{"createdTime":1741256271759,"shownInTree":["algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html","algorithms/algo/backtracking/backtracking.html","algorithms/algo/divide-and-conquer/divide-and-conquer.html","algorithms/algo/dynamic-programming/dynamic-programing.html","algorithms/algo/dynamic-programming/kadane's-algorithm.html","algorithms/algo/graph/alpha-beta.html","algorithms/algo/graph/floyd's-cycle-finding-algorithm.html","algorithms/algo/graph/minimax.html","algorithms/algo/graph/union-find.html","algorithms/algo/greedy/greedy.html","algorithms/algo/recursion/recursion.html","algorithms/algo/search/a-star.html","algorithms/algo/search/breadth-first-search.html","algorithms/algo/search/depth-first-search.html","algorithms/algo/search/dfs-vs-bfs.html","algorithms/algo/search/dijkstra.html","algorithms/algo/search/iterative-deepening.html","algorithms/algo/sliding-window/sliding-window.html","algorithms/algo/sorting/binary-search.html","algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html","algorithms/algo/two-pointers/two-pointer.html","algorithms/leetcode/array/dp/45.-jump-game-ii.html","algorithms/leetcode/array/dp/62.-unique-paths.html","algorithms/leetcode/array/dp/70.-climbing-stairs.html","algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html","algorithms/leetcode/array/dp/198.-house-robber.html","algorithms/leetcode/array/dp/213.-house-robber-ii.html","algorithms/leetcode/array/dp/221.-maximal-square.html","algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html","algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html","algorithms/leetcode/array/dp/413.-arithmetic-slices.html","algorithms/leetcode/array/dp/472.-concatenated-words.html","algorithms/leetcode/array/dp/542.-01-matrix.html","algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html","algorithms/leetcode/array/dp/740.-delete-and-earn.html","algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html","algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html","algorithms/leetcode/array/hash/454.-4sum-ii.html","algorithms/leetcode/array/interval/57.-insert-interval.html","algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html","algorithms/leetcode/array/matrix/79.-word-search.html","algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html","algorithms/leetcode/array/matrix/695.-max-area-of-island.html","algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html","algorithms/leetcode/array/matrix/934.-shortest-bridge.html","algorithms/leetcode/array/permutation/46.-permutations.html","algorithms/leetcode/array/permutation/77.-combinations.html","algorithms/leetcode/array/sort/912.-sort-an-array.html","algorithms/leetcode/array/subarray/53.-maximum-subarray.html","algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html","algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html","algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html","algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html","algorithms/leetcode/array/two-pointers/15.-3sum.html","algorithms/leetcode/array/two-pointers/16.-3sum-closest.html","algorithms/leetcode/array/two-pointers/18.-4sum.html","algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html","algorithms/leetcode/array/two-pointers/27.-remove-element.html","algorithms/leetcode/array/two-pointers/69.-sqrt(x).html","algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html","algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html","algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html","algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html","algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html","algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html","algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html","algorithms/leetcode/array/two-pointers/392.-is-subsequence.html","algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html","algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html","algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html","algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html","algorithms/leetcode/bit/190.-reverse-bits.html","algorithms/leetcode/bit/461.-hamming-distance.html","algorithms/leetcode/geometry/149.-max-points-on-a-line.html","algorithms/leetcode/graph/490.-the-maze.html","algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html","algorithms/leetcode/graph/841.-keys-and-rooms.html","algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html","algorithms/leetcode/greedy/134.-gas-station.html","algorithms/leetcode/greedy/135.-candy.html","algorithms/leetcode/greedy/435.-non-overlapping-intervals.html","algorithms/leetcode/greedy/455.-assign-cookies.html","algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html","algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html","algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html","algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html","algorithms/leetcode/sql/select/183.-customers-who-never-order.html","algorithms/leetcode/sql/select/584.-find-customer-referee.html","algorithms/leetcode/sql/select/595.-big-countries.html","algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html","algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html","algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html","algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html","algorithms/leetcode/sql/union/197.-rising-temperature.html","algorithms/leetcode/sql/union&select/608.-tree-node.html","algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html","algorithms/leetcode/string/6.-zigzag-conversion.html","algorithms/leetcode/string/131.-palindrome-partitioning.html","algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html","algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html","algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html","algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html","algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html","algorithms/leetcode/tree/100.-same-tree.html","algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html","algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html","algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html","algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html","algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html","algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html","algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html","algorithms/leetcode/tree/1302.-deepest-leaves-sum.html","algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html","algorithms/leetcode/tree/1490.-clone-n-ary-tree.html","algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html","algorithms/leetcode/tree/1660.-correct-a-binary-tree.html","algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html","algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html","algorithms/leetcode/road-map.html","algorithms/algorithms.html","artificial-intelligence/computer-vision/0.-image-processing.html","artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html","artificial-intelligence/computer-vision/2.-neural-networks.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html","artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html","artificial-intelligence/data-science/data-wrangling/0.-concepts.html","artificial-intelligence/data-science/data-wrangling/1.-process.html","artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html","artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html","artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html","artificial-intelligence/data-science/data-wrangling/5.-bins.html","artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html","artificial-intelligence/data-science/data-wrangling/7.-record-classification.html","artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html","artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html","artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html","artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html","artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html","artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html","artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html","artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html","artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html","artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html","artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html","artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html","artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html","artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html","artificial-intelligence/deep-learning/0.-basic/2.-resnet.html","artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html","artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html","artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html","artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html","artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html","artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html","artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html","artificial-intelligence/deep-learning/5.-transformer/3.-bert.html","artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html","artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html","artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html","artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html","artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html","artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html","artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html","artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html","artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html","artificial-intelligence/deep-learning/0.2.-resnet.html","artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html","artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html","artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html","artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html","artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html","artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html","artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html","artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html","artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html","artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html","artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html","artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html","artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html","artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html","artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html","artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html","artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html","artificial-intelligence/machine-learning/algorithm/7.-random-forest.html","artificial-intelligence/machine-learning/algorithm/8.-adaboost.html","artificial-intelligence/machine-learning/algorithm/8.-k-means.html","artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html","artificial-intelligence/machine-learning/algorithm/9.-k-means.html","artificial-intelligence/machine-learning/algorithm/10.-gbdt.html","artificial-intelligence/machine-learning/algorithm/11.-adaboost.html","artificial-intelligence/machine-learning/algorithm/12.-xgboost.html","artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html","artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html","artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html","artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html","artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html","artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html","artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html","artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html","artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html","artificial-intelligence/machine-learning/data/preprocessing.html","artificial-intelligence/machine-learning/math/1.-linear-algebra.html","artificial-intelligence/machine-learning/math/2.-gradient-descent.html","artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html","artificial-intelligence/machine-learning/stats/formula/1.-variance.html","artificial-intelligence/machine-learning/stats/formula/2.-covariance.html","artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html","artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html","artificial-intelligence/machine-learning/stats/formula/4.-correlation.html","artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html","artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html","artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html","artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html","artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html","artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html","artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html","artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html","artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html","artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html","artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html","artificial-intelligence/natural-language-processing/basics/1.-components.html","artificial-intelligence/natural-language-processing/basics/1.-pipeline.html","artificial-intelligence/natural-language-processing/basics/2.-tasks.html","artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html","artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html","artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html","artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html","artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html","artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html","artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html","artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html","artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html","artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html","artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html","artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html","artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html","artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html","artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html","artificial-intelligence/reinforcement-learning/0.-q-learning.html","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html","artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html","artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html","big-data/0.-data-lake/0.-introduction.html","big-data/0.-data-lake/1.-architecture.html","big-data/0.-data-lake/2.-workflow.html","big-data/0.-data-lake/3.-datax.html","big-data/1.-hadoop/1.-hadoop.html","big-data/1.-hadoop/2.-hdfs.html","big-data/1.-hadoop/3.-mapreduce-&-yarn.html","big-data/1.-hadoop/3.-metadata.html","big-data/1.-hadoop/4.-hdfs-shell.html","big-data/1.-hadoop/5.-java-api.html","big-data/1.-hadoop/6.-mapreduce-&-yarn.html","big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html","big-data/2.-scala/0.-comprehensive-guide.html","big-data/2.-scala/1.-variables-and-data-types.html","big-data/2.-scala/2.-operators.html","big-data/2.-scala/3.-control-flow.html","big-data/2.-scala/4.-functional-programming.html","big-data/2.-scala/5.-package-management.html","big-data/2.-scala/6.-object-oriented-programming.html","big-data/2.-scala/7.-collections.html","big-data/2.-scala/8.-pattern-matching.html","big-data/2.-scala/9.-exception-handling.html","big-data/2.-scala/10.-sbt.html","big-data/3.-spark/0.-concept-&-architecture/0.-spark.html","big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html","big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html","big-data/3.-spark/1.-rdd/0.-rdd.html","big-data/3.-spark/1.-rdd/1.-transformations.html","big-data/3.-spark/1.-rdd/2.-actions.html","big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html","big-data/3.-spark/1.-rdd/4.-collaboration.html","big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html","big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html","big-data/3.-spark/2.-spark-sql/0.-sparksql.html","big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html","big-data/3.-spark/2.-spark-sql/2.-coding.html","big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html","big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html","big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html","big-data/3.-spark/3.-streaming/0.-spark-streaming.html","big-data/3.-spark/3.-streaming/1.-kafka-data-source.html","big-data/3.-spark/3.-streaming/2.-dstream-transformations.html","big-data/3.-spark/3.-streaming/3.-dstream-output.html","big-data/3.-spark/4.-core/0.-spark-kernel.html","big-data/3.-spark/4.-core/1.-deployment.html","big-data/3.-spark/4.-core/2.-spark-communication-architecture.html","big-data/3.-spark/4.-core/3.-task-scheduling.html","big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html","big-data/3.-spark/5.-pyspark/0.-installation.html","big-data/3.-spark/5.-pyspark/1.-dataframe.html","big-data/3.-spark/5.-pyspark/2.-spark-connect.html","big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html","big-data/3.-spark/6.-source-code/0.-spark-storage.html","big-data/4.-hive/0.-hive.html","big-data/4.-hive/1.-data-definition-language-(ddl).html","big-data/4.-hive/2.-data-manipulation-language-(dml).html","big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html","big-data/5.-hbase/0.-hbase.html","big-data/5.-hbase/2.-shell.html","big-data/5.-hbase/3.-processes.html","big-data/5.-hbase/4.-api.html","big-data/5.-hbase/5.-mapreduce.html","big-data/5.-hbase/6.-integration-with-hive.html","big-data/5.-hbase/7.-optimization.html","big-data/6.-flink/0.-key-features-of-flink.html","big-data/6.-flink/1.-deployment-and-startup.html","big-data/6.-flink/2.-architecture.html","big-data/6.-flink/3.-flink-operators.html","big-data/6.-flink/4.-time-and-window-in-stream-processing.html","big-data/7.-kafka/0.-kafka.html","big-data/7.-kafka/1.-deployment-&-commands.html","big-data/7.-kafka/2.-architecture.html","big-data/7.-kafka/3.-api.html","big-data/7.-kafka/4.-monitoring.html","big-data/7.-kafka/5.-flume-integration.html","big-data/8.-doris/0.-doris.html","big-data/8.-doris/1.-installation.html","big-data/8.-doris/2.-usage.html","big-data/8.-doris/3.-monitoring-and-alerting.html","big-data/9.-elastic-search/0.-overview.html","big-data/9.-elastic-search/1.-index-&-shard.html","big-data/10.-clickhouse/0.-introduction-and-installation.html","big-data/10.-clickhouse/1.-data-types.html","big-data/10.-clickhouse/2.-database-engines.html","big-data/10.-clickhouse/3.-table-engines.html","big-data/11.-data-migration/0.-sqoop.html","big-data/11.-data-migration/1.-datax.html","big-data/12.-schedule/1.-cron-expressions.html","big-data/12.-schedule/2.-quartz-cron-expressions.html","big-data/13.-certificate/cdga/0.-data-management.html","big-data/13.-certificate/cdga/1.-data-handling-ethics.html","big-data/13.-certificate/cdga/2.-data-governance.html","big-data/13.-certificate/cdga/3.-data-architecture.html","big-data/13.-certificate/cdga/4.-data-modeling-and-design.html","big-data/13.-certificate/cdga/5.-data-storage-and-operations.html","big-data/13.-certificate/cdga/6.-data-security.html","big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html","big-data/13.-certificate/cdga/8.-document-and-content-management.html","big-data/13.-certificate/cdga/9.-reference-and-master-data.html","big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html","big-data/13.-certificate/cdga/11.-metadata-management.html","big-data/13.-certificate/cdga/12.-data-quality.html","big-data/13.-certificate/cdga/13.-big-data-and-data-science.html","big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html","c++/auto-keyword.html","c++/c++.html","c++/const-keyword.html","c++/enum-and-enum-class.html","c++/header-files.html","c++/inline-keyword.html","c++/oop.html","c++/static-keyword.html","c++/timing.html","c++/type-punning.html","data-base/data-base.html","data-base/mongodb.html","data-base/mongodb-cheat-sheet.html","data-base/sql-cheat-sheet.html","data-structures/aggregation-analysis.html","data-structures/analyzing-exchanging-sorting-algorithms.html","data-structures/avl-tree.html","data-structures/binary-search-tree.html","data-structures/comparison-based-sorting.html","data-structures/data-structures.html","data-structures/expression-tree.html","data-structures/hashing.html","data-structures/heaps-&-priority-queues.html","data-structures/insertion-sort.html","data-structures/linear-sorting-algorithm.html","data-structures/lists.html","data-structures/master-theorem.html","data-structures/open-addressing.html","data-structures/order.html","data-structures/priority-queues.html","data-structures/recusrsion.html","data-structures/red-black-tree.html","data-structures/sorting-algorithms.html","data-structures/stacks-&-queues.html","data-structures/time-complexity-comparison.html","data-structures/tree-and-trie.html","data-structures/trees.html","git/git.html","git/git-cheat-sheet.html","graph/dijkstra.html","graph/graph.html","graph/max-flow.html","graph/scc.html","markdown/markdown.html","markdown/markdown-cheat-sheet.html","markdown/mathjax.html","python/beautiful-soup.html","python/python.html","python/python-oop.html","home.html"],"attachments":["site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/theme.css","site-lib/styles/global-variable-styles.css","site-lib/styles/supported-plugins.css","site-lib/styles/main-styles.css","excalidraw/codeverse.svg","excalidraw/avl-tree.excalidraw.svg","excalidraw/black-and-red-tree-height-diff.excalidraw.svg","excalidraw/expression-tree.svg","excalidraw/red-black-tree.excalidraw.svg"],"allFiles":["home.html","data-structures/avl-tree.html","algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html","algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html","algorithms/algo/backtracking/backtracking.html","algorithms/algo/divide-and-conquer/divide-and-conquer.html","algorithms/algo/dynamic-programming/dynamic-programing.html","algorithms/algo/dynamic-programming/kadane's-algorithm.html","algorithms/algo/graph/alpha-beta.html","algorithms/algo/graph/floyd's-cycle-finding-algorithm.html","algorithms/algo/graph/minimax.html","algorithms/algo/graph/union-find.html","algorithms/algo/greedy/greedy.html","algorithms/algo/recursion/recursion.html","algorithms/algo/search/a-star.html","algorithms/algo/search/breadth-first-search.html","algorithms/algo/search/depth-first-search.html","algorithms/algo/search/dijkstra.html","algorithms/algo/search/iterative-deepening.html","algorithms/algo/sliding-window/sliding-window.html","algorithms/algo/sorting/binary-search.html","algorithms/algo/two-pointers/two-pointer.html","algorithms/leetcode/array/dp/45.-jump-game-ii.html","algorithms/leetcode/array/dp/62.-unique-paths.html","algorithms/leetcode/array/dp/70.-climbing-stairs.html","algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html","algorithms/leetcode/array/dp/198.-house-robber.html","algorithms/leetcode/array/dp/213.-house-robber-ii.html","algorithms/leetcode/array/dp/221.-maximal-square.html","algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html","algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html","algorithms/leetcode/array/dp/413.-arithmetic-slices.html","algorithms/leetcode/array/dp/472.-concatenated-words.html","algorithms/leetcode/array/dp/542.-01-matrix.html","algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html","algorithms/leetcode/array/dp/740.-delete-and-earn.html","algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html","algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html","algorithms/leetcode/array/hash/454.-4sum-ii.html","algorithms/leetcode/array/interval/57.-insert-interval.html","algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html","algorithms/leetcode/array/matrix/79.-word-search.html","algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html","algorithms/leetcode/array/matrix/695.-max-area-of-island.html","algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html","algorithms/leetcode/array/matrix/934.-shortest-bridge.html","algorithms/leetcode/array/permutation/46.-permutations.html","algorithms/leetcode/array/permutation/77.-combinations.html","algorithms/leetcode/array/sort/912.-sort-an-array.html","algorithms/leetcode/array/subarray/53.-maximum-subarray.html","algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html","algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html","algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html","algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html","algorithms/leetcode/array/two-pointers/15.-3sum.html","algorithms/leetcode/array/two-pointers/16.-3sum-closest.html","algorithms/leetcode/array/two-pointers/18.-4sum.html","algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html","algorithms/leetcode/array/two-pointers/27.-remove-element.html","algorithms/leetcode/array/two-pointers/69.-sqrt(x).html","algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html","algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html","algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html","algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html","algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html","algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html","algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html","algorithms/leetcode/array/two-pointers/392.-is-subsequence.html","algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html","algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html","algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html","algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html","algorithms/leetcode/bit/190.-reverse-bits.html","algorithms/leetcode/bit/461.-hamming-distance.html","algorithms/leetcode/geometry/149.-max-points-on-a-line.html","algorithms/leetcode/graph/490.-the-maze.html","algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html","algorithms/leetcode/graph/841.-keys-and-rooms.html","algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html","algorithms/leetcode/greedy/134.-gas-station.html","algorithms/leetcode/greedy/135.-candy.html","algorithms/leetcode/greedy/435.-non-overlapping-intervals.html","algorithms/leetcode/greedy/455.-assign-cookies.html","algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html","algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html","algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html","algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html","algorithms/leetcode/sql/select/183.-customers-who-never-order.html","algorithms/leetcode/sql/select/584.-find-customer-referee.html","algorithms/leetcode/sql/select/595.-big-countries.html","algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html","algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html","algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html","algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html","algorithms/leetcode/sql/union/197.-rising-temperature.html","algorithms/leetcode/sql/union&select/608.-tree-node.html","algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html","algorithms/leetcode/string/6.-zigzag-conversion.html","algorithms/leetcode/string/131.-palindrome-partitioning.html","algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html","algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html","algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html","algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html","algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html","algorithms/leetcode/tree/100.-same-tree.html","algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html","algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html","algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html","algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html","algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html","algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html","algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html","algorithms/leetcode/tree/1302.-deepest-leaves-sum.html","algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html","algorithms/leetcode/tree/1490.-clone-n-ary-tree.html","algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html","algorithms/leetcode/tree/1660.-correct-a-binary-tree.html","algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html","algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html","artificial-intelligence/computer-vision/0.-image-processing.html","artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html","artificial-intelligence/computer-vision/2.-neural-networks.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html","artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html","artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html","artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html","artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html","artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html","artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html","artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html","artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html","artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html","artificial-intelligence/data-science/data-wrangling/0.-concepts.html","artificial-intelligence/data-science/data-wrangling/1.-process.html","artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html","artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html","artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html","artificial-intelligence/data-science/data-wrangling/5.-bins.html","artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html","artificial-intelligence/data-science/data-wrangling/7.-record-classification.html","artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html","artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html","artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html","artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html","artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html","artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html","artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html","artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html","artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html","artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html","artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html","artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html","artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html","artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html","artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html","artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html","artificial-intelligence/deep-learning/0.-basic/2.-resnet.html","artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html","artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html","artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html","artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html","artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html","artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html","artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html","artificial-intelligence/deep-learning/5.-transformer/3.-bert.html","artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html","artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html","artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html","artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html","artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html","artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html","artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html","artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html","artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html","artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html","artificial-intelligence/deep-learning/0.2.-resnet.html","artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html","artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html","artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html","artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html","artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html","artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html","artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html","artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html","artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html","artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html","artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html","artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html","artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html","artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html","artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html","artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html","artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html","artificial-intelligence/machine-learning/algorithm/7.-random-forest.html","artificial-intelligence/machine-learning/algorithm/8.-adaboost.html","artificial-intelligence/machine-learning/algorithm/8.-k-means.html","artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html","artificial-intelligence/machine-learning/algorithm/9.-k-means.html","artificial-intelligence/machine-learning/algorithm/10.-gbdt.html","artificial-intelligence/machine-learning/algorithm/11.-adaboost.html","artificial-intelligence/machine-learning/algorithm/12.-xgboost.html","artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html","artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html","artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html","artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html","artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html","artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html","artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html","artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html","artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html","artificial-intelligence/machine-learning/data/preprocessing.html","artificial-intelligence/machine-learning/math/1.-linear-algebra.html","artificial-intelligence/machine-learning/math/2.-gradient-descent.html","artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html","artificial-intelligence/machine-learning/stats/formula/1.-variance.html","artificial-intelligence/machine-learning/stats/formula/2.-covariance.html","artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html","artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html","artificial-intelligence/machine-learning/stats/formula/4.-correlation.html","artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html","artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html","artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html","artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html","artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html","artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html","artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html","artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html","artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html","artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html","artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html","artificial-intelligence/natural-language-processing/basics/1.-components.html","artificial-intelligence/natural-language-processing/basics/1.-pipeline.html","artificial-intelligence/natural-language-processing/basics/2.-tasks.html","artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html","artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html","artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html","artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html","artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html","artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html","artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html","artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html","artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html","artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html","artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html","artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html","artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html","artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html","artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html","artificial-intelligence/reinforcement-learning/0.-q-learning.html","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html","artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html","artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html","artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html","artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html","artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html","artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html","big-data/0.-data-lake/0.-introduction.html","big-data/0.-data-lake/1.-architecture.html","big-data/0.-data-lake/2.-workflow.html","big-data/0.-data-lake/3.-datax.html","big-data/1.-hadoop/1.-hadoop.html","big-data/1.-hadoop/2.-hdfs.html","big-data/1.-hadoop/3.-mapreduce-&-yarn.html","big-data/1.-hadoop/3.-metadata.html","big-data/1.-hadoop/4.-hdfs-shell.html","big-data/1.-hadoop/5.-java-api.html","big-data/1.-hadoop/6.-mapreduce-&-yarn.html","big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html","big-data/2.-scala/0.-comprehensive-guide.html","big-data/2.-scala/1.-variables-and-data-types.html","big-data/2.-scala/2.-operators.html","big-data/2.-scala/3.-control-flow.html","big-data/2.-scala/4.-functional-programming.html","big-data/2.-scala/5.-package-management.html","big-data/2.-scala/6.-object-oriented-programming.html","big-data/2.-scala/7.-collections.html","big-data/2.-scala/8.-pattern-matching.html","big-data/2.-scala/9.-exception-handling.html","big-data/2.-scala/10.-sbt.html","big-data/3.-spark/0.-concept-&-architecture/0.-spark.html","big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html","big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html","big-data/3.-spark/1.-rdd/0.-rdd.html","big-data/3.-spark/1.-rdd/1.-transformations.html","big-data/3.-spark/1.-rdd/2.-actions.html","big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html","big-data/3.-spark/1.-rdd/4.-collaboration.html","big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html","big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html","big-data/3.-spark/2.-spark-sql/0.-sparksql.html","big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html","big-data/3.-spark/2.-spark-sql/2.-coding.html","big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html","big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html","big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html","big-data/3.-spark/3.-streaming/0.-spark-streaming.html","big-data/3.-spark/3.-streaming/1.-kafka-data-source.html","big-data/3.-spark/3.-streaming/2.-dstream-transformations.html","big-data/3.-spark/3.-streaming/3.-dstream-output.html","big-data/3.-spark/4.-core/0.-spark-kernel.html","big-data/3.-spark/4.-core/1.-deployment.html","big-data/3.-spark/4.-core/2.-spark-communication-architecture.html","big-data/3.-spark/4.-core/3.-task-scheduling.html","big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html","big-data/3.-spark/5.-pyspark/0.-installation.html","big-data/3.-spark/5.-pyspark/1.-dataframe.html","big-data/3.-spark/5.-pyspark/2.-spark-connect.html","big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html","big-data/3.-spark/6.-source-code/0.-spark-storage.html","big-data/4.-hive/0.-hive.html","big-data/4.-hive/1.-data-definition-language-(ddl).html","big-data/4.-hive/2.-data-manipulation-language-(dml).html","big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html","big-data/5.-hbase/0.-hbase.html","big-data/5.-hbase/2.-shell.html","big-data/5.-hbase/3.-processes.html","big-data/5.-hbase/4.-api.html","big-data/5.-hbase/5.-mapreduce.html","big-data/5.-hbase/6.-integration-with-hive.html","big-data/5.-hbase/7.-optimization.html","big-data/6.-flink/0.-key-features-of-flink.html","big-data/6.-flink/1.-deployment-and-startup.html","big-data/6.-flink/2.-architecture.html","big-data/6.-flink/3.-flink-operators.html","big-data/6.-flink/4.-time-and-window-in-stream-processing.html","big-data/7.-kafka/0.-kafka.html","big-data/7.-kafka/1.-deployment-&-commands.html","big-data/7.-kafka/2.-architecture.html","big-data/7.-kafka/3.-api.html","big-data/7.-kafka/4.-monitoring.html","big-data/7.-kafka/5.-flume-integration.html","big-data/8.-doris/0.-doris.html","big-data/8.-doris/1.-installation.html","big-data/8.-doris/2.-usage.html","big-data/8.-doris/3.-monitoring-and-alerting.html","big-data/9.-elastic-search/0.-overview.html","big-data/9.-elastic-search/1.-index-&-shard.html","big-data/10.-clickhouse/0.-introduction-and-installation.html","big-data/10.-clickhouse/1.-data-types.html","big-data/10.-clickhouse/2.-database-engines.html","big-data/10.-clickhouse/3.-table-engines.html","big-data/11.-data-migration/0.-sqoop.html","big-data/11.-data-migration/1.-datax.html","big-data/12.-schedule/1.-cron-expressions.html","big-data/12.-schedule/2.-quartz-cron-expressions.html","big-data/13.-certificate/cdga/0.-data-management.html","big-data/13.-certificate/cdga/1.-data-handling-ethics.html","big-data/13.-certificate/cdga/2.-data-governance.html","big-data/13.-certificate/cdga/3.-data-architecture.html","big-data/13.-certificate/cdga/4.-data-modeling-and-design.html","big-data/13.-certificate/cdga/5.-data-storage-and-operations.html","big-data/13.-certificate/cdga/6.-data-security.html","big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html","big-data/13.-certificate/cdga/8.-document-and-content-management.html","big-data/13.-certificate/cdga/9.-reference-and-master-data.html","big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html","big-data/13.-certificate/cdga/11.-metadata-management.html","big-data/13.-certificate/cdga/12.-data-quality.html","big-data/13.-certificate/cdga/13.-big-data-and-data-science.html","big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html","c++/const-keyword.html","graph/scc.html","graph/max-flow.html","graph/dijkstra.html","excalidraw/red-black-tree.excalidraw.svg","excalidraw/expression-tree.svg","excalidraw/black-and-red-tree-height-diff.excalidraw.svg","excalidraw/avl-tree.excalidraw.svg","data-structures/data-structures.html","graph/graph.html","data-structures/open-addressing.html","data-structures/expression-tree.html","data-structures/red-black-tree.html","data-structures/binary-search-tree.html","data-structures/stacks-&-queues.html","data-structures/hashing.html","data-structures/master-theorem.html","data-structures/tree-and-trie.html","algorithms/algo/search/dfs-vs-bfs.html","algorithms/algorithms.html","data-structures/priority-queues.html","data-structures/order.html","data-structures/linear-sorting-algorithm.html","data-structures/comparison-based-sorting.html","data-structures/heaps-&-priority-queues.html","data-structures/sorting-algorithms.html","algorithms/leetcode/road-map.html","python/beautiful-soup.html","python/python.html","c++/c++.html","markdown/mathjax.html","markdown/markdown.html","c++/static-keyword.html","data-base/data-base.html","c++/enum-and-enum-class.html","c++/type-punning.html","c++/timing.html","c++/oop.html","c++/inline-keyword.html","c++/header-files.html","c++/auto-keyword.html","git/git.html","data-base/sql-cheat-sheet.html","data-base/mongodb-cheat-sheet.html","data-base/mongodb.html","data-structures/insertion-sort.html","data-structures/time-complexity-comparison.html","data-structures/analyzing-exchanging-sorting-algorithms.html","data-structures/recusrsion.html","data-structures/aggregation-analysis.html","data-structures/lists.html","markdown/markdown-cheat-sheet.html","python/python-oop.html","git/git-cheat-sheet.html","data-structures/trees.html","site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/theme.css","site-lib/styles/global-variable-styles.css","site-lib/styles/supported-plugins.css","site-lib/styles/main-styles.css"],"webpages":{"home.html":{"title":"Home","icon":"RiHome3Fill","description":"<img alt=\"codeverse.svg\" src=\"excalidraw/codeverse.svg\" target=\"_self\">Welcome to my personal knowledge base and digital garden! I'm sharing my notes and thoughts on various coding topics.\nData Structures - Notes on data structures like arrays, linked lists, trees, etc.\nC++ - C++ concepts, syntax, STL, tips and tricks.\nGit - Git and GitHub tutorials, workflows, etc.\nMarkdown - Markdown syntax and examples.\nPython - Python tutorials for beginners to advanced.\nData Base - SQL and NoSQL syntax and examples.\nAlgorithms - Roadmap for solving Algorithm problems primarily on LeetCode\nGraph - Algorithms in graph for finding path, max flow, min cut, etc.\nArtificial Intelligence - Machine Learning, Data Science and other fields in AI. Big Data - data stores like Kafka &amp; ...\nI'm always adding new content, so check back often! Feel free to browse and learn.Get in touch if you have any questions or just want to connect!\n<br>✉️Email: <a data-tooltip-position=\"top\" aria-label=\"mailto:fkarimi8320@gmail.com\" rel=\"noopener nofollow\" class=\"external-link\" href=\".html\" target=\"_self\">fkarimi8320@gmail.com</a>\n<br>🎓LinkedIn: <a data-tooltip-position=\"top\" aria-label=\"https://www.linkedin.com/in/farid-kmi/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.linkedin.com/in/farid-kmi/\" target=\"_self\">Farid-kmi</a>\n<br>💻GitHub: <a data-tooltip-position=\"top\" aria-label=\"https://github.com/Farid-Karimi\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://github.com/Farid-Karimi\" target=\"_self\">Farid-Karimi</a>\n<br>🎨My Portfolio: <a data-tooltip-position=\"top\" aria-label=\"https://farid-karimi.github.io/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://farid-karimi.github.io/\" target=\"_self\">Portfolio</a>\nLet's talk code!","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"RiHome3FillHome","level":1,"id":"RiHome3FillHome_0"},{"heading":"📒 Contents","level":2,"id":"📒_Contents_0"},{"heading":"💬Connect","level":2,"id":"💬Connect_0"}],"links":[".html"],"author":"","coverImageURL":"","fullURL":"home.html","pathToRoot":".","attachments":["excalidraw/codeverse.svg"],"createdTime":1690548373184,"modifiedTime":1741268362292,"sourceSize":1237,"sourcePath":"Home.md","exportPath":"home.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html":{"title":"A general approach to backtracking questions","icon":"","description":"This structure might apply to many other backtracking questions, but here I am just going to demonstrate Subsets, Permutations, and Combination Sum.-<a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/subsets/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/subsets/\" target=\"_self\">Subsets</a>class Solution {\npublic: vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) { vector&lt;vector&lt;int&gt;&gt; res = {{}}; for (int i=0 ; i&lt;nums.size() ; i++){ int s = res.size(); for(int j=0 ; j&lt;s ; j++){ res.push_back(res[j]); res.back().push_back(nums[i]); } } return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/subsets-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/subsets-ii/\" target=\"_self\">Subsets II (contains duplicates)</a>class Solution {\npublic: void solve(vector&lt;vector&lt;int&gt;&gt; &amp;res, vector&lt;int&gt; ph, vector&lt;int&gt;&amp; nums, int begin){ res.push_back(ph); for(int i=begin ; i&lt;nums.size() ; i++){ if(i &gt; begin &amp;&amp; nums[i]==nums[i-1]){ continue; } ph.push_back(nums[i]); solve(res, ph, nums, i+1); ph.pop_back(); } } vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt;&amp; nums) { sort(nums.begin(), nums.end()); vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; ph; solve(res, ph, nums, 0); return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/permutations/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/permutations/\" target=\"_self\">Permutations</a>class Solution {\npublic: void solve(vector&lt;vector&lt;int&gt;&gt; &amp;res, vector&lt;int&gt; placeHolder, vector&lt;int&gt;&amp; nums){ if(placeHolder.size()==nums.size()){ res.push_back(placeHolder); } else{ for(int i=0 ; i&lt;nums.size() ; i++){ if (find(placeHolder.begin(), placeHolder.end(), nums[i]) != placeHolder.end()) continue; placeHolder.push_back(nums[i]); solve(res, placeHolder, nums); placeHolder.pop_back(); } } } vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) { vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; placeHolder; solve(res, placeHolder, nums); return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/permutations-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/permutations-ii/\" target=\"_self\">Permutations II (contains duplicates)</a>class Solution {\npublic: void backtrack(vector&lt;vector&lt;int&gt;&gt;&amp; res, vector&lt;int&gt;&amp; ph, vector&lt;int&gt;&amp; nums, vector&lt;bool&gt;&amp; used) { if (ph.size() == nums.size()) { res.push_back(ph); } else { for (int i = 0; i &lt; nums.size(); i++) { if (used[i] || (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1] &amp;&amp; !used[i - 1])){ continue; } used[i] = true; ph.push_back(nums[i]); backtrack(res, ph, nums, used); used[i] = false; ph.pop_back(); } } } vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) { vector&lt;vector&lt;int&gt;&gt; res; sort(nums.begin(), nums.end()); vector&lt;bool&gt; used(nums.size(), false); vector&lt;int&gt; ph; backtrack(res, ph, nums, used); return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/combination-sum/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/combination-sum/\" target=\"_self\">Combination Sum</a>class Solution {\nprivate: void combination(vector&lt;int&gt;&amp; candidates, int target, vector&lt;int&gt;&amp; currComb, int currSum, int index, vector&lt;vector&lt;int&gt;&gt;&amp; res) { if (currSum == target) { res.push_back(currComb); return; } if (currSum &gt; target || index &gt;= candidates.size()) { return; } for (int i = index; i &lt; candidates.size(); i++) { currComb.push_back(candidates[i]); currSum += candidates[i]; combination(candidates, target, currComb, currSum, i, res); currComb.pop_back(); currSum -= candidates[i]; } } public: vector&lt;vector&lt;int&gt;&gt; combinationSum(vector&lt;int&gt;&amp; candidates, int target) { vector&lt;vector&lt;int&gt;&gt; res; vector&lt;int&gt; currComb; combination(candidates, target, currComb, 0, 0, res); return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/combination-sum-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/combination-sum-ii/\" target=\"_self\">Combination Sum II (can't reuse same element)</a>class Solution {\npublic: void backtrack(vector&lt;vector&lt;int&gt;&gt;&amp; res, vector&lt;int&gt;&amp; ph, vector&lt;int&gt;&amp; nums, long remain, int start) { if (remain &lt; 0) { return; } else if (remain == 0) { res.push_back(ph); } else { for (int i = start; i &lt; nums.size(); i++) { if (i &gt; start &amp;&amp; nums[i] == nums[i - 1]) { continue; } ph.push_back(nums[i]); backtrack(res, ph, nums, remain - nums[i], i + 1); ph.pop_back(); } } } vector&lt;vector&lt;int&gt;&gt; combinationSum2(vector&lt;int&gt;&amp; nums, int target) { vector&lt;vector&lt;int&gt;&gt; res; sort(nums.begin(), nums.end()); vector&lt;int&gt; ph; backtrack(res, ph, nums, target, 0); return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/palindrome-partitioning/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/palindrome-partitioning/\" target=\"_self\">Palindrome Partitioning</a>class Solution {\npublic: bool isPalindrome(const string&amp; s, int left, int right) { while (left &lt; right) { if (s[left++] != s[right--]) { return false; } } return true; } void solve(vector&lt;vector&lt;string&gt;&gt;&amp; res, vector&lt;string&gt;&amp; ph, const string&amp; s, int begin) { if (begin &gt;= s.length()) { res.push_back(ph); } else { for (int i = begin; i &lt; s.length(); i++) { if (isPalindrome(s, begin, i)) { ph.push_back(s.substr(begin, i - begin + 1)); solve(res, ph, s, i + 1); ph.pop_back(); } } } } vector&lt;vector&lt;string&gt;&gt; partition(string s) { vector&lt;vector&lt;string&gt;&gt; res; vector&lt;string&gt; ph; solve(res, ph, s, 0); return res; }\n};\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/letter-combinations-of-a-phone-number/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/letter-combinations-of-a-phone-number/\" target=\"_self\">letter combinations of a phone number</a>class Solution { unordered_map&lt;int, string&gt; pad{ {2, \"abc\"}, {3, \"def\"}, {4, \"ghi\"}, {5, \"jkl\"}, {6, \"mno\"}, {7, \"pqrs\"}, {8, \"tuv\"}, {9, \"wxyz\"}, }; public: void solve(vector&lt;string&gt;&amp; res, string&amp; ph, const string&amp; digits, int begin) { if (begin &gt;= digits.length()) { res.push_back(ph); } else { int digit = digits[begin] - '0'; for (char c : pad[digit]) { ph += c; solve(res, ph, digits, begin + 1); ph.pop_back(); } } } vector&lt;string&gt; letterCombinations(const string&amp; digits) { vector&lt;string&gt; res; string ph; if (!digits.empty()) { solve(res, ph, digits, 0); } return res; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"A general approach to backtracking questions","level":1,"id":"A_general_approach_to_backtracking_questions_1"},{"heading":"Subsets","level":2,"id":"Subsets_0"},{"heading":"Subsets II (contains duplicates)","level":2,"id":"Subsets_II_(contains_duplicates)_0"},{"heading":"Permutations","level":2,"id":"Permutations_0"},{"heading":"Permutations II (contains duplicates)","level":2,"id":"Permutations_II_(contains_duplicates)_0"},{"heading":"Combination Sum","level":2,"id":"Combination_Sum_0"},{"heading":"Combination Sum II (can't reuse same element)","level":2,"id":"Combination_Sum_II_(can't_reuse_same_element)_0"},{"heading":"Palindrome Partitioning","level":2,"id":"Palindrome_Partitioning_0"},{"heading":"letter combinations of a phone number","level":2,"id":"letter_combinations_of_a_phone_number_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html","pathToRoot":"../../..","attachments":[],"createdTime":1695306687377,"modifiedTime":1741171987763,"sourceSize":7278,"sourcePath":"Algorithms/Algo/Backtracking/A general approach to backtracking questions.md","exportPath":"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/backtracking/backtracking.html":{"title":"Backtracking","icon":"","description":"\n<a data-href=\"491. Non-decreasing Subsequences\" href=\"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">491. Non-decreasing Subsequences</a>\n<br><a data-href=\"131. Palindrome Partitioning\" href=\"algorithms/leetcode/string/131.-palindrome-partitioning.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">131. Palindrome Partitioning</a>\n<br><a data-href=\"79. Word Search\" href=\"algorithms/leetcode/array/matrix/79.-word-search.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">79. Word Search</a>\n<br><a data-href=\"797. All Paths From Source to Target\" href=\"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">797. All Paths From Source to Target</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html#_0","algorithms/leetcode/string/131.-palindrome-partitioning.html#_0","algorithms/leetcode/array/matrix/79.-word-search.html#_0","algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/backtracking/backtracking.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475860,"modifiedTime":1737554783000,"sourceSize":138,"sourcePath":"Algorithms/Algo/Backtracking/Backtracking.md","exportPath":"algorithms/algo/backtracking/backtracking.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/divide-and-conquer/divide-and-conquer.html":{"title":"Divide and Conquer","icon":"","description":"","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/divide-and-conquer/divide-and-conquer.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475871,"modifiedTime":1737554783000,"sourceSize":1,"sourcePath":"Algorithms/Algo/Divide and Conquer/Divide and Conquer.md","exportPath":"algorithms/algo/divide-and-conquer/divide-and-conquer.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/dynamic-programming/dynamic-programing.html":{"title":"Dynamic Programing","icon":"","description":"Classic:\n<a data-href=\"70. Climbing Stairs\" href=\"algorithms/leetcode/array/dp/70.-climbing-stairs.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">70. Climbing Stairs</a>\n<br><a data-href=\"413. Arithmetic Slices\" href=\"algorithms/leetcode/array/dp/413.-arithmetic-slices.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">413. Arithmetic Slices</a>\n<br><a data-href=\"221. Maximal Square\" href=\"algorithms/leetcode/array/dp/221.-maximal-square.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">221. Maximal Square</a>\n<br><a data-href=\"1143. Longest Common Subsequence\" href=\"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1143. Longest Common Subsequence</a>\n<br><a data-href=\"300. Longest Increasing Subsequence\" href=\"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">300. Longest Increasing Subsequence</a>\n<br><a data-href=\"1626. Best Team With No Conflicts\" href=\"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1626. Best Team With No Conflicts</a>\n<br><a data-href=\"213. House Robber II\" href=\"algorithms/leetcode/array/dp/213.-house-robber-ii.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">213. House Robber II</a>\n<br><a data-href=\"62. Unique Paths\" href=\"algorithms/leetcode/array/dp/62.-unique-paths.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">62. Unique Paths</a>\nCaching:\n<br><a data-href=\"309. Best Time to Buy and Sell Stock with Cooldown\" href=\"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">309. Best Time to Buy and Sell Stock with Cooldown</a>\n<br><a data-href=\"714. Best Time to Buy and Sell Stock with Transaction Fee\" href=\"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">714. Best Time to Buy and Sell Stock with Transaction Fee</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/array/dp/70.-climbing-stairs.html#_0","algorithms/leetcode/array/dp/413.-arithmetic-slices.html#_0","algorithms/leetcode/array/dp/221.-maximal-square.html#_0","algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html#_0","algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html#_0","algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html#_0","algorithms/leetcode/array/dp/213.-house-robber-ii.html#_0","algorithms/leetcode/array/dp/62.-unique-paths.html#_0","algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html#_0","algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/dynamic-programming/dynamic-programing.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475883,"modifiedTime":1737554783000,"sourceSize":399,"sourcePath":"Algorithms/Algo/Dynamic Programming/Dynamic Programing.md","exportPath":"algorithms/algo/dynamic-programming/dynamic-programing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/dynamic-programming/kadane's-algorithm.html":{"title":"Kadane's Algorithm","icon":"","description":"\n<a data-href=\"53. Maximum Subarray\" href=\"algorithms/leetcode/array/subarray/53.-maximum-subarray.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">53. Maximum Subarray</a>\n<br><a data-href=\"918. Maximum Sum Circular Subarray\" href=\"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">918. Maximum Sum Circular Subarray</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/array/subarray/53.-maximum-subarray.html#_0","algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/dynamic-programming/kadane's-algorithm.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475888,"modifiedTime":1737554783000,"sourceSize":68,"sourcePath":"Algorithms/Algo/Dynamic Programming/Kadane's Algorithm.md","exportPath":"algorithms/algo/dynamic-programming/kadane's-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/graph/alpha-beta.html":{"title":"Alpha-Beta","icon":"","description":"In the alpha-beta pruning algorithm, when we are trying to minimize the potential search space, we keep track of two variables: alpha and beta. Alpha represents the best option for the maximizing player so far, and beta represents the best option for the minimizing player so far.When we are evaluating a node and we are in the maximizing player's turn, if the current alpha value is greater than or equal to the current beta value, we stop evaluating that node and any subsequent nodes, because we know that the minimizing player has already found a better move in a previous branch.This may seem counterintuitive at first, because we're using the alpha variable (associated with the maximizing player) to determine when to cut off the search, instead of the beta variable (associated with the minimizing player). However, this is simply a clever trick that allows us to save computational resources while still finding the optimal move. By considering alpha as the upper bound for the maximizing player, we can eliminate entire subtrees of the game tree that we know will not yield a better outcome than what has already been found.\nAlpha-Beta Pruning is a search algorithm commonly used in game theory to improve the efficiency of the Minimax algorithm.\nIt works by pruning branches in the game tree that are guaranteed to be worse than previously explored branches, thereby reducing the number of nodes that need to be explored.\nThe algorithm keeps track of two values, alpha and beta, which represent the best score the maximizing player (alpha) and the minimizing player (beta) can achieve respectively.\nAs the algorithm explores the game tree, it updates these values and uses them to prune branches that are guaranteed to lead to worse scores.\nThe algorithm is called Alpha-Beta because alpha represents the best score for the maximizing player found so far and beta represents the best score for the minimizing player found so far.\nBy using Alpha-Beta Pruning, we can potentially reduce the number of nodes that need to be explored, which can greatly improve the efficiency of the Minimax algorithm, especially when the game tree is very large.\ndef minimax_alpha_beta(node, depth, alpha, beta, maximizing_player): # Check if the maximum depth is reached or the node is terminal if depth == 0 or node.is_terminal(): return node.evaluate(), None # Maximizing player's turn if maximizing_player: max_val = float('-inf') best_child = None for child in node.generate_children(): child_val, _ = minimax_alpha_beta(child, depth - 1, alpha, beta, False) if child_val &gt; max_val: max_val = child_val best_child = child alpha = max(alpha, max_val) if beta &lt;= alpha: break return max_val, best_child # Minimizing player's turn else: min_val = float('inf') best_child = None for child in node.generate_children(): child_val, _ = minimax_alpha_beta(child, depth - 1, alpha, beta, True) if child_val &lt; min_val: min_val = child_val best_child = child beta = min(beta, min_val) if beta &lt;= alpha: break return min_val, best_child\nThe node parameter represents the current state of the game or problem, and the generate_children() method should be implemented to generate all possible next states. The is_terminal() method should be implemented to check if a state is terminal, and the evaluate() method should be implemented to evaluate the utility of a terminal state. The maximizing_player parameter should be set to True if the current player is maximizing, and False if the current player is minimizing. The alpha and beta parameters are used for pruning, and should be initialized to -inf and inf, respectively, at the root node.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/graph/alpha-beta.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475922,"modifiedTime":1737554783000,"sourceSize":4002,"sourcePath":"Algorithms/Algo/Graph/Alpha-Beta.md","exportPath":"algorithms/algo/graph/alpha-beta.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html":{"title":"Floyd's cycle-finding algorithm","icon":"","description":"\n<a data-href=\"142. Linked List Cycle II\" href=\"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">142. Linked List Cycle II</a>\nThe algorithm works by using two pointers, one that moves one step at a time (the \"slow\" pointer) and another that moves two steps at a time (the \"fast\" pointer). If there is a cycle in the linked list, eventually the fast pointer will catch up to the slow pointer, indicating the presence of a cycle.Here's how the algorithm works in more detail:\nInitialize the slow and fast pointers to the first node in the linked list.\nWhile the fast pointer is not null and the node it points to is not the same as the slow pointer: a. Move the slow pointer one step forward. b. Move the fast pointer two steps forward.\nIf the fast pointer becomes null, then there is no cycle in the linked list. Otherwise, there is a cycle, and the slow and fast pointers have met at a node in the cycle.\nHere's some sample code that implements the Floyd's cycle-finding algorithm in Python:def has_cycle(head): if head is None: return False slow = head fast = head.next while fast is not None and fast.next is not None: if slow == fast: return True slow = slow.next fast = fast.next.next return False\nIn this code, head is the first node of the linked list, and the has_cycle function returns True if there is a cycle in the linked list, and False otherwise. The algorithm runs in O(n) time, where n is the length of the linked list.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Explantion:","level":3,"id":"Explantion_0"}],"links":["algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475956,"modifiedTime":1737554783000,"sourceSize":1502,"sourcePath":"Algorithms/Algo/Graph/Floyd's cycle-finding algorithm.md","exportPath":"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/graph/minimax.html":{"title":"MiniMax","icon":"","description":"\nThe Minimax algorithm is a recursive algorithm used for decision making in games with perfect information (i.e., games where all players have complete information about the state of the game).\nIt works by evaluating each possible move from the current state of the game, assuming that the opponent will choose the move that minimizes the player's score and the player will choose the move that maximizes their score.\nThe algorithm is based on the assumption that both players play optimally (i.e., they always choose the best possible move) and therefore it is guaranteed to find the optimal move for the player, assuming that the opponent plays optimally as well.\nThe algorithm works by building a game tree, where each node represents a possible state of the game and the edges represent the possible moves that can be made from that state.\nThe algorithm evaluates each node by recursively applying the Minimax algorithm to each child node and choosing the move that leads to the highest score for the player, assuming that the opponent will choose the move that leads to the lowest score for the player.\nThe algorithm assumes that the utility function (i.e., the function that assigns a score to each terminal state of the game) is known and can be evaluated for each leaf node in the game tree.\nThe Minimax algorithm can be optimized using techniques such as alpha-beta pruning, which can eliminate large parts of the game tree that are unlikely to lead to the optimal move.\n# Define the utility function that returns the score for a given state of the game\ndef utility(state, player): # Return a positive score if the player has won, a negative score if the opponent has won, # and 0 if the game is a tie or has not ended yet. pass # Define the Minimax function that returns the best move for a given state of the game\ndef minimax(state, player): # Check if the game has ended if is_terminal(state): return None, utility(state, player) # If it is the player's turn, maximize their score if player == MAX_PLAYER: best_score = -float('inf') for move in get_possible_moves(state): new_state = make_move(state, move, player) _, score = minimax(new_state, MIN_PLAYER) if score &gt; best_score: best_score = score best_move = move # If it is the opponent's turn, minimize the player's score else: best_score = float('inf') for move in get_possible_moves(state): new_state = make_move(state, move, player) _, score = minimax(new_state, MAX_PLAYER) if score &lt; best_score: best_score = score best_move = move return best_move, best_score\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Tic-Tac-Toe","level":3,"id":"Tic-Tac-Toe_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/graph/minimax.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475982,"modifiedTime":1737554783000,"sourceSize":2827,"sourcePath":"Algorithms/Algo/Graph/MiniMax.md","exportPath":"algorithms/algo/graph/minimax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/graph/union-find.html":{"title":"Union Find","icon":"","description":"\n<a data-href=\"1061. Lexicographically Smallest Equivalent String\" href=\"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1061. Lexicographically Smallest Equivalent String</a> which also can be done by DFS\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/graph/union-find.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475990,"modifiedTime":1737554783000,"sourceSize":87,"sourcePath":"Algorithms/Algo/Graph/Union Find.md","exportPath":"algorithms/algo/graph/union-find.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/greedy/greedy.html":{"title":"Greedy","icon":"","description":"As the name implies, the greedy algorithm or greedy thought adopts a greedy strategy to ensure that each operation is locally optimal, so that the most\nThe result obtained is the global optimum.\n<a data-href=\"1833. Maximum Ice Cream Bars\" href=\"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1833. Maximum Ice Cream Bars</a>\n<br><a data-href=\"134. Gas Station\" href=\"algorithms/leetcode/greedy/134.-gas-station.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">134. Gas Station</a>\n<br><a data-href=\"455. Assign Cookies\" href=\"algorithms/leetcode/greedy/455.-assign-cookies.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">455. Assign Cookies</a>\n<br><a data-href=\"435. Non-overlapping Intervals\" href=\"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">435. Non-overlapping Intervals</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html#_0","algorithms/leetcode/greedy/134.-gas-station.html#_0","algorithms/leetcode/greedy/455.-assign-cookies.html#_0","algorithms/leetcode/greedy/435.-non-overlapping-intervals.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/greedy/greedy.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476010,"modifiedTime":1737554783000,"sourceSize":322,"sourcePath":"Algorithms/Algo/Greedy/Greedy.md","exportPath":"algorithms/algo/greedy/greedy.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/recursion/recursion.html":{"title":"Recursion","icon":"","description":"Linkedlist:\n<a data-href=\"24. Swap Nodes in Pairs\" href=\"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">24. Swap Nodes in Pairs</a>\nTree:\n<br><a data-href=\"1490. Clone N-ary Tree\" href=\"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1490. Clone N-ary Tree</a>\nString:\n<br><a data-href=\"1071. Greatest Common Divisor of Strings\" href=\"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1071. Greatest Common Divisor of Strings</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html#_0","algorithms/leetcode/tree/1490.-clone-n-ary-tree.html#_0","algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/recursion/recursion.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476021,"modifiedTime":1737554783000,"sourceSize":134,"sourcePath":"Algorithms/Algo/Recursion/Recursion.md","exportPath":"algorithms/algo/recursion/recursion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/search/a-star.html":{"title":"A star","icon":"","description":"A* search is a widely used informed search algorithm that combines elements of both uniform cost search and greedy search. It uses a heuristic function to guide the search towards the goal state and is often used in pathfinding and graph traversal problems.In A* search, each node in the search space is assigned a cost, which is the sum of the cost of the path from the start node to that node (called the g-score) and an estimate of the remaining cost to reach the goal state from that node (called the h-score). The algorithm maintains a priority queue of nodes to be expanded based on their f-score (the sum of the g-score and h-score).At each step, the algorithm expands the node with the lowest f-score and updates the g-score and h-score of its neighbors. If the goal state is reached, the algorithm terminates and returns the optimal path from the start node to the goal state. Otherwise, the algorithm continues to expand nodes until the goal state is reached or there are no more nodes to expand.A key advantage of A* search is that it can often find the optimal solution to a problem efficiently by intelligently selecting which nodes to expand based on their estimated distance to the goal state. However, the accuracy of the heuristic function used to estimate the remaining cost can greatly affect the performance of the algorithm, and in some cases, the algorithm may be slow or fail to find the optimal solution.import heapq def heuristic(a, b): \"\"\" A heuristic function that returns the Manhattan distance between two points a and b. \"\"\" return abs(a[0] - b[0]) + abs(a[1] - b[1]) def a_star_search(grid, start, target): \"\"\" An implementation of A* search algorithm on a given grid with start and target positions. Returns a list of positions representing the path from start to target. :param grid: a Grid object representing the environment :param start: a tuple representing the starting position :param target: a tuple representing the target position :return: a list of positions representing the path from start to target \"\"\" # Calculate the heuristic value of start to target distance h = heuristic(start, target) # Initialize the frontier and visited set frontier, moves = [], [] heapq.heappush(frontier, (0, (start, moves))) visited = {start: 0} while frontier: _, cur = heapq.heappop(frontier) curPos, move = cur # If current position is target position, return the path if curPos == target: return move # Explore the successors of current position successors = grid.neighbors(curPos) for next_State in successors: # Calculate the cost of reaching the next state next_cost = visited[curPos]+1 # If next state has not been visited or has been visited with higher cost, add it to the frontier if next_State not in visited or visited[next_State] &gt; next_cost: visited[next_State] = next_cost temp = move.copy() temp.append(next_State) f = next_cost + heuristic(next_State, target) heapq.heappush(frontier, (f, (next_State, temp))) # If the frontier is empty but target position not reached, return an empty path return []\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/search/a-star.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476056,"modifiedTime":1737554783000,"sourceSize":3357,"sourcePath":"Algorithms/Algo/Search/A star.md","exportPath":"algorithms/algo/search/a-star.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/search/breadth-first-search.html":{"title":"Breadth First Search","icon":"","description":"\n<a data-href=\"103. Binary Tree Zigzag Level Order Traversal\" href=\"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">103. Binary Tree Zigzag Level Order Traversal</a>\n<br><a data-href=\"102. Binary Tree Level Order Traversal\" href=\"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">102. Binary Tree Level Order Traversal</a>\n<br><a data-href=\"490. The Maze\" href=\"algorithms/leetcode/graph/490.-the-maze.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">490. The Maze</a>\n<br><a data-href=\"934. Shortest Bridge\" href=\"algorithms/leetcode/array/matrix/934.-shortest-bridge.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">934. Shortest Bridge</a>\n<br><a data-href=\"1602. Find Nearest Right Node in Binary Tree\" href=\"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1602. Find Nearest Right Node in Binary Tree</a>\nclass Node: def __init__(self, value): self.value = value self.left = None self.right = None def bfs_queue(node): if node is None: return queue = [] queue.append(node) while queue: curr_node = queue.pop(0) print(curr_node.value) # process current node if curr_node.left: queue.append(curr_node.left) # enqueue left child if curr_node.right: queue.append(curr_node.right) # enqueue right child # create a binary tree\nroot = Node(1)\nroot.left = Node(2)\nroot.right = Node(3)\nroot.left.left = Node(4)\nroot.left.right = Node(5) # perform BFS on the binary tree using queue\nbfs_queue(root)\nIn this implementation, we use a queue (implemented as a Python list) to keep track of the nodes to be processed. We start by enqueueing the root node onto the queue. Then, while the queue is not empty, we dequeue the first node from the queue and process it by printing its value. We then check if the dequeued node has a left child, and if so, enqueue the left child onto the queue. Similarly, we check if the dequeued node has a right child, and if so, enqueue the right child onto the queue.The bfs_queue function is similar to the previous bfs function, except that it uses a queue instead of recursion to perform the traversal.In the main part of the code, we create the same binary tree as before and call bfs_queue with the root node to perform the breadth-first traversal. The output of the program would be:1\n2\n3\n4\n5\nalternatively, use recurions instead of queue:\ndef bfs_recursion(queue): if not queue: return node = queue.pop(0) print(node.value) # process current node if node.left: queue.append(node.left) # enqueue left child if node.right: queue.append(node.right) # enqueue right child bfs_recursion(queue) # process remaining nodes in queue # create a binary tree\nroot = Node(1)\nroot.left = Node(2)\nroot.right = Node(3)\nroot.left.left = Node(4)\nroot.left.right = Node(5) # perform BFS on the binary tree using recursion\nbfs_recursion([root])\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Explanation:","level":3,"id":"Explanation_0"}],"links":["algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html#_0","algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html#_0","algorithms/leetcode/graph/490.-the-maze.html#_0","algorithms/leetcode/array/matrix/934.-shortest-bridge.html#_0","algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/search/breadth-first-search.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476060,"modifiedTime":1737554783000,"sourceSize":4052,"sourcePath":"Algorithms/Algo/Search/Breadth First Search.md","exportPath":"algorithms/algo/search/breadth-first-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/search/depth-first-search.html":{"title":"Depth First Search","icon":"","description":"\n<a data-href=\"100. Same Tree\" href=\"algorithms/leetcode/tree/100.-same-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">100. Same Tree</a>\n<br><a data-href=\"1061. Lexicographically Smallest Equivalent String\" href=\"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1061. Lexicographically Smallest Equivalent String</a>\n<br><a data-href=\"417. Pacific Atlantic Water Flow\" href=\"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">417. Pacific Atlantic Water Flow</a>\n<br><a data-href=\"472. Concatenated Words\" href=\"algorithms/leetcode/array/dp/472.-concatenated-words.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">472. Concatenated Words</a>\n<br><a data-href=\"1971. Find if Path Exists in Graph\" href=\"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1971. Find if Path Exists in Graph</a>\n<br><a data-href=\"841. Keys and Rooms\" href=\"algorithms/leetcode/graph/841.-keys-and-rooms.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">841. Keys and Rooms</a>\n<br><a data-href=\"1123. Lowest Common Ancestor of Deepest Leaves\" href=\"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1123. Lowest Common Ancestor of Deepest Leaves</a>\n<br><a data-href=\"309. Best Time to Buy and Sell Stock with Cooldown\" href=\"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">309. Best Time to Buy and Sell Stock with Cooldown</a>\n<br><a data-href=\"714. Best Time to Buy and Sell Stock with Transaction Fee\" href=\"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">714. Best Time to Buy and Sell Stock with Transaction Fee</a>\nclass Node: def __init__(self, value): self.value = value self.left = None self.right = None def dfs(node): if node is None: return print(node.value) # process current node dfs(node.left) # recursively process left subtree dfs(node.right) # recursively process right subtree # create a binary tree\nroot = Node(1)\nroot.left = Node(2)\nroot.right = Node(3)\nroot.left.left = Node(4)\nroot.left.right = Node(5) # perform DFS on the binary tree\ndfs(root)\nIn this example, we define a Node class to represent a node in the binary tree. Each node has a value attribute to store the node's value, and left and right attributes to point to its left and right children.The dfs function performs a depth-first traversal of the binary tree. It takes a node argument that represents the current node being processed. The function first checks if the node argument is None. If it is, then it simply returns without doing anything. Otherwise, it processes the current node by printing its value, and then recursively processes its left and right subtrees by calling dfs with the left and right children of the current node, respectively.In the main part of the code, we create a binary tree with the same structure as in the previous example, and then call dfs with the root node of the binary tree to perform the depth-first traversal. The output of the program would be:1\n2\n4\n5\n3\nThis output shows the values of the nodes in the binary tree in pre-order traversal order, which is the order in which the nodes are processed by the dfs function.alternatively, use stack instead of recursion:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Explanation:","level":3,"id":"Explanation_0"}],"links":["algorithms/leetcode/tree/100.-same-tree.html#_0","algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html#_0","algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html#_0","algorithms/leetcode/array/dp/472.-concatenated-words.html#_0","algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html#_0","algorithms/leetcode/graph/841.-keys-and-rooms.html#_0","algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html#_0","algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html#_0","algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/search/depth-first-search.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476122,"modifiedTime":1737554783000,"sourceSize":5374,"sourcePath":"Algorithms/Algo/Search/Depth First Search.md","exportPath":"algorithms/algo/search/depth-first-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/search/dfs-vs-bfs.html":{"title":"DFS vs BFS","icon":"","description":"When it comes to solving traversal related algorithmic problems, two popular algorithms come to mind: Depth-First Search (DFS) and Breadth-First Search (BFS).\nWhile they both aim to explore and traverse a graph, they have distinct strategies for visiting nodes and excel in different problem domains.Imagine you're exploring a maze, and DFS is your strategy. You start at an entrance and venture as far as you can along each path before backtracking. It's all about going as deep as possible! DFS uses recursion to keep track of the order of traversal.DFS is a memory-saver compared to BFS. It only needs to store the path from the starting node to the current node, reducing the memory load.In terms of time complexity, DFS performs with a worst-case scenario of O(V + E), where V represents the number of vertices and E represents the number of edges in the graph.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"A Comparative Analysis of DFS and BFS Algorithms for Problem Solving","level":1,"id":"A_Comparative_Analysis_of_DFS_and_BFS_Algorithms_for_Problem_Solving_0"},{"heading":"Introduction","level":2,"id":"Introduction_0"},{"heading":"Depth-First Search (DFS)","level":2,"id":"Depth-First_Search_(DFS)_0"},{"heading":"Exploring the Depths","level":3,"id":"Exploring_the_Depths_0"},{"heading":"Going Light on Memory","level":3,"id":"Going_Light_on_Memory_0"},{"heading":"Timing It Right","level":3,"id":"Timing_It_Right_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/search/dfs-vs-bfs.html","pathToRoot":"../../..","attachments":[],"createdTime":1697824454391,"modifiedTime":1704139676000,"sourceSize":3966,"sourcePath":"Algorithms/Algo/Search/DFS vs BFS.md","exportPath":"algorithms/algo/search/dfs-vs-bfs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/search/dijkstra.html":{"title":"Dijkstra","icon":"","description":"<a data-href=\"787. Cheapest Flights Within K Stops\" href=\"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">787. Cheapest Flights Within K Stops</a>Dijkstra's algorithm is a graph traversal algorithm used to find the shortest path between a source node and all other nodes in a weighted graph. It uses a priority queue to explore the vertices in order of their distance from the source vertex. Here is an example implementation of Dijkstra's algorithm using a priority queue in Python:import heapq def dijkstra(graph, source): distances = {vertex: float('infinity') for vertex in graph} distances[source] = 0 pq = [(0, source)] while pq: (current_distance, current_vertex) = heapq.heappop(pq) if current_distance &gt; distances[current_vertex]: continue for neighbor, weight in graph[current_vertex].items(): distance = current_distance + weight if distance &lt; distances[neighbor]: distances[neighbor] = distance heapq.heappush(pq, (distance, neighbor)) return distances\nIn this implementation, we first initialize a dictionary distances to hold the shortest distance from the source node to all other nodes in the graph. We initialize the distance to infinity for all nodes except the source node, which has a distance of 0. We then create a priority queue pq to hold the vertices in order of their distance from the source vertex. The queue is initially populated with the source vertex and its distance of 0.We then repeatedly extract the vertex with the smallest distance from the priority queue and explore its neighbors. For each neighbor, we calculate its tentative distance from the source node by adding the weight of the edge connecting it to the current node to the current distance. If this tentative distance is less than its current distance, we update the distance and add the neighbor to the priority queue.Finally, we return the distances dictionary containing the shortest distances from the source node to all other nodes in the graph.Here is an example usage of the dijkstra function:graph = { 'A': {'B': 5, 'D': 9, 'E': 2}, 'B': {'C': 2}, 'C': {}, 'D': {'F': 2}, 'E': {'B': 1, 'D': 6}, 'F': {'C': 3}\n} distances = dijkstra(graph, 'A')\nprint(distances)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Explanation:","level":3,"id":"Explanation_0"}],"links":["algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/search/dijkstra.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476188,"modifiedTime":1737554783000,"sourceSize":3758,"sourcePath":"Algorithms/Algo/Search/Dijkstra.md","exportPath":"algorithms/algo/search/dijkstra.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/search/iterative-deepening.html":{"title":"Iterative Deepening","icon":"","description":"Iterative deepening search (IDS) is a search algorithm that combines the benefits of depth-first search (DFS) and breadth-first search (BFS). It is a brute-force search algorithm that systematically searches for a goal node by incrementally increasing the maximum depth limit of DFS until a goal node is found. The algorithm performs DFS to a maximum depth limit in a loop until a goal node is found or the maximum depth limit is reached, then it increases the maximum depth limit by 1 and performs another DFS iteration. This process continues until a goal node is found or the entire search space has been explored.IDS is complete, meaning that it is guaranteed to find a solution if one exists, and optimal, meaning that it will find the shortest path to the goal node if it exists. However, the time complexity of IDS is still exponential in the worst case, although it is more efficient than BFS in terms of space complexity because it only stores a single path at any given time.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/search/iterative-deepening.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476197,"modifiedTime":1737554783000,"sourceSize":988,"sourcePath":"Algorithms/Algo/Search/Iterative Deepening.md","exportPath":"algorithms/algo/search/iterative-deepening.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/sliding-window/sliding-window.html":{"title":"Sliding Window","icon":"","description":"\n<a data-href=\"209. Minimum Size Subarray Sum\" href=\"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">209. Minimum Size Subarray Sum</a>\n<br><a data-href=\"713. Subarray Product Less Than K\" href=\"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">713. Subarray Product Less Than K</a>\n<br><a data-href=\"1004. Max Consecutive Ones III\" href=\"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1004. Max Consecutive Ones III</a>\n<br><a data-href=\"340. Longest Substring with At Most K Distinct Characters\" href=\"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">340. Longest Substring with At Most K Distinct Characters</a>\n<br><a data-href=\"438. Find All Anagrams in a String\" href=\"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">438. Find All Anagrams in a String</a>\n<br><a data-href=\"1493. Longest Subarray of 1's After Deleting One Element\" href=\"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1493. Longest Subarray of 1's After Deleting One Element</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html#_0","algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html#_0","algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html#_0","algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html#_0","algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html#_0","algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/sliding-window/sliding-window.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476231,"modifiedTime":1737554783000,"sourceSize":281,"sourcePath":"Algorithms/Algo/Sliding Window/Sliding Window.md","exportPath":"algorithms/algo/sliding-window/sliding-window.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/sorting/binary-search.html":{"title":"Binary Search","icon":"","description":"\n<a data-href=\"81. Search in Rotated Sorted Array II\" href=\"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">81. Search in Rotated Sorted Array II</a>\n<br><a data-href=\"74. Search a 2D Matrix\" href=\"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">74. Search a 2D Matrix</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html#_0","algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/sorting/binary-search.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476241,"modifiedTime":1737554783000,"sourceSize":73,"sourcePath":"Algorithms/Algo/Sorting/Binary Search.md","exportPath":"algorithms/algo/sorting/binary-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html":{"title":"Floyd’s Cycle Finding Algorithm","icon":"","description":"Floyd's Cycle Finding Algorithm, also known as the tortoise and hare algorithm, is used to detect cycles in a linked list or any sequence of elements. It works by using two pointers, one moving at a slower pace (tortoise) and the other at a faster pace (hare), to traverse the sequence. If there is a cycle present, the two pointers will eventually meet at the same node.The algorithm consists of the following steps:\nInitialize two pointers, the tortoise and the hare, to the starting node of the sequence.\nMove the tortoise one step at a time and the hare two steps at a time.\nContinue moving the pointers until they either meet at the same node or reach the end of the sequence. If they reach the end without meeting, there is no cycle present.\nIf the pointers meet at the same node, it indicates the presence of a cycle in the sequence.\nThe algorithm can be summarized using the following pseudocode:class Node {\npublic: int data; Node* next; Node(int data) { this-&gt;data = data; next = NULL; }\n}; Node* head = NULL; class Linkedlist {\npublic: void insert(int value){ Node* newNode = new Node(value); if (head == NULL) head = newNode; else { newNode-&gt;next = head; head = newNode; } } bool detectLoop(){ Node *slowPointer = head, *fastPointer = head; while (slowPointer != NULL &amp;&amp; fastPointer != NULL &amp;&amp; fastPointer-&gt;next != NULL) { slowPointer = slowPointer-&gt;next; fastPointer = fastPointer-&gt;next-&gt;next; if (slowPointer == fastPointer) return 1; } return 0; }\n}; int main(){ Linkedlist l1; l1.insert(10); l1.insert(20); l1.insert(30); l1.insert(40); l1.insert(50); Node* temp = head; while (temp-&gt;next != NULL) temp = temp-&gt;next; temp-&gt;next = head; if (l1.detectLoop()) cout &lt;&lt; \"Loop exists in the\" &lt;&lt; \" Linked List\" &lt;&lt; endl; else cout &lt;&lt; \"Loop does not exists \" &lt;&lt; \"in the Linked List\" &lt;&lt; endl; return 0;\n}\nlets assume that :\nP = Distance between the head(starting) to the loop starting point.\nX = Distance between the loop starting point and the first meeting point of both the pointers.\nC = The length of the loop\nSo before both the pointer meets:\nThe slow pointer has traveled distance.\nThe fast pointer has traveled distance.\nSince the fast pointer is moving twice as fast as the slow pointer, we can say that the fast pointer covered twice the distance the slow pointer covered. Thereforeso we have proved the distances are equal now if we start two slow pointers from these 2 points they will meet each other at the start of the loop which is the duplicate number and we the return value we wanted. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Floyd's Cycle Finding Algorithm","level":1,"id":"Floyd's_Cycle_Finding_Algorithm_0"},{"heading":"Proof of Equal Distances","level":2,"id":"Proof_of_Equal_Distances_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html","pathToRoot":"../../..","attachments":[],"createdTime":1695306688443,"modifiedTime":1741084570719,"sourceSize":2865,"sourcePath":"Algorithms/Algo/Two Pointers/Floyd’s Cycle Finding Algorithm.md","exportPath":"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algo/two-pointers/two-pointer.html":{"title":"Two Pointer","icon":"","description":"\n<a data-href=\"26. Remove Duplicates from Sorted Array\" href=\"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">26. Remove Duplicates from Sorted Array</a>\n<br><a data-href=\"27. Remove Element\" href=\"algorithms/leetcode/array/two-pointers/27.-remove-element.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">27. Remove Element</a> <br><a data-href=\"11. Container With Most Water\" href=\"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">11. Container With Most Water</a>\n<br><a data-href=\"633. Sum of Square Numbers\" href=\"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">633. Sum of Square Numbers</a>\n<br><a data-href=\"69. Sqrt(x)\" href=\"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">69. Sqrt(x)</a> <br><a data-href=\"Floyd's cycle-finding algorithm\" href=\"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Floyd's cycle-finding algorithm</a>\n<br><a data-href=\"1004. Max Consecutive Ones III\" href=\"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1004. Max Consecutive Ones III</a>\n<br><a data-href=\"455. Assign Cookies\" href=\"algorithms/leetcode/greedy/455.-assign-cookies.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">455. Assign Cookies</a>\n<br><a data-href=\"392. Is Subsequence\" href=\"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">392. Is Subsequence</a> <br><a data-href=\"15. 3Sum\" href=\"algorithms/leetcode/array/two-pointers/15.-3sum.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">15. 3Sum</a>\n<br><a data-href=\"16. 3Sum Closest\" href=\"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">16. 3Sum Closest</a>\n<br><a data-href=\"18. 4Sum\" href=\"algorithms/leetcode/array/two-pointers/18.-4sum.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">18. 4Sum</a>\n<br><a data-href=\"259. 3Sum Smaller\" href=\"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">259. 3Sum Smaller</a>\n<br><a data-href=\"253. Meeting Rooms II\" href=\"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">253. Meeting Rooms II</a> which can be done by heap as well\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Basic two-pointers:","level":4,"id":"Basic_two-pointers_0"},{"heading":"Left - Right pointers:","level":4,"id":"Left_-_Right_pointers_0"},{"heading":"Fast-slow pointers:","level":4,"id":"Fast-slow_pointers_0"},{"heading":"Sort then pointers:","level":4,"id":"Sort_then_pointers_0"}],"links":["algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html#_0","algorithms/leetcode/array/two-pointers/27.-remove-element.html#_0","algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html#_0","algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html#_0","algorithms/leetcode/array/two-pointers/69.-sqrt(x).html#_0","algorithms/algo/graph/floyd's-cycle-finding-algorithm.html#_0","algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html#_0","algorithms/leetcode/greedy/455.-assign-cookies.html#_0","algorithms/leetcode/array/two-pointers/392.-is-subsequence.html#_0","algorithms/leetcode/array/two-pointers/15.-3sum.html#_0","algorithms/leetcode/array/two-pointers/16.-3sum-closest.html#_0","algorithms/leetcode/array/two-pointers/18.-4sum.html#_0","algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html#_0","algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algo/two-pointers/two-pointer.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084476250,"modifiedTime":1737554783000,"sourceSize":533,"sourcePath":"Algorithms/Algo/Two Pointers/Two Pointer.md","exportPath":"algorithms/algo/two-pointers/two-pointer.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/45.-jump-game-ii.html":{"title":"45. Jump Game II","icon":"","description":"You are given a&nbsp;0-indexed&nbsp;array of integers&nbsp;nums&nbsp;of length&nbsp;n. You are initially positioned at&nbsp;nums[0].Each element&nbsp;nums[i]&nbsp;represents the maximum length of a forward jump from index&nbsp;i. In other words, if you are at&nbsp;nums[i], you can jump to any&nbsp;nums[i + j]&nbsp;where:\n0 &lt;= j &lt;= nums[i]&nbsp;and\ni + j &lt; n\nReturn&nbsp;the minimum number of jumps to reach&nbsp;nums[n - 1]. The test cases are generated such that you can reach&nbsp;nums[n - 1].class Solution: def jump(self, nums: List[int]) -&gt; int: n = len(nums) jump_count = 0 max_reach = 0 # initialize the current reachable index to 0 reach = 0 for i in range(n-1): max_reach = max(max_reach, i + nums[i]) # if we have reached the last index we can reach from the current position, # update the jump count and the current reachable index if i == reach: jump_count += 1 reach = max_reach return jump_count\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/45.-jump-game-ii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473623,"modifiedTime":1737554783000,"sourceSize":1055,"sourcePath":"Algorithms/Leetcode/Array/DP/45. Jump Game II.md","exportPath":"algorithms/leetcode/array/dp/45.-jump-game-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/62.-unique-paths.html":{"title":"62. Unique Paths","icon":"","description":"There is a robot on an&nbsp;m x n&nbsp;grid. The robot is initially located at the&nbsp;top-left corner&nbsp;(i.e.,&nbsp;grid[0][0]). The robot tries to move to the&nbsp;bottom-right corner&nbsp;(i.e.,&nbsp;grid[m - 1][n - 1]). The robot can only move either down or right at any point in time.Given the two integers&nbsp;m&nbsp;and&nbsp;n, return&nbsp;the number of possible unique paths that the robot can take to reach the bottom-right corner.The test cases are generated so that the answer will be less than or equal to&nbsp;2 * 109.class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: dp = [[0]*(n) for _ in range(m)] for i in range(m): for j in range(n): if i == 0: dp[i][j] = 1 continue if j == 0: dp[i][j] = 1 else: dp[i][j] = dp[i-1][j] + dp[i][j-1] return dp[m-1][n-1]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/62.-unique-paths.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473685,"modifiedTime":1737554783000,"sourceSize":972,"sourcePath":"Algorithms/Leetcode/Array/DP/62. Unique Paths.md","exportPath":"algorithms/leetcode/array/dp/62.-unique-paths.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/70.-climbing-stairs.html":{"title":"70. Climbing Stairs","icon":"","description":"You are climbing a staircase. It takes&nbsp;n&nbsp;steps to reach the top.Each time you can either climb&nbsp;1&nbsp;or&nbsp;2&nbsp;steps. In how many distinct ways can you climb to the top?class Solution: def climbStairs(self, n: int) -&gt; int: if n==1: return 1 dp = [None]*(n+1); dp[1]=1; dp[2] = 2 for i in range(3,n+1): dp[i] = dp[i-1]+dp[i-2] return dp[n]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/70.-climbing-stairs.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473690,"modifiedTime":1737554783000,"sourceSize":435,"sourcePath":"Algorithms/Leetcode/Array/DP/70. Climbing Stairs.md","exportPath":"algorithms/leetcode/array/dp/70.-climbing-stairs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html":{"title":"96. Unique Binary Search Trees","icon":"","description":"Given an integer&nbsp;n, return&nbsp;the number of structurally unique&nbsp;BST's (binary search trees) which has exactly&nbsp;n&nbsp;nodes of unique values from&nbsp;1&nbsp;to&nbsp;n.class Solution: def numTrees(self, n: int) -&gt; int: G = [0]*(n+1) G[0], G[1] = 1, 1 for i in range(2, n+1): for j in range(1, i+1): G[i] += G[j-1] * G[i-j] return G[n]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473701,"modifiedTime":1737554783000,"sourceSize":419,"sourcePath":"Algorithms/Leetcode/Array/DP/96. Unique Binary Search Trees.md","exportPath":"algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/198.-house-robber.html":{"title":"198. House Robber","icon":"","description":"You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and&nbsp;it will automatically contact the police if two adjacent houses were broken into on the same night.Given an integer array&nbsp;nums&nbsp;representing the amount of money of each house, return&nbsp;the maximum amount of money you can rob tonight&nbsp;without alerting the police.class Solution: def rob(self, nums: List[int]) -&gt; int: if nums==[]:return 0 n = len(nums) dp = [0]*(n+1) dp[1] = nums[0] for i in range(2,n+1): dp[i] = max (dp[i-1], dp[i-2]+nums[i-1]) return dp[-1]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/198.-house-robber.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473382,"modifiedTime":1737554783000,"sourceSize":795,"sourcePath":"Algorithms/Leetcode/Array/DP/198. House Robber.md","exportPath":"algorithms/leetcode/array/dp/198.-house-robber.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/213.-house-robber-ii.html":{"title":"213. House Robber II","icon":"","description":"You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed. All houses at this place are&nbsp;arranged in a circle.&nbsp;That means the first house is the neighbor of the last one. Meanwhile, adjacent houses have a security system connected, and&nbsp;it will automatically contact the police if two adjacent houses were broken into on the same night.Given an integer array&nbsp;nums&nbsp;representing the amount of money of each house, return&nbsp;the maximum amount of money you can rob tonight&nbsp;without alerting the police.class Solution {\npublic: int rob(vector&lt;int&gt;&amp; nums) { if (nums.size() == 1){ return nums[0]; } const int n = nums.size()+2; int dp1[n]; dp1[n-1] = 0; dp1[n-2] = 0; for(int i=n-3; i&gt;=1; i--){ //don't rob the first house dp1[i] = max(nums[i]+ dp1[i+2],dp1[i+1]); } //if we rob the first house, we couldn't rob the last one int dp2[n]; dp2[n-1] = 0; dp2[n-2] = 0; dp2[n-3] = 0;//we set up the profit of last house as 0 for(int i=n-4; i&gt;=0; i--){ // rob the first house dp2[i] = max(nums[i]+ dp2[i+2],dp2[i+1]); } return max(dp1[1], dp2[0]); }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/213.-house-robber-ii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473404,"modifiedTime":1737554783000,"sourceSize":1317,"sourcePath":"Algorithms/Leetcode/Array/DP/213. House Robber II.md","exportPath":"algorithms/leetcode/array/dp/213.-house-robber-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/221.-maximal-square.html":{"title":"221. Maximal Square","icon":"","description":"Given an&nbsp;m x n&nbsp;binary&nbsp;matrix&nbsp;filled with&nbsp;0's and&nbsp;1's,&nbsp;find the largest square containing only&nbsp;1's&nbsp;and return its area.class Solution {\npublic: int maximalSquare(vector&lt;vector&lt;char&gt;&gt;&amp; matrix) { //base case if(matrix.size()==0) return 0; //initializing our sentinal variables int maxSqr = 0, rows = matrix.size() , column = matrix[0].size(); //dp matrix that we will be making vector&lt;vector&lt;int&gt;&gt; dp(rows+1,vector&lt;int&gt;(column+1,0)); //Iterate over the matrix for(int i=1;i&lt;=rows;++i){ for(int j=1;j&lt;=column;++j){ //we found the 1 in our binary matrix if(matrix[i-1][j-1]=='1'){ dp[i][j] = min({dp[i-1][j-1], dp[i-1][j], dp[i][j-1]})+ 1; maxSqr=max(maxSqr,dp[i][j]); } } } return maxSqr*maxSqr; }\n}; ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/221.-maximal-square.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473457,"modifiedTime":1737554783000,"sourceSize":950,"sourcePath":"Algorithms/Leetcode/Array/DP/221. Maximal Square.md","exportPath":"algorithms/leetcode/array/dp/221.-maximal-square.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html":{"title":"300. Longest Increasing Subsequence","icon":"","description":"Given an integer array&nbsp;nums, return&nbsp;the length of the longest&nbsp;strictly increasing&nbsp;subsequence.class Solution: def lengthOfLIS(self, nums: List[int]) -&gt; int: n,res = len(nums),0 dp = [1]*(n+1) for i in range(n): for j in range(i): if nums[i]&gt;nums[j]: dp[i] = max(dp[i], dp[j]+1) res = max(res, dp[i]) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473569,"modifiedTime":1737554783000,"sourceSize":463,"sourcePath":"Algorithms/Leetcode/Array/DP/300. Longest Increasing Subsequence.md","exportPath":"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html":{"title":"309. Best Time to Buy and Sell Stock with Cooldown","icon":"","description":"You are given an array&nbsp;prices&nbsp;where&nbsp;prices[i]&nbsp;is the price of a given stock on the&nbsp;ith&nbsp;day.Find the maximum profit you can achieve. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times) with the following restrictions:\nAfter you sell your stock, you cannot buy stock on the next day (i.e., cooldown one day).\nNote:&nbsp;You may not engage in multiple transactions simultaneously (i.e., you must sell the stock before you buy again).class Solution: def maxProfit(self, prices: List[int]) -&gt; int: dp = {} def dfs(i,buying): if i&gt;=len(prices): return 0 if (i,buying) in dp: return dp[(i,buying)] if buying: buy = dfs(i+1,not buying)-prices[i] cooldown = dfs(i+1,buying) dp[(i,buying)] = max(buy,cooldown) else: sell = dfs(i+2, not buying)+prices[i] cooldown = dfs(i+1,buying) dp[(i,buying)] = max(sell,cooldown) return dp[(i,buying)] return dfs(0,True)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473573,"modifiedTime":1737554783000,"sourceSize":1200,"sourcePath":"Algorithms/Leetcode/Array/DP/309. Best Time to Buy and Sell Stock with Cooldown.md","exportPath":"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/413.-arithmetic-slices.html":{"title":"413. Arithmetic Slices","icon":"","description":"An integer array is called arithmetic if it consists of&nbsp;at least three elements&nbsp;and if the difference between any two consecutive elements is the same.\nFor example,&nbsp;[1,3,5,7,9],&nbsp;[7,7,7,7], and&nbsp;[3,-1,-5,-9]&nbsp;are arithmetic sequences.\nGiven an integer array&nbsp;nums, return&nbsp;the number of arithmetic&nbsp;subarrays&nbsp;of&nbsp;nums.A&nbsp;subarray&nbsp;is a contiguous subsequence of the array.class Solution {\npublic: int numberOfArithmeticSlices(vector&lt;int&gt;&amp; nums) { int n = nums.size(); if (n&lt;3) return 0; vector&lt;int&gt; dp(n,0); for (int i=2;i&lt;n;i++){ if (nums[i]-nums[i-1] == nums[i-1]-nums[i-2]){ dp[i] = dp[i-1]+1; } } return accumulate(dp.begin(),dp.end(),0); }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/413.-arithmetic-slices.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473589,"modifiedTime":1737554783000,"sourceSize":803,"sourcePath":"Algorithms/Leetcode/Array/DP/413. Arithmetic Slices.md","exportPath":"algorithms/leetcode/array/dp/413.-arithmetic-slices.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/472.-concatenated-words.html":{"title":"472. Concatenated Words","icon":"","description":"Given an array of strings&nbsp;words&nbsp;(without duplicates), return&nbsp;all the&nbsp;concatenated words&nbsp;in the given list of&nbsp;words.A&nbsp;concatenated word&nbsp;is defined as a string that is comprised entirely of at least two shorter words in the given array.class Solution: def findAllConcatenatedWordsInADict(self, words: List[str]) -&gt; List[str]: wordSet,res = set(words),[] def dfs(word): for i in range(1,len(word)): prefix = word[:i] suffix = word[i:] if ((prefix in wordSet and suffix in wordSet) or (prefix in wordSet and dfs(suffix))): return True for x in words: if dfs(x): res.append(x) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/472.-concatenated-words.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473649,"modifiedTime":1737554783000,"sourceSize":799,"sourcePath":"Algorithms/Leetcode/Array/DP/472. Concatenated Words.md","exportPath":"algorithms/leetcode/array/dp/472.-concatenated-words.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/542.-01-matrix.html":{"title":"542. 01 Matrix","icon":"","description":"Given an&nbsp;m x n&nbsp;binary matrix&nbsp;mat, return&nbsp;the distance of the nearest&nbsp;0&nbsp;for each cell.The distance between two adjacent cells is&nbsp;1.class Solution {\npublic: vector&lt;vector&lt;int&gt;&gt; updateMatrix(vector&lt;vector&lt;int&gt;&gt;&amp; mat) { vector&lt;vector&lt;int&gt;&gt;dp(mat.size(), vector&lt;int&gt;(mat[0].size(), INT_MAX-100000)); for(int i=0; i&lt;mat.size(); i++){ for(int j=0; j&lt;mat[0].size(); j++){ if(mat[i][j]==0){ dp[i][j]=0; } else{ if(i&gt;0){ dp[i][j]= min(dp[i][j], dp[i-1][j]+1); } if(j&gt;0){ dp[i][j]= min(dp[i][j], dp[i][j-1]+1); } } } } for(int i=mat.size()-1; i&gt;=0; i--){ for(int j=mat[0].size()-1; j&gt;=0; j--){ if(mat[i][j]==0){ dp[i][j]=0; } else{ if(i&lt;mat.size()-1){ dp[i][j]= min(dp[i][j], dp[i+1][j]+1); } if(j&lt;mat[0].size()-1){ dp[i][j]= min(dp[i][j], dp[i][j+1]+1); } } } } return dp; } };\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/542.-01-matrix.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473681,"modifiedTime":1737554783000,"sourceSize":1352,"sourcePath":"Algorithms/Leetcode/Array/DP/542. 01 Matrix.md","exportPath":"algorithms/leetcode/array/dp/542.-01-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html":{"title":"714. Best Time to Buy and Sell Stock with Transaction Fee","icon":"","description":"You are given an array&nbsp;prices&nbsp;where&nbsp;prices[i]&nbsp;is the price of a given stock on the&nbsp;ith&nbsp;day, and an integer&nbsp;fee&nbsp;representing a transaction fee.Find the maximum profit you can achieve. You may complete as many transactions as you like, but you need to pay the transaction fee for each transaction.Note:&nbsp;You may not engage in multiple transactions simultaneously (i.e., you must sell the stock before you buy again).class Solution: def maxProfit(self, prices: List[int], fee: int) -&gt; int: dp = {} def dfs(i,buying): if i &gt;=len(prices): return 0 if (i,buying) in dp: return dp[(i,buying)] if buying: buy = dfs(i+1,not buying) - prices[i] cool = dfs(i+1,buying) dp[(i,buying)] = max (buy,cool) else: sell = dfs(i+1, not buying) + prices[i] - fee cool = dfs (i+1, buying) dp[(i,buying)] = max (sell,cool) return dp[(i,buying)] return dfs(0,True)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473693,"modifiedTime":1737554783000,"sourceSize":1160,"sourcePath":"Algorithms/Leetcode/Array/DP/714. Best Time to Buy and Sell Stock with Transaction Fee.md","exportPath":"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/740.-delete-and-earn.html":{"title":"740. Delete and Earn","icon":"","description":"You are given an integer array&nbsp;nums. You want to maximize the number of points you get by performing the following operation any number of times:\nPick any&nbsp;nums[i]&nbsp;and delete it to earn&nbsp;nums[i]&nbsp;points. Afterwards, you must delete&nbsp;every&nbsp;element equal to&nbsp;nums[i] - 1&nbsp;and&nbsp;every&nbsp;element equal to&nbsp;nums[i] + 1.\nReturn&nbsp;the&nbsp;maximum number of points&nbsp;you can earn by applying the above operation some number of times.class Solution: def deleteAndEarn(self, nums: List[int]) -&gt; int: count = Counter(nums) nums = sorted(list(set(nums))) e1,e2 = 0,0 for i in range(len(nums)): cur = nums[i]*count[nums[i]] if i&gt;0 and nums[i]==nums[i-1]+1: temp = e2 e2 = max(cur+e1, e2) e1 = temp else: temp = e2 e2 = cur+e2 e1 = temp return e2\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/740.-delete-and-earn.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473697,"modifiedTime":1737554783000,"sourceSize":1028,"sourcePath":"Algorithms/Leetcode/Array/DP/740. Delete and Earn.md","exportPath":"algorithms/leetcode/array/dp/740.-delete-and-earn.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html":{"title":"1143. Longest Common Subsequence","icon":"","description":"Given two strings&nbsp;text1&nbsp;and&nbsp;text2, return&nbsp;the length of their longest&nbsp;common subsequence.&nbsp;If there is no&nbsp;common subsequence, return&nbsp;0.A&nbsp;subsequence&nbsp;of a string is a new string generated from the original string with some characters (can be none) deleted without changing the relative order of the remaining characters.\nFor example,&nbsp;\"ace\"&nbsp;is a subsequence of&nbsp;\"abcde\".\nA&nbsp;common subsequence&nbsp;of two strings is a subsequence that is common to both strings.class Solution: def longestCommonSubsequence(self, text1: str, text2: str) -&gt; int: m,n = len(text1), len(text2) dp = [[0 for j in range(n+1)] for i in range(m+1)] for i in range(1,m+1): for j in range(1,n+1): if text1[i-1] == text2[j-1]: dp[i][j] = dp[i-1][j-1]+1 else: dp[i][j] = max (dp[i-1][j] , dp[i][j-1]) return dp[m][n]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473329,"modifiedTime":1737554783000,"sourceSize":974,"sourcePath":"Algorithms/Leetcode/Array/DP/1143. Longest Common Subsequence.md","exportPath":"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html":{"title":"1626. Best Team With No Conflicts","icon":"","description":"You are the manager of a basketball team. For the upcoming tournament, you want to choose the team with the highest overall score. The score of the team is the&nbsp;sum&nbsp;of scores of all the players in the team.However, the basketball team is not allowed to have&nbsp;conflicts. A&nbsp;conflict&nbsp;exists if a younger player has a&nbsp;strictly higher&nbsp;score than an older player. A conflict does&nbsp;not&nbsp;occur between players of the same age.Given two lists,&nbsp;scores&nbsp;and&nbsp;ages, where each&nbsp;scores[i]&nbsp;and&nbsp;ages[i]&nbsp;represents the score and age of the&nbsp;ith&nbsp;player, respectively, return&nbsp;the highest overall score of all possible basketball teams.class Solution: def bestTeamScore(self, scores: List[int], ages: List[int]) -&gt; int: pairs = [[scores[i],ages[i]] for i in range(len(scores))] pairs.sort() dp = [pairs[i][0] for i in range(len(pairs))] for i in range(len(pairs)): ms, ma = pairs[i] for j in range(i): s,a = pairs[j] if ma&gt;=a: dp[i] = max(dp[i],ms+dp[j]) return max(dp)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473372,"modifiedTime":1737554783000,"sourceSize":1177,"sourcePath":"Algorithms/Leetcode/Array/DP/1626. Best Team With No Conflicts.md","exportPath":"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/hash/454.-4sum-ii.html":{"title":"454. 4Sum II","icon":"","description":"Given four integer arrays&nbsp;nums1,&nbsp;nums2,&nbsp;nums3, and&nbsp;nums4&nbsp;all of length&nbsp;n, return the number of tuples&nbsp;(i, j, k, l)&nbsp;such that:\n0 &lt;= i, j, k, l &lt; n\nnums1[i] + nums2[j] + nums3[k] + nums4[l] == 0\nclass Solution: def fourSumCount(self, nums1: List[int], nums2: List[int], nums3: List[int], nums4: List[int]) -&gt; int: count = 0 mp = collections.defaultdict(int) for a in nums1: for b in nums2: mp[(a+b)]+=1 for c in nums3: for d in nums4: count+=mp[-(c+d)] return count\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/hash/454.-4sum-ii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473711,"modifiedTime":1737554783000,"sourceSize":640,"sourcePath":"Algorithms/Leetcode/Array/Hash/454. 4Sum II.md","exportPath":"algorithms/leetcode/array/hash/454.-4sum-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/interval/57.-insert-interval.html":{"title":"57. Insert Interval","icon":"","description":"You are given an array of non-overlapping intervals&nbsp;intervals&nbsp;where&nbsp;intervals[i] = [starti, endi]&nbsp;represent the start and the end of the&nbsp;ith&nbsp;interval and&nbsp;intervals&nbsp;is sorted in ascending order by&nbsp;starti. You are also given an interval&nbsp;newInterval = [start, end]&nbsp;that represents the start and end of another interval.Insert&nbsp;newInterval&nbsp;into&nbsp;intervals&nbsp;such that&nbsp;intervals&nbsp;is still sorted in ascending order by&nbsp;starti&nbsp;and&nbsp;intervals&nbsp;still does not have any overlapping intervals (merge overlapping intervals if necessary).Return&nbsp;intervals&nbsp;after the insertion.class Solution: def insert(self, intervals: List[List[int]], newInterval: List[int]) -&gt; List[List[int]]: res = [] for i in range(len(intervals)): if newInterval[1]&lt;intervals[i][0]: res.append(newInterval) return res+intervals[i:] elif newInterval[0] &gt; intervals[i][1]: res.append(intervals[i]) else: newInterval = [min (newInterval[0], intervals[i][0]), max (newInterval[1], intervals[i][1])] res.append(newInterval) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/interval/57.-insert-interval.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473719,"modifiedTime":1737554783000,"sourceSize":1237,"sourcePath":"Algorithms/Leetcode/Array/Interval/57. Insert Interval.md","exportPath":"algorithms/leetcode/array/interval/57.-insert-interval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html":{"title":"74. Search a 2D Matrix","icon":"","description":"You are given an&nbsp;m x n&nbsp;integer matrix&nbsp;matrix&nbsp;with the following two properties:\nEach row is sorted in non-decreasing order.\nThe first integer of each row is greater than the last integer of the previous row.\nGiven an integer&nbsp;target, return&nbsp;true&nbsp;if&nbsp;target&nbsp;is in&nbsp;matrix&nbsp;or&nbsp;false&nbsp;otherwise.You must write a solution in&nbsp;O(log(m * n))&nbsp;time complexity.class Solution: def searchMatrix(self, matrix: List[List[int]], target: int) -&gt; bool: lo, hi = 0, len(matrix) * len(matrix[0]) - 1 while lo &lt;= hi: mid = (lo + hi) // 2 r = mid // len(matrix[0]) c = mid % len(matrix[0]) if matrix[r][c] == target: return True if target &gt; matrix[r][c]: lo = mid + 1 else: hi = mid - 1 return False\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473778,"modifiedTime":1737554783000,"sourceSize":852,"sourcePath":"Algorithms/Leetcode/Array/Matrix/74. Search a 2D Matrix.md","exportPath":"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/matrix/79.-word-search.html":{"title":"79. Word Search","icon":"","description":"Given an&nbsp;m x n&nbsp;grid of characters&nbsp;board&nbsp;and a string&nbsp;word, return&nbsp;true&nbsp;if&nbsp;word&nbsp;exists in the grid.The word can be constructed from letters of sequentially adjacent cells, where adjacent cells are horizontally or vertically neighboring. The same letter cell may not be used more than once.class Solution: def exist(self, board: List[List[str]], word: str) -&gt; bool: def helper(i,j,word,start,board): if start&gt;=len(word): return True if i&lt;0 or i&gt;=len(board) or j&lt;0 or j&gt;=len(board[0]): return False if board[i][j]==word[start]: start+=1 c = board[i][j] board[i][j] = '#' res = helper(i-1,j,word,start,board) or helper(i+1,j,word,start,board) or helper(i,j-1,word,start,board) or helper(i,j+1,word,start,board) board[i][j]=c return res return False for i in range(len(board)): for j in range(len(board[0])): if helper(i,j,word,0,board): return True return False\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/matrix/79.-word-search.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473795,"modifiedTime":1737554783000,"sourceSize":1169,"sourcePath":"Algorithms/Leetcode/Array/Matrix/79. Word Search.md","exportPath":"algorithms/leetcode/array/matrix/79.-word-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html":{"title":"417. Pacific Atlantic Water Flow","icon":"","description":"There is an&nbsp;m x n&nbsp;rectangular island that borders both the&nbsp;Pacific Ocean&nbsp;and&nbsp;Atlantic Ocean. The&nbsp;Pacific Ocean&nbsp;touches the island's left and top edges, and the&nbsp;Atlantic Ocean&nbsp;touches the island's right and bottom edges.The island is partitioned into a grid of square cells. You are given an&nbsp;m x n&nbsp;integer matrix&nbsp;heights&nbsp;where&nbsp;heights[r][c]&nbsp;represents the&nbsp;height above sea level&nbsp;of the cell at coordinate&nbsp;(r, c).The island receives a lot of rain, and the rain water can flow to neighboring cells directly north, south, east, and west if the neighboring cell's height is&nbsp;less than or equal to&nbsp;the current cell's height. Water can flow from any cell adjacent to an ocean into the ocean.Return&nbsp;a&nbsp;2D list&nbsp;of grid coordinates&nbsp;result&nbsp;where&nbsp;result[i] = [ri, ci]&nbsp;denotes that rain water can flow from cell&nbsp;(ri, ci)&nbsp;to&nbsp;both&nbsp;the Pacific and Atlantic oceans.class Solution: def pacificAtlantic(self, heights: List[List[int]]) -&gt; List[List[int]]: M, N = len(heights), len(heights[0]) visitedP=set() visitedA=set() def dfs(i, j, preHeight, visited): if 0&lt;=i&lt;M and 0&lt;=j&lt;N and heights[i][j]&gt;=preHeight and (i,j) not in visited: visited.add((i,j)) dfs(i-1, j, heights[i][j], visited) dfs(i+1, j, heights[i][j], visited) dfs(i, j-1, heights[i][j], visited) dfs(i, j+1, heights[i][j], visited) for j in range(N): dfs(0, j, heights[0][j], visitedP) dfs(M-1, j, heights[M-1][j], visitedA) for i in range(M): dfs(i, 0, heights[i][0], visitedP) dfs(i, N-1, heights[i][N-1], visitedA) return list(visitedP &amp; visitedA) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473743,"modifiedTime":1737554783000,"sourceSize":1872,"sourcePath":"Algorithms/Leetcode/Array/Matrix/417. Pacific Atlantic Water Flow.md","exportPath":"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/matrix/695.-max-area-of-island.html":{"title":"695. Max Area of Island","icon":"","description":"You are given an&nbsp;m x n&nbsp;binary matrix&nbsp;grid. An island is a group of&nbsp;1's (representing land) connected&nbsp;4-directionally&nbsp;(horizontal or vertical.) You may assume all four edges of the grid are surrounded by water.The&nbsp;area&nbsp;of an island is the number of cells with a value&nbsp;1&nbsp;in the island.Return&nbsp;the maximum&nbsp;area&nbsp;of an island in&nbsp;grid. If there is no island, return&nbsp;0.class Solution: def maxAreaOfIsland(self, grid: List[List[int]]) -&gt; int: def dfs(x,y): if x &lt; 0 or y &lt; 0 or x &gt;= m or y &gt;= n: return 0 if not grid[x][y]: return 0 if grid[x][y]: grid[x][y] = 0 return 1+dfs(x-1,y)+dfs(x+1,y)+dfs(x,y-1)+dfs(x,y+1) m = len(grid) n = len(grid[0]) area = 0 for i in range(m): for j in range(n): if grid[i][j]: area = max(dfs(i,j), area) return area\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/matrix/695.-max-area-of-island.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473748,"modifiedTime":1737554783000,"sourceSize":1033,"sourcePath":"Algorithms/Leetcode/Array/Matrix/695. Max Area of Island.md","exportPath":"algorithms/leetcode/array/matrix/695.-max-area-of-island.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html":{"title":"909. Snakes and Ladders","icon":"","description":"You are given an&nbsp;n x n&nbsp;integer matrix&nbsp;board&nbsp;where the cells are labeled from&nbsp;1&nbsp;to&nbsp;n2&nbsp;in a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Boustrophedon\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Boustrophedon\" target=\"_self\"><strong></strong></a>Boustrophedon style&nbsp;starting from the bottom left of the board (i.e.&nbsp;board[n - 1][0]) and alternating direction each row.You start on square&nbsp;1&nbsp;of the board. In each move, starting from square&nbsp;curr, do the following:\nChoose a destination square&nbsp;next&nbsp;with a label in the range&nbsp;[curr + 1, min(curr + 6, n2)]. This choice simulates the result of a standard&nbsp;6-sided die roll: i.e., there are always at most 6 destinations, regardless of the size of the board. If&nbsp;next&nbsp;has a snake or ladder, you&nbsp;must&nbsp;move to the destination of that snake or ladder. Otherwise, you move to&nbsp;next.\nThe game ends when you reach the square&nbsp;n2.\nA board square on row&nbsp;r&nbsp;and column&nbsp;c&nbsp;has a snake or ladder if&nbsp;board[r][c] != -1. The destination of that snake or ladder is&nbsp;board[r][c]. Squares&nbsp;1&nbsp;and&nbsp;n2&nbsp;do not have a snake or ladder.Note that you only take a snake or ladder at most once per move. If the destination to a snake or ladder is the start of another snake or ladder, you do&nbsp;not&nbsp;follow the subsequent&nbsp;snake or ladder.\nFor example, suppose the board is&nbsp;[[-1,4],[-1,3]], and on the first move, your destination square is&nbsp;2. You follow the ladder to square&nbsp;3, but do&nbsp;not&nbsp;follow the subsequent ladder to&nbsp;4.\nReturn&nbsp;the least number of moves required to reach the square&nbsp;n2. If it is not possible to reach the square, return&nbsp;-1.class Solution: def snakesAndLadders(self, board: List[List[int]]) -&gt; int: m,q,visit = len(board),deque(),set() board.reverse() def intToPos(square): r = (square-1)//m c = (square-1)%m if r%2: c = m-1-c return [r,c] q.append([1,0]) #[square, moves] while q: square,moves = q.popleft() for i in range(1,7): nextSquare = square+i r,c = intToPos(nextSquare) if board[r][c] != -1: nextSquare= board[r][c] if nextSquare == m*m: return moves+1 if nextSquare not in visit: visit.add(nextSquare) q.append([nextSquare,moves+1]) return -1\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473805,"modifiedTime":1737554783000,"sourceSize":2497,"sourcePath":"Algorithms/Leetcode/Array/Matrix/909. Snakes and Ladders.md","exportPath":"algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/matrix/934.-shortest-bridge.html":{"title":"934. Shortest Bridge","icon":"","description":"You are given an&nbsp;n x n&nbsp;binary matrix&nbsp;grid&nbsp;where&nbsp;1&nbsp;represents land and&nbsp;0&nbsp;represents water.An&nbsp;island&nbsp;is a 4-directionally connected group of&nbsp;1's not connected to any other&nbsp;1's. There are&nbsp;exactly two islands&nbsp;in&nbsp;grid.You may change&nbsp;0's to&nbsp;1's to connect the two islands to form&nbsp;one island.Return&nbsp;the smallest number of&nbsp;0's you must flip to connect the two islands.class Solution: def shortestBridge(self, grid: List[List[int]]) -&gt; int: def dfs(r, c) : if grid[r][c] == 1 : q.append((r, c, 0)) grid[r][c] = '$' if r-1 &gt;= 0 : dfs(r-1, c) if r+1 &lt; len(grid) : dfs(r+1, c) if c-1 &gt;= 0 : dfs(r, c-1) if c+1 &lt; len(grid) : dfs(r, c+1) def bfs(q) : seen = set() while q : i, j, d = q.popleft() if grid[i][j] == 1 : return d else : for x, y in ((i-1, j), (i, j-1), (i+1, j), (i, j+1)) : if 0 &lt;= x &lt; len(grid) and 0 &lt;= y &lt; len(grid[0]) : if (x, y) not in seen : seen.add((x, y)) q.append((x, y, d+1)) q = deque() flag = False for i in range(len(grid)) : for j in range(len(grid[0])) : if grid[i][j] == 1 : dfs(i, j) flag = True break if flag : break return bfs(q) - 1 ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/matrix/934.-shortest-bridge.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473893,"modifiedTime":1737554783000,"sourceSize":1678,"sourcePath":"Algorithms/Leetcode/Array/Matrix/934. Shortest Bridge.md","exportPath":"algorithms/leetcode/array/matrix/934.-shortest-bridge.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/permutation/46.-permutations.html":{"title":"46. Permutations","icon":"","description":"Given an array&nbsp;nums&nbsp;of distinct integers, return&nbsp;all the possible permutations. You can return the answer in&nbsp;any order.class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: res = [] def bt(temp): if len(temp) == len(nums): res.append(temp[:]) return for x in nums: if x not in temp: temp.append(x) bt(temp) temp.remove(x) bt([]) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/permutation/46.-permutations.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473950,"modifiedTime":1737554783000,"sourceSize":582,"sourcePath":"Algorithms/Leetcode/Array/Permutation/46. Permutations.md","exportPath":"algorithms/leetcode/array/permutation/46.-permutations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/permutation/77.-combinations.html":{"title":"77. Combinations","icon":"","description":"Given two integers&nbsp;n&nbsp;and&nbsp;k, return&nbsp;all possible combinations of&nbsp;k&nbsp;numbers chosen from the range&nbsp;[1, n].You may return the answer in&nbsp;any order.class Solution: def combine(self, n: int, k: int) -&gt; List[List[int]]: res = [] def BT(temp, start): if len(temp)==k: res.append(temp[:]) return for x in range(start+1,n+1): temp.append(x) BT(temp,x) temp.pop() BT([],0) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/permutation/77.-combinations.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473955,"modifiedTime":1737554783000,"sourceSize":590,"sourcePath":"Algorithms/Leetcode/Array/Permutation/77. Combinations.md","exportPath":"algorithms/leetcode/array/permutation/77.-combinations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/sort/912.-sort-an-array.html":{"title":"912. Sort an Array","icon":"","description":"Given an array of integers&nbsp;nums, sort the array in ascending order and return it.You must solve the problem&nbsp;without using any built-in&nbsp;functions in&nbsp;O(nlog(n))&nbsp;time complexity and with the smallest space complexity possible.class Solution: def sortArray(self, nums: List[int]) -&gt; List[int]: # merge sort + two pointers # Divide Merge # 5 2 3 1 1 2 3 5 # / \\ -&gt; Three pointers (one pointer in [2,5] one pointer in [1,3], one pointer in [1,2,3,5] # 5 2 3 1 | | # /\\ /\\ 2 5 1 3 # 5 2 3 1 def merge(arr,l,m,r): left, right = arr[l:m+1], arr[m+1:r+1] i,j,k = l,0,0 while j&lt;len(left) and k &lt; len(right): if left[j] &lt;= right[k]: arr[i] = left[j] j+=1 else: arr[i] = right[k] k+=1 i+=1 while j &lt; len(left): nums[i] = left[j] i+=1 j+=1 while k&lt;len(right): nums[i] = right[k] i+=1 k+=1 def mergeSort(arr,l,r): if l==r: return arr m = (l+r)//2 # divide until base case mergeSort(arr,l,m) mergeSort(arr,m+1,r) # merge subarrays merge(arr,l,m,r) return arr return mergeSort(nums,0,len(nums)-1) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/sort/912.-sort-an-array.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084473968,"modifiedTime":1737554783000,"sourceSize":1855,"sourcePath":"Algorithms/Leetcode/Array/Sort/912. Sort an Array.md","exportPath":"algorithms/leetcode/array/sort/912.-sort-an-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/subarray/53.-maximum-subarray.html":{"title":"53. Maximum Subarray","icon":"","description":"Given an integer array&nbsp;nums, find the&nbsp;subarray&nbsp;with the largest sum, and return&nbsp;its sum.class Solution { public int maxSubArray(int[] nums) { int maxSumSoFar = nums[0] , curMaxSum = nums[0]; for (int i = 1; i &lt; nums.length; i++) { curMaxSum=Math.max(nums[i],curMaxSum+nums[i]); maxSumSoFar=Math.max(curMaxSum,maxSumSoFar); } return maxSumSoFar; }\n}\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/subarray/53.-maximum-subarray.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474104,"modifiedTime":1737554783000,"sourceSize":437,"sourcePath":"Algorithms/Leetcode/Array/Subarray/53. Maximum Subarray.md","exportPath":"algorithms/leetcode/array/subarray/53.-maximum-subarray.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html":{"title":"491. Non-decreasing Subsequences","icon":"","description":"Given an integer array&nbsp;nums, return&nbsp;all the different possible non-decreasing subsequences of the given array with at least two elements. You may return the answer in&nbsp;any order.class Solution: def findSubsequences(self, nums: List[int]) -&gt; List[List[int]]: res = set() def BT(i,subsequence): nonlocal res if len(subsequence)&gt;1: res.add(tuple(subsequence)) if i==len(nums): return if not subsequence or nums[i] &gt;= subsequence[-1]: BT(i+1, subsequence+[nums[i]]) BT(i+1, subsequence) BT(0,[]) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474038,"modifiedTime":1737554783000,"sourceSize":697,"sourcePath":"Algorithms/Leetcode/Array/Subarray/491. Non-decreasing Subsequences.md","exportPath":"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html":{"title":"918. Maximum Sum Circular Subarray","icon":"","description":"Given a&nbsp;circular integer array&nbsp;nums&nbsp;of length&nbsp;n, return&nbsp;the maximum possible sum of a non-empty&nbsp;subarray&nbsp;of&nbsp;nums.A&nbsp;circular array&nbsp;means the end of the array connects to the beginning of the array. Formally, the next element of&nbsp;nums[i]&nbsp;is&nbsp;nums[(i + 1) % n]&nbsp;and the previous element of&nbsp;nums[i]&nbsp;is&nbsp;nums[(i - 1 + n) % n].A&nbsp;subarray&nbsp;may only include each element of the fixed buffer&nbsp;nums&nbsp;at most once. Formally, for a subarray&nbsp;nums[i], nums[i + 1], ..., nums[j], there does not exist&nbsp;i &lt;= k1,&nbsp;k2 &lt;= j&nbsp;with&nbsp;k1 % n == k2 % n.class Solution: def maxSubarraySumCircular(self, nums: List[int]) -&gt; int: globmax, globmin = nums[0],nums[0] curmax,curmin = 0,0 total = 0 for n in nums: curmax = max(curmax+n,n) curmin = min(curmin+n,n) total +=n globmax = max(globmax,curmax) globmin = min(globmin,curmin) return max(globmax,total-globmin) if globmax&gt;0 else globmax\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474121,"modifiedTime":1737554783000,"sourceSize":1073,"sourcePath":"Algorithms/Leetcode/Array/Subarray/918. Maximum Sum Circular Subarray.md","exportPath":"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html":{"title":"974. Subarray Sums Divisible by K","icon":"","description":"Given an integer array&nbsp;nums&nbsp;and an integer&nbsp;k, return&nbsp;the number of non-empty&nbsp;subarrays&nbsp;that have a sum divisible by&nbsp;k.A&nbsp;subarray&nbsp;is a&nbsp;contiguous&nbsp;part of an array.class Solution: def subarraysDivByK(self, nums: List[int], k: int) -&gt; int: # frequency table to store the frequency of the remainder remainderFrq = defaultdict(int) # Empty sub array will have a sum of 0 and remainder of 0, thus the frequency of 0 is 1 before we go into the array remainderFrq[0] = 1 res = prefixSum = 0 for n in nums: prefixSum += n remainder = prefixSum % k res += remainderFrq[remainder] remainderFrq[remainder] += 1 return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474146,"modifiedTime":1737554783000,"sourceSize":779,"sourcePath":"Algorithms/Leetcode/Array/Subarray/974. Subarray Sums Divisible by K.md","exportPath":"algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html":{"title":"11. Container With Most Water","icon":"","description":"You are given an integer array&nbsp;height&nbsp;of length&nbsp;n. There are&nbsp;n&nbsp;vertical lines drawn such that the two endpoints of the&nbsp;ith&nbsp;line are&nbsp;(i, 0)&nbsp;and&nbsp;(i, height[i]).Find two lines that together with the x-axis form a container, such that the container contains the most water.Return&nbsp;the maximum amount of water a container can store.Example1:\nInput: height = [1,8,6,2,5,4,8,3,7]\nOutput: 49Example2:\nInput: height = [1,1]\nOutput: 1This is an interesting question as obviously you can simply use brute force to sovle this problems.class Solution: def maxArea(self, height: List[int]) -&gt; int: res = 0 n = len(height) for i in range(n): temp = 0 for j in range(i+1,n): t = (j-i)*min(height[i],height[j]) res = max(res,t) return res\nComplexity Analysis:\nTime complexity:&nbsp;O(n^2). Space complexity:&nbsp;O(1). Constant extra space is used.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474238,"modifiedTime":1737554783000,"sourceSize":2247,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/11. Container With Most Water.md","exportPath":"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/15.-3sum.html":{"title":"15. 3Sum","icon":"","description":"Given an integer array nums, return all the triplets&nbsp;[nums[i], nums[j], nums[k]]&nbsp;such that&nbsp;i != j,&nbsp;i != k, and&nbsp;j != k, and&nbsp;nums[i] + nums[j] + nums[k] == 0.Notice that the solution set must not contain duplicate triplets.class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: def twoSum(i,nums,res): l,r = i+1, len(nums)-1 # left,right pointers while l&lt;r: summ = nums[i]+nums[l]+nums[r] if summ&lt;0: l+=1 elif summ&gt;0: r-=1 else: res.append([nums[i],nums[l],nums[r]]) l+=1 r-=1 while l&lt;r and nums[l]==nums[l-1]: l+=1 res=[] nums.sort() for i in range(len(nums)): if nums[i]&gt;0: break if i==0 or nums[i]!=nums[i-1]: # avoid repeat twoSum(i,nums,res) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/15.-3sum.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474299,"modifiedTime":1737554783000,"sourceSize":1214,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/15. 3Sum.md","exportPath":"algorithms/leetcode/array/two-pointers/15.-3sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html":{"title":"16. 3Sum Closest","icon":"","description":"Given an integer array&nbsp;nums&nbsp;of length&nbsp;n&nbsp;and an integer&nbsp;target, find three integers in&nbsp;nums&nbsp;such that the sum is closest to&nbsp;target.Return&nbsp;the sum of the three integers.You may assume that each input would have exactly one solution.class Solution: def threeSumClosest(self, nums: List[int], target: int) -&gt; int: diff = float(\"inf\") nums.sort() for i in range(len(nums)): l,r = i+1,len(nums)-1 while l&lt;r: sum = nums[i] + nums[l] + nums[r] if abs(target - sum) &lt; abs(diff): diff = target - sum if sum &lt; target: l += 1 else: r -= 1 if diff == 0: break return target-diff\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474306,"modifiedTime":1737554783000,"sourceSize":848,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/16. 3Sum Closest.md","exportPath":"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/18.-4sum.html":{"title":"18. 4Sum","icon":"","description":"Given an array&nbsp;nums&nbsp;of&nbsp;n&nbsp;integers, return&nbsp;an array of all the&nbsp;unique&nbsp;quadruplets&nbsp;[nums[a], nums[b], nums[c], nums[d]]&nbsp;such that:\n0 &lt;= a, b, c, d&nbsp;&lt; n\na,&nbsp;b,&nbsp;c, and&nbsp;d&nbsp;are&nbsp;distinct.\nnums[a] + nums[b] + nums[c] + nums[d] == target\nYou may return the answer in&nbsp;any order.class Solution: def fourSum(self, nums: List[int], target: int) -&gt; List[List[int]]: res,n = [],len(nums) if n&lt;4: return res nums.sort() for i in range(n): if nums[i]*4 &gt; target: break if nums[i]+nums[-1]*3&lt;target: continue for j in range(i+1,n): if nums[i]+nums[j]*3 &gt; target: break if nums[i]+ nums[j]+nums[-1]*2&lt;target: continue if i&gt;0 and nums[i]==nums[i-1]: continue if j&gt;i+1 and nums[j]==nums[j-1]: continue l,r = j+1,n-1 while l&lt;r: summ = nums[i]+nums[j]+nums[l]+nums[r] if summ&gt;target: r-=1 elif summ&lt;target: l+=1 else: res.append([nums[i],nums[j],nums[l],nums[r]]) l+=1 r-=1 while l&lt;r and nums[l]==nums[l-1]: l+=1 while l&lt;r and nums[r]==nums[r+1]: r-=1 return res ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/18.-4sum.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474335,"modifiedTime":1737554783000,"sourceSize":1582,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/18. 4Sum.md","exportPath":"algorithms/leetcode/array/two-pointers/18.-4sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html":{"title":"26. Remove Duplicates from Sorted Array","icon":"","description":"Given an integer array&nbsp;nums&nbsp;sorted in&nbsp;non-decreasing order, remove the duplicates&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/In-place_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/In-place_algorithm\" target=\"_self\"><strong></strong></a>in-place&nbsp;such that each unique element appears only&nbsp;once. The&nbsp;relative order&nbsp;of the elements should be kept the&nbsp;same.Since it is impossible to change the length of the array in some languages, you must instead have the result be placed in the&nbsp;first part&nbsp;of the array&nbsp;nums. More formally, if there are&nbsp;k&nbsp;elements after removing the duplicates, then the first&nbsp;k&nbsp;elements of&nbsp;nums&nbsp;should hold the final result. It does not matter what you leave beyond the first&nbsp;k&nbsp;elements.Return&nbsp;k&nbsp;after placing the final result in the first&nbsp;k&nbsp;slots of&nbsp;nums.<br>Do&nbsp;not&nbsp;allocate extra space for another array. You must do this by&nbsp;modifying the input array&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/In-place_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/In-place_algorithm\" target=\"_self\">in-place</a>&nbsp;with O(1) extra memory.Example 1:\nInput: nums = [1,1,2]\nOutput: 2, nums = [1,2,_]Example 2:\nInput: nums = [0,0,1,1,1,2,2,3,3,4]\nOutput: 5, nums = [0,1,2,3,4,,,,,_]Combine two-pointers with some swap conditions:class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: i,j=0,1 while j&lt;len(nums): if nums[i]==nums[j]: j+=1 else: i+=1 temp = nums[i] nums[i] = nums[j] nums[j] = temp j+=1 return i+1\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474532,"modifiedTime":1737554783000,"sourceSize":1535,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/26. Remove Duplicates from Sorted Array.md","exportPath":"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/27.-remove-element.html":{"title":"27. Remove Element","icon":"","description":"Given an integer array&nbsp;nums&nbsp;and an integer&nbsp;val, remove all occurrences of&nbsp;val&nbsp;in&nbsp;nums&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/In-place_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/In-place_algorithm\" target=\"_self\"><strong></strong></a>in-place. The relative order of the elements may be changed.Since it is impossible to change the length of the array in some languages, you must instead have the result be placed in the&nbsp;first part&nbsp;of the array&nbsp;nums. More formally, if there are&nbsp;k&nbsp;elements after removing the duplicates, then the first&nbsp;k&nbsp;elements of&nbsp;nums&nbsp;should hold the final result. It does not matter what you leave beyond the first&nbsp;k&nbsp;elements.Return&nbsp;k&nbsp;after placing the final result in the first&nbsp;k&nbsp;slots of&nbsp;nums.<br>Do&nbsp;not&nbsp;allocate extra space for another array. You must do this by&nbsp;modifying the input array&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/In-place_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/In-place_algorithm\" target=\"_self\">in-place</a>&nbsp;with O(1) extra memory.Example1:\nInput: nums = [3,2,2,3], val = 3\nOutput: 2, nums = [2,2,,]Example2:\nInput: nums = [0,1,2,2,3,0,4,2], val = 2\nOutput: 5, nums = [0,1,4,0,3,,,_]class Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: i = 0 for j in range(len(nums)): if nums[j] != val: nums[i] = nums[j] i+=1 return i\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/27.-remove-element.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474570,"modifiedTime":1737554783000,"sourceSize":1308,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/27. Remove Element.md","exportPath":"algorithms/leetcode/array/two-pointers/27.-remove-element.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html":{"title":"69. Sqrt(x)","icon":"","description":"Given a non-negative integer&nbsp;x, return&nbsp;the square root of&nbsp;x&nbsp;rounded down to the nearest integer. The returned integer should be&nbsp;non-negative&nbsp;as well.You&nbsp;must not use&nbsp;any built-in exponent function or operator.\nFor example, do not use&nbsp;pow(x, 0.5)&nbsp;in c++ or&nbsp;x ** 0.5&nbsp;in python. class Solution {\npublic: int mySqrt(int x) { if (x==0) return x; int l = 1, r = x, mid, sqrt; while (l&lt;=r){ mid = l+(r-l)/2; sqrt = x/mid; if (sqrt == mid){ return sqrt; } else if (mid &gt; sqrt){ r = mid-1; } else l = mid+1; } return r; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474645,"modifiedTime":1737554783000,"sourceSize":767,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/69. Sqrt(x).md","exportPath":"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html":{"title":"81. Search in Rotated Sorted Array II","icon":"","description":"There is an integer array&nbsp;nums&nbsp;sorted in non-decreasing order (not necessarily with&nbsp;distinct&nbsp;values).Before being passed to your function,&nbsp;nums&nbsp;is&nbsp;rotated&nbsp;at an unknown pivot index&nbsp;k&nbsp;(0 &lt;= k &lt; nums.length) such that the resulting array is&nbsp;[nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]&nbsp;(0-indexed). For example,&nbsp;[0,1,2,4,4,4,5,6,6,7]&nbsp;might be rotated at pivot index&nbsp;5&nbsp;and become&nbsp;[4,5,6,6,7,0,1,2,4,4].Given the array&nbsp;nums&nbsp;after&nbsp;the rotation and an integer&nbsp;target, return&nbsp;true&nbsp;if&nbsp;target&nbsp;is in&nbsp;nums, or&nbsp;false&nbsp;if it is not in&nbsp;nums.You must decrease the overall operation steps as much as possible.public: bool search(vector&lt;int&gt;&amp; nums, int target) { int start = 0, end = nums.size()-1; while (start&lt;=end){ int mid = (start+end)/2; if (nums[mid]==target) return true; if (nums[start] == nums[mid]) start++; else if (nums[mid]&lt;=nums[end]){ // right side ascending if (target&gt;nums[mid] &amp;&amp; target &lt;= nums[end]){ start = mid+1; }else{ end = mid-1; } }else{ if (target&gt;=nums[start] &amp;&amp; target &lt; nums[mid]){ end = mid-1; }else{ start = mid+1; } } } return false; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474692,"modifiedTime":1737554783000,"sourceSize":1572,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/81. Search in Rotated Sorted Array II.md","exportPath":"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html":{"title":"88. Merge Sorted Array","icon":"","description":"You are given two integer arrays&nbsp;nums1&nbsp;and&nbsp;nums2, sorted in&nbsp;non-decreasing order, and two integers&nbsp;m&nbsp;and&nbsp;n, representing the number of elements in&nbsp;nums1&nbsp;and&nbsp;nums2&nbsp;respectively.Merge&nbsp;nums1&nbsp;and&nbsp;nums2&nbsp;into a single array sorted in&nbsp;non-decreasing order.The final sorted array should not be returned by the function, but instead be&nbsp;stored inside the array&nbsp;nums1. To accommodate this,&nbsp;nums1&nbsp;has a length of&nbsp;m + n, where the first&nbsp;m&nbsp;elements denote the elements that should be merged, and the last&nbsp;n&nbsp;elements are set to&nbsp;0&nbsp;and should be ignored.&nbsp;nums2&nbsp;has a length of&nbsp;n.class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None: \"\"\" Do not return anything, modify nums1 in-place instead. \"\"\" i = m-1 j = n-1 index1 = m+n-1 while i &gt;= 0 and j &gt;= 0: if nums1[i] &gt; nums2[j]: nums1[index1] = nums1[i] i -= 1 index1 -= 1 elif nums2[j] &gt; nums1[i]: nums1[index1] = nums2[j] j -= 1 index1 -= 1 else: nums1[index1] = nums1[i] nums1[index1-1] = nums2[j] i -= 1 j -= 1 index1 -= 2 while j &gt;= 0: nums1[index1] = nums2[j] j -= 1 index1 -= 1 ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474717,"modifiedTime":1737554783000,"sourceSize":1525,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/88. Merge Sorted Array.md","exportPath":"algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html":{"title":"142. Linked List Cycle II","icon":"","description":"Given the&nbsp;head&nbsp;of a linked list, return&nbsp;the node where the cycle begins. If there is no cycle, return&nbsp;null.There is a cycle in a linked list if there is some node in the list that can be reached again by continuously following the&nbsp;next&nbsp;pointer. Internally,&nbsp;pos&nbsp;is used to denote the index of the node that tail's&nbsp;next&nbsp;pointer is connected to (0-indexed). It is&nbsp;-1&nbsp;if there is no cycle.&nbsp;Note that&nbsp;pos&nbsp;is not passed as a parameter.Do not modify&nbsp;the linked list.class Solution {\npublic: ListNode *detectCycle(ListNode *head) { ListNode *slow = head, *fast = head; do{ if (!fast || !fast-&gt;next) return nullptr; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; }while(fast!=slow); // when first time meets, put fast at head fast = head; while (fast!=slow){ // when second time meets, meeting point is the start of the loop slow = slow-&gt;next; fast = fast-&gt;next; } return fast; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474272,"modifiedTime":1737554783000,"sourceSize":1102,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/142. Linked List Cycle II.md","exportPath":"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html":{"title":"209. Minimum Size Subarray Sum","icon":"","description":"Given an array of positive integers&nbsp;nums&nbsp;and a positive integer&nbsp;target, return&nbsp;the&nbsp;minimal length&nbsp;of a&nbsp;subarray&nbsp;whose sum is greater than or equal to&nbsp;target. If there is no such subarray, return&nbsp;0&nbsp;instead.class Solution: def minSubArrayLen(self, target: int, nums: List[int]) -&gt; int: left = total = 0 res = len(nums) + 1 for i in range(len(nums)): total = total + nums[i] while total &gt;= target: res = min(res,i-left+1) total = total - nums[left] left = left+1 return res if res &lt;= len(nums) else 0\nclass Solution {\npublic: int minSubArrayLen(int target, vector&lt;int&gt;&amp; nums) { // using sliding window, move right pointer if the sum of subarray is smaller than target, move left pointer if the sum is larger than target, update the min length evey time deducing the left value int l=0,r=0, n=nums.size(),summ=0, res = INT_MAX; for(r = 0; r &lt; n; r++) { summ += nums[r]; while (summ &gt;= target) { res = min(res, r - l + 1); summ -= nums[l]; l++; } } return res == INT_MAX ? 0 : res; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474383,"modifiedTime":1737554783000,"sourceSize":1300,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/209. Minimum Size Subarray Sum.md","exportPath":"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html":{"title":"253. Meeting Rooms II","icon":"","description":"Given an array of meeting time intervals&nbsp;intervals&nbsp;where&nbsp;intervals[i] = [starti, endi], return&nbsp;the minimum number of conference rooms required.class Solution: def minMeetingRooms(self, intervals: List[List[int]]) -&gt; int: n = len(intervals) start = [intervals[i][0] for i in range(n)] end = [intervals[i][1] for i in range(n)] start.sort(); end.sort() i,j,count,res = 0,0,0,0 while i&lt;n and j&lt;n: if start[i]&lt;end[j]: count+=1 i+=1 res = max(count,res) else: count-=1 j+=1 return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474458,"modifiedTime":1737554783000,"sourceSize":732,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/253. Meeting Rooms II.md","exportPath":"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html":{"title":"259. 3Sum Smaller","icon":"","description":"Given an array of&nbsp;n&nbsp;integers&nbsp;nums&nbsp;and an integer&nbsp;target, find the number of index triplets&nbsp;i,&nbsp;j,&nbsp;k&nbsp;with&nbsp;0 &lt;= i &lt; j &lt; k &lt; n&nbsp;that satisfy the condition&nbsp;nums[i] + nums[j] + nums[k] &lt; target.class Solution: def threeSumSmaller(self, nums: List[int], target: int) -&gt; int: res,n = 0,len(nums) nums.sort() for i in range(n): l,r = i+1,n-1 while l&lt;r: summ = nums[i]+nums[l]+nums[r] if summ&lt;target: res += r-l l += 1 else: r -= 1 return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474525,"modifiedTime":1737554783000,"sourceSize":678,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/259. 3Sum Smaller.md","exportPath":"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html":{"title":"360. Sort Transformed Array","icon":"","description":"Given a&nbsp;sorted&nbsp;integer array&nbsp;nums&nbsp;and three integers&nbsp;a,&nbsp;b&nbsp;and&nbsp;c, apply a quadratic function of the form&nbsp;f(x) = ax2&nbsp;+ bx + c&nbsp;to each element&nbsp;nums[i]&nbsp;in the array, and return&nbsp;the array in a sorted order.class Solution: def sortTransformedArray(self, nums: List[int], a: int, b: int, c: int) -&gt; List[int]: res = [] if a==0: res = [b*x+c for x in nums] if b&lt;0: res.reverse() else: flag = a&gt;0 mid = -b/(2*a) l,r = 0, len(nums)-1 while l&lt;=r: if abs(mid-nums[l])&gt;abs(mid-nums[r]): res.append(a*nums[l]*nums[l]+b*nums[l]+c) l+=1 else: res.append(a*nums[r]*nums[r]+b*nums[r]+c) r-=1 if flag: res.reverse() return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474586,"modifiedTime":1737554783000,"sourceSize":934,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/360. Sort Transformed Array.md","exportPath":"algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html":{"title":"392. Is Subsequence","icon":"","description":"Given two strings&nbsp;s&nbsp;and&nbsp;t, return&nbsp;true&nbsp;if&nbsp;s&nbsp;is a&nbsp;subsequence&nbsp;of&nbsp;t, or&nbsp;false&nbsp;otherwise.A&nbsp;subsequence&nbsp;of a string is a new string that is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e.,&nbsp;\"ace\"&nbsp;is a subsequence of&nbsp;\"abcde\"&nbsp;while&nbsp;\"aec\"&nbsp;is not).class Solution: def isSubsequence(self, s: str, t: str) -&gt; bool: LEFT_BOUND, RIGHT_BOUND = len(s), len(t) p_left = p_right = 0 while p_left &lt; LEFT_BOUND and p_right &lt; RIGHT_BOUND: # move both pointers or just the right pointer if s[p_left] == t[p_right]: p_left += 1 p_right += 1 return p_left == LEFT_BOUND\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474618,"modifiedTime":1737554783000,"sourceSize":819,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/392. Is Subsequence.md","exportPath":"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html":{"title":"633. Sum of Square Numbers","icon":"","description":"Given a non-negative integer&nbsp;c, decide whether there're two integers&nbsp;a&nbsp;and&nbsp;b&nbsp;such that&nbsp;a2&nbsp;+ b2&nbsp;= c.class Solution: def judgeSquareSum(self, c: int) -&gt; bool: rootC = int(c ** 0.5) low, high = 0, rootC while low &lt;= high: result = low ** 2 + high ** 2 if result == c: return True elif result &gt; c: high -= 1 else: low += 1 return False ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474641,"modifiedTime":1737554783000,"sourceSize":548,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/633. Sum of Square Numbers.md","exportPath":"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html":{"title":"713. Subarray Product Less Than K","icon":"","description":"Given an array of integers&nbsp;nums&nbsp;and an integer&nbsp;k, return&nbsp;the number of contiguous subarrays where the product of all the elements in the subarray is strictly less than&nbsp;k.class Solution {\npublic: int numSubarrayProductLessThanK(vector&lt;int&gt;&amp; nums, int k) { if (k&lt;=1) return 0; int j=0; int prod = 1; int count = 0; for (int i=0; i&lt;nums.size(); i++) { if (j&lt;i) { j = i; prod = 1; } while (j&lt;nums.size() &amp;&amp; prod*nums[j]&lt;k) { prod = prod*nums[j]; j++; } count += j-i; prod = prod/nums[i]; } return count; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474676,"modifiedTime":1737554783000,"sourceSize":792,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/713. Subarray Product Less Than K.md","exportPath":"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html":{"title":"1004. Max Consecutive Ones III","icon":"","description":"Given a binary array&nbsp;nums&nbsp;and an integer&nbsp;k, return&nbsp;the maximum number of consecutive&nbsp;1's in the array if you can flip at most&nbsp;k&nbsp;0's.class Solution: def longestOnes(self, nums: List[int], k: int) -&gt; int: n,j,res,count = len(nums),0,0,0 for i in range(n): while j&lt;n and (nums[j]==1 or (nums[j]==0 and count&lt;k)): if nums[j]==0: count+=1 j+=1 res = max(res,j-i) if nums[i]==0: count-=1 return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474203,"modifiedTime":1737554783000,"sourceSize":582,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/1004. Max Consecutive Ones III.md","exportPath":"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html":{"title":"1493. Longest Subarray of 1's After Deleting One Element","icon":"","description":"Given a binary array&nbsp;nums, you should delete one element from it.Return&nbsp;the size of the longest non-empty subarray containing only&nbsp;1's in the resulting array. Return&nbsp;0&nbsp;if there is no such subarray.class Solution {\npublic: int longestSubarray(vector&lt;int&gt;&amp; nums) { // SlideWindow // Fast pointer will go until it reach the max zero limit // Then, the slow pointer will move towards faster pointer and calculate max length untill the max zero limit is removed by reducing it whenver the slow pointer is 0 and going to move right. int i = 0, res = 0, n = nums.size(), zeros = 0; for (int j=0; j&lt;n;j++){ if (nums[j]==0){ zeros ++; } while (zeros&gt;1){ if (nums[i]==0){ zeros--; } i++; } res = max(res, j - i); } return res; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084474283,"modifiedTime":1737554783000,"sourceSize":1007,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/1493. Longest Subarray of 1's After Deleting One Element.md","exportPath":"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/bit/190.-reverse-bits.html":{"title":"190. Reverse Bits","icon":"","description":"Reverse bits of a given 32 bits unsigned integer.Note:\nNote that in some languages, such as Java, there is no unsigned integer type. In this case, both input and output will be given as a signed integer type. They should not affect your implementation, as the integer's internal binary representation is the same, whether it is signed or unsigned.\nIn Java, the compiler represents the signed integers using&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Two%27s_complement\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Two%27s_complement\" target=\"_self\">2's complement notation</a>. Therefore, in&nbsp;Example 2&nbsp;above, the input represents the signed integer&nbsp;-3&nbsp;and the output represents the signed integer&nbsp;-1073741825.\nclass Solution: def reverseBits(self, n: int) -&gt; int: ret, power = 0, 31 while n: ret += (n &amp; 1) &lt;&lt; power n = n &gt;&gt; 1 power -= 1 return ret ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/bit/190.-reverse-bits.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474744,"modifiedTime":1737554783000,"sourceSize":865,"sourcePath":"Algorithms/Leetcode/Bit/190. Reverse Bits.md","exportPath":"algorithms/leetcode/bit/190.-reverse-bits.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/bit/461.-hamming-distance.html":{"title":"461. Hamming Distance","icon":"","description":"The&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Hamming_distance\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Hamming_distance\" target=\"_self\">Hamming distance</a>&nbsp;between two integers is the number of positions at which the corresponding bits are different.Given two integers&nbsp;x&nbsp;and&nbsp;y, return&nbsp;the&nbsp;Hamming distance&nbsp;between them.class Solution {\npublic: int hammingDistance(int x, int y) { int diff = x^y, ans = 0; while (diff){ ans+=diff&amp;1; diff&gt;&gt;=1; } return ans; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/bit/461.-hamming-distance.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474759,"modifiedTime":1737554783000,"sourceSize":492,"sourcePath":"Algorithms/Leetcode/Bit/461. Hamming Distance.md","exportPath":"algorithms/leetcode/bit/461.-hamming-distance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/geometry/149.-max-points-on-a-line.html":{"title":"149. Max Points on a Line","icon":"","description":"Given an array of&nbsp;points&nbsp;where&nbsp;points[i] = [xi, yi]&nbsp;represents a point on the&nbsp;X-Y&nbsp;plane, return&nbsp;the maximum number of points that lie on the same straight line.class Solution: def maxPoints(self, points: List[List[int]]) -&gt; int: n = len(points) if n == 1: return 1 result = 2 for i in range(n): cnt = collections.defaultdict(int) for j in range(n): if j != i: cnt[math.atan2(points[j][1] - points[i][1], points[j][0] - points[i][0])] += 1 result = max(result, max(cnt.values()) + 1) return result\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/geometry/149.-max-points-on-a-line.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474769,"modifiedTime":1737554783000,"sourceSize":693,"sourcePath":"Algorithms/Leetcode/Geometry/149. Max Points on a Line.md","exportPath":"algorithms/leetcode/geometry/149.-max-points-on-a-line.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/graph/490.-the-maze.html":{"title":"490. The Maze","icon":"","description":"There is a ball in a&nbsp;maze&nbsp;with empty spaces (represented as&nbsp;0) and walls (represented as&nbsp;1). The ball can go through the empty spaces by rolling&nbsp;up, down, left or right, but it won't stop rolling until hitting a wall. When the ball stops, it could choose the next direction.Given the&nbsp;m x n&nbsp;maze, the ball's&nbsp;start&nbsp;position and the&nbsp;destination, where&nbsp;start = [startrow, startcol]&nbsp;and&nbsp;destination = [destinationrow, destinationcol], return&nbsp;true&nbsp;if the ball can stop at the destination, otherwise return&nbsp;false.You may assume that&nbsp;the borders of the maze are all walls&nbsp;(see examples).class Solution: def hasPath(self, maze: List[List[int]], start: List[int], destination: List[int]) -&gt; bool: Q = [start] n = len(maze) m = len(maze[0]) dirs = ((0, 1), (0, -1), (1, 0), (-1, 0)) while Q: i, j = Q.pop(0) maze[i][j] = 2 if i == destination[0] and j == destination[1]: return True for x, y in dirs: row = i + x col = j + y while 0 &lt;= row &lt; n and 0 &lt;= col &lt; m and maze[row][col] != 1: row += x col += y row -= x col -= y if maze[row][col] == 0: Q.append([row, col]) return False\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/graph/490.-the-maze.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474806,"modifiedTime":1737554783000,"sourceSize":1451,"sourcePath":"Algorithms/Leetcode/Graph/490. The Maze.md","exportPath":"algorithms/leetcode/graph/490.-the-maze.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html":{"title":"797. All Paths From Source to Target","icon":"","description":"Given a directed acyclic graph (DAG) of&nbsp;n&nbsp;nodes labeled from&nbsp;0&nbsp;to&nbsp;n - 1, find all possible paths from node&nbsp;0&nbsp;to node&nbsp;n - 1&nbsp;and return them in&nbsp;any order.The graph is given as follows:&nbsp;graph[i]&nbsp;is a list of all nodes you can visit from node&nbsp;i&nbsp;(i.e., there is a directed edge from node&nbsp;i&nbsp;to node&nbsp;graph[i][j]).class Solution: def allPathsSourceTarget(self, graph: List[List[int]]) -&gt; List[List[int]]: g, res, n= defaultdict(list), [] , len(graph) for i, v in enumerate(graph): for x in v: g[i].append(x) def dfs(node, temp, res): if temp!=[] and temp[-1]==n-1: res.append(temp[:]) return children = g[node] for x in children: temp.append(x) dfs(x,temp,res) temp.pop() dfs(0,[0],res) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474838,"modifiedTime":1737554783000,"sourceSize":988,"sourcePath":"Algorithms/Leetcode/Graph/797. All Paths From Source to Target.md","exportPath":"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/graph/841.-keys-and-rooms.html":{"title":"841. Keys and Rooms","icon":"","description":"There are&nbsp;n&nbsp;rooms labeled from&nbsp;0&nbsp;to&nbsp;n - 1&nbsp;and all the rooms are locked except for room&nbsp;0. Your goal is to visit all the rooms. However, you cannot enter a locked room without having its key.When you visit a room, you may find a set of&nbsp;distinct keys&nbsp;in it. Each key has a number on it, denoting which room it unlocks, and you can take all of them with you to unlock the other rooms.Given an array&nbsp;rooms&nbsp;where&nbsp;rooms[i]&nbsp;is the set of keys that you can obtain if you visited room&nbsp;i, return&nbsp;true&nbsp;if you can visit&nbsp;all&nbsp;the rooms, or&nbsp;false&nbsp;otherwise.class Solution: def canVisitAllRooms(self, rooms: List[List[int]]) -&gt; bool: g, v = defaultdict(), set() for i in range(len(rooms)): g[i] = rooms[i] def dfs(node,count=set(),visited=set()): temp = False if len(count) == len(rooms): return True for x in g[node]: if x not in visited: count.add(x) visited.add(x) temp = temp or dfs(x,count) return temp count = set([0]) v = set([0]) return dfs(0,count,v)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/graph/841.-keys-and-rooms.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474894,"modifiedTime":1737554783000,"sourceSize":1257,"sourcePath":"Algorithms/Leetcode/Graph/841. Keys and Rooms.md","exportPath":"algorithms/leetcode/graph/841.-keys-and-rooms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html":{"title":"1971. Find if Path Exists in Graph","icon":"","description":"There is a&nbsp;bi-directional&nbsp;graph with&nbsp;n&nbsp;vertices, where each vertex is labeled from&nbsp;0&nbsp;to&nbsp;n - 1&nbsp;(inclusive). The edges in the graph are represented as a 2D integer array&nbsp;edges, where each&nbsp;edges[i] = [ui, vi]&nbsp;denotes a bi-directional edge between vertex&nbsp;ui&nbsp;and vertex&nbsp;vi. Every vertex pair is connected by&nbsp;at most one&nbsp;edge, and no vertex has an edge to itself.You want to determine if there is a&nbsp;valid path&nbsp;that exists from vertex&nbsp;source&nbsp;to vertex&nbsp;destination.Given&nbsp;edges&nbsp;and the integers&nbsp;n,&nbsp;source, and&nbsp;destination, return&nbsp;true&nbsp;if there is a&nbsp;valid path&nbsp;from&nbsp;source&nbsp;to&nbsp;destination, or&nbsp;false&nbsp;otherwise__.class Solution: def validPath(self, n: int, edges: List[List[int]], source: int, destination: int) -&gt; bool: tree = defaultdict(set) for u,v in edges: tree[u].add(v) tree[v].add(u) seen = {source} stack = [source] while stack: v = stack.pop() if v == destination: return True for u in tree[v]: if u not in seen: seen.add(u) stack.append(u) return False\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474785,"modifiedTime":1737554783000,"sourceSize":1263,"sourcePath":"Algorithms/Leetcode/Graph/1971. Find if Path Exists in Graph.md","exportPath":"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/greedy/134.-gas-station.html":{"title":"134. Gas Station","icon":"","description":"There are&nbsp;n&nbsp;gas stations along a circular route, where the amount of gas at the&nbsp;ith&nbsp;station is&nbsp;gas[i].You have a car with an unlimited gas tank and it costs&nbsp;cost[i]&nbsp;of gas to travel from the&nbsp;ith&nbsp;station to its next&nbsp;(i + 1)th&nbsp;station. You begin the journey with an empty tank at one of the gas stations.Given two integer arrays&nbsp;gas&nbsp;and&nbsp;cost, return&nbsp;the starting gas station's index if you can travel around the circuit once in the clockwise direction, otherwise return&nbsp;-1. If there exists a solution, it is&nbsp;guaranteed&nbsp;to be&nbsp;uniqueclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: total = 0 n = len(gas) mini = float(\"inf\") index = n+1 for i in range(n): total += gas[i]-cost[i] if total &lt; mini: mini = total index = i return -1 if total&lt;0 else (index+1)%n\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/greedy/134.-gas-station.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474928,"modifiedTime":1737554783000,"sourceSize":982,"sourcePath":"Algorithms/Leetcode/Greedy/134. Gas Station.md","exportPath":"algorithms/leetcode/greedy/134.-gas-station.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/greedy/135.-candy.html":{"title":"135. Candy","icon":"","description":"There are&nbsp;n&nbsp;children standing in a line. Each child is assigned a rating value given in the integer array&nbsp;ratings.You are giving candies to these children subjected to the following requirements:\nEach child must have at least one candy.\nChildren with a higher rating get more candies than their neighbors.\nReturn&nbsp;the minimum number of candies you need to have to distribute the candies to the children.class Solution {\npublic: int candy(vector&lt;int&gt;&amp; ratings) { int size = ratings.size(); if (size&lt;2) return size; vector&lt;int&gt; num(size,1); // initialize to array with ones for (int i=1;i&lt;size;i++){ if (ratings[i]&gt;ratings[i-1]){ num[i] = num[i-1]+1; } } for (int i= size-1;i&gt;0;i--){ if (ratings[i]&lt;ratings[i-1]){ num[i-1] = max(num[i-1],num[i]+1); } } return accumulate(num.begin(),num.end(),0) ; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/greedy/135.-candy.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474991,"modifiedTime":1737554783000,"sourceSize":1032,"sourcePath":"Algorithms/Leetcode/Greedy/135. Candy.md","exportPath":"algorithms/leetcode/greedy/135.-candy.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html":{"title":"435. Non-overlapping Intervals","icon":"","description":"Given an array of intervals&nbsp;intervals&nbsp;where&nbsp;intervals[i] = [starti, endi], return&nbsp;the minimum number of intervals you need to remove to make the rest of the intervals non-overlapping.class Solution {\npublic: static bool cmp(vector&lt;int&gt; &amp;a, vector&lt;int&gt; &amp;b){ return a[1]&lt;b[1]; } int eraseOverlapIntervals(vector&lt;vector&lt;int&gt;&gt;&amp; intervals) { sort(intervals.begin(),intervals.end(),cmp); int count=0; int lastInterval=intervals[0][1]; for(int i=1;i&lt;intervals.size();i++){ if(intervals[i][0]&lt;lastInterval){ count++; } else{ lastInterval=intervals[i][1]; } } return count; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475001,"modifiedTime":1737554783000,"sourceSize":747,"sourcePath":"Algorithms/Leetcode/Greedy/435. Non-overlapping Intervals.md","exportPath":"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/greedy/455.-assign-cookies.html":{"title":"455. Assign Cookies","icon":"","description":"Assume you are an awesome parent and want to give your children some cookies. But, you should give each child at most one cookie.Each child&nbsp;i&nbsp;has a greed factor&nbsp;g[i], which is the minimum size of a cookie that the child will be content with; and each cookie&nbsp;j&nbsp;has a size&nbsp;s[j]. If&nbsp;s[j] &gt;= g[i], we can assign the cookie&nbsp;j&nbsp;to the child&nbsp;i, and the child&nbsp;i&nbsp;will be content. Your goal is to maximize the number of your content children and output the maximum number.class Solution {\npublic: int findContentChildren(vector&lt;int&gt;&amp; g, vector&lt;int&gt;&amp; s) { sort(g.begin(),g.end()); sort(s.begin(),s.end()); int child = 0 , cookies = 0; while ( child&lt;g.size() &amp;&amp; cookies &lt; s.size() ){ if (g[child]&lt;=s[cookies]) ++child; ++cookies; } return child; }\n};\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/greedy/455.-assign-cookies.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475010,"modifiedTime":1737554783000,"sourceSize":862,"sourcePath":"Algorithms/Leetcode/Greedy/455. Assign Cookies.md","exportPath":"algorithms/leetcode/greedy/455.-assign-cookies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html":{"title":"1833. Maximum Ice Cream Bars","icon":"","description":"It is a sweltering summer day, and a boy wants to buy some ice cream bars.At the store, there are&nbsp;n&nbsp;ice cream bars. You are given an array&nbsp;costs&nbsp;of length&nbsp;n, where&nbsp;costs[i]&nbsp;is the price of the&nbsp;ith&nbsp;ice cream bar in coins. The boy initially has&nbsp;coins&nbsp;coins to spend, and he wants to buy as many ice cream bars as possible.&nbsp;Return&nbsp;the&nbsp;maximum&nbsp;number of ice cream bars the boy can buy with&nbsp;coins&nbsp;coins.Note:&nbsp;The boy can buy the ice cream bars in any order.class Solution: def maxIceCream(self, costs: List[int], coins: int) -&gt; int: costs.sort() c = 0 for x in costs: coins-=x if coins&lt;0: break c+=1 return c\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084474997,"modifiedTime":1737554783000,"sourceSize":759,"sourcePath":"Algorithms/Leetcode/Greedy/1833. Maximum Ice Cream Bars.md","exportPath":"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html":{"title":"24. Swap Nodes in Pairs","icon":"","description":"Given a&nbsp;linked list, swap every two adjacent nodes and return its head. You must solve the problem without&nbsp;modifying the values in the list's nodes (i.e., only nodes themselves may be changed.)class Solution: def swapPairs(self, head: Optional[ListNode]) -&gt; Optional[ListNode]: # If the list has no node or has only one node left. if not head or not head.next: return head # Nodes to be swapped first_node = head second_node = head.next # Swapping first_node.next = self.swapPairs(second_node.next) second_node.next = first_node # Now the head is the second node return second_node ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475031,"modifiedTime":1737554783000,"sourceSize":701,"sourcePath":"Algorithms/Leetcode/Linked List/24. Swap Nodes in Pairs.md","exportPath":"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html":{"title":"453. Minimum Moves to Equal Array Elements","icon":"","description":"Given an integer array&nbsp;nums&nbsp;of size&nbsp;n, return&nbsp;the minimum number of moves required to make all array elements equal.In one move, you can increment&nbsp;n - 1&nbsp;elements of the array by&nbsp;1.class Solution: def minMoves(self, nums: List[int]) -&gt; int: nums.sort() c = 0 for i in range(len(nums)-1,0,-1): c += nums[i]-nums[0] return c\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475073,"modifiedTime":1737554783000,"sourceSize":408,"sourcePath":"Algorithms/Leetcode/Math/453. Minimum Moves to Equal Array Elements.md","exportPath":"algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html":{"title":"787. Cheapest Flights Within K Stops","icon":"","description":"There are&nbsp;n&nbsp;cities connected by some number of flights. You are given an array&nbsp;flights&nbsp;where&nbsp;flights[i] = [fromi, toi, pricei]&nbsp;indicates that there is a flight from city&nbsp;fromi&nbsp;to city&nbsp;toi&nbsp;with cost&nbsp;pricei.You are also given three integers&nbsp;src,&nbsp;dst, and&nbsp;k, return&nbsp;the cheapest price&nbsp;from&nbsp;src&nbsp;to&nbsp;dst&nbsp;with at most&nbsp;k&nbsp;stops.&nbsp;If there is no such route, return&nbsp;-1.\nclass Solution: def findCheapestPrice(self, n: int, flights: List[List[int]], src: int, dst: int, k: int) -&gt; int: graph = defaultdict(dict) for u, v, p in flights: graph[u][v] = p seen = {} pq = [] heappush(pq, (0, 0, src)) while pq: cost, hops, city = heappop(pq) seen[city] = hops if city == dst: return cost if hops &gt; k: continue for next_city, next_cost in graph[city].items(): if next_city in seen and seen[next_city] &lt;= hops: continue heappush(pq, (cost + next_cost, hops + 1, next_city)) return -1\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475100,"modifiedTime":1737554783000,"sourceSize":1235,"sourcePath":"Algorithms/Leetcode/Shortest Path/787. Cheapest Flights Within K Stops.md","exportPath":"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/select/183.-customers-who-never-order.html":{"title":"183. Customers Who Never Order","icon":"","description":"Table:&nbsp;Customers+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| id | int |\n| name | varchar |\n+-------------+---------+\nid is the primary key column for this table.\nEach row of this table indicates the ID and name of a customer.\nselect name as customers from customers where id not in ( select customerid from orders\n)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/select/183.-customers-who-never-order.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475178,"modifiedTime":1737554783000,"sourceSize":403,"sourcePath":"Algorithms/Leetcode/SQL/Select/183. Customers Who Never Order.md","exportPath":"algorithms/leetcode/sql/select/183.-customers-who-never-order.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/select/584.-find-customer-referee.html":{"title":"584. Find Customer Referee","icon":"","description":"Table:&nbsp;Customer+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| id | int |\n| name | varchar |\n| referee_id | int |\n+-------------+---------+\nid is the primary key column for this table.\nEach row of this table indicates the id of a customer, their name, and the id of the customer who referred them.\nselect name from customer where referee_id!=2 or referee_id is null\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/select/584.-find-customer-referee.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475182,"modifiedTime":1737554783000,"sourceSize":453,"sourcePath":"Algorithms/Leetcode/SQL/Select/584. Find Customer Referee.md","exportPath":"algorithms/leetcode/sql/select/584.-find-customer-referee.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/select/595.-big-countries.html":{"title":"595. Big Countries","icon":"","description":"Table:&nbsp;World+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| name | varchar |\n| continent | varchar |\n| area | int |\n| population | int |\n| gdp | int |\n+-------------+---------+\nname is the primary key column for this table.\nEach row of this table gives information about the name of a country, the continent to which it belongs, its area, the population, and its GDP value.\nA country is&nbsp;big&nbsp;if:\nit has an area of at least&nbsp;three million (i.e.,&nbsp;3000000 km2), or\nit has a population of at least&nbsp;twenty-five million (i.e.,&nbsp;25000000).\nWrite an SQL query to report the name, population, and area of the&nbsp;big countries.Return the result table in&nbsp;any order.The query result format is in the following example.select name, population, area from world where area&gt;=3000000 or population&gt;=25000000\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/select/595.-big-countries.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475205,"modifiedTime":1737554783000,"sourceSize":925,"sourcePath":"Algorithms/Leetcode/SQL/Select/595. Big Countries.md","exportPath":"algorithms/leetcode/sql/select/595.-big-countries.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html":{"title":"1757. Recyclable and Low Fat Products","icon":"","description":"Table:&nbsp;Products+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| product_id | int |\n| low_fats | enum |\n| recyclable | enum |\n+-------------+---------+\nproduct_id is the primary key for this table.\nlow_fats is an ENUM of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.\nrecyclable is an ENUM of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.\nselect product_id from products where low_fats='Y' and recyclable='Y'\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475174,"modifiedTime":1737554783000,"sourceSize":555,"sourcePath":"Algorithms/Leetcode/SQL/Select/1757. Recyclable and Low Fat Products.md","exportPath":"algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html":{"title":"1484. Group Sold Products By The Date","icon":"","description":"Table&nbsp;Activities:+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| sell_date | date |\n| product | varchar |\n+-------------+---------+\nThere is no primary key for this table, it may contain duplicates.\nEach row of this table contains the product name and the date it was sold in a market.\nselect sell_date, count(distinct product) as num_sold, group_concat(Distinct product order by product ASC separator ',') as products from activities group by sell_date order by sell_date asc;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475215,"modifiedTime":1737554783000,"sourceSize":553,"sourcePath":"Algorithms/Leetcode/SQL/String Processing Functions/1484. Group Sold Products By The Date.md","exportPath":"algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html":{"title":"1527. Patients With a Condition","icon":"","description":"Table:&nbsp;Patients| Column Name | Type |\n+--------------+---------+\n| patient_id | int |\n| patient_name | varchar |\n| conditions | varchar |\n+--------------+---------+\npatient_id is the primary key for this table.\n'conditions' contains 0 or more code separated by spaces. This table contains information of the patients in the hospital.\nselect * from patients where conditions like \"% DIAB1%\" or conditions like 'DIAB1%';\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475219,"modifiedTime":1737554783000,"sourceSize":488,"sourcePath":"Algorithms/Leetcode/SQL/String Processing Functions/1527. Patients With a Condition.md","exportPath":"algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html":{"title":"1667. Fix Names in a Table","icon":"","description":"Table:&nbsp;Users+----------------+---------+\n| Column Name | Type |\n+----------------+---------+\n| user_id | int |\n| name | varchar |\n+----------------+---------+\nuser_id is the primary key for this table.\nThis table contains the ID and the name of the user. The name consists of only lowercase and uppercase characters.\nselect user_id, CONCAT(UPPER(SUBSTR(name,1,1)),LOWER(SUBSTR(name,2))) AS name from users ORDER BY user_id ASC\nTips:\nsubstr(name,start,length)","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475230,"modifiedTime":1737554783000,"sourceSize":519,"sourcePath":"Algorithms/Leetcode/SQL/String Processing Functions/1667. Fix Names in a Table.md","exportPath":"algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/union/197.-rising-temperature.html":{"title":"197. Rising Temperature","icon":"","description":"Table:&nbsp;Weather+---------------+---------+\n| Column Name | Type |\n+---------------+---------+\n| id | int |\n| recordDate | date |\n| temperature | int |\n+---------------+---------+\nid is the primary key for this table.\nThis table contains information about the temperature on a certain day. select w1.id from weather w1 join weather w2 on w1.recordDate = date_add(w2.recordDate, interval 1 day) where w1.temperature &gt; w2.temperature\nTips:MySQL&nbsp;DATE_ADD()&nbsp;Function: Definition and Usage\nThe DATE_ADD() function adds a time/date interval to a date and then returns the date. Syntax\nDATE_ADD(_date_, INTERVAL&nbsp;_value addunit_) addunit_ Required. The type of interval to add. Can be one of the following values: - MICROSECOND\n- SECOND\n- MINUTE\n- HOUR\n- DAY\n- WEEK\n- MONTH\n- QUARTER\n- YEAR\n- SECOND_MICROSECOND\n- MINUTE_MICROSECOND\n- MINUTE_SECOND\n- HOUR_MICROSECOND\n- HOUR_SECOND\n- HOUR_MINUTE\n- DAY_MICROSECOND\n- DAY_SECOND\n- DAY_MINUTE\n- DAY_HOUR\n- YEAR_MONTH\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/union/197.-rising-temperature.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475238,"modifiedTime":1737554783000,"sourceSize":1107,"sourcePath":"Algorithms/Leetcode/SQL/Union/197. Rising Temperature.md","exportPath":"algorithms/leetcode/sql/union/197.-rising-temperature.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/union&select/608.-tree-node.html":{"title":"608. Tree Node","icon":"","description":"Table:&nbsp;Tree+-------------+------+\n| Column Name | Type |\n+-------------+------+\n| id | int |\n| p_id | int |\n+-------------+------+\nid is the primary key column for this table.\nEach row of this table contains information about the id of a node and the id of its parent node in a tree.\nThe given structure is always a valid tree.\nselect id, case when p_id is null then 'Root'\nwhen p_id in (select id from tree) and id in (select p_id from tree) then 'Inner'\nELSE 'Leaf'\nend as type\nfrom Tree\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/union&select/608.-tree-node.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475276,"modifiedTime":1737554783000,"sourceSize":535,"sourcePath":"Algorithms/Leetcode/SQL/Union&Select/608. Tree Node.md","exportPath":"algorithms/leetcode/sql/union&select/608.-tree-node.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html":{"title":"1965. Employees With Missing Information","icon":"","description":"Table:&nbsp;Employees+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| employee_id | int |\n| name | varchar |\n+-------------+---------+\nemployee_id is the primary key for this table.\nEach row of this table indicates the name of the employee whose ID is employee_id.\nTable:&nbsp;Salaries+-------------+---------+\n| Column Name | Type |\n+-------------+---------+\n| employee_id | int |\n| salary | int |\n+-------------+---------+\nemployee_id is the primary key for this table.\nEach row of this table indicates the salary of the employee whose ID is employee_id.\nSELECT employee_id from Employees WHERE employee_id not in (Select employee_id from Salaries)\nUNION SELECT employee_id from Salaries WHERE employee_id not in (Select employee_id from Employees)\nORDER BY employee_id;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084475262,"modifiedTime":1737554783000,"sourceSize":861,"sourcePath":"Algorithms/Leetcode/SQL/Union&Select/1965. Employees With Missing Information.md","exportPath":"algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/6.-zigzag-conversion.html":{"title":"6. Zigzag Conversion","icon":"","description":"The string&nbsp;\"PAYPALISHIRING\"&nbsp;is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility)P A H N\nA P L S I I G\nY I RAnd then read line by line:&nbsp;\"PAHNAPLSIIGYIR\"Write the code that will take a string and make this conversion given a number of rows:string convert(string s, int numRows);class Solution: def convert(self, s: str, numRows: int) -&gt; str: if numRows == 1: return s res = \"\" for r in range(numRows): increment = 2*(numRows-1) for i in range(r,len(s),increment): res+=s[i] if (r&gt;0 and r&lt;numRows-1 and i+increment-2*r&lt;len(s)): res+=s[i+increment-2*r] return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/6.-zigzag-conversion.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475478,"modifiedTime":1737554783000,"sourceSize":815,"sourcePath":"Algorithms/Leetcode/String/6. Zigzag Conversion.md","exportPath":"algorithms/leetcode/string/6.-zigzag-conversion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/131.-palindrome-partitioning.html":{"title":"131. Palindrome Partitioning","icon":"","description":"Given a string&nbsp;s, partition&nbsp;s&nbsp;such that every&nbsp;substring&nbsp;of the partition is a&nbsp;palindrome. Return&nbsp;all possible palindrome partitioning of&nbsp;s.class Solution: def isPali(self, s,l,r): while l&lt;r: if s[l]!=s[r]: return False l,r = l+1, r-1 return True def partition(self, s: str) -&gt; List[List[str]]: res = [] part = [] def dfs(i): if i&gt;=len(s): res.append(part[:]) for j in range(i, len(s)): if self.isPali(s,i,j): part.append(s[i:j+1]) dfs(j+1) part.pop() dfs(0) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/131.-palindrome-partitioning.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475307,"modifiedTime":1737554783000,"sourceSize":731,"sourcePath":"Algorithms/Leetcode/String/131. Palindrome Partitioning.md","exportPath":"algorithms/leetcode/string/131.-palindrome-partitioning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html":{"title":"340. Longest Substring with At Most K Distinct Characters","icon":"","description":"Given a string&nbsp;s&nbsp;and an integer&nbsp;k, return&nbsp;the length of the longest&nbsp;substring&nbsp;of&nbsp;s&nbsp;that contains at most&nbsp;k&nbsp;distinct&nbsp;characters.class Solution: def lengthOfLongestSubstringKDistinct(self, s: str, k: int) -&gt; int: d = {} low, res = 0, 0 for i, c in enumerate(s): d[c] = i if len(d) &gt; k: low = min(d.values()) del d[s[low]] low += 1 res = max(i - low + 1, res) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475324,"modifiedTime":1737554783000,"sourceSize":546,"sourcePath":"Algorithms/Leetcode/String/340. Longest Substring with At Most K Distinct Characters.md","exportPath":"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html":{"title":"395. Longest Substring with At Least K Repeating Characters","icon":"","description":"Given a string&nbsp;s&nbsp;and an integer&nbsp;k, return&nbsp;the length of the longest substring of&nbsp;s&nbsp;such that the frequency of each character in this substring is greater than or equal to&nbsp;k.class Solution: def longestSubstring(self, s: str, k: int) -&gt; int: if len(s) &lt; k: return 0 count = Counter(s) if min(count.values()) &gt;= k: return len(s) for i,c in enumerate(s): # If the current character appears less than k times in the string # We need to find the longest substring before and after it. # As it cannot be included in any valid substring if count[c] &lt; k: length1 = self.longestSubstring(s[:i], k) length2 = self.longestSubstring(s[i+1:], k) return max(length1, length2) return len(s) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475380,"modifiedTime":1737554783000,"sourceSize":897,"sourcePath":"Algorithms/Leetcode/String/395. Longest Substring with At Least K Repeating Characters.md","exportPath":"algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html":{"title":"438. Find All Anagrams in a String","icon":"","description":"Given two strings&nbsp;s&nbsp;and&nbsp;p, return&nbsp;an array of all the start indices of&nbsp;p's anagrams in&nbsp;s. You may return the answer in&nbsp;any order.An&nbsp;Anagram&nbsp;is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.class Solution: def findAnagrams(self, s: str, p: str) -&gt; List[int]: if len(s) &lt; len(p): return [] p_count = [0] * 26 s_count = [0] * 26 for char in p: p_count[ord(char) - ord('a')] += 1 result = [] for i in range(len(s)): s_count[ord(s[i]) - ord('a')] += 1 if i &gt;= len(p): s_count[ord(s[i - len(p)]) - ord('a')] -= 1 if p_count == s_count: result.append(i - len(p) + 1) return result\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475431,"modifiedTime":1737554783000,"sourceSize":875,"sourcePath":"Algorithms/Leetcode/String/438. Find All Anagrams in a String.md","exportPath":"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html":{"title":"1061. Lexicographically Smallest Equivalent String","icon":"","description":"class Solution: def smallestEquivalentString(self, s1: str, s2: str, baseStr: str) -&gt; str: adj = defaultdict(list) ans = \"\" def dfs(root): if root in visited: return root visited.add(root) min_char = root for child in adj[root]: min_char = min(min_char, dfs(child)) return min_char for i in range(len(s1)): adj[s1[i]].append(s2[i]) adj[s2[i]].append(s1[i]) for char in baseStr: visited = set() ans += dfs(char) return ans\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475285,"modifiedTime":1737554783000,"sourceSize":653,"sourcePath":"Algorithms/Leetcode/String/1061. Lexicographically Smallest Equivalent String.md","exportPath":"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html":{"title":"1071. Greatest Common Divisor of Strings","icon":"","description":"For two strings&nbsp;s&nbsp;and&nbsp;t, we say \"t&nbsp;divides&nbsp;s\" if and only if&nbsp;s = t + ... + t&nbsp;(i.e.,&nbsp;t&nbsp;is concatenated with itself one or more times).Given two strings&nbsp;str1&nbsp;and&nbsp;str2, return&nbsp;the largest string&nbsp;x&nbsp;such that&nbsp;x&nbsp;divides both&nbsp;str1&nbsp;and&nbsp;str2.class Solution: def gcdOfStrings(self, str1: str, str2: str) -&gt; str: if len(str1) &lt; len(str2): str1, str2 = str2, str1 if str1 == str2: return str1 if str1[:len(str2)] != str2: return \"\" return self.gcdOfStrings(str1[len(str2):], str2) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475299,"modifiedTime":1737554783000,"sourceSize":621,"sourcePath":"Algorithms/Leetcode/String/1071. Greatest Common Divisor of Strings.md","exportPath":"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/100.-same-tree.html":{"title":"100. Same Tree","icon":"","description":"Given the roots of two binary trees&nbsp;p&nbsp;and&nbsp;q, write a function to check if they are the same or not.Two binary trees are considered the same if they are structurally identical, and the nodes have the same value.\nElegant\n```java\nclass Solution { public boolean isSameTree(TreeNode p, TreeNode q) { // DFS if (p==null &amp;&amp; q==null){ return true; } else if (p==null || q==null){ return false; } else if (p.val!=q.val){ return false; } else { return isSameTree(p.left,q.left) &amp;&amp; isSameTree(p.right,q.right); } }\n} Ugly\nclass Solution: def isSameTree(self, p: Optional[TreeNode], q: Optional[TreeNode]) -&gt; bool: def dfs(node): res = [] if not node: res.append(0) return res res.append(node.val) res += dfs(node.left) res+=dfs(node.right) return res return dfs(p)==dfs(q)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/100.-same-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475510,"modifiedTime":1737554783000,"sourceSize":1041,"sourcePath":"Algorithms/Leetcode/Tree/100. Same Tree.md","exportPath":"algorithms/leetcode/tree/100.-same-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html":{"title":"102. Binary Tree Level Order Traversal","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, return&nbsp;the level order traversal of its nodes' values. (i.e., from left to right, level by level).class Solution: def levelOrder(self, root: Optional[TreeNode]) -&gt; List[List[int]]: if not root: return [] queue, level, next_, res = [root],[],[],[] while queue!=[]: for root in queue: level.append(root.val) if root.left: next_.append(root.left) if root.right: next_.append(root.right) res.append(level) queue, level, next_ = next_,[],[] return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475541,"modifiedTime":1737554783000,"sourceSize":701,"sourcePath":"Algorithms/Leetcode/Tree/102. Binary Tree Level Order Traversal.md","exportPath":"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html":{"title":"103. Binary Tree Zigzag Level Order Traversal","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, return&nbsp;the zigzag level order traversal of its nodes' values. (i.e., from left to right, then right to left for the next level and alternate between). if root == None: return [] queue = [root] next_ = [] level = [] res = [] count = 1 while queue!=[]: for root in queue: level.append(root.val) if root.right: next_.append(root.right) if root.left: next_.append(root.left) if count % 2==0: res.append(level) else: level.reverse() res.append(level) count+=1 level = [] queue = next_ next_ = [] return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475546,"modifiedTime":1737554783000,"sourceSize":907,"sourcePath":"Algorithms/Leetcode/Tree/103. Binary Tree Zigzag Level Order Traversal.md","exportPath":"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html":{"title":"114. Flatten Binary Tree to Linked List","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, flatten the tree into a \"linked list\":\nThe \"linked list\" should use the same&nbsp;TreeNode&nbsp;class where the&nbsp;right&nbsp;child pointer points to the next node in the list and the&nbsp;left&nbsp;child pointer is always&nbsp;null.\nThe \"linked list\" should be in the same order as a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Tree_traversal#Pre-order,_NLR\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Tree_traversal#Pre-order,_NLR\" target=\"_self\"><strong></strong>&nbsp;<strong></strong></a>pre-ordertraversal&nbsp;of the binary tree. # Definition for a binary tree node.\n# class TreeNode:\n# def __init__(self, val=0, left=None, right=None):\n# self.val = val\n# self.left = left\n# self.right = right\nclass Solution: def flatten(self, root: Optional[TreeNode]) -&gt; None: \"\"\" Do not return anything, modify root in-place instead. \"\"\" def dfs(node): nonlocal prev if not node: return dfs(node.right) dfs(node.left) node.right = prev node.left = None prev = node prev = None dfs(root)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475555,"modifiedTime":1737554783000,"sourceSize":1073,"sourcePath":"Algorithms/Leetcode/Tree/114. Flatten Binary Tree to Linked List.md","exportPath":"algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html":{"title":"144. Binary Tree Preorder Traversal","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, return&nbsp;the preorder traversal of its nodes' values.class Solution: def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]: res = [] if not root: return [] res.append(root.val) if root.left: res+=self.preorderTraversal(root.left) if root.right: res+=self.preorderTraversal(root.right) return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475573,"modifiedTime":1737554783000,"sourceSize":481,"sourcePath":"Algorithms/Leetcode/Tree/144. Binary Tree Preorder Traversal.md","exportPath":"algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html":{"title":"429. N-ary Tree Level Order Traversal","icon":"","description":"Given an n-ary tree, return the&nbsp;level order&nbsp;traversal of its nodes' values.Nary-Tree input serialization is represented in their level order traversal, each group of children is separated by the null value (See examples).\"\"\"\n# Definition for a Node.\nclass Node: def __init__(self, val=None, children=None): self.val = val self.children = children\n\"\"\" class Solution: def levelOrder(self, root: 'Node') -&gt; List[List[int]]: if not root: return [] res,level,next_,q = ([] for i in range(4)) q.append(root) while q: for x in q: level.append(x.val) for y in x.children: next_.append(y) if level: res.append(level[:]) q,level,next_ = next_,[],[] return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475803,"modifiedTime":1737554783000,"sourceSize":876,"sourcePath":"Algorithms/Leetcode/Tree/429. N-ary Tree Level Order Traversal.md","exportPath":"algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html":{"title":"637. Average of Levels in Binary Tree","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, return&nbsp;the average value of the nodes on each level in the form of an array. Answers within&nbsp;10-5&nbsp;of the actual answer will be accepted.class Solution: def averageOfLevels(self, root: Optional[TreeNode]) -&gt; List[float]: res,queue,level,next_ = [],[root],[],[] while queue!=[]: for root in queue: level.append(root.val) if root.right: next_.append(root.right) if root.left: next_.append(root.left) res.append(sum(level)/len(level)) queue,next_,level = next_,[],[] return res\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475807,"modifiedTime":1737554783000,"sourceSize":730,"sourcePath":"Algorithms/Leetcode/Tree/637. Average of Levels in Binary Tree.md","exportPath":"algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html":{"title":"1123. Lowest Common Ancestor of Deepest Leaves","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, return&nbsp;the lowest common ancestor of its deepest leaves.Recall that:\nThe node of a binary tree is a leaf if and only if it has no children\nThe depth of the root of the tree is&nbsp;0. if the depth of a node is&nbsp;d, the depth of each of its children is&nbsp;d + 1.\nThe lowest common ancestor of a set&nbsp;S&nbsp;of nodes, is the node&nbsp;A&nbsp;with the largest depth such that every node in&nbsp;S&nbsp;is in the subtree with root&nbsp;A.\n# Definition for a binary tree node.\n# class TreeNode:\n# def __init__(self, val=0, left=None, right=None):\n# self.val = val\n# self.left = left\n# self.right = right\nclass Solution: def lcaDeepestLeaves(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]: def dfs(node): if not node: return 0, None l, r = dfs(node.left), dfs(node.right) if l[0] &gt; r[0]: return l[0] + 1, l[1] if l[0] &lt; r[0]: return r[0] + 1, r[1] return l[0] + 1, node return dfs(root)[1]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475550,"modifiedTime":1737554783000,"sourceSize":1110,"sourcePath":"Algorithms/Leetcode/Tree/1123. Lowest Common Ancestor of Deepest Leaves.md","exportPath":"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/1302.-deepest-leaves-sum.html":{"title":"1302. Deepest Leaves Sum","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree, return&nbsp;the sum of values of its deepest leaves. def deepestLeavesSum(self, root: TreeNode) -&gt; int: maxlevel = 0 tsum = 0 def dfs(node=root, level=0): nonlocal maxlevel, tsum if not node: return dfs(node.left, level+1) dfs(node.right, level+1) if maxlevel &lt; level: maxlevel = level tsum = 0 if maxlevel == level: tsum += node.val dfs() return tsum\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/1302.-deepest-leaves-sum.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475568,"modifiedTime":1737554783000,"sourceSize":434,"sourcePath":"Algorithms/Leetcode/Tree/1302. Deepest Leaves Sum.md","exportPath":"algorithms/leetcode/tree/1302.-deepest-leaves-sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html":{"title":"1443. Minimum Time to Collect All Apples in a Tree","icon":"","description":"Given an undirected tree consisting of&nbsp;n&nbsp;vertices numbered from&nbsp;0&nbsp;to&nbsp;n-1, which has some apples in their vertices. You spend 1 second to walk over one edge of the tree.&nbsp;Return the minimum time in seconds you have to spend to collect all apples in the tree, starting at&nbsp;vertex 0&nbsp;and coming back to this vertex.The edges of the undirected tree are given in the array&nbsp;edges, where&nbsp;edges[i] = [ai, bi]&nbsp;means that exists an edge connecting the vertices&nbsp;ai&nbsp;and&nbsp;bi. Additionally, there is a boolean array&nbsp;hasApple, where&nbsp;hasApple[i] = true&nbsp;means that vertex&nbsp;i&nbsp;has an apple; otherwise, it does not have any apple.class Solution: def minTime(self, n: int, edges: List[List[int]], hasApple: List[bool]) -&gt; int: tree = defaultdict(list) for s,e in edges: tree[s].append(e) tree[e].append(s) def dfs(node,par): res = 0 for nei in tree[node]: if nei != par: res += dfs(nei,node) if res or hasApple[node]: return res + 2 return res return max(dfs(0,-1)-2, 0)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475577,"modifiedTime":1737554783000,"sourceSize":1205,"sourcePath":"Algorithms/Leetcode/Tree/1443. Minimum Time to Collect All Apples in a Tree.md","exportPath":"algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html":{"title":"1490. Clone N-ary Tree","icon":"","description":"Given a&nbsp;root&nbsp;of an N-ary tree, return a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Object_copying#Deep_copy\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Object_copying#Deep_copy\" target=\"_self\"><strong></strong></a>deep copy&nbsp;(clone) of the tree.class Solution: def cloneTree(self, root: 'Node') -&gt; 'Node': if not root: return root copy = Node(root.val) for c in root.children: copy.children.append(self.cloneTree(c)) return copy\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475617,"modifiedTime":1737554783000,"sourceSize":420,"sourcePath":"Algorithms/Leetcode/Tree/1490. Clone N-ary Tree.md","exportPath":"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html":{"title":"1602. Find Nearest Right Node in Binary Tree","icon":"","description":"Given the&nbsp;root&nbsp;of a binary tree and a node&nbsp;u&nbsp;in the tree, return&nbsp;the&nbsp;nearest&nbsp;node on the&nbsp;same level&nbsp;that is to the&nbsp;right&nbsp;of&nbsp;u, or return&nbsp;null&nbsp;if&nbsp;u&nbsp;is the rightmost node in its level.# Definition for a binary tree node.\n# class TreeNode:\n# def __init__(self, val=0, left=None, right=None):\n# self.val = val\n# self.left = left\n# self.right = right\nclass Solution: def findNearestRightNode(self, root: TreeNode, u: TreeNode) -&gt; Optional[TreeNode]: q,level,next_= [root],[],[] while q: for cur in q: level.append(cur) if cur.left: next_.append(cur.left) if cur.right: next_.append(cur.right) if u in level: if level[-1]==u: return None else: a = level.index(u)+1 return level[a] q = next_ level, next_ = [], [] return None\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475624,"modifiedTime":1737554783000,"sourceSize":1117,"sourcePath":"Algorithms/Leetcode/Tree/1602. Find Nearest Right Node in Binary Tree.md","exportPath":"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/1660.-correct-a-binary-tree.html":{"title":"1660. Correct a Binary Tree","icon":"","description":"You have a binary tree with a small defect. There is&nbsp;exactly one&nbsp;invalid node where its right child incorrectly points to another node at the&nbsp;same depth&nbsp;but to the&nbsp;invalid node's right.Given the root of the binary tree with this defect,&nbsp;root, return&nbsp;the root of the binary tree after&nbsp;removing&nbsp;this invalid node&nbsp;and every node underneath it&nbsp;(minus the node it incorrectly points to).Custom testing:The test input is read as 3 lines:\nTreeNode root\nint fromNode&nbsp;(not available to&nbsp;correctBinaryTree)\nint toNode&nbsp;(not available to&nbsp;correctBinaryTree)\nAfter the binary tree rooted at&nbsp;root&nbsp;is parsed, the&nbsp;TreeNode&nbsp;with value of&nbsp;fromNode&nbsp;will have its right child pointer pointing to the&nbsp;TreeNode&nbsp;with a value of&nbsp;toNode. Then,&nbsp;root&nbsp;is passed to&nbsp;correctBinaryTree.# Definition for a binary tree node.\n# class TreeNode:\n# def __init__(self, val=0, left=None, right=None):\n# self.val = val\n# self.left = left\n# self.right = right\nclass Solution: def correctBinaryTree(self, root: TreeNode) -&gt; TreeNode: seen = set() def dfs(root): if not root or (root.right and root.right.val in seen): return seen.add(root.val) root.right = dfs(root.right) root.left = dfs(root.left) return root return dfs(root)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/1660.-correct-a-binary-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475668,"modifiedTime":1737554783000,"sourceSize":1461,"sourcePath":"Algorithms/Leetcode/Tree/1660. Correct a Binary Tree.md","exportPath":"algorithms/leetcode/tree/1660.-correct-a-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html":{"title":"2196. Create Binary Tree From Descriptions","icon":"","description":"You are given a 2D integer array&nbsp;descriptions&nbsp;where&nbsp;descriptions[i] = [parenti, childi, isLefti]&nbsp;indicates that&nbsp;parenti&nbsp;is the&nbsp;parent&nbsp;of&nbsp;childi&nbsp;in a&nbsp;binary&nbsp;tree of&nbsp;unique&nbsp;values. Furthermore,\nIf&nbsp;isLefti&nbsp;== 1, then&nbsp;childi&nbsp;is the left child of&nbsp;parenti.\nIf&nbsp;isLefti&nbsp;== 0, then&nbsp;childi&nbsp;is the right child of&nbsp;parenti.\nConstruct the binary tree described by&nbsp;descriptions&nbsp;and return&nbsp;its&nbsp;root.The test cases will be generated such that the binary tree is&nbsp;valid.# Definition for a binary tree node.\n# class TreeNode:\n# def __init__(self, val=0, left=None, right=None):\n# self.val = val\n# self.left = left\n# self.right = right\nclass Solution: def createBinaryTree(self, descriptions: List[List[int]]) -&gt; Optional[TreeNode]: hashmap = {} nodes = set() children = set() for parent,child,isLeft in descriptions: nodes.add(parent) nodes.add(child) children.add(child) if parent not in hashmap: hashmap[parent] = TreeNode(parent) if child not in hashmap: hashmap[child] = TreeNode(child) if isLeft: hashmap[parent].left = hashmap[child] if not isLeft: hashmap[parent].right = hashmap[child] for node in nodes: if node not in children: return hashmap[node]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475703,"modifiedTime":1737554783000,"sourceSize":1508,"sourcePath":"Algorithms/Leetcode/Tree/2196. Create Binary Tree From Descriptions.md","exportPath":"algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html":{"title":"2415. Reverse Odd Levels of Binary Tree","icon":"","description":"Given the&nbsp;root&nbsp;of a&nbsp;perfect&nbsp;binary tree, reverse the node values at each&nbsp;odd&nbsp;level of the tree.\nFor example, suppose the node values at level 3 are&nbsp;[2,1,3,4,7,11,29,18], then it should become&nbsp;[18,29,11,7,4,3,1,2].\nReturn&nbsp;the root of the reversed tree.A binary tree is&nbsp;perfect&nbsp;if all parent nodes have two children and all leaves are on the same level.The&nbsp;level&nbsp;of a node is the number of edges along the path between it and the root node.class Solution: def reverseOddLevels(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]: q,level = deque([root]),0 while q: if level %2 != 0: l = 0 r = len(q)-1 while l&lt;r: q[l].val,q[r].val = q[r].val,q[l].val l+=1 r-=1 for _ in range(len(q)): cur = q.popleft() if cur.left: q.append(cur.left) if cur.right: q.append(cur.right) level += 1 return root\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084475748,"modifiedTime":1737554783000,"sourceSize":1149,"sourcePath":"Algorithms/Leetcode/Tree/2415. Reverse Odd Levels of Binary Tree.md","exportPath":"algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/leetcode/road-map.html":{"title":"Road Map","icon":"","description":"these are neetcode’s 150 problems roadmap also the problems are sorted from easy to hard in each section\n<a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/contains-duplicate/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/contains-duplicate/\" target=\"_self\">Contains Duplicate </a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/two-sum/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/two-sum/\" target=\"_self\">Two Sum</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/valid-anagram/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/valid-anagram/\" target=\"_self\">Valid Anagram </a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/group-anagrams/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/group-anagrams/\" target=\"_self\">Group Anagrams </a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/top-k-frequent-elements/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/top-k-frequent-elements/\" target=\"_self\">Top K Frequent Elements </a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/product-of-array-except-self/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/product-of-array-except-self/\" target=\"_self\">Product of Array Except Self </a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/valid-sudoku/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/valid-sudoku/\" target=\"_self\">Valid Sudoku </a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/longest-consecutive-sequence/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/longest-consecutive-sequence/\" target=\"_self\">Longest Consecutive Sequence </a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/valid-parentheses/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/valid-parentheses/\" target=\"_self\">Valid Parentheses</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/min-stack/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/min-stack/\" target=\"_self\">Min Stack</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/evaluate-reverse-polish-notation/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/evaluate-reverse-polish-notation/\" target=\"_self\">Evaluate Reverse Polish Notation</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/generate-parentheses/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/generate-parentheses/\" target=\"_self\">Generate Parentheses</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/daily-temperatures/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/daily-temperatures/\" target=\"_self\">Daily Temperatures</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/car-fleet/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/car-fleet/\" target=\"_self\">Car Fleet</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/largest-rectangle-in-histogram/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/largest-rectangle-in-histogram/\" target=\"_self\">Largest Rectangle In Histogram</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/valid-palindrome/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/valid-palindrome/\" target=\"_self\">Valid Palindrome</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/\" target=\"_self\">Two Sum II - Input Array Is Sorted</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/3sum/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/3sum/\" target=\"_self\">3Sum</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/container-with-most-water/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/container-with-most-water/\" target=\"_self\">Container With Most Water</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/trapping-rain-water/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/trapping-rain-water/\" target=\"_self\">Trapping Rain Water</a>\n<br><a data-href=\"Floyd’s Cycle Finding Algorithm\" href=\"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Floyd’s Cycle Finding Algorithm</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/reverse-linked-list/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/reverse-linked-list/\" target=\"_self\">Reverse Linked List</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/merge-two-sorted-lists/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/merge-two-sorted-lists/\" target=\"_self\">Merge Two Sorted Lists</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/reorder-list/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/reorder-list/\" target=\"_self\">Reorder List</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/remove-nth-node-from-end-of-list/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/remove-nth-node-from-end-of-list/\" target=\"_self\">Remove Nth Node From End of List</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/copy-list-with-random-pointer/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/copy-list-with-random-pointer/\" target=\"_self\">Copy List With Random Pointer</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/add-two-numbers/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/add-two-numbers/\" target=\"_self\">Add Two Numbers</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/linked-list-cycle/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/linked-list-cycle/\" target=\"_self\">Linked List Cycle</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/find-the-duplicate-number/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/find-the-duplicate-number/\" target=\"_self\">Find The Duplicate Number</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/lru-cache/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/lru-cache/\" target=\"_self\">LRU Cache</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/merge-k-sorted-lists/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/merge-k-sorted-lists/\" target=\"_self\">Merge K Sorted Lists</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/reverse-nodes-in-k-group/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/reverse-nodes-in-k-group/\" target=\"_self\">Reverse Nodes In K Group</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/best-time-to-buy-and-sell-stock/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/best-time-to-buy-and-sell-stock/\" target=\"_self\">Best Time to Buy And Sell Stock</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/longest-substring-without-repeating-characters/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/longest-substring-without-repeating-characters/\" target=\"_self\">Longest Substring Without Repeating Characters</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/longest-repeating-character-replacement/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/longest-repeating-character-replacement/\" target=\"_self\">Longest Repeating Character Replacement</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/permutation-in-string/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/permutation-in-string/\" target=\"_self\">Permutation In String</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/minimum-window-substring/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/minimum-window-substring/\" target=\"_self\">Minimum Window Substring</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/minimum-window-substring/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/minimum-window-substring/\" target=\"_self\">Sliding Window Maximum</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/binary-search/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/binary-search/\" target=\"_self\">Binary Search</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/search-a-2d-matrix/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/search-a-2d-matrix/\" target=\"_self\">Search a 2D Matrix</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/koko-eating-bananas/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/koko-eating-bananas/\" target=\"_self\">Koko Eating Bananas</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/find-minimum-in-rotated-sorted-array/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/find-minimum-in-rotated-sorted-array/\" target=\"_self\">Find Minimum in Rotated Sorted Array</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/search-in-rotated-sorted-array/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/search-in-rotated-sorted-array/\" target=\"_self\">Search in Rotated Sorted Array</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/time-based-key-value-store/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/time-based-key-value-store/\" target=\"_self\">Time Based Key Value Store</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/median-of-two-sorted-arrays/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/median-of-two-sorted-arrays/\" target=\"_self\">Median of Two Sorted Arrays</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/invert-binary-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/invert-binary-tree/\" target=\"_self\">Invert Binary Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/maximum-depth-of-binary-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/maximum-depth-of-binary-tree/\" target=\"_self\">Maximum Depth of Binary Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/diameter-of-binary-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/diameter-of-binary-tree/\" target=\"_self\">Diameter of Binary Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/balanced-binary-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/balanced-binary-tree/\" target=\"_self\">Balanced Binary Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/same-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/same-tree/\" target=\"_self\">Same Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/subtree-of-another-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/subtree-of-another-tree/\" target=\"_self\">Subtree of Another Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-search-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-search-tree/\" target=\"_self\">Lowest Common Ancestor of a Binary Search Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/binary-tree-level-order-traversal/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/binary-tree-level-order-traversal/\" target=\"_self\">Binary Tree Level Order Traversal</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/binary-tree-right-side-view/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/binary-tree-right-side-view/\" target=\"_self\">Binary Tree Right Side View</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/count-good-nodes-in-binary-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/count-good-nodes-in-binary-tree/\" target=\"_self\">Count Good Nodes in Binary Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/validate-binary-search-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/validate-binary-search-tree/\" target=\"_self\">Validate Binary Search Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/kth-smallest-element-in-a-bst/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/kth-smallest-element-in-a-bst/\" target=\"_self\">Kth Smallest Element in a BST</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/\" target=\"_self\">Construct Binary Tree from Preorder and Inorder Traversal</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/binary-tree-maximum-path-sum/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/binary-tree-maximum-path-sum/\" target=\"_self\">Binary Tree Maximum Path Sum</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/serialize-and-deserialize-binary-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/serialize-and-deserialize-binary-tree/\" target=\"_self\">Serialize and Desterilize Binary Tree</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/implement-trie-prefix-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/implement-trie-prefix-tree/\" target=\"_self\">Implement Trie (Prefix Tree)</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/design-add-and-search-words-data-structure/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/design-add-and-search-words-data-structure/\" target=\"_self\">Design Add and Search Words Data Structure</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/word-search-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/word-search-ii/\" target=\"_self\">Word Search II</a>\n<br><a data-href=\"A general approach to backtracking questions\" href=\"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">A general approach to backtracking questions</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/subsets/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/subsets/\" target=\"_self\">Subsets</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/combination-sum/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/combination-sum/\" target=\"_self\">Combination Sum</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/permutations/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/permutations/\" target=\"_self\">Permutations</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/subsets-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/subsets-ii/\" target=\"_self\">Subsets II</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/combination-sum-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/combination-sum-ii/\" target=\"_self\">Combination Sum II</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/word-search/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/word-search/\" target=\"_self\">Word Search</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/palindrome-partitioning/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/palindrome-partitioning/\" target=\"_self\">Palindrome Partitioning</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/letter-combinations-of-a-phone-number/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/letter-combinations-of-a-phone-number/\" target=\"_self\">Letter Combinations of a Phone Number</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/n-queens/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/n-queens/\" target=\"_self\">N Queens</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/kth-largest-element-in-a-stream/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/kth-largest-element-in-a-stream/\" target=\"_self\">Kth Largest Element in a Stream</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/last-stone-weight/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/last-stone-weight/\" target=\"_self\">Last Stone Weight</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/k-closest-points-to-origin/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/k-closest-points-to-origin/\" target=\"_self\">K Closest Points to Origin</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/kth-largest-element-in-an-array/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/kth-largest-element-in-an-array/\" target=\"_self\">Kth Largest Element in an Array</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/task-scheduler/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/task-scheduler/\" target=\"_self\">Task Scheduler</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/design-twitter/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/design-twitter/\" target=\"_self\">Design Twitter</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/find-median-from-data-stream/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/find-median-from-data-stream/\" target=\"_self\">Find Median from Data Stream</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/number-of-islands/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/number-of-islands/\" target=\"_self\">Number of Islands</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/clone-graph/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/clone-graph/\" target=\"_self\">Clone Graph</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/max-area-of-island/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/max-area-of-island/\" target=\"_self\">Max Area of Island</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/pacific-atlantic-water-flow/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/pacific-atlantic-water-flow/\" target=\"_self\">Pacific Atlantic Water Flow</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/surrounded-regions/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/surrounded-regions/\" target=\"_self\">Surrounded Regions</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/rotting-oranges/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/rotting-oranges/\" target=\"_self\">Rotting Oranges</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/walls-and-gates/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/walls-and-gates/\" target=\"_self\">Walls and Gates</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/course-schedule/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/course-schedule/\" target=\"_self\">Course Schedule</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/course-schedule-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/course-schedule-ii/\" target=\"_self\">Course Schedule II</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/redundant-connection/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/redundant-connection/\" target=\"_self\">Redundant Connection</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/\" target=\"_self\">Number of Connected Components in an Undirected Graph</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/graph-valid-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/graph-valid-tree/\" target=\"_self\">Graph Valid Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/word-ladder/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/word-ladder/\" target=\"_self\">Word Ladder</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/climbing-stairs/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/climbing-stairs/\" target=\"_self\">Climbing Stairs</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/min-cost-climbing-stairs/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/min-cost-climbing-stairs/\" target=\"_self\">Min Cost Climbing Stairs</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/house-robber/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/house-robber/\" target=\"_self\">House Robber</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/house-robber-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/house-robber-ii/\" target=\"_self\">House Robber II</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/longest-palindromic-substring/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/longest-palindromic-substring/\" target=\"_self\">Longest Palindromic Substring</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/palindromic-substrings/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/palindromic-substrings/\" target=\"_self\">Palindromic Substrings</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/decode-ways/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/decode-ways/\" target=\"_self\">Decode Ways</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/coin-change/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/coin-change/\" target=\"_self\">Coin Change</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/maximum-product-subarray/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/maximum-product-subarray/\" target=\"_self\">Maximum Product Subarray</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/word-break/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/word-break/\" target=\"_self\">Word Break</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/longest-increasing-subsequence/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/longest-increasing-subsequence/\" target=\"_self\">Longest Increasing Subsequence</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/partition-equal-subset-sum/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/partition-equal-subset-sum/\" target=\"_self\">Partition Equal Subset Sum</a> <br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/insert-interval/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/insert-interval/\" target=\"_self\">Insert Interval</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/merge-intervals/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/merge-intervals/\" target=\"_self\">Merge Intervals</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/non-overlapping-intervals/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/non-overlapping-intervals/\" target=\"_self\">Non-overlapping Intervals</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/meeting-rooms/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/meeting-rooms/\" target=\"_self\">Meeting Rooms</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/meeting-rooms-ii/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/meeting-rooms-ii/\" target=\"_self\">Meeting Rooms II</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://leetcode.com/problems/minimum-interval-to-include-each-query/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://leetcode.com/problems/minimum-interval-to-include-each-query/\" target=\"_self\">Minimum Interval to Include Each Query</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"LeetCode Problems Roadmap","level":1,"id":"LeetCode_Problems_Roadmap_0"},{"heading":"Array","level":2,"id":"Array_0"},{"heading":"Stack","level":2,"id":"Stack_0"},{"heading":"Two Pointers","level":2,"id":"Two_Pointers_0"},{"heading":"Linked List","level":2,"id":"Linked_List_0"},{"heading":"Sliding Window","level":2,"id":"Sliding_Window_0"},{"heading":"Binary Search","level":2,"id":"Binary_Search_0"},{"heading":"Trees","level":2,"id":"Trees_0"},{"heading":"Tries","level":2,"id":"Tries_0"},{"heading":"Backtracking","level":2,"id":"Backtracking_0"},{"heading":"Heap / Priority Queue","level":2,"id":"Heap_/_Priority_Queue_0"},{"heading":"Graphs","level":2,"id":"Graphs_0"},{"heading":"1-D Dynamic Programming","level":2,"id":"1-D_Dynamic_Programming_0"},{"heading":"Intervals","level":2,"id":"Intervals_0"}],"links":["algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html#_0","algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/leetcode/road-map.html","pathToRoot":"../..","attachments":[],"createdTime":1693680696995,"modifiedTime":1695308042284,"sourceSize":13635,"sourcePath":"Algorithms/Leetcode/Road Map.md","exportPath":"algorithms/leetcode/road-map.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"algorithms/algorithms.html":{"title":"Algorithms","icon":"","description":"\n<a data-href=\"Road Map\" href=\"algorithms/leetcode/road-map.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Road Map</a>\n<br><a data-href=\"Floyd’s Cycle Finding Algorithm\" href=\"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Floyd’s Cycle Finding Algorithm</a>\n<br><a data-href=\"A general approach to backtracking questions\" href=\"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">A general approach to backtracking questions</a>\n<br><a data-href=\"Heaps &amp; Priority Queues\" href=\"data-structures/heaps-&amp;-priority-queues.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Heaps &amp; Priority Queues</a>\n<br><a data-href=\"DFS vs BFS\" href=\"algorithms/algo/search/dfs-vs-bfs.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">DFS vs BFS</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://neetcode.io/roadmap\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://neetcode.io/roadmap\" target=\"_self\">neetcode</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Algorithms","level":1,"id":"Algorithms_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["algorithms/leetcode/road-map.html#_0","algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html#_0","algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html#_0","data-structures/heaps-&-priority-queues.html#_0","algorithms/algo/search/dfs-vs-bfs.html#_0"],"author":"","coverImageURL":"","fullURL":"algorithms/algorithms.html","pathToRoot":"..","attachments":[],"createdTime":1693680747190,"modifiedTime":1697824483947,"sourceSize":251,"sourcePath":"Algorithms/Algorithms.md","exportPath":"algorithms/algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/computer-vision/0.-image-processing.html":{"title":"0. Image Processing","icon":"","description":"\nFiltering: Gaussian filter: Blurs an image to remove high-frequency noise.\nimport cv2 img = cv2.imread('image.jpg')\nblur = cv2.GaussianBlur(img, (5, 5), 0)\ncv2.imshow('Blur', blur)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, load an image using cv2.imread(), apply a Gaussian filter using cv2.GaussianBlur(), and display the blurred image using cv2.imshow().\nMedian filter: Replaces the central pixel with the median value of the surrounding pixels to remove impulse noise.\nimport cv2 img = cv2.imread('image.jpg')\nmedian = cv2.medianBlur(img, 5)\ncv2.imshow('Median', median)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, we load an image using cv2.imread(), apply a median filter using cv2.medianBlur(), and display the filtered image using cv2.imshow().\nBilateral filter: Smoothes an image while preserving its edges.\nimport cv2 img = cv2.imread('image.jpg')\nbilateral = cv2.bilateralFilter(img, 9, 75, 75)\ncv2.imshow('Bilateral', bilateral)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, we load an image using cv2.imread(), apply a bilateral filter using cv2.bilateralFilter(), and display the filtered image using cv2.imshow().\nEnhancement: Histogram equalization: Improves the contrast of an image by spreading out the intensity values over a wider range.\nimport cv2 img = cv2.imread('image.jpg', 0)\nequ = cv2.equalizeHist(img)\ncv2.imshow('Equalized', equ)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, we load a grayscale image using cv2.imread() with the second argument set to 0 to load the image in grayscale, apply histogram equalization using cv2.equalizeHist(), and display the equalized image using cv2.imshow().\nContrast stretching: Expands the intensity range of an image to improve its contrast.\nimport cv2\nimport numpy as np img = cv2.imread('image.jpg', 0)\nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(img)\nstretch = np.uint8(255 * (img - min_val) / (max_val - min_val))\ncv2.imshow('Stretched', stretch)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, we load a grayscale image using cv2.imread() with the second argument set to 0 to load the image in grayscale, find the minimum and maximum pixel values using cv2.minMaxLoc(), apply contrast stretching using a formula that maps the pixel values to a wider range, and display the stretched image using cv2.imshow().\nSharpening: Enhances the edges of an image to make it appear sharper.\nimport cv2\nimport numpy as np img = cv2.imread('image.jpg')\nkernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\nsharp = cv2.filter2D(img, -1, kernel)\ncv2.imshow('Sharpened', sharp)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/computer-vision/0.-image-processing.html","pathToRoot":"../..","attachments":[],"createdTime":1741084156443,"modifiedTime":1737554783000,"sourceSize":2787,"sourcePath":"Artificial Intelligence/Computer Vision/0. Image Processing.md","exportPath":"artificial-intelligence/computer-vision/0.-image-processing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html":{"title":"1. Loss Function & Optimization","icon":"","description":"A loss function is a mathematical function that measures the difference between the predicted output of a model and the actual output. It provides a quantitative measure of the error between the predicted values and the true values. The choice of loss function is an important decision that can have a significant impact on the accuracy and generalization ability of the model. Commonly used loss functions in computer vision include mean squared error (MSE), cross-entropy loss, and hinge loss. Check:\n<a data-href=\"3.1 Cost function - Least Square\" href=\"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3.1 Cost function - Least Square</a>\n<br><a data-href=\"3.2 Cost Function - Logistic Regression\" href=\"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3.2 Cost Function - Logistic Regression</a>\nOptimization is the process of minimizing the loss function by adjusting the parameters of the model. In other words, it involves finding the set of model parameters that result in the smallest possible loss value. Optimization algorithms such as stochastic gradient descent (SGD), Adam, and Adagrad are commonly used in computer vision to update the model parameters during training. These algorithms adjust the parameters based on the gradient of the loss function, with the goal of finding the optimal set of parameters that minimize the loss.Check:\n<br><a data-href=\"2. Gradient descent\" href=\"artificial-intelligence/machine-learning/math/2.-gradient-descent.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. Gradient descent</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Loss Function &amp; Optimization","level":1,"id":"1._Loss_Function_&_Optimization_0"}],"links":["artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html#_0","artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html#_0","artificial-intelligence/machine-learning/math/2.-gradient-descent.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html","pathToRoot":"../..","attachments":[],"createdTime":1741084156447,"modifiedTime":1737554783000,"sourceSize":1260,"sourcePath":"Artificial Intelligence/Computer Vision/1. Loss Function & Optimization.md","exportPath":"artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/computer-vision/2.-neural-networks.html":{"title":"2. Neural Networks","icon":"","description":"Check:\n<a data-href=\"0. Neural Network Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">0. Neural Network Learning</a>\n<br><a data-href=\"Convolutional neural networks (CNNs)\" href=\"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Convolutional neural networks (CNNs)</a>\n<br><a data-href=\"1. Recurrent Neural Networks (RNNs)\" href=\"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Recurrent Neural Networks (RNNs)</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[".html","artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html#_0","artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/computer-vision/2.-neural-networks.html","pathToRoot":"../..","attachments":[],"createdTime":1741084156451,"modifiedTime":1737554783000,"sourceSize":127,"sourcePath":"Artificial Intelligence/Computer Vision/2. Neural Networks.md","exportPath":"artificial-intelligence/computer-vision/2.-neural-networks.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html":{"title":"1.1 Why Data Mining","icon":"","description":"The Explosive Growth of Data: from terabytes to petabytes\nData collection and data availability Automated data collection tools, database systems, Web, sensors and scientific instruments, computerised society Major sources of abundant data Business: Web, e-commerce, transactions, stocks, Web shopping…\nScience: Remote sensing, bioinformatics, instrumentation and simulation, astronomy, IoT Australia's ASKAP now generating 5.2 terabytes/sec (ie 1 024 Gb/s) [<a data-tooltip-position=\"top\" aria-label=\"http://theconversation.com/the-australian-square-kilometre-array-pathfinder-finally-hits-the-big-data-highway-71217\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://theconversation.com/the-australian-square-kilometre-array-pathfinder-finally-hits-the-big-data-highway-71217\" target=\"_self\">Conversation</a>] Society and everyone: news, digital cameras, YouTube, twitter, health records and imaging Government: Medicare claims, Tax returns, cash transaction reports,&nbsp; arrival cards, business surveys, census We are drowning in data, but starving for knowledge!\"Necessity is the mother of invention\"- hence Data mining—Automated analysis of massive data sets","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156474,"modifiedTime":1737554783000,"sourceSize":1110,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.1 Why Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html":{"title":"1.2 What is Data Mining","icon":"","description":"Definition:&nbsp;The process of discovering interesting patterns and knowledge&nbsp;from large amounts of data.Interesting&nbsp;patterns are&nbsp;non-trivial,&nbsp;implicit,&nbsp;previously unknown&nbsp;and&nbsp;potentially useful.This can also be described as&nbsp;building models&nbsp;of data&nbsp;and&nbsp;evaluating the models&nbsp;for usefulness.Alternative names:&nbsp;Knowledge discovery in databases (KDD), knowledge extraction, data analytics, data archaeology, data dredging, information harvesting, business intelligence, machine learning, pattern recognition and more.Data Mining vs Machine Learning vs Statistics?No strict distinction, but indicative of a research or business community.&nbsp;Machine Learning&nbsp;as induction, abduction and generalisation is older and arose as a branch of AI in the very early days of computing.&nbsp;Data mining&nbsp; (and KDD) arose from Database research in the early 90s.&nbsp; Machine learning may aim at more agent-oriented methods such as incremental, active and bio-inspired learning. Data mining is more likely to be batch- and large-data- oriented.&nbsp; Both schools draw heavily on&nbsp;Statistics&nbsp;for data characterisation. Some machine-learning/data-mining techniques also arise in Statistics e.g. Naive Bayes, Decision trees.Due to the very mixed heritage of data mining techniques, you will see data mining language arising from each of these three discipline areas, with very often different names used for the same or a very similar idea. For example, the very fundamental idea of an atomic chunk of data that contributes to the description of some object of interest is variously called an&nbsp;attribute, a&nbsp;feature, or a&nbsp;variable. This course primarily uses the language in the text, which is primarily derived from a&nbsp; database background, but as a practicing data miner, you must be able to recognise and interchangeably use the various terms.The Knowledge Discovery in Databases (KDD) process is a widely-used framework for data mining. It consists of several stages that help in discovering useful knowledge from large amounts of data. The following are the steps involved in the KDD process:\nData cleaning: In this stage, the data is preprocessed to remove noise, inconsistent data, and irrelevant attributes.\nData integration: In this stage, data from multiple sources are combined into a single dataset.\nData selection: In this stage, a subset of the data is selected for analysis.\nData transformation: In this stage, data is transformed into a form appropriate for mining. This may involve normalization, aggregation, or other transformations.\nData mining: In this stage, various data mining techniques are applied to the data to discover patterns, associations, and other insights.\nPattern evaluation: In this stage, the discovered patterns are evaluated to determine their usefulness and reliability.\nKnowledge representation: In this stage, the discovered patterns are represented in a form that can be easily understood by the user.\nDeployment: In this stage, the knowledge gained from the data is deployed to solve real-world problems. The KDD process is an iterative process, meaning that each stage can be revisited if necessary. It is important to note that the KDD process is not a linear process and that the steps may overlap or occur in a different order.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"KDD Process","level":1,"id":"KDD_Process_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156478,"modifiedTime":1737554783000,"sourceSize":3891,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.2 What is Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html":{"title":"1.3. What makes a pattern useful","icon":"","description":"One of the challenges in data mining is that automated tools can typically generate thousands or millions of patterns. Ultimately, not all can be used, so data miners and their tools need to be able to select&nbsp;interesting&nbsp;patterns for further study and possible application.Typically a pattern is&nbsp;interesting&nbsp;if\nit is easily understood by humans working in the domain of application\nit is&nbsp;valid&nbsp;(true), with some required degree of&nbsp;certainty, on sample data and real&nbsp; data in practice\nit is potentially useful or actionable\nit is novel, that is not already well known or else&nbsp;it validates a hypothesis that needed confirmation\nAn interesting pattern can be characterised as representing knowledge.However, not all of these conditions are achieved by all mining methods, and not all these conditions&nbsp;can&nbsp;be met in automated ways.Objective&nbsp;and automatable measures of pattern interestingness do exist, based on pattern structure and statistical measures.Commonly such a measure would need to exceed some user-selected&nbsp;threshold&nbsp;to be considered interesting, or alternatively, just some user-selected number of the&nbsp;best-ranked&nbsp;patterns by that measure are considered interesting. For example,&nbsp;support&nbsp;and&nbsp;confidence&nbsp;&nbsp;are used to assess association rules. These indicate respectively&nbsp; how much of the data the rule has any relevance to, and how much of the relevant data it is valid for. Coverage and accuracy&nbsp; have respectively&nbsp; similar roles for classification rules. The&nbsp;simplicity&nbsp;of the pattern is commonly assesed, too. The formulation of these objective measures may be&nbsp; specific to a mining method and is not directly comparable across methods.Subjective&nbsp;measures of interestingness, based on user beliefs about the data,&nbsp; are also generally required. These may be domain-dependent and it is typically the task of a data miner (you!) to know enough about the domain of the data and the proposed application of the mining results to be able to apply judgement here.&nbsp; Results will generally need to be communicated and shared with other domain experts prior to accepting and acting on them.Subjectively, interesting patterns may be&nbsp;unexpected (I did not know this!), actionable (I can use that result to change what we do),&nbsp;or&nbsp;confirming (I had a hunch that might be true, but now I have evidence for it)&nbsp;It is highly desirable for data mining methods to generate only interesting patterns for users. It is also efficient for data mining methods to use concepts of interestingness internally during the search for patterns, so that computations that will result in uninteresting patterns need not be explored. This means that in order for a method to be&nbsp;&nbsp;a&nbsp; viable candidate for a data mining problem, the notion of interestingness built in to the method must be aligned to the user's concept of interestingness associated with the problem.Interestingness, and how it can be used for efficient mining, is an important aspect of every data mining technique, and it affects the nature of problems to which the technique can be applied.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156483,"modifiedTime":1737554783000,"sourceSize":3451,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.3. What makes a pattern useful.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html":{"title":"1.4 What kind of patterns can be mined","icon":"","description":"\nData warehousing multidimensional data model\nData cube technology\nOLAP (online analytical processing) Multidimensional class or concept description: Characterization and discrimination\nGeneralize, summarize, and contrast data characteristics, e.g., dry vs. wet region\nData characterised by a user specified class description can be retrieved from a warehouse\nData discrimination can compare features of selected classes\nOLAP operations can be used to summarise data along user-specified dimensions Frequent patterns (or frequent itemsets) What prescribed drugs are frequently taken together? What welfare payments are frequently received together? Association, correlation vs. causality A typical association rule: Tertiary Education -&gt; Atheist&nbsp; [10%, 20%]&nbsp; (support, confidence)\nAre strongly associated items also strongly correlated? How to mine such patterns and rules efficiently in large datasets?\nHow to use such patterns for classification, clustering, and other applications? Classification&nbsp;&nbsp; Construct models (functions) based on some training examples (supervised&nbsp;learning)\nDescribe and distinguish classes or concepts for future prediction\nPredict unknown discrete class labels\ne.g., classify countries based on climate, or classify cars based on fuel efficiency Regression (also called&nbsp;numerical prediction) Construct models based on some training examples (supervised learning) Predict unknown continuous values e.g. predict weight from height and age, or predict precipitation based on geo-location and cloud patterns. Typical methods - Decision trees, naïve Bayesian classification, support vector machines, neural networks, rule-based classification, pattern-based classification, logistic regression, … Typical applications - Credit card or taxation fraud detection, direct marketing, classifying stars, diseases,&nbsp; web-pages, … ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Mining for Generalisation","level":4,"id":"Data_Mining_for_Generalisation_0"},{"heading":"Frequent Patterns, Association and Correlation Analysis","level":4,"id":"Frequent_Patterns,_Association_and_Correlation_Analysis_0"},{"heading":"Classification and Regression for Prediction","level":4,"id":"Classification_and_Regression_for_Prediction_0"},{"heading":"Clustering/ Cluster Analysis","level":4,"id":"Clustering/_Cluster_Analysis_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156486,"modifiedTime":1737554783000,"sourceSize":3361,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.4 What kind of patterns can be mined.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html":{"title":"1.5 Challenges in Data Mining","icon":"","description":"It does not take a lot of skill to pick up a data mining tool and to push its buttons to run some algorithms&nbsp; over some data and to get some output back again.&nbsp;&nbsp; But this approach will not provide the required insight into application problems and can be very harmful in some circumstances.The competent data miner&nbsp;must&nbsp;be capable of&nbsp;\nSelecting the right tool for the job. This will be informed by both the question being asked and the nature of the data that is used; &nbsp;Evaluating&nbsp; the output of the tool. What is it telling you? How good are the results? Is there a way of obtaining better results? What does \"better\" mean for the particular question being asked?; and\nInterpreting the results in the context of the question. What can be learnt from the results and&nbsp; for what purpose can that information be ethically and robustly used?&nbsp;&nbsp;\nThe following challenges are indicative of the issues which professional data miners encounter and for which data mining researchers attempt to create new methods.\nMining various and new kinds of knowledge\nMining knowledge in multi-dimensional space\nInterdisciplinary skills\nBoosting the power of discovery in a networked environment\nHandling noise, uncertainty, and incompleteness of data\nPattern evaluation and pattern- or constraint-guided mining Goal-directed mining\nInteractive mining\nIncorporation of background knowledge\nPresentation and visualization of data mining results\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Basic Competency","level":4,"id":"Basic_Competency_0"},{"heading":"Mining Methodology","level":4,"id":"Mining_Methodology_0"},{"heading":"Leveraging Human Knowledge","level":4,"id":"Leveraging_Human_Knowledge_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156490,"modifiedTime":1737554783000,"sourceSize":2075,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.5 Challenges in Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html":{"title":"1.6 Privacy Implications of Data Mining","icon":"","description":"\nPeople often feel that this information is private to themselves, and only the person in question has the right to decide who to share it with, and for what purpose.\nPeople often willingly or unconsciously but consentingly share such private information.\nThe information may be shared for reasons quite independently of data mining, but it may be mined for&nbsp;secondary use.\ne.g. credit card transactions, health records, personal financial records, individual biological traits, criminal or justice investigations, ethnicity, educational performance.\nNot all data mining uses personal data, although it may nevertheless be&nbsp;confidential&nbsp;data.\ne.g. natural resources, climate and weather, water storage and floods, astronomy, geography, geology, biology\nother scientific and engineering data\nAccess to individual records directly attributable to individual people is the most common source of concern. This data may leak in ways that have no connection to data mining (e.g Web shopping). Nevertheless, when it is collected initially for the purposes of mining, or when mining is a secondary use, the data miner should be aware of their legal and ethical obligations to protect privacy.The OECD (Organisation for Economic Cooperation),&nbsp;of which Australia is a member, It&nbsp; provides some useful&nbsp;principles&nbsp;for personal data, summarised here: Collection Limitation: Data should be collected by lawful and fair means, with the knowledge and consent of the individual where appropriate.\nData Quality: For the purpose that data is used, it should be relevant, accurate, complete and up-to-date.\nPurpose Specification:&nbsp;Data should be collected only for a specified purpose, and used for that purpose (although alternative uses are conditionally possible).\nUse Limitation: Data should not be disclosed or used for other purposes unless authorised by the person or by the law.\nSecurity Safeguards: Data should be protected by reasonable security safeguards.\nOpenness Principle:&nbsp; Developments, practices and policies with respect to data should be open, including the existence and nature of data, the main purposes of use, and the identity and contact method for a data controller.\nIndividual Participation: Individuals have the right to obtain confirmation of existence or absence of data relating to them, to obtain such data, to challenge such data, and if successful,&nbsp; to have the data erased, rectified, completed or amended.\nAccountability Principle: A data controller is accountable for complying with the principles.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data mining often uses information about individual people.","level":3,"id":"Data_mining_often_uses_information_about_individual_people._0"},{"heading":"What is fair use of the private data?","level":3,"id":"What_is_fair_use_of_the_private_data?_0"},{"heading":"How to protect privacy when data mining?","level":3,"id":"How_to_protect_privacy_when_data_mining?_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156522,"modifiedTime":1737554783000,"sourceSize":5844,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.6 Privacy Implications of Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html":{"title":"2.1 Data Types and Representations","icon":"","description":"Data is generally structured as&nbsp;objects,&nbsp;with each object described by some kind of&nbsp;properties, or&nbsp;attributes, or&nbsp;features,&nbsp; of some real-world or abstract entity. Some of those attributes may include&nbsp;relationships&nbsp;to other objects. The objects may alternatively be called&nbsp;samples, examples, instances, data points, observations, rows, tuples, records,&nbsp;or&nbsp;vectors.&nbsp;The attributes may be called&nbsp;fields, variables, dimensions, properties,&nbsp;or&nbsp;features.&nbsp;This will depend on the data model or data mining discipline you are working with, and you need to be comfortable with all of those names.Commonly, a&nbsp;data model&nbsp;&nbsp;(also called a&nbsp;meta-model)&nbsp;dictates how these concepts are materialised in a data structure, such as the relational data model, for example.&nbsp; Alternatively,&nbsp;&nbsp;data model&nbsp;(but not&nbsp;meta-model) could refer to a model that instantiates a particular meta-model for some domain of application, for example, the particular relational schema that is used in your hospital's medical records system.The type of an attribute defines the way its values for some object are represented, the ways it can be manipulated, and appropriate methods for mining.\ncategories, states, classes, or “names of things”\nalso called&nbsp;categorical\ne.g. Hair_color = {auburn, black, blond, brown, grey, red, white}\ne.g. marital status, occupation, ID numbers, postcodes Nominal attribute with only 2 values&nbsp; (commonly the values 0 and 1)\nSymmetric binary: both outcomes equally important e.g., resides-with-family: yes or no Asymmetric binary: outcomes not equally important.&nbsp; e.g., medical test (positive vs. negative). e.g. attended class (no, yes)\nConvention: assign 1 to most important outcome (e.g., HIV positive) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Attribute Types","level":4,"id":"Attribute_Types_0"},{"heading":"<strong><em>Nominal</em></strong>","level":5,"id":"***Nominal***_0"},{"heading":"<strong>*Binary</strong>","level":5,"id":"***Binary**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156611,"modifiedTime":1737554783000,"sourceSize":4305,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.1 Data Types and Representations.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html":{"title":"2.2 Basic Statistical Descriptions of a Single Data Variable","icon":"","description":"In statistics, central tendency is a measure that represents the typical or central value of a dataset. The three most common measures of central tendency are the mean, median, and mode.The mean is the sum of all values in a dataset divided by the number of values. It represents the average value of the dataset.Formula:where is the mean, are the values in the dataset, and is the number of values.The mean is sensitive to outliers, meaning that extreme values can skew the value towards one direction or the other.The median is the middle value of a dataset when the values are arranged in order. If the dataset has an odd number of values, the median is the middle value. If the dataset has an even number of values, the median is the average of the two middle values.Formula:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Mean","level":3,"id":"Mean_0"},{"heading":"Median","level":3,"id":"Median_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156622,"modifiedTime":1737554783000,"sourceSize":2597,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.2 Basic Statistical Descriptions of a Single Data Variable.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html":{"title":"2.3 Measuring the Dispersion of Data","icon":"","description":"Range is the difference between the maximum and minimum values in a dataset:Quartiles divide a dataset into four equal parts. The first quartile (Q1) is the 25th percentile, the second quartile (Q2) is the median, and the third quartile (Q3) is the 75th percentile.“quartile.png” could not be found.Interquartile Range (IQR) is the difference between the third quartile and the first quartile:Variance measures how much the data deviate from the mean:where is the mean of the data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Range, Quartile, Variance, Standard Deviation, and Interquartile Range","level":1,"id":"Range,_Quartile,_Variance,_Standard_Deviation,_and_Interquartile_Range_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156626,"modifiedTime":1737554783000,"sourceSize":3479,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.3 Measuring the Dispersion of Data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html":{"title":"2.4 Computation of Measures","icon":"","description":"\nWhen dealing with big data, computational cost of all processing stages should be considered.\nSome measures of data character, data summarization, and data pattern interestingness are computationally more difficult to compute than others.\nThe classification of measure functions as distributive, algebraic or holistic helps to understand their computational complexity. A function is distributive if the data is partitioned into n sets, and the function is applied to each of those sets.\nThen, suppose the function is applied to those n results, and the result is the same as applying the function to all of the data without partitioning.\nExamples include count(), sum(), min(), max().\nSuch computations can be made efficient by distributing the computation. A function is algebraic if it can be computed by a function with M arguments (where M is a bounded integer), each of which is obtained by applying a distributive function.\nExamples include average(), min_N(), max_N(), standard_deviation().\nSuch computations can take advantage of the distributive sub-functions to be made efficient. A function is holistic if there is no constant bound on the storage size needed to describe sub-functions.\nThere is no algebraic function with M arguments (where M is a bounded integer) that characterises the computation.\nExamples include median(), mode(), rank().\nSuch computations are difficult to compute efficiently and often efficient approximations are used instead.\nExercises:1.Suppose that the data for analysis includes the attribute&nbsp;age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70. (a) What is the mean of the data? What is the median? (b) What is the mode of the data? Comment on the data’s modality (i.e., bimodal, trimodal, etc.). (c) What is the midrange of the data? (d) Can you find (roughly) the first quartile (Q1) and the third quartile (Q3) of the data? (e) Give the five-number summary of the data. (f) Show a boxplot of the data.\n(a) The mean can be calculated by summing up all the values and dividing by the total number of values.The median (middle valueof the ordered set, as the number of values in the set is odd) of the data is: (b) The mode is the most common value in the dataset. In this case, there are two modes: 25 and 35. The data is bimodal.(c) The midrange is the average of the smallest and largest values in the dataset.(d) The first quartile (Q1) is the value below which 25% of the data falls. The third quartile (Q3) is the value below which 75% of the data falls.(e) The five-number summary consists of the minimum value, Q1, median, Q3, and maximum value:Minimum: 13 Q1: 20 Median: 25 Q3: 35 Maximum: 70\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Distributive","level":4,"id":"Distributive_0"},{"heading":"Algebraic","level":4,"id":"Algebraic_0"},{"heading":"Holistic","level":4,"id":"Holistic_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156641,"modifiedTime":1737554783000,"sourceSize":3487,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.4 Computation of Measures.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html":{"title":"2.5 Measuring Correlation amongst Two Variables","icon":"","description":"There are some measures commonly used to assess whether variables are&nbsp;correlated, that is that one can (approximately) be computed directly from the other.Pearson's Correlation\nMeasures how well a straight line can be fitted to two continuous attributes (one on each axis).\nAlso called Pearson's r, Pearson product-moment correlation, Pearson correlation coefficient, bivariate correlation\nRanges from -1 (perfectly negatively correlated), through 0 (no correlation) to 1 (perfectly positively correlated).\nThe formula for Pearson's Correlation is:where:\nn is the number of observations and are the ith observations of X and Y, respectively and are the means of X and Y, respectively\nHere's an example code snippet in Python to calculate Pearson's Correlation:import numpy as np # Sample data\nX = np.array([1, 2, 3, 4, 5])\nY = np.array([2, 4, 5, 7, 8]) # Calculate means\nx_mean = np.mean(X)\ny_mean = np.mean(Y) # Calculate Pearson's Correlation\nnumerator = np.sum((X - x_mean) * (Y - y_mean))\ndenominator = np.sqrt(np.sum((X - x_mean)**2) * np.sum((Y - y_mean)**2))\nr = numerator / denominator print(f\"Pearson's Correlation: {r}\")\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156644,"modifiedTime":1737554783000,"sourceSize":1775,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.5 Measuring Correlation amongst Two Variables.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html":{"title":"2.6 Measuring Similarity and Dissimilarity of Multivariate Data","icon":"","description":"We need algorithmic ways to assess how alike or unlike objects are to others. For example, are some set of taxpayers similar in their characteristics of age, employment status and income?\nNumerical measure of how alike two data objects are\nValue is higher when objects are more alike\nOften falls in the range [0,1] Numerical measure of how different two data objects are\nLower when objects are more alike\nMinimum dissimilarity is often 0\nUpper limit varies\nProximity refers to a similarity or dissimilarity Common data structure used to represent dissimilarity amongst objects\nUsed e.g. in main-memory based algorithms for clustering and nearest neighbour\nFor n data points, an n x n matrix data structure that shows dissimilarity or distance d(i,j) for all pairs of n objects i, and j\nd(i,j) is close to 0 when objects i, and j are near to each other, i.e. the distance is small\nIt is defined that d(i,j) = d(j,i) and d(i,i) = 0\nTherefore, the matrix is symmetric or triangular with zeros along the diagonal and duplicates (often not represented at all) in the upper right triangle\nSingle mode or one-mode matrix because rows and columns represent the same things (i.e. objects)\n“Dissimilarity Matrix.png” could not be found. Similarity, sim(i,j) is often expressed as a function of dissimilarity, e.g. when d(i,j) is normalised to [0,1] we may have sim(i,j) = 1 - d(i,j) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Similarity and Dissimilarity Measures","level":2,"id":"Similarity_and_Dissimilarity_Measures_0"},{"heading":"Similarity","level":3,"id":"Similarity_0"},{"heading":"Dissimilarity (e.g., distance)","level":3,"id":"Dissimilarity_(e.g.,_distance)_0"},{"heading":"Dissimilarity Matrix","level":3,"id":"Dissimilarity_Matrix_0"},{"heading":"Similarity from Dissimilarity","level":3,"id":"Similarity_from_Dissimilarity_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156648,"modifiedTime":1737554783000,"sourceSize":2113,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.6 Measuring Similarity and Dissimilarity of Multivariate Data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html":{"title":"2.7 Proximity Measures for Nominal Attributes","icon":"","description":"A nominal attribute may have two or more states as values (if only two, then it is binary).Let the number of states for a nominal attribute be .For convenience, we denote the states by integers but note that no ordering is implied here.Method 1: Simple matching\nLet be the number of matches i.e. the number of attributes for which object and have the same value.\nLet be the number of variables (i.e attributes for each object).\nThen, defineSimilarity may then be defined asMethod 2: Map nominals to a large number of binary attributesAlso called one-hot encoding.\nCreate a new binary attribute for each of the nominal values of the attribute.\ne.g. for attribute Colour = {red, blue, green}, introduce binary attributes Red, Blue and Green and map an object with Colour = red to the corresponding object with attribute Red = 1, attribute Blue = 0 and attribute Green = 0.\nThen use any method for the proximity of binary attributes.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156652,"modifiedTime":1737554783000,"sourceSize":1047,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.7 Proximity Measures for Nominal Attributes.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html":{"title":"2.8  Proximity Measures for Binary Attributes","icon":"","description":"\nBinary attributes have only one of two values: 0 or 1.\nThe attributes may be symmetric or asymmetric: asymmetric means the 1-value state is more important than the 0-value state.\nTreating the values as numeric can be misleading.\nThe methods presented in this subsection require that all the attributes of the objects are binary. Let us consider a pair of objects i and j with a total number of attributes p.\nLet q be the number of attributes that equal 1 for both i and j.\nLet r be the number of attributes that equal 1 for object i but 0 for object j.\nLet s be the number of attributes that equal 0 for object i but 1 for object j. Let t be the number of attributes that equal 0 for both object i and object j. Then p = q+r+s+t and we have the contingency table below. Then the dissimilarity is defined as the proportion of non-matching attributes amongst all attributes.\nDissimilarity for symmetric binary attributes d(i,j) = (r + s) / (q + r + s + t)\nThe agreement of two 1 values (i.e. the agreement of the important values, a positive match) counts more than the agreement of two 0 values (i.e. a negative match).\nDissimilarity is defined as the corresponding similarity, asymmetric binary similarity, is also called the Jaccard coefficient and is widely used. It is also called coherence.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Contingency Table","level":3,"id":"Contingency_Table_0"},{"heading":"Contingency table","level":3,"id":"Contingency_table_0"},{"heading":"Symmetric binary dissimilarity","level":3,"id":"Symmetric_binary_dissimilarity_0"},{"heading":"Asymmetric binary dissimilarity","level":3,"id":"Asymmetric_binary_dissimilarity_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156656,"modifiedTime":1737554783000,"sourceSize":2170,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.8  Proximity Measures for Binary Attributes.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html":{"title":"2.9 Normalisation of numeric data","icon":"","description":"Normalization of numeric data is a process in which the numeric data is rescaled to fit within a specific range, typically between 0 and 1. This is done to eliminate the effect of differences in the scale of variables in the dataset, which can lead to inaccurate and biased results in data analysis.Normalization is performed by subtracting the minimum value from each observation and then dividing it by the range of values. The range is calculated as the difference between the maximum and minimum values in the dataset.For example, if we have a dataset of exam scores ranging from 50 to 100, we can normalize the scores as follows:\nFind the range: range = maximum score - minimum score = 100 - 50 = 50\nSubtract the minimum value from each score: new score = original score - minimum score\nDivide each new score by the range: normalized score = new score / range\nAfter normalization, all scores will fall within the range of 0 and 1. This makes it easier to compare the scores across different variables or datasets.There are different methods of normalization, such as min-max scaling, z-score normalization, and decimal scaling. The choice of method depends on the specific requirements of the data analysis and the characteristics of the dataset.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156659,"modifiedTime":1737554783000,"sourceSize":1466,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.9 Normalisation of numeric data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html":{"title":"2.10 Minkowski Distance","icon":"","description":"Minkowski distance is a distance metric used to calculate the distance between two points in a multidimensional space. It is a generalization of the Euclidean distance and can be defined for any value of \"p\", where \"p\" is a positive real number.The Minkowski distance between two points x and yinn`-dimensional space is defined as:where p is the degree of the distance.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156615,"modifiedTime":1737554783000,"sourceSize":496,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.10 Minkowski Distance.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html":{"title":"2.11 Exercises","icon":"","description":"\nAn attribute&nbsp;favourite sport&nbsp;with values drawn from the set&nbsp;{soccer, rugby, hockey, netball, tennis, other}&nbsp;is&nbsp; likely to be of what attribute type?\nNominal attributes are categories, states, or “names of things”, with no expected ordering amongst the states. Also discrete because it has only a finite or countably infinite set of values. Nominal\nDiscrete Year, (e.g. 1960, 1984, 2000, 2005),&nbsp; according to the Gregorian calendar (i.e. the calendar we use every day), is a ratio-scaled attribute.\nIt&nbsp; does not have a \"true zero\" and you cannot sensibly talk about the ratio like&nbsp; 2005/1984. False The function average() is\nAlgebraic because it is not distributive itself (i.e you cannot compute it from averages of data subsets) but it can be computed from the&nbsp; 2 distributive functions sum() and count(). Both of those can be computed by summing and (respectively) counting the sums and (respectively) counts of subsets of data.&nbsp; Algebraic The approximate equality mean - mode ≈ 3 x (mean - median)\nis a&nbsp; rule-of thumb that works well for data with a single peak in the distribution and a spread that is not too asymmetric. The inter-quartile range\nis indicated by the length of the box on a box-plot diagram is the difference between the third quartile and the first quartile is useful to indicate the spread of the data Standard deviation is the square root of&nbsp;variance.\nTrue By Chebyshev's inequality,we know that\nAt least&nbsp;(1 - 1/k2)&nbsp;x&nbsp;100%&nbsp;of the observations are no more than&nbsp;k&nbsp;standard deviations from the mean.\nIf&nbsp; we set k to 2 , then this is a convenient explanation of how standard deviation measures dispersion.\nHow would you explain standard deviation to someone who does not understand it using k=2?\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. ","level":3,"id":"1._0"},{"heading":"2. ","level":3,"id":"2._0"},{"heading":"3. ","level":3,"id":"3._0"},{"heading":"4. ","level":3,"id":"4._0"},{"heading":"5. ","level":3,"id":"5._0"},{"heading":"6. ","level":3,"id":"6._0"},{"heading":"7. ","level":3,"id":"7._0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156618,"modifiedTime":1737554783000,"sourceSize":3365,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.11 Exercises.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html":{"title":"0. Basic concepts","icon":"","description":"What is a Data Warehouse?\nDefined in many different ways, but not rigorously.\nA decision support database that is maintained separately from the organisation’s operational database\nSupports information processing by providing a solid platform of consolidated, historical data for analysis.\nA data warehouse is a&nbsp;subject-oriented,&nbsp;integrated,&nbsp;time-variant, and&nbsp;nonvolatile&nbsp;collection of data in support of management’s decision-making process.”—W. H. Inmon\nData warehousing&nbsp;is the process of constructing and using data warehouses\nTypical Characteristics&nbsp; of a Data WarehouseSubject-Oriented\nOrganised around major subjects, such as customer, product, sales\nFocusing on the modelling and analysis of data for decision makers, not on daily operations or transaction processing\nProvides a simple and concise view around particular subject issues by excluding data that are not useful in the decision support process\nIntegrated\nConstructed by integrating multiple, heterogeneous data source e.g. relational databases, flat files, on-line transaction records\nData cleaning and data integration techniques are applied. Ensure consistency in naming conventions, encoding structures, attribute measures, etc. among different data sources\ne.g., Hotel price: currency, tax, breakfast covered, etc.\nWhen data is moved to the warehouse, it is converted (this process is called&nbsp;ETL, for Extract, Transform, Load) Time Variant\nThe time horizon for the data warehouse is significantly longer than that of operational systems Operational database: current value data\nData warehouse data: provide information from a historical perspective (e.g., past 5-10 years) Every key structure in the data warehouse Contains an element of time, explicitly or implicitly\nBut the key of operational data may or may not contain “time element” Nonvolatile\nA physically separate store of data transformed from the operational environment\nOperational update of data does not occur in the data warehouse environment Does not require transaction processing, recovery, and concurrency control mechanisms\nRequires only two operations in data accessing:&nbsp;Initial loading of data and access of data Comparison with typical operational databases:Most databases are used for&nbsp;online transaction processing (OLTP)&nbsp;and querying for day-to-day operations. On the other hand, data warehouses are built specifically for analytics to support decision making, that is,&nbsp;online analytical processing (OLAP).The key difference between OLTP and OLAP is that OLTP is focused on capturing and processing transactions in real-time, while OLAP is focused on analyzing and reporting on data over a longer period of time. As such, data warehouses are typically used to support strategic decision-making processes, while operational databases are used for day-to-day operations.“data warehouse comparision.png” could not be found.Multi-tiered Architecture:“data warehouse architecture.png” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156667,"modifiedTime":1737554783000,"sourceSize":4293,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/0. Basic concepts.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html":{"title":"1. Multi-dimensional Data Cubes","icon":"","description":"\nA data warehouse is based on a&nbsp;multidimensional data model&nbsp;which views data in the form of a data cube\nA data cube, such as&nbsp;sales, allows data to be modeled and viewed in multiple dimensions.\nThe cube axes are the&nbsp;dimensions&nbsp;such as item (e.g. item_name, brand, type), or time (e.g. day, week, month, quarter, year) or location (e.g. store, city, state, country) . The values of those dimensions (e.g. Item = home_entertainment, Quarter = Q4, City = Vancouver) uniqely identify a cube&nbsp;cell.&nbsp;\nThe cube cells hold&nbsp; the&nbsp;measures&nbsp;(such as dollars_sold,&nbsp; quantity-sold and quantity-returned).\n“data cube 1.png” could not be found. It is easy to see why this&nbsp; structure is commonly called an (n-dimensional)&nbsp;cube.&nbsp; However, in&nbsp;data warehousing literature,&nbsp;this structure may be called a&nbsp;cuboid&nbsp;as a component of a&nbsp;more elaborate&nbsp;&nbsp;data cube. An n-dimensional cuboid of unaggregated data is called a&nbsp;base cuboid. Measures along one or more dimensions of the base cuboid may be aggregated to form another&nbsp; cuboid with dimensions that are a subset of the original dimensions.\n- The top-most 0-D cuboid, which holds the highest-level of summarisation in a single cell, is called the&nbsp;apex cuboid,&nbsp;commonly denoted&nbsp;all. The lattice of cuboids forms a data cube. In this case the&nbsp;all&nbsp;cuboid contains a single cell&nbsp; that represents aggregated dollars_sold over all time, for all items,&nbsp; at all locations and for every supplier. An intermediate cuboid, for example the 2-D ( item, location) cuboid, will contain cells such as (Mars bar, Belconnen Mall) with measure $20,000, indicating that $20,000 worth of Mars Bars have been sold at the Belconnen Mall shop, over all time and for all suppliers. The bottom-most&nbsp;base&nbsp;cuboid contains ||time|| x || item|| x || location|| x ||supplier||&nbsp;cells, where each cell represents the dollars-sold for a possible combination of&nbsp; specific values for&nbsp;time, item, location and supplier. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156671,"modifiedTime":1737554783000,"sourceSize":2040,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/1. Multi-dimensional Data Cubes.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html":{"title":"2. Concept Hierarchies","icon":"","description":"A&nbsp;concept hierarchy&nbsp;defines a sequence of mappings from a set of low-level concepts to higher-level, more general concepts. They support the modelling of data at varying levels of abstraction. Geography (top level) Country State/Province City Store location At the top level, the data is aggregated by country, so you can see the total sales for each country where the retail business operates. As you move down the hierarchy, the data becomes more specific and granular, so you can see the total sales for each state/province within each country, then the total sales for each city within each state/province, and finally the total sales for each store location within each city.Another example of a concept hierarchy could be for a sales organization:\nTime (top level) Year Quarter Month Week Day At the top level, the data is aggregated by year, so you can see the total sales for each year. As you move down the hierarchy, the data becomes more specific and granular, so you can see the total sales for each quarter, then the total sales for each month within each quarter, and so on, down to the total sales for each day within each week.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Examples:","level":3,"id":"Examples_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156675,"modifiedTime":1737554783000,"sourceSize":1406,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/2. Concept Hierarchies.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html":{"title":"3. Modelling the Cube in a Relational Database","icon":"","description":"Need:\nDimension tables, such as item (item_name, brand, type), or time (day, week, month, quarter, year)\nA fact table that contains measures (such as dollars_sold) and keys to each of the related dimension tables\nStar schema: A fact table in the middle connected to a set of dimension tables. The primary key for the fact table is composed of a key for each dimension and the remaining attributes are the measures. Fact Table: - Sales (measures) Dimension Tables: - Product (attributes)\n- Time (attributes)\n- Geography (attributes)\nSnowflake schema:&nbsp; A refinement of star schema where some dimensional hierarchy is normalised into a set of smaller dimension tables, forming a shape similar to snowflake.Fact Table: - Sales (measures) Dimension Tables: - Product (attributes) - Product Category (attributes) - Product Subcategory (attributes)\n- Time (attributes) - Year (attributes) - Quarter (attributes) - Month (attributes) - Day (attributes)\n- Geography (attributes) - Country (attributes) - State/Province (attributes) - City (attributes) - Zip/Postal Code (attributes)\nFact constellation:&nbsp; Multiple fact tables share dimension tables, viewed as a collection of stars, therefore called galaxy schema or fact constellation.Snowflake and Constellation schemes are fairly obvious extensions of the basic Star schema to more complex data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156678,"modifiedTime":1737554783000,"sourceSize":1462,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/3. Modelling the Cube in a Relational Database.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html":{"title":"4. Data Mining in Data Warehouses","icon":"","description":"Three kinds of data warehouse applications&nbsp;(in order of increasing sophistication)(1) Information processing\nsupports querying, basic statistical analysis, and reporting using crosstabs, tables, charts and graphs provides information directly stored or aggregated. (2) Analytical processing:&nbsp;Online analytical processing (OLAP) multidimensional&nbsp;analysis of data warehouse data. supports basic OLAP operations, slice-dice, drilling, pivoting.\nderives information summarised at multiple granularites from user specified subsets (i.e user-specified concepts). (3) multidimensional data mining:&nbsp;&nbsp;also called&nbsp;Exploratory multidimensional data mining,&nbsp;Online Analytical Mining,&nbsp; and&nbsp;OLAM knowledge discovery from hidden patterns Many of the data mining methods covered in the course are implemented directly in warehouse platforms, which can assist in mining by easing data discovery and data preparation by close integration with mining.&nbsp;\nsupports finding associations, constructing analytical models, performing classification and prediction, and presenting the mining results using visualization tools\nautomated&nbsp;discovery of implicit patterns\nintegrates OLAP with data mining;&nbsp;OLAP can be used for inital exploration\nbenefits from the high quality of data warehouse data\ninformation processing infrastructure of the data warehouse can be used (access, APIs, storage, security, maintenance etc). The typical OLAP operations are implemented in OLAP servers.The SQL standard also defines some OLAP operators but these are generally implemented inconsistently in relational databases.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156682,"modifiedTime":1737554783000,"sourceSize":2917,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/4. Data Mining in Data Warehouses.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html":{"title":"5. Typical OLAP Operations","icon":"","description":"Rollup&nbsp;(also called&nbsp;drill-up) summarises data in one of two ways.1. By dimension reductionMove from one cuboid to another&nbsp;higher up the lattice towards the apex, where some dimension of the cube is removed and the remaining dimensions are represented by measures that are now aggregated over the missing dimension.2 . By climbing up the concept hierarchyMove from&nbsp;one cuboid to another by&nbsp;stepping up a level in a concept hierarchy on one dimension. This&nbsp; does not remove a dimension from the cube but aggregates the measures for that dimension into bigger chunks, and so reduces the number of data points along that dimension.Example:Consider the cube of&nbsp;sales&nbsp;with dimensions&nbsp;item x time x location&nbsp;&nbsp;and&nbsp; the concept hierarchy&nbsp; along the location dimension of (offices &lt; cities &lt; countries &lt; regions).\nStarting from the cuboid&nbsp;types x quarters x cities, &nbsp;a rollup operation:roll-up&nbsp;on location&nbsp; from cities to countrieswould move to the cubiod types x quarters x countries. Now, instead of being grouped by cities, the data is grouped by countries. Another subsequent rollup&nbsp; along the same&nbsp; location dimension would move to the cuboid&nbsp;&nbsp;types x quarters x regions.Drill-down&nbsp;&nbsp;(also called&nbsp;roll down) is the reverse of rollup, navigating from less detailed data to more detailed data. As for rollup, there are two ways to do this.1. SliceSlice cuts off one dimension of the cube, not by aggregating but by selecting only one fixed value along one dimension. It corresponds to a&nbsp; relational algebra&nbsp;select&nbsp; (to choose which fixed value from which dimension) then&nbsp;&nbsp;project&nbsp; (on the remaining dimensions). Example Consider the cube of&nbsp;sales&nbsp;with dimensions&nbsp;item x time x location.&nbsp;Starting from the cuboid&nbsp;types x quarters x cities, &nbsp;a slice operation:slice&nbsp;for time = \"Q1\"\nwould move to the cubiod types x&nbsp;cities&nbsp;and every value represented holds only for the quarter Q1. Another subsequent slice, say for&nbsp;item = \"computer\"&nbsp;&nbsp;would&nbsp; result in a cuboid&nbsp;&nbsp;for&nbsp;&nbsp;cities&nbsp;alone,&nbsp;with all data for each city being values only for computers in Q1.2. DiceDice&nbsp;cuts out&nbsp; a sub-cube, not by aggregating but by selecting multiple fixed values for each of multiple dimensions. When only one dimension is selected, a&nbsp;dice&nbsp;is a&nbsp;slice. It corresponds to some relational algebra&nbsp;selects&nbsp; to choose which fixed values from which dimensions. Example Consider the cube of&nbsp;sales&nbsp;with dimensions&nbsp;item x time x location.&nbsp;Starting from the cuboid&nbsp;types x quarters x cities, &nbsp;a dice&nbsp; operation:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Slice and Dice","level":4,"id":"Slice_and_Dice_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156770,"modifiedTime":1737554783000,"sourceSize":4723,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/5. Typical OLAP Operations.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html":{"title":"6. Processing OLAP queries","icon":"","description":"Data Warehouses contain huge volumes of data, yet aim to answer queries in interactive query time time-frames.One way to deliver fast response times is to pre-compute all the aggregate measures required, at a much increased storage cost.Data Warehouses must support efficient cube computation, access methods and query processing techniques.Data Cubes\nA Data cube can be viewed as a lattice of cuboids where each cuboid represents a choice of&nbsp;group-by&nbsp;attributes. The bottom-most cuboid is the base cuboid\nThe top-most cuboid (apex) contains only one cell\n“data cube 1.png” could not be found. The 3-D base cuboid&nbsp;represents the answer to queries such as&nbsp; \"What&nbsp; are the sales for each city, item and year?\".&nbsp;&nbsp;It is the least generalised (most specific) cuboid. The 2-D&nbsp; cuboids represent the answer to queries such as&nbsp; \"What is the sum of sales, grouping by city and item?\". The level-2 1-D cuboids represent the answer to queries such as&nbsp;&nbsp;\"_What is&nbsp;the sum of sales, grouping by city?\". The&nbsp; 0-D apex&nbsp;cuboid, often denoted&nbsp;all,&nbsp; represents the the answer to&nbsp;\"What is&nbsp;the sum of sales?\".&nbsp;It is the most generalised (least specific) cuboid. The total number of cuboids for this 3-D data cube is 2^3&nbsp;= 8. Compute cubeThis cube may be only conceptual, but it can also be&nbsp;materialised&nbsp;(that is, pre-computed) in full to reduce run-time query processing costs.It can be generated by a sequence of&nbsp; SQL group-by queries, one for each cuboid.In a generalised syntax, the complete data cube above could be defined by the querydefine cube sales_cube - city, item, year: sum(sales in dollars)And it&nbsp;could them be fully&nbsp;materialised as all 8 cuboids bycompute cube sales_cube&nbsp;In extended SQL bySELECT city, item, year, SUM(sales in dollars) FROM SALES GROUP BY CUBE (city, item, year)\nHowever, the full materialisation can be useful for fast query response times, but very expensive in storage.The number of cuboids in a data cube with n dimensions and m levels for each dimension is given by:C = (m+1)^n - 1where n is the number of dimensions and m is the number of levels for each dimension.The reason why there is an additional \"+1\" in the formula for the number of cuboids in a data cube is because of the inclusion of the base cuboid, which is the cuboid that contains the lowest level of aggregation for each dimension.For example, consider a data cube with 2 dimensions (i.e., n=2) and 2 levels for each dimension (i.e., m=2). Without the inclusion of the base cuboid, there would be 4 cuboids in total (2 cuboids for each dimension). However, with the base cuboid included, there would be an additional cuboid, bringing the total number of cuboids to 5.In general, for a data cube with n dimensions and m levels for each dimension, there are (m+1)^n possible combinations of levels for each dimension, including the base cuboid. Hence, the formula for the number of cuboids is (m+1)^n - 1.Question:Consider the&nbsp; (4D) &nbsp; cube of&nbsp;sales&nbsp;with dimensions&nbsp;item x time x location x supplier&nbsp;&nbsp;and&nbsp; the concept hierarchy&nbsp; along the location dimension of (offices &lt; cities &lt; countries &lt; regions)&nbsp;and the concept hierarchy along the time dimension of (days &lt; quarters &lt; years).“cuboids cal.png” could not be found.How many cuboids are there in the materialised data cube?T = 2 * 4 * 5 * 2 = 80 Precomputing all of a data cube is often prohibitively expensive in storage, and typically large parts of the cubiod are never used.We can\nMaterialise&nbsp;every&nbsp;(cuboid) (full materialisation),\nnone&nbsp;(no materialisation),\nor&nbsp;some (partial materialisation)\nThe latter is the typical choice, but it requires intelligent strategies for run-time query processing.In general, query processing must","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Cube Materialisation","level":4,"id":"Data_Cube_Materialisation_0"},{"heading":"Processing OLAP queries","level":4,"id":"Processing_OLAP_queries_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156774,"modifiedTime":1737554783000,"sourceSize":8000,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/6. Processing OLAP queries.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html":{"title":"7. Exercises","icon":"","description":"\nIn comparison with a traditional OLTP database, an OLAP data base is used for decision support\nhistorical rather than current and up-to-date\nneeds to support complex queries OLAP servers are typically categorised according to one of three architectures with various cost/benefit tradeoffs.&nbsp; Name the three architectures (in the uppercase acronym form) and state one benefit of each. ROLAP - uses familiar, scalable, relational technology\nMOLAP - efficient access to summarised data and efficient storage&nbsp; of sparse data\nHOLAP - scalability of ROLAP and fast computation of MOLAP\nmore details1. MOLAP (Multidimensional OLAP): It stores data in a multidimensional cube format and provides fast query response times. The benefit of MOLAP is that it supports complex calculations and aggregations, which can be processed very quickly due to its storage format. 2. ROLAP (Relational OLAP): It stores data in a traditional relational database management system (RDBMS) and provides good scalability and flexibility. The benefit of ROLAP is that it can handle large amounts of data, and can integrate with existing data warehouses and business intelligence systems. 3. HOLAP (Hybrid OLAP): It is a combination of MOLAP and ROLAP and provides the benefits of both approaches. HOLAP allows users to store summary data in a multidimensional cube format, while the detailed data is stored in a relational database. The benefit of HOLAP is that it can handle both detailed and summary data efficiently, while providing flexibility and scalability An OLAP&nbsp;roll-up&nbsp;operation is likely to be used to: To look at more general, i.e., more summarised data A&nbsp;slice&nbsp;differs from a&nbsp;dice&nbsp;by: Anything that can be done with a slice can also be done with a dice.\nA dice allows an interval of a dimension to be selected, for example to to ignore extremities, but a slice requires only a single value for a dimension to be selected.\nA slice can select over a single dimension only, but a dice can select over multiple dimensions.\nA slice operation moves to a sub-cuboid that is a plane (ie, a cuboid with the dimension reduced by one) of the current cuboid without any change to the&nbsp; measure values that remain. A&nbsp;pivot&nbsp;operation does not affect the data available in the cuboid under investigation.\nTrue.\nA pivot operation is a process in OLAP where the dimensions of a cube are rotated, allowing the user to view the data from a different perspective. This operation does not change the data available in the cube, but instead changes the way it is viewed. For example, if a user is viewing sales data by product and region, they can pivot the cube to view the same data by region and product. This does not affect the underlying data, only the way it is presented to the user. How many cuboids are generated from a base data cuboid with 11 dimensions, including the base and apex cuboids?\nFor a base data cuboid with 11 dimensions, there will be 2^11 cuboids, including the base and apex cuboids. Therefore, the total number of cuboids generated will be 2^11 = 2048.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. ","level":3,"id":"1._0"},{"heading":"2. ","level":3,"id":"2._0"},{"heading":"3. ","level":3,"id":"3._0"},{"heading":"4. ","level":3,"id":"4._0"},{"heading":"5. ","level":3,"id":"5._0"},{"heading":"6. ","level":3,"id":"6._0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156777,"modifiedTime":1737554783000,"sourceSize":10891,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/7. Exercises.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html":{"title":"4.1 Motivation","icon":"","description":"\nFrequent pattern mining discovers associations and correlations in itemsets in transactional and relational databases.\nTechniques were developed for commercial market basket analysis, which led to the birth of data mining and KDD from a database perspective in the 90s.\nThe language of frequent pattern mining assumes data items are grouped into transactions and aims to find patterns of items that occur in a high proportion of transactions.\nAn example is supermarket shopping, where mining revealed purchasers of nappies are also likely to purchase beer in the same transaction.\nFrequent pattern mining is used in many other applications, such as the pharmaceutical benefits scheme, shopping basket analysis, cross marketing, catalogue design, sale campaign analysis, Web click stream analysis, and DNA sequence analysis.\nOur work on frequent pattern mining focuses on association mining to learn patterns of association rules.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156875,"modifiedTime":1737554783000,"sourceSize":1045,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.1 Motivation.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html":{"title":"4.2 Basic Concepts","icon":"","description":"\nItem: an atomic piece of nominal data, e.g. apple, orange, or banana.\nItemset: A set of one or more items, denoted by , e.g. .\nk-itemset: An itemset of cardinality k, where k is a positive integer. Denoted by , e.g. is a 2-itemset.\nTransaction: A non-empty itemset with a unique identifier in a group called a database. Denoted by , where is a set of task-relevant transactions (or a dataset or a database), e.g. , , . While each transaction, , has a unique identifier, say , this is not always explicit in the transaction but is needed to ensure that multiple transactions containing the same items are distinguished when required. Alternatively, a database could be a bag of transactions, but more usually it is assumed to be a set, with multiple transactions containing the same itemset assumed to be distinguished via a .\nSupport Count: The number of transactions that contain an itemset, denoted by . Also called occurrence frequency, frequency, absolute support, and count, e.g. the support count of is 2 in dataset .\nSupport: The proportion of transactions that contain an itemset, denoted by . Also called relative support and sometimes frequency, e.g. the support of in dataset above is .\nFrequent Itemset: An itemset with support greater than or equal to a minimum support threshold, denoted by , where is a user-defined, application-specific parameter.\nAssociation Rule: An implication of the form , where , , , , and .\nRule Support: The proportion of transactions that contain every one of both 's and 's items. Denoted by , it measures the significance of the association rule in the dataset. Example: If and , then the rule support for the association rule is the proportion of transactions that contain all three items: .\nConfidence: The conditional probability of given , denoted by . Also called the support ratio, it is the support count of in as a proportion of the support count of in . Example: If and , then the confidence for the association rule is the proportion of transactions that contain both and , out of the transactions that contain : .\nStrong Rule: An association rule is strong when is frequent. Example: If the minimum support threshold is 0.5 and the minimum confidence threshold is 0.8, then the association rule is considered strong if has a support of at least 0.5 and the confidence of the rule is at least 0.8.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Preliminary Definitions","level":3,"id":"Preliminary_Definitions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156883,"modifiedTime":1737554783000,"sourceSize":3730,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.2 Basic Concepts.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html":{"title":"4.3 Interesting pattern","icon":"","description":"In addition to support and confidence which measure association, lift is used to evaluate patterns to measure dependence (also called correlation).LiftInspired by probability theory, itemset is considered independent of itemset if . Otherwise they are dependent and correlated.Therefore we define the lift of rule byThe numerator is the probability of the customer purchasing both, while the denominator is what the probability of purchasing both would have been if the items were independent.If then and are independent.If then they are positively correlated as the presence of one suggests the presence of the other. In other words, it suggests that each lifts the likelihood of the other.If the itemsets are negatively correlated and the occurrence of one itemset suggests the more likely absence of the other.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Lift","level":4,"id":"Lift_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156887,"modifiedTime":1737554783000,"sourceSize":4819,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.3 Interesting pattern.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html":{"title":"4.4 Frequent Itemset Mining","icon":"","description":"Association mining can be viewed as a two-step process:(1) Find all frequent itemsets--i.e. all the itemsets that occur at least times in the dataset.(2) Use the frequent itemsets to generate strong association rules that satisfy .The first step is much more computationally demanding than the second, especially when is low.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156890,"modifiedTime":1737554783000,"sourceSize":434,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.4 Frequent Itemset Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html":{"title":"4.5 Apriori algorithm","icon":"","description":"The&nbsp;Apriori algorithm&nbsp;is the archetypical algorithm for finding frequent itemsets. It implements an iterative level-wise search for frequent itemsets, where&nbsp; the k-itemsets (itemsets of cardinality&nbsp;k) at level&nbsp;k&nbsp;are used to explore the k+1-itemsets at level&nbsp;k+1.&nbsp;At each level, the transaction database is scanned to count items in transactions and&nbsp; to collect the items that satisfy minimum support.Apriori PropertyThe following observation is used to reduce the search space. This property is very closely related to the&nbsp;&nbsp;iceberg condition&nbsp;we saw in data cube materialisation.All non-empty subsets of a frequent itemset are also frequentRecall that a frequent itemset is one with at least&nbsp; least&nbsp;&nbsp;min_sup&nbsp;support, i.e. one that occurs in at least&nbsp;min_sup t_ransactions. Since each of its subsets occur in all those same transactions (and possibly in additional transactions as well),&nbsp; the support of sub-itemsets cannot be less than&nbsp;_min_sup&nbsp;either.A consequence of this is that&nbsp;if an itemset is&nbsp;not&nbsp;frequent, then any of its supersets cannot be frequent either&nbsp;(otherwise the apriori property would be violated).This can be used in a level-wise algorithm like the&nbsp;apriori algorithm&nbsp;to reduce the search space, because any k+1-itemset with a non-frequent k-itemset subset (all of which have already been found in the previous iteration) can be ignored.\nStart by scanning the database of transactions to determine the support of each item, i.e., the number of transactions in which it occurs.\nGenerate a list of frequent 1-itemsets by selecting those items whose support is greater than or equal to a predefined minimum support threshold.\nUse the frequent 1-itemsets to generate candidate 2-itemsets, i.e., pairs of items, by joining each frequent itemset with itself.\nScan the database again to determine the support of each candidate 2-itemset and generate a list of frequent 2-itemsets by selecting those whose support is greater than or equal to the minimum support threshold.\nUse the frequent k-itemsets to generate candidate (k+1) itemsets by joining each frequent itemset with itself.\nRepeat steps 4 and 5 until no new frequent itemsets are generated, i.e., until the list of frequent k-itemsets is empty.\nUse the frequent itemsets to generate association rules, i.e., rules of the form A→B, where A and B are disjoint itemsets and the support and confidence of the rule are greater than or equal to predefined minimum thresholds.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Steps:","level":3,"id":"Steps_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156894,"modifiedTime":1737554783000,"sourceSize":2867,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.5 Apriori algorithm.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html":{"title":"4.6 Generating Association rules","icon":"","description":"Note that all frequent itemsets and all their non-empty subsets already satisfy support, so we only need to check confidence of the rule, which is, for rule is For each frequent itemset ,generate all non-empty proper subsets of for each non-empty proper subset of if thenoutput the rule ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156935,"modifiedTime":1737554783000,"sourceSize":511,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.6 Generating Association rules.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html":{"title":"4.7 Efficient frequent Data Mining","icon":"","description":"An important optimization in frequent itemset mining algorithms based on the fact that if an itemset is frequent, then all of its non-empty subsets are also frequent. This allows for significant memory-saving in algorithms like Apriori.Example:\nIf a frequent itemset A contains 100 items, then all non-empty subsets of A, which includes more than 1.27 x 10^30 subsets, are also frequent itemsets. This is due to the fact that if an itemset is frequent, then all of its non-empty subsets are frequent as well.In data warehouse cuboids, a similar problem of combinatorial explosion is solved by looking at closed cells.Since every non-empty subset of a frequent itemset is also frequent, we don't need to explicitly store and count the support of every single subset. Instead, we only need to store and count the support of the individual items that make up the frequent itemsets. This significantly reduces the memory requirements and computation time for frequent itemset mining.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156939,"modifiedTime":1737554783000,"sourceSize":1029,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.7 Efficient frequent Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html":{"title":"4.8 Closed and Maximal Frequent","icon":"","description":"The concept of closed frequent itemsets and maximal frequent itemsets are used to efficiently mine frequent itemsets from large datasets without losing important information.An itemset X is considered closed in the dataset D if there is no other itemset Y in D that is a superset of X and has the same support count as X. This means that X is as big as it can be without reducing the support count. A closed frequent itemset is an itemset that is both closed and frequent, meaning it occurs in the dataset with a frequency that is at least the minimum support threshold.On the other hand, a maximal frequent itemset is an itemset that is frequent in the dataset and has no superset that is also frequent. In other words, it is as large as possible while still satisfying the minimum support threshold.The closed frequent itemsets with their support counts represent all the frequent itemsets in the dataset, including those that are not closed. However, the maximal frequent itemsets represent all the frequent itemsets but may not give the exact counts for all of them.By mining closed and maximal frequent itemsets, we can efficiently discover all frequent itemsets in a large dataset without generating redundant or unnecessary itemsets. This can significantly reduce the computational cost of frequent itemset mining while preserving the important information about the frequency of itemsets in the dataset.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156943,"modifiedTime":1737554783000,"sourceSize":1818,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.8 Closed and Maximal Frequent.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html":{"title":"4.9 Adavanced pattern Mining","icon":"","description":"Algorithms other than a-priori have been developed to reduce database scans or reduce main memory usage and these may scale better over large datasets.Some of the most well-known are:\nFP-growth, frequent pattern growth\nMining closed and max-patterns\nSome algorithms rely on representing transactions in a&nbsp;vertical&nbsp;form&nbsp; as &lt;item, set of TIDs&gt; instead of the&nbsp;horizontal&nbsp;form &lt;TID, set of items&gt; that we saw previously. However, many of these aternative approaches use the apriori property like the apriori algorithm.Mining Advanced PatternsSo far, we have spoken of very simple pattern mining where all the data in a transaction corresponds to multiple values drawn from a single nominal domain. Many extensions have been developed that&nbsp; apply more structure over the input data handled and the rules produced.&nbsp; For example,Multilevel association rules: employing a concept hierarchy to discover rules at different levels of abstraction,&nbsp; and to do association mining at each level&nbsp; in a top-down manner to prune some lower-level, low-support items from the search. Multidimensional associations:&nbsp;replacing items in itemsets by instantiated relational predicates,&nbsp; so that properties of&nbsp; key objects in the transaction can also participate in mining (e.g. the price of each item purchased) and that mined rules become more expressive. Rare patterns and negative patterns:&nbsp;looking for patterns&nbsp; that are surprisingly&nbsp;infrequent&nbsp;or that are strongly negatively correlated by lift.&nbsp; Apriori is inefficient in this scenario.Compressed or approximate patterns: looking for the&nbsp;top-k&nbsp;most frequent patterns or constraint-based patterns (for which the constraints can be used while pattern mining to reduce the search space), or clustering patterns so that a cluster representative itemset can&nbsp; stand for&nbsp; all the patterns in the cluster more efficiently. Sequential patterns: where the order of items is significant, for example in text or DNA sequences. Graph patterns:&nbsp;where frequent sub-graphs may be discovered in graph&nbsp; network structures.Until now, we have assumed that all&nbsp;items&nbsp;are pretty much the same, i.e. they&nbsp; can all be seen as alternative values of a single nominal variable (say,&nbsp;item) without any further structure. We can view the problem as single-dimensional analysis over the dimension&nbsp;item, although multiple&nbsp;items&nbsp;occur in a transaction.&nbsp; This assumption seriously limits the scope of application of association mining. Multi-dimensional patterns over nominal datawe mixed (controllable) lifestyle attributes with disease attributes for a person as indistinguishable items for a&nbsp;single-dimensional rule.&nbsp;Such rules are also called&nbsp;Boolean association rules&nbsp;since the binary&nbsp; presence or absence of an item is represented. &nbsp; Some advanced pattern mining methods rely on explicitly distinguishing the dimensions for&nbsp;multidimensional association rules, but the apriori algorithm does not do this. A simple solution to distinguish the attributes&nbsp; for single-dimensional algorithms like apriori is to transform the values of nominal attributes to explicit attribute-value pairs and so to transfer all distinct nominal domains to the one nominal item domain as required by a-priori.ExampleFor the domains&nbsp;Disease = (diabetes, cerebrovascular, liver),&nbsp; and&nbsp;Lifestyle = (drinking, overweight, smoking, exercise)&nbsp;a transaction such as {diabetes, overweight, exercise} would be transformed to the transaction&nbsp;{Disease=diabetes, Lifestyle=overweight, Lifestyle=exercise}&nbsp;and these can then be treated homogenously as if from a single domain.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Extended data types","level":4,"id":"Extended_data_types_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156946,"modifiedTime":1737554783000,"sourceSize":8141,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.9 Adavanced pattern Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html":{"title":"4.10 Exercises","icon":"","description":"[Association mining]&nbsp;is classically suited to [nominal] attributes where attribute values are interpreted as items associated in a set called a&nbsp; [transaction].&nbsp; A set of these, in turn,&nbsp; make up a database (or dataset) for mining.[numeric or continuous]&nbsp; attributes can be converted to [nominal] by grouping small ranges of values into categories, sometimes called \"bins\".[ordinal] attributes, including [binary]&nbsp; attributes, are&nbsp; treated as [nominal] attributes, meaning the information of the order is not used, although an alternative for [ordinal]&nbsp; attributes with a large range of possible values could be to bin into ranged categories as for [numeric or continuous] attributes.\nWhat is the definition of&nbsp;support&nbsp;of an itemset? Consider the dataset, D, of transactions as follows.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. ","level":3,"id":"1._0"},{"heading":"2. ","level":3,"id":"2._0"},{"heading":"3. ","level":3,"id":"3._0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156879,"modifiedTime":1737554783000,"sourceSize":5303,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.10 Exercises.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html":{"title":"0. Classification","icon":"","description":"\nClassification in machine learning is the process of building models to describe interesting classes of data, where the models are called classifiers. The classification models are constructed by a learning method or algorithm and can be used to classify unseen data into pre-determined class labels.\nThe models may also provide a compact summary of the data that is explanatory for humans. Typically, classification is binary, where objects are classified as belonging to a class or not, but generalizations to more than two classes are important. Classification is commonly used in science, marketing, fraud detection, performance prediction, medical diagnosis, and fault diagnosis. The distinction between classification and numerical prediction is that classification deals with categorical class labels, while numerical prediction builds models of continuous-valued functions to predict unknown or missing numeric values. Supervised learning is the classification approach where the training data has labels indicating the class of observations. Batch\nIncremental learning Unsupervised learning has no class labels in the training data, and the algorithm must find interesting classes with which to classify new data, a process commonly called clustering. Other problem variants in classification include semi-supervised learning and zero-shot learning, where the target classes lack labelled data. The process of constructing a classification model involves two main steps: Training phase In the training phase (supervised), a model is built from the labelled training dataset, where each tuple is assumed to belong to a predefined class. The training set is represented as T = {X: X = (x1, x2, ..., xn)}, where each xi is an attribute value and X belongs to some predefined class Cj. The model can be represented as classification rules, decision trees, mathematical formulae, or a \"black box\" and is used to predict the class label for some unlabelled tuple X. Classification phase. In the classification phase, the accuracy of the model is evaluated using independent test samples, and if it is considered acceptable, the model is used to classify new, unlabelled data tuples. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Model to describe Interesting classes","level":3,"id":"Model_to_describe_Interesting_classes_0"},{"heading":"Types of Classification","level":3,"id":"Types_of_Classification_0"},{"heading":"Construct and Apply","level":3,"id":"Construct_and_Apply_0"},{"heading":"Evaluation and Selection of Learning Methods:","level":3,"id":"Evaluation_and_Selection_of_Learning_Methods_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156954,"modifiedTime":1737554783000,"sourceSize":3640,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/0. Classification.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html":{"title":"1. Decision Tree Induction","icon":"","description":"\nA popular machine learning method used for classification and regression problems. The process of decision tree induction involves creating a tree-like model that classifies data by splitting it based on a set of attributes. At each level of the tree, the algorithm selects the attribute that best splits the data into classes, based on a criterion such as information gain or Gini impurity. This process is repeated recursively for each subset of data until a stopping criterion is met. The resulting tree can be used to classify new, unseen instances by following the path from the root to a leaf node, which indicates the class assignment. The basic, greedy, decision tree algorithm is a commonly used approach for decision tree induction. It follows a top-down, recursive approach to divide a dataset into smaller subsets based on the values of the input attributes. At each step, the algorithm chooses the attribute that provides the most information gain, which is a measure of the reduction in entropy or impurity of the dataset.\nThe basic, greedy decision tree algorithm selects the test attribute that optimizes the next step locally, without considering the effect on future steps. It uses a heuristic or statistical measure, such as information gain or Gini index, to choose the attribute that provides the most useful information for partitioning the data at a given node. However, this does not necessarily result in the best overall decision tree. This process is repeated recursively for each subset until a stopping criterion is met, such as reaching a maximum depth, a minimum number of instances in a leaf node, or a threshold for information gain.\nIt can handle both categorical and continuous attributes, but it requires a discrete set of thresholds for each continuous attribute. It may also suffer from overfitting, where the tree is too complex and fits the training data too closely, resulting in poor performance on new, unseen data. To prevent overfitting, the algorithm can be pruned by removing branches or nodes that do not improve the performance on a validation set. Additionally, ensemble methods, such as random forests or boosting, can be used to combine multiple decision trees to improve the predictive performance and reduce overfitting.\nAttribute selection methods are used to identify the most relevant features or attributes that contribute the most to the target variable in a given dataset. The main objective of attribute selection is to improve the performance of a machine learning algorithm by reducing the number of irrelevant and redundant features, and thus, simplifying the model.\nTechniques&nbsp; to choose a&nbsp;splitting criterion&nbsp;comprised of a&nbsp;splitting attribute&nbsp;and a&nbsp;split point&nbsp;or&nbsp;splitting subset\nAim to have partitions at each branch as&nbsp;pure&nbsp;as possible -- i.e. all examples at each sub- node&nbsp; belong in the same class.\n(or&nbsp;attribute selection measures) are used to choose the best splitting criterion. Information Gain, Gain ratio and Gini index are most popular. Information gain: biased towards multivalued attributes Gain ratio: tends to prefer unbalanced splits in which one partition is much smaller than the others Gini index: biased towards multivalued attributes\nhas difficulty when number of classes is large\ntends to favour tests that result in equal-sized partitions and purity in both partitions Filter methods: These methods use statistical measures to rank the importance of each feature and select a subset of relevant features. These measures are applied to the dataset independently of the machine learning algorithm.\nWrapper methods: These methods use a specific machine learning algorithm to evaluate the performance of different subsets of features, and select the subset that yields the best performance.\nEmbedded methods: These methods incorporate the attribute selection process into the model-building algorithm itself, so that the selection and building of the model are done simultaneously. This approach is often used in algorithms that have built-in feature selection capabilities, such as decision trees and support vector machines.\ncheck: <a data-href=\"5. Feature Selection\" href=\"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">5. Feature Selection</a>\nCHAID: a popular decision tree algorithm, measure based on χ2 test for independence\nC-SEP: performs better than info. gain and gini index in certain cases\nG-statistic: has a close approximation to χ2 distribution\nMDL (Minimal Description Length) principle&nbsp;(i.e., the simplest solution is preferred): The best tree as the one that requires the fewest number of bits to both (1) encode the tree, and (2) encode the exceptions to the tree Multivariate splits&nbsp;(partition based on multiple variable combinations), e.g.&nbsp;CART: finds multivariate splits based on a linear combination of attributes.\nInformation Gain is a measure of the amount of information gained about a class variable when a certain attribute is used to split the data into subgroups. The basic idea behind Information Gain is to calculate the difference between the entropy of the target variable before and after the split on the given attribute. The attribute with the highest Information Gain is chosen as the splitting attribute.Information Gain where: is the original set of examples is the attribute being considered for splitting the set is the subset of examples in that have the value for attribute is the entropy of the set is the entropy of the subset The entropy formula is:where is a set of examples, is the number of classes, is the proportion of examples in that belong to class .Let attribute&nbsp;A&nbsp;be a continuous-valued attributeTo determine the&nbsp;best split point&nbsp;for A Sort the values of&nbsp;&nbsp;A&nbsp;in increasing order Typically, the&nbsp;midpoint between each pair of adjacent values&nbsp;is considered as a possible split point (ai+ai+1)/2&nbsp;is the midpoint between the values of&nbsp;a_i&nbsp;and&nbsp;_ai+1 Of these, the point with the minimum expected information requirement for&nbsp;A, InforA(D) is selected as the split-point for&nbsp;A Then Split:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Decision tree induction","level":3,"id":"Decision_tree_induction_0"},{"heading":"Basic, greedy, decision tree algorithm","level":3,"id":"Basic,_greedy,_decision_tree_algorithm_0"},{"heading":"Attribute Selection Methods","level":3,"id":"Attribute_Selection_Methods_0"},{"heading":"Splitting rules","level":4,"id":"Splitting_rules_0"},{"heading":"Heuristics","level":4,"id":"Heuristics_0"},{"heading":"Three main types of attribute selection methods:","level":4,"id":"Three_main_types_of_attribute_selection_methods_0"},{"heading":"Other Attribute Selection Methods","level":4,"id":"Other_Attribute_Selection_Methods_0"},{"heading":"Information Gain","level":3,"id":"Information_Gain_0"},{"heading":"Information Gain for continuous-valued attributes","level":4,"id":"Information_Gain_for_continuous-valued_attributes_0"}],"links":["artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156958,"modifiedTime":1737554783000,"sourceSize":9630,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/1. Decision Tree Induction.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html":{"title":"2. Overfitting and tree pruning","icon":"","description":"\nOverfitting is a common problem in decision tree induction. It occurs when the model is too complex and fits the training data too well, leading to poor generalization on new, unseen data. Overfitting can lead to poor predictive performance, and the decision tree may become too large and complex, making it difficult to interpret.\nTree pruning is a technique used to prevent overfitting by reducing the size of the decision tree. It involves removing branches from the tree that do not contribute to improving its accuracy on new data. The goal is to create a smaller tree that is more interpretable and has better predictive performance.\nPre-pruning, which involves stopping the tree construction process before it reaches a fully-grown tree. This is done by setting a threshold on some measure of impurity or information gain, such that if the gain falls below the threshold, the tree is not expanded further. This can help to prevent overfitting by limiting the complexity of the decision tree. Post-pruning, which involves constructing a complete decision tree and then removing branches to create a smaller tree. This is done by recursively removing the subtrees with the least improvement in accuracy on a validation set. The decision tree is then pruned at the node where removing the subtree leads to the highest improvement in accuracy. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Problems","level":3,"id":"Problems_0"},{"heading":"Tree Pruning","level":3,"id":"Tree_Pruning_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156966,"modifiedTime":1737554783000,"sourceSize":1574,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/2. Overfitting and tree pruning.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html":{"title":"3. Enhancements to the Basic Algorithm","icon":"","description":"Handle missing attribute values\nAssign the most common value of the attribute\nAssign probability to each of the possible values\nAttribute construction\nCreate new attributes based on existing ones that are sparsely represented\nThis reduces fragmentation, repetition, and replication\nContinuous target variable\nIn this case the tree is called a&nbsp;regression tree, the leaf node classes are represented by their mean values, and&nbsp; the tree performs prediction (using that mean value) rather than classification.\nProbabilistic classifier\nInstead of majority voting to assign a class label to a leaf node, the&nbsp;proportion&nbsp;of&nbsp;training data&nbsp;objects of each class in the leaf node can be interpreted to as the&nbsp;probability&nbsp;of the class, and this probability can be assigned to the classification&nbsp; for unknown objects falling in that node at use-time. A useful process for understanding the decision-making process and explaining the results to stakeholders. A decision tree can be transformed into a set of if-then rules, which can be used to classify new data. The rules can also be analyzed to identify patterns and gain insights into the data. Traversing the decision tree from the root node to the leaf nodes and creating rules for each path. The rule for each path is created by combining the conditions along the path to form an if-then statement.\nThe condition of each internal node is combined using the logical AND operator, while the condition at the leaf node is used as the output or decision.\nThe rules can be simplified by removing redundant conditions and combining similar rules. This process is called rule pruning and can be performed manually or using algorithms. The resulting rules can be expressed in a human-readable format and used to explain the decision-making process to stakeholders\nMutually exclusive We cannot have rule conflicts here because no two rules will be triggered for the same tuple.\nWe have one rule per leaf, and any tuple can map to only one leaf. Exhaustive: There is one rule for each possible attribute–value combination, so that this set of rules does not require a default rule.\nThe order of rules is irrelevant. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Extracting rules from decision trees","level":3,"id":"Extracting_rules_from_decision_trees_0"},{"heading":"Process of rule extraction","level":3,"id":"Process_of_rule_extraction_0"},{"heading":"Properties of extracted rules","level":3,"id":"Properties_of_extracted_rules_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156999,"modifiedTime":1737554783000,"sourceSize":2743,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/3. Enhancements to the Basic Algorithm.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html":{"title":"4. Rule-Based Classification","icon":"","description":"Rules are a good way of representing information or bits of knowledge. A rule-based classifier uses a set of IF-THEN rules for classification.\n\"IF\" part Rule antecedent / precondition\nCondition consists of one or more attribute tests that are logically ANDed \"Then\" part Rule consequent\nContains a class prediction How to measure the quality of a single rule&nbsp;R? Use both&nbsp;coverage&nbsp;and&nbsp;accuracy.\nThe key thing to note is that the accuracy of a rule is in proportion to its coverage, not to the size of the dataset as a whole.&nbsp;\nn_covers&nbsp;= number of tuples covered by&nbsp;R,&nbsp;i.e. that satisfy the antecedent of&nbsp;R\nn_correct&nbsp;= number of tuples correctly classified by&nbsp;R&nbsp;i.e that satisfy both the antecedent and the consequent\nD =&nbsp;training data set\ncoverage(R) =&nbsp;n_covers&nbsp;/|D| the proportion of tuples that are covered by the rule&nbsp; accuracy(R) =&nbsp;n_correct&nbsp;/&nbsp;n_covers\nProportion of covered tuples that are correctly labelled\nKnowledge graph learning, is usually called&nbsp;standard confidence. If more than one rule is triggered, need conflict resolution\nSize ordering: assign the highest priority to the triggering rules that has the “toughest” requirement i.e., with the most attribute tests Rule ordering: prioritise the rules beforehand by class-based or rule-based Class-based ordering: prioritise classes beforehand. If a tuple is classified into multiple classes, choose a class by the class order.\nRule-based ordering:&nbsp;the rules are organised into one long priority list, according to some measure of rule quality ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"IF-THEN rules for classification","level":3,"id":"IF-THEN_rules_for_classification_0"},{"heading":"Rule Evaluation","level":3,"id":"Rule_Evaluation_0"},{"heading":"Conflict Resolution","level":3,"id":"Conflict_Resolution_0"},{"heading":"Default Rule","level":3,"id":"Default_Rule_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157002,"modifiedTime":1737554783000,"sourceSize":3903,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/4. Rule-Based Classification.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html":{"title":"5. Bayes Classifiers","icon":"","description":"\nBayesian classifiers are a family of probabilistic classifiers based on Bayes' theorem, which assumes that the probability of a particular class given an observation or input is proportional to the probability of that observation given the class. These classifiers are widely used in various fields, including natural language processing, machine learning, and computer vision.\nThe Bayes theorem states that:where: is the probability of class given the observation , is the probability of observation given class is the prior probability of class is the probability of observation .\nTo classify an observation or input x, the Bayesian classifier calculates the probability of each class given the observation x, and then selects the class with the highest probability as the predicted class.Bayesian classifiers can be trained using a set of labeled training data, and can be used to classify new and unseen data. They are relatively simple and efficient, and can be used in a variety of classification tasks. However, they may not be suitable for complex classification problems with a large number of features or classes.The Bayesian classifier is an incremental method, which means that it can adapt over time to gradual or incremental changes in labelled training data.\nIt is a \"black box\" method, which means that it is not easily interpretable by humans, although its relationship to its training data is straightforward to understand.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Limitation","level":3,"id":"Limitation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157005,"modifiedTime":1737554783000,"sourceSize":6786,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/5. Bayes Classifiers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html":{"title":"6. Evaluation of Classifiers","icon":"","description":"\nThe confusion matrix is a tool for evaluating the performance of a classifier on a binary classification problem.\nIt is a 2 by 2 matrix where each entry represents the number of tuples classified as belonging to a particular class (positive or negative) versus the actual class of those tuples.\nThe entries of the confusion matrix are calculated as follows: True Positives (TP): number of positive tuples that were correctly labeled as positive by the classifier.\nTrue Negatives (TN): number of negative tuples that were correctly labeled as negative by the classifier.\nFalse Positives (FP): number of negative tuples that were incorrectly labeled as positive by the classifier.\nFalse Negatives (FN): number of positive tuples that were incorrectly labeled as negative by the classifier. The TP and TN entries indicate correct predictions, while the FP and FN entries indicate incorrect predictions.\nThe confusion matrix can be used to calculate various evaluation metrics such as accuracy, precision, recall, and F1-score.\nAccuracy is the proportion of correctly labeled tuples out of the total number of tuples. However, it can be misleading if the dataset is imbalanced.\nPrecision is the proportion of true positive predictions out of all positive predictions, while recall (or sensitivity) is the proportion of true positive predictions out of all actual positive tuples.\nF1-score is the harmonic mean of precision and recall, and is useful when you want to balance precision and recall.\nIt's important to choose an appropriate evaluation metric based on the specific problem and the domain of application.\nConfusion matrices can also be used for multiclass classification problems by extending the matrix to include all possible combinations of predicted and actual classes.\nGeneral Format:\nThe distribution of classes in a dataset is not balanced. This can lead to skewed performance evaluation, particularly if the evaluation measure used is accuracy. When dealing with a rare class, a high accuracy may not indicate good classification performance.\nInstead, evaluation measures such as sensitivity, specificity, precision, recall, and F1-score can be used to better assess the performance of the classifier. Consider a cancer classification problem where only a small proportion of patients have cancer. A classifier with high accuracy may still fail to identify many cancer patients, leading to a high rate of false negatives. In this case, sensitivity (the proportion of actual positives that are correctly identified by the classifier) becomes a more relevant evaluation measure than specificity (the proportion of actual negatives that are correctly identified by the classifier). Precision and recall are alternative measures that can be useful in evaluating classification performance, particularly when the classes are imbalanced. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive tuples. These measures are often combined into an F1-score, which provides a single measure of performance that balances precision and recall. However, it's important to choose the appropriate evaluation metric for the specific problem and domain of application, as different evaluation measures may emphasize different aspects of classification performance. The accuracy or error rate on the training data is not a good way to judge how well a model will perform on new data because the model may have learned to remember the training data instead of learning to generalize to new data. This is called overfitting and can result in poor performance on new data.\nAccuracy or error rate on the training data is not a reliable indicator of future performance, as the model may be overfitting to the training data, leading to poor performance on new, unseen data.\nThe holdout method is a technique used to estimate the performance of a model on new, unseen data by randomly partitioning the available data into two sets: a training set, used for model construction, and a test set, used for accuracy estimation.\nTypically, around two-thirds of the data is used for training and one-third for testing, although this split can vary depending on the size and nature of the dataset.\nThe holdout method provides a simple and effective way to estimate the generalization error of a model and to compare the performance of different models. However, it has some limitations, such as potentially yielding unstable estimates for small datasets, and potentially introducing bias if the test set is not representative of the overall population.\nThe difference between the holdout method and other training and testing approaches lies mainly in the way the data is split. In the holdout method, the data is usually split into two roughly equal parts, whereas other approaches such as k-fold cross-validation involve splitting the data into multiple parts and using each part in turn for testing and training. In this method, the given data is randomly partitioned into three different sets Training set\nValidation set\nand Test set The training set is used to construct a classification model, while the validation set is used to find the best parameters for the model. This is typically done by studying the effect of various parameters on the accuracy or other quality measures of the training set. Finally, the test set is used to measure the final performance of the model, as it would be reported.\nThis method is useful when you want to experiment with the parameters of the model-building algorithm. By having a separate validation set, you can ensure that you're not overfitting to the training set and that you're selecting the best parameters for the model. The test set provides a final evaluation of the model's performance on unseen data. It's important to keep the test set separate from the training and validation sets to avoid bias and ensure that the reported performance is accurate.\nCross-validation is a popular technique used in machine learning to evaluate the accuracy of a model's performance. The most commonly used method is k-fold cross-validation, where k=10, which is preferred due to its low bias and variance. In this method, the data is randomly partitioned into k mutually exclusive subsets, each of approximately equal size. In each iteration, one of the subsets is used as the test set, while the remaining subsets are used as the training set. The leave-one-out method is a special case for small-sized datasets, use k-fold where k is equal to the number of tuples. Another special case is stratified cross-validation, where folds are not randomly selected but stratified so that the class distribution in each fold is approximately the same as that in the initial data to achieve low bias. The overall accuracy of the model is computed by averaging the accuracy of each model on its respective test set. Works well for small data sets where, otherwise, the requirement to split the data into training and testing sets makes both sets too small for purpose.\nSamples the given training tuples uniformly with replacement, i.e., each time a tuple is selected, it is equally likely to be selected again and added to the training set again. Acommon one is .632 bootstrap. A data set with tuples is sampled times, with replacement, resulting in a training set of samples. The data tuples that did not make it into the training set end up forming the test set. About 63.2% of the original data end up in the training set, and the remaining 36.8% form the test set (since if is very large).\nRepeat the sampling procedure times. The overall accuracy of the model is:\nwhere is the accuracy of the model trained with training set when it is applied to test set , and is the accuracy of the model obtained with training set when it is applied to the training set .In comparing two classifiers, M1 and M2, mean error rates are obtained using 10-fold cross-validation. While it may seem intuitive to choose the classifier with the lowest error rate, these mean error rates are only estimates of future data cases. There may also be a chance that the difference between the two error rates is attributed to chance. In order to confirm the statistical significance, confidence limits are obtained for the error estimates. Once the confidence limits have been obtained, it can be concluded whether one classifier is statistically better than the other.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Confusion Matrix","level":3,"id":"Confusion_Matrix_0"},{"heading":"Evaluation Measures:","level":3,"id":"Evaluation_Measures_0"},{"heading":"Class Imbalance Problem","level":3,"id":"Class_Imbalance_Problem_0"},{"heading":"Cancer example","level":4,"id":"Cancer_example_0"},{"heading":"Estimating a classifier’s accuracy","level":3,"id":"Estimating_a_classifier’s_accuracy_0"},{"heading":"Holdout method","level":3,"id":"Holdout_method_0"},{"heading":"Training / Validation / Test","level":3,"id":"Training_/_Validation_/_Test_0"},{"heading":"Cross-validation","level":3,"id":"Cross-validation_0"},{"heading":"Bootstrap","level":3,"id":"Bootstrap_0"},{"heading":"Several bootstrap methods","level":4,"id":"Several_bootstrap_methods_0"},{"heading":"Comparing classifiers using t-tests","level":3,"id":"Comparing_classifiers_using_t-tests_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157009,"modifiedTime":1737554783000,"sourceSize":18194,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/6. Evaluation of Classifiers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html":{"title":"7. Linear Regression & Neural Nets","icon":"","description":"Check: <a data-href=\"1. Linear Regression\" href=\"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Linear Regression</a><br>Check: <a data-href=\"0. Neural Network Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">0. Neural Network Learning</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"7. Linear Regression &amp; Neural Nets","level":1,"id":"7._Linear_Regression_&_Neural_Nets_0"},{"heading":"Linear Regression","level":3,"id":"Linear_Regression_0"},{"heading":"Neural Network Learning","level":3,"id":"Neural_Network_Learning_0"}],"links":["artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html#_0",".html"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157013,"modifiedTime":1737554783000,"sourceSize":125,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/7. Linear Regression & Neural Nets.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html":{"title":"8. SVM, Lazy Learners, & Variants","icon":"","description":"Check: <a data-href=\"5. Support Vector Machine\" href=\"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">5. Support Vector Machine</a><br>Check: <a data-href=\"7. Lazy Leaner\" href=\"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">7. Lazy Leaner</a>\nLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple\nEager learning (the discussed methods so far): Given a set of training tuples, constructs a classification model before receiving new (e.g., test) data to classify\nLazy: less time in training but more time in predicting\nEager: must commit to a single hypothesis that covers the entire instance space\nAccuracy\nLazy method effectively uses a richer hypothesis space since it uses many local linear functions to form an implicit global approximation to the target function\nEager: must commit to a single hypothesis that covers the entire instance space\nLazy Learner: Instance based method\nInstance-based learning Store training examples and delay the processing (“lazy evaluation”) until a new instance must be classified Typical approaches k-nearest neighbor approach (KNN) Instances represented as points in a Euclidean space. Locally weighted regression Constructs local approximation Case-based reasoning Uses symbolic representations and knowledge-based inference ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"8. SVM, Lazy Learners, &amp; Variants","level":1,"id":"8._SVM,_Lazy_Learners,_&_Variants_0"},{"heading":"Support vector machines (SVMs)","level":3,"id":"Support_vector_machines_(SVMs)_0"},{"heading":"Lazy Learners","level":3,"id":"Lazy_Learners_0"},{"heading":"Lazy vs. Eager Learning","level":3,"id":"Lazy_vs._Eager_Learning_0"}],"links":["artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html#_0","artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157017,"modifiedTime":1737554783000,"sourceSize":1413,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/8. SVM, Lazy Learners, & Variants.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html":{"title":"9. Classification, Decision Trees, Bayes, Evaluation","icon":"","description":"Consider the Athletics dataset, D, above which is a training set for a decision tree classifier aiming to predict the variable \"wins\" from age and club membership. Let be the probability that an arbitrary tuple in D belongs to class , of m classes estimated by . Then the information needed to classify a tuple in D is defined by and after using attribute A to split D into v partitions, the information needed is So the information gain by splitting is defined as What is the information gain for splitting D on the three categories of attribute \"age\"? To calculate the information gain for a given attribute A, we first need to calculate the information needed to classify a tuple in D, , and the information needed to classify a tuple in D after using A to split D into v partitions, . We can then calculate the information gain as the difference between these two values. is the entropy of the class distribution in D, and can be calculated as formula:where is the probability that an arbitrary tuple in D belongs to class , and m is the number of classes. Also, can be written as:\nI(p,n) = -(p/(p+n)log_2(p/(p+n)) + n/(p+n) log_2(n/(p+n)))In this case, we have two classes: \"yes\" and \"no\". So we can calculate :Next, we need to calculate , which is the weighted average of the entropy of the class distribution in each partition created by splitting D on attribute A. It can be calculated as:where is the number of tuples in partition j, and |D| is the total number of tuples in D.To calculate the information gain for each attribute, we need to split D on that attribute and calculate for each resulting partition. We can then calculate the information gain as the difference between ) and for each attribute.For example, if we split D on the attribute \"age\", we get three partitions:\nPartition 1: age = \"child\", with 2 tuples (1 \"yes\" and 1 \"no\")\nPartition 2: age = \"youth\", with 1 tuple (1 \"yes\" and 0 \"no\")\nPartition 3: age = \"adult\", with 2 tuples (1 \"yes\" and 1 \"no\")\nThe entropy of the class distribution in each partition can be calculated as: Next, we need to calculate the weighted average of the entropy of the class distribution in each partition:Therefore, the information gain of splitting the data on \"age\" into three partitions with the updated class distribution is:This means that splitting the data on \"age\" into three partitions with the updated class distribution results in a reduction in the entropy of the class distribution in D by 0.171 bits, which is the amount of information gained by using the attribute \"age\" to split the data. If we split the data on the attribute \"club\" into two partitions based on the categories \"red\" and \"blue\", we get the following partitions: To calculate the entropy of the class distribution in each partition, we can use the same formula as before: Next, we need to calculate the weighted average of the entropy of the class distribution in each partition:Therefore, the information gain of splitting the data on the attribute \"club\" into two categories with the updated class distribution is:This means that splitting the data on the attribute \"club\" into two categories with the updated class distribution results in a reduction in the entropy of the class distribution in D by 0.100 bits, which is the amount of information gained by using the attribute \"club\" to split the data. A higher information gain from splitting an attribute indicates that the attribute is more useful for predicting the target variable, and splitting on that attribute will result in a greater reduction in the entropy of the class distribution in the data.In the context of a decision tree classifier, we want to choose the attribute that provides the highest information gain as the root of the tree, as this will result in the most effective split of the data and the most accurate predictions for the target variable.So in the case of the Athletics dataset, we saw that splitting on the attribute \"age\" resulted in a higher information gain than splitting on the attribute \"club\", indicating that \"age\" is a more useful attribute for predicting \"wins\". Therefore, we would choose \"age\" as the root of the decision tree and split the data accordingly.And using Naive Bayes to predict the class that maximises Calculate the prior probability of an arbitrary&nbsp; customer buying a computer (i.e belonging to the class buys_computer = yes) regardless of other information about the customer. Calculate the prior probability of an arbitrary&nbsp; customer&nbsp;not&nbsp;buying a computer (i.e belonging to the class buys_computer = no) regardless of other information about the customer. Calculate the&nbsp; likelihood probability P(age=middle_aged)| buys=yes) number of instances where age = middle_aged and buys_computer = yes = 2 number of instances where buys_computer = yes = 3\nAs a result: Calculate&nbsp; the likelihood probability P(credit=fair|&nbsp; buys=yes) number of instances where credit = fair and buys_computer = yes = 2\nnumber of instances where buys_computer = yes = 3\nlikelihood probability = 2 / 3 = 0.67 (rounded to 2 decimal places) Calculate the likelihood&nbsp;P(X = (middle-aged, fair) | buys = yes) P((middle-aged, fair) | buys = yes))\n= P(age=middle_aged)| buys=yes) P(credit=fair|&nbsp; buys=yes)\n= (2/3) (2/3) = 4/9 Using - Calculate P(X = (middle_aged, fair)) = P(X = (middle_aged, fair) | buys_computer = yes) P(buys_computer = yes) = (4/9 3/7) = 12/63 Calculate the likelihood probability P(age=middle_aged| buys=no) 2/4 = 1/2 Calculate&nbsp; the likelihood probability P(credit=fair|&nbsp; buys=no) 2/4 = 1/2 Calculate the likelihood&nbsp;P(X = (middle-aged, fair) | buys = no) 1/4 Calculate P(X = (middle-aged, fair) | buys = no) * P(buys = no) 1/4 * 4/7 = 1/7 So X = (middle_aged, fair) is classified as someone who buys a computer, right? True as 12/63 &gt; 1/7. Information:\nTrue Positives = 90\nTrue Negatives = 9560\nFalse Positives = 140\nAccuracy = Correct predictions / Size of the data\nError Rate = 3.5%\nError Rate = ( False Positives + False Negatives )/ Total Size of Data Say you have a labelled dataset D of 50 labelled objects and you want to evaluate the performance of your fantastic new learning algorithm you wrote yourself, called Z.&nbsp; So you decide to evaluate the&nbsp; performance of Z using 5-fold cross-validation. This would be a very small dataset and you need to get all you can out of the data to train your models.&nbsp; Maybe use leave-one-out, or better,&nbsp; bootstrap.\nNeed 5 disjoint subsets and you make 5 training sets by each combination of 4 of them.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157021,"modifiedTime":1737554783000,"sourceSize":10590,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/9. Classification, Decision Trees, Bayes, Evaluation.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html":{"title":"10. Other Classification and Prediction Methods","icon":"","description":"\nYou use multiple linear regression to predict the value for a response variable yearsto_live from 4 numeric variables, age, income, weight and numchildren. Your trained model gives intercept = 9.0 and coefficients $w{age}$ = 0.13 and = 0.0000 and = 0.19 and = 0.5. What does your model predict for the years to live for Joe who is 30 years old, earns $70000 p.a., weighs 80 kg and has no children? Give your answer to 1 decimal place.\nyears_to_live = 9.0 + 0.13*30 + 0.0000*70000 + 0.19*80 + 0.5*0 = 9.0 + 3.9 + 15.2 + 0 = 28.1 Multiple linear regression and linearly separable SVM are similar in that they both find a hyperplane, but different because linear regression&nbsp; finds a plane that&nbsp;minimises&nbsp;the distance to the&nbsp;target&nbsp;variable (numerical) values in the training set wheras the latter finds a plane that&nbsp;maximises&nbsp;the distance to the independent (input)&nbsp;variable values of each class in the training set.\nTrue The SVM plane maximises the distance to the independent variable values&nbsp; (specifically the distance to the support vectors, which are those&nbsp; objects that turn out to be the minimally distant ones from the plane)&nbsp; but also must divide the dependent variable (which is a class label) into two, ie one class on each side of the plane. SVM does not care how far away the target variable (which is a class label) is from the plane. While a low RMSE and a high R-squared are great things to have, often when evaluating a linear regression model&nbsp; it is a good idea to look at a&nbsp; scatter plot of the predicted values for objects vs the observed values, together with the ideal line showing predicted = observed.&nbsp; This may not be objective, but it is a useful subjective view of the quality.\nTrue An analysis of variance (ANOVA) over a linear model built using linear regression can identify the input variables of high signficance in the model, using a t-test to determine the significance.&nbsp; Highly significant variables\na.have non-zero coefficients in the model b.are typically indicated by \"***\" c.can be regarded as the most influential variables on the response variable One of the things&nbsp; for the data miner to decide when training a neural network model&nbsp; is what&nbsp;activation function&nbsp;to use. What is the role of an activation function in a neural network learner?\nTo produce an output value of a network node based on a function of the weighted linear sum of input values and a bias value\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156962,"modifiedTime":1737554783000,"sourceSize":7867,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/10. Other Classification and Prediction Methods.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html":{"title":"1. Basics","icon":"","description":"\nA collection of data objects that are similar or related to each other within the same group, but dissimilar or unrelated to objects in other groups. Cluster analysis, also known as clustering or data segmentation, aims to find similarities between data objects based on their characteristics and group them into clusters. It is an unsupervised learning technique, meaning there are no predefined classes. Stand-alone tool: Clustering can be used independently to gain insights into the distribution of data.\nPreprocessing step: Clustering is often used as a preprocessing tool for other algorithms, such as regression, principal components analysis, classification, and association analysis. Summarization: It can be used to summarize data and reduce its complexity.\nCompression: In image processing, clustering techniques like vector quantization can be used for compression purposes.\nFinding K-nearest Neighbors: Clustering helps in localizing searches to a specific cluster or a small number of clusters.\nOutlier detection: Outliers, which are data points significantly distant from any cluster, can be identified using clustering methods. Biology: Taxonomy of living things, such as grouping organisms into kingdom, phylum, class, etc.\nInformation retrieval: Document clustering to organize and retrieve similar documents.\nLand use: Identification of areas with similar land use patterns in Earth observation databases.\nMarketing: Discovery of distinct customer groups to develop targeted marketing strategies.\nCity planning: Grouping houses based on their type, value, and geographical location.\nEarthquake studies: Clustering observed earthquake epicenters along continental faults.\nClimate research: Understanding Earth's climate by identifying atmospheric and oceanic patterns.\nEconomic science: Market research and identifying market segments.\nA good clustering method is characterized by producing high-quality clusters. These clusters exhibit the following characteristics:\nHigh intra-class similarity: The objects within a cluster should be cohesive and similar to each other.\nLow inter-class similarity: The clusters should be distinct from each other, exhibiting dissimilarity between clusters. Similarity measure: The method should employ an appropriate similarity measure or distance function, denoted as d(i, j), to quantify the similarity between data objects. Different types of variables (interval-scaled, boolean, categorical, ordinal ratio, and vector variables) may require different distance functions.\nImplementation: The effectiveness of the clustering method is influenced by its implementation, including the algorithms and techniques used.\nAbility to discover hidden patterns: A good clustering method should be able to uncover some or all of the hidden patterns within the data, providing valuable insights. Similarity or dissimilarity metrics: Similarity between data objects is typically expressed using a distance function, such as d(i, j). The definitions of distance functions differ depending on the types of variables being considered. It is essential to assign appropriate weights to different variables based on the application and the semantic meaning of the data. Quality of clustering: In addition to the similarity metric, clustering methods often incorporate a separate \"quality\" function to evaluate the goodness of a cluster. This function assesses the overall quality of the clustering results. However, determining what constitutes \"similar enough\" or \"good enough\" clusters is often subjective, and there is no universally defined threshold. Single level vs. hierarchical partitioning: Choosing between a single-level partitioning approach or a hierarchical partitioning approach, where multi-level hierarchical partitioning is often desirable. Exclusive vs. non-exclusive: Determining whether clusters should be exclusive (e.g., each data point belongs to only one cluster) or non-exclusive (e.g., a data point can belong to multiple clusters).\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Cluster","level":3,"id":"Cluster_0"},{"heading":"Typical applications of clustering","level":3,"id":"Typical_applications_of_clustering_0"},{"heading":"Purposes:","level":3,"id":"Purposes_0"},{"heading":"Different domains and applications:","level":3,"id":"Different_domains_and_applications_0"},{"heading":"Quality of Clustering","level":3,"id":"Quality_of_Clustering_0"},{"heading":"The quality of a clustering method depends on several factors","level":4,"id":"The_quality_of_a_clustering_method_depends_on_several_factors_0"},{"heading":"Measuring the Quality of Clustering:","level":4,"id":"Measuring_the_Quality_of_Clustering_0"},{"heading":"Algorithmic Considerations","level":3,"id":"Algorithmic_Considerations_0"},{"heading":"Partitioning criteria:","level":4,"id":"Partitioning_criteria_0"},{"heading":"Separation of clusters:","level":4,"id":"Separation_of_clusters_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157029,"modifiedTime":1737554783000,"sourceSize":9085,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/1. Basics.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html":{"title":"2. Partitioning method","icon":"","description":"Partitioning a database D of n objects into a set of k clusters. The quality of cluster can be measured by the within-cluster variation, which is the sum of squared distances between all objects in and the centroid , defined as: Given k, the goal is to find a partition of k clusters that optimizes the chosen partitioning criterion. This can be achieved using the following methods: Globally optimal: Exhaustively enumerate all possible partitions and select the one with the minimum within-cluster variation. However, this approach is computationally expensive and not feasible for large datasets. Heuristic methods: These are more efficient and commonly used in practice. Two popular heuristic methods are: k-means and k-medoids algorithms k-means algorithm: In this algorithm, each cluster is represented by the calculated mean center of the cluster. The steps involved are: Arbitrarily choose k objects to be the initial cluster centroids.\nAssign each object to the cluster that has the closest centroid.\nUpdate the cluster means by calculating the mean value of the objects for each cluster.\nRepeat the assignment and update steps until the assignments no longer change. k-medoids (or PAM) algorithm: In this algorithm, each cluster is represented by one of the objects in the cluster, called a medoid. The steps are similar to k-means, but instead of using the mean center, the medoid is chosen as the representative. k-modes algorithm: This is a variant of k-means for categorical data, where each cluster is represented by the most frequent category in the cluster. “K-MEANS.png” could not be found.(a) Green points denote the data set in a two-dimensional Euclidean space. The initial choices for centres for and are shown by the red and blue crosses, respectively.(b) Each data point is assigned either to the red cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta line, they lie on.(c) In the subsequent step, each cluster centre is re-computed to be the mean of the points assigned to the corresponding cluster.(d)–(i) show successive steps through to final convergence of the algorithm.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Strength and Weakness","level":3,"id":"Strength_and_Weakness_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157033,"modifiedTime":1737554783000,"sourceSize":9859,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/2. Partitioning method.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html":{"title":"3. Hierachical Clustering","icon":"","description":"A method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types: Agglomerative: This is a \"bottom up\" approach: each observation starts in its own cluster, and a pair of clusters is merged in each step of moving up the hierarchy. Divisive: This is a \"top down\" approach: all observations start in one cluster, and a cluster is split into two at each step of moving down the hierarchy. In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendogram.Here is an example of the agglomerative and divisive hierarchical clustering approaches on data objects {a,b,c,d,e}.“hierarchical_clustering.png” could not be found.Initially, the agglomerative method places each object into a cluster of its own. The clusters are then merged step-by-step according to some criterion. The merging process is repeated until all the objects are eventually merged to one cluster.The divisive method proceeds in the oppostive direction. All the objects are used to form one initial cluster. The cluster is split according to some principle. The splitting process repeats until each new cluster contains only a single object.\nUses the single-link method for determining the distance (dissimilarity) between clusters. Other methods can instead be applied, see Distance between clusters, together with a dissimilarity matrix\nMerges nodes that have the least dissimilarity\nGo on until all nodes are in the same cluster Inverse order of AGNES\nGo on until each distinct data object forms its own cluster\nWhether using an agglomerative method or a divisive method, a core need is to measure the distance between two clusters, , where each cluster is a set of objects. Single link (minimum distance, nearest-neighbour clustering): smallest distance between an element in one cluster and an element in the other\ni.e., Complete link (maximum distance): largest distance between an element in one cluster and an element in the other\ni.e., Average (average distance): average distance between an element in one cluster and an element in the other\ni.e., Centroid: distance between the centroids of two clusters\ni.e., Medoid: distance between the medoids of two clusters\ni.e., A binary-tree-structured diagram, called a dendrogram, is commonly used to represent the process of hierarchical clustering.&nbsp;It shows how objects are grouped together (in an agglomerative method) or partitioned (in a divisive method) step-by-step.&nbsp;The similarity of&nbsp;the cluster pairs selected at the step of their agglomeration or division may be shown on a similarity scale. A final clustering of the data objects is obtained by cutting the dendrogram at the desired level, then each connected component at that level forms a cluster. The desired level is usually determined by selecting a threshold for&nbsp; similarity amongst clusters, but the desired number of clusters could be a factor too. Here's an example of a dendrogram on data objects&nbsp;{a,b,c,d,e}","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Hierarchical clustering","level":3,"id":"Hierarchical_clustering_0"},{"heading":"AGNES (AGglomerative NESting)","level":4,"id":"AGNES_(AGglomerative_NESting)_0"},{"heading":"DIANA (DIvisive ANAlysis)","level":4,"id":"DIANA_(DIvisive_ANAlysis)_0"},{"heading":"Distance between Clusters","level":3,"id":"Distance_between_Clusters_0"},{"heading":"Dendrogram","level":3,"id":"Dendrogram_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157036,"modifiedTime":1737554783000,"sourceSize":3995,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/3. Hierachical Clustering.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html":{"title":"4. Density-Based Methods","icon":"","description":"\nAll points within the cluster C are mutually density-connected, and\nThere is no point outside C that is density-connected to a point inside C.\nExample of density-reachable and density-connected:\n“dbscan (1).png” could not be found.\nLet \\epsilon be the radius of the circles and MinPts 3. m,p,o, r are core objects. Object q is directly density-reachable from m. Object m is directly density-reachable from p and vice versa. Object q is density-reachable from p because q is directly density reachable from m and m is directly density-reachable from p. However, p is not density reachable from q because q is not a core object. r and s are density-reachable from o o is density-reachable from r. o, r, and s are all density-connected.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition of Cluster in DBSCAN","level":3,"id":"Definition_of_Cluster_in_DBSCAN_0"},{"heading":"A subset C <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2286\"></mjx-c></mjx-mo></mjx-math></mjx-container></span> D is a cluster if","level":4,"id":"A_subset_C_$\\subseteq$_D_is_a_cluster_if_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157040,"modifiedTime":1737554783000,"sourceSize":880,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/4. Density-Based Methods.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html":{"title":"5. Grid-Based Approach","icon":"","description":" Space driven approach by partitioning the input space into cells. Multiresolution grid data structure approach is used: quantize the object space into a finite number of cells that forms a grid structure. Fast processing time which is independent of the number of data objects. Possible clusters are predefined by the design of the grid and therefore the defined allocation of objects to grid cells; the problem becomes to retrieve the clusters that statisfy a query that includes statistical properties of desirable clusters.In STING, the input space is divided in a hierarchical way\nAt the first layer, the input space is divided into some rectangular cells.\nEach cell at a high level is partitioned to form a number of cells at the next level.\n“sting.png” could not be found.\n-&gt; Hierarchical structure for STING clustering.\nPrecomputed statistical parameters: Statistical informations such as count, mean, maximum, and minimum, of each grid cell is precomputed and stored.\nStatistics of the bottom level cells are directly computed from the data.\nStatistics of a higher level cell can be computed based on lower-level cells Answer spatial data queries using precomputed statistics Top down approach: Start from a pre-selected layer - typically with a small number of cells\nCompute the confidence interval reflecting the cell’s relevance to the given query Irrelevant cells are removed\nChildren of remaining cells will be examined\nRepeat until the bottom layer is reached The regions of relevant cells, which satisfy the query, are returned\nQuery example: Select the maximal regions that have at least 100 houses per unit area and at least 70% of the house prices are above $400K and with total area at least 100 units with 90% confidence “example_sting.png” could not be found.-&gt; Example of clustering result obtained from a query (Wang et al, 1997)Advantage\nGrid-based computation is query-independent\nGrid structure facilitates parallel processing and incremental updating\nComputational efficiency: STING goes through the database once to compute the statistical parameters of the cells\nDisadvantage\nSensitive to bottom level granularity If the granularity is very fine, the cost of processing will increase substantially if the bottom level of the grid structure is too coarse, it may reduce the quality of cluster analysis All the cluster boundaries are either horizontal or vertical, and no diagonal boundary is detected\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Grid-based clustering","level":3,"id":"Grid-based_clustering_0"},{"heading":"STING: STatistical INformation Grid","level":3,"id":"STING_STatistical_INformation_Grid_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157044,"modifiedTime":1737554783000,"sourceSize":2816,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/5. Grid-Based Approach.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html":{"title":"6. Evaluation of Clustering","icon":"","description":"\nBefore you start: Assess clustering tendency. Assess whether a non-random structure exists in the data. Clustering analysis on a data set is meaningful only when there is a non-random structure in the data. Next:&nbsp;Determine the number of clusters in a dataset. How many clusters are there to find? Like k-means, many methods require the number of clusters in advance as a parameter to the method. After clustering: Measure the clustering quality. There are various quality measures according to different criteria.\nClustering tendency assessment is a method used to determine if a given dataset has a non-random structure that can lead to meaningful clusters. It involves assessing whether the data follows a uniform distribution and testing for spatial randomness using the Hopkins Statistic.\nMeasure Probability of Uniform Distribution: The goal is to determine how far the dataset D deviates from being uniformly distributed in the data space. This is done by sampling points from D and calculating their nearest neighbors.\nRandom Sampling: Select n points () uniformly from the range of the dataset D. Each point is chosen randomly within the range, with each value being equally likely to be picked.\nNearest Neighbor Calculation: For each randomly sampled point , find its nearest neighbor in the dataset D. This is done by calculating the distance between and all other points in D, and selecting the minimum distance as .\nExisting Point Sampling: Sample another set of n points () uniformly from the dataset D. These points are selected from the existing values in D.\nNearest Neighbor Calculation: For each point , find its nearest neighbor in the dataset D excluding itself. Calculate the minimum distance between and all other points in D (excluding ) as .\nCalculate the Hopkins Statistic: Compute the Hopkins Statistic (H) using the formula: If the dataset D is uniformly distributed, H will be close to 0.5. This suggests the absence of meaningful clusters in the data.\nIf the sum of is significantly larger than the sum of , H tends towards 1. This indicates that the data points have many more distant neighbors than expected in a uniform distribution, implying randomness and irregularity without distinct clusters.\nIf the sum of is much larger than the sum of , H tends towards 0. This implies that the distribution of the data is non-uniform, with unusually low distances to nearest neighbors, indicating strong clustering.\nWhile the best&nbsp;choice for&nbsp;k&nbsp;may be inspired by something you know about why you are attempting the custering in the first place, a few&nbsp;general-purpose methods are given here.&nbsp; Each can be varied acording to the clustering method,&nbsp; the cluster quality heuristic or domain knowledge.&nbsp; Note that the Elbow and Cross-validation methods require many clustering attempts with different numbers of clusters, so could be prohibitively expensive over the full data set. Consider selecting a random sample of the data for this purpose.\nTry number of clusters for a dataset of n points. Then each cluster would be expected to have points. As the number of clusters goes up, the within-cluster variance, that is the distances amongst points in the cluster (defined as&nbsp;the sum of squared distances between each object and the centroid)&nbsp;&nbsp;decreases to zero. So aim to choose a number that tends to reduce the sum of each within-cluster variance, but&nbsp; increasing the number any further would have only have marginal effect on the variance.\nUse the turning point in the curve of sum of within cluster variance w.r.t the number of clusters.\nTo implement: For many choices of k&gt;0 (in the extreme, k = 1.., n),&nbsp; execute&nbsp; the clustering with parameter k and calculate sum of within-cluster variances for that k.&nbsp; Plot&nbsp; each k against its sum. Choose the k corresponding to a notable bend in the curve to be the \"right\" number of clusters. Cross validation method <a data-href=\"1. Cross Validation\" href=\"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Cross Validation</a>\nDivide a given data set into m parts\nUse m – 1 parts to obtain a clustering model.\nUse the remaining part to test the quality of the clustering. E.g., For each point in the test set, find the closest centroid, and use the sum of squared distance. between all points in the test set and the closest centroids to measure how well the model fits the test set For several choices of&nbsp; k &gt; 0, repeat it m times and compute the overall quality as the average for each of the m times.&nbsp; Compare the overall quality measure w.r.t. different k’s, and choose the&nbsp; number of clusters that&nbsp; corresponds to the k that has the best overall quality. Extrinsic: supervised, i.e., the ground truth is available Compare a clustering against the ground truth using certain clustering quality measure\nEx. BCubed precision and recall metrics Intrinsic: unsupervised, i.e., the ground truth is unavailable Evaluate the goodness of a clustering by considering how well the clusters are separated, and how compact the clusters are\ne.g. Silhouette coefficient below is objectively quantitative, but subjective judgement can be just as useful. To measure clustering quality, we need to define a score function for a clustering C and a ground truth clusters .Four criteria of a good score function:\nCluster homogeneity: The more pure the clusters, the better the clustering.\nCluster completeness: The couterpart of homogeneity. Any two objects belonging to the same category in the ground truth, should be assigned to the same cluster.\nRag bag: Rag bag category: objects that cannot be merged with other objects. Putting a heterogeneous object into a pure cluster should be penalised more than putting it into a rag bag.\nSmall cluster preservation: Splitting a small category into pieces is more harmful than splitting a large category into pieces.\nBCubed precision and recall metrics satisfy the all four criteria. Let be the cluster number of object , be the category of given by the ground truth, and be 1 if and , otherwise 0. BCubed precision: how many other objects in the same cluster belong to the same category as the object. BCubed recall: how many objects of the same category are assigned to the same cluster. Two criteria for intrinsic method:\nHow well the clusters are separated\nHow compact the clusters are Silhouette coefficient satisfies above two conditions.\nLet be the ith cluster from a clustering and let o be an object in the cluster .\ncompactness, where ,\nseparation, a(o) reflects the compactness of the cluster , being the average distance of the object o in the cluster from every other object in the cluster. Low compactness is good.b(o) reflects the degree to which object o is separated from other clusters it does not belong to, being the average distance to all objects in the next-closest cluster. High separation is good.The silhouette coefficient of o is then defined as: The value lies between -1 and 1. A negative value means o is closer to the objects in another cluster than to the objects in the same cluster in expectation, and this is normally undesirable.To evaluate a particular cluster, average the silhouette coefficient for every object in the cluster. To evaluate a clustering, average the silhouette coefficient for every object in the dataset. Negative values are poor. Visual inspection Plot the clusters (or a small random sample of the data instead of the whole dataset) in 2 dimensions (choose several pairs of dimensions for several plots, or choose pairs of dimensions that are expected to be important in the problem domain. You can plot in 3 dimensions if you prefer, but more than that and it gets very hard to inspect visually). Indicate the cluster membership by the colour coding of points on the plot. Do the clusters seem to be well separated and internally compact ? Elbow method If you used the elbow method to choose an optimal number of clusters, was there a clear turning point in the plot, indicating that there is some inherent structural meaning to the k you chose? ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Major tasks of clustering evaluation","level":3,"id":"Major_tasks_of_clustering_evaluation_0"},{"heading":"Assessing Clustering Tendency","level":3,"id":"Assessing_Clustering_Tendency_0"},{"heading":"Assessing clustering tendency using the Hopkins Statistic:","level":4,"id":"Assessing_clustering_tendency_using_the_Hopkins_Statistic_0"},{"heading":"Interpreting the Hopkins Statistic:","level":4,"id":"Interpreting_the_Hopkins_Statistic_0"},{"heading":"Determine the Number of Clusters, k","level":3,"id":"Determine_the_Number_of_Clusters,_k_0"},{"heading":"<strong>Empirical method</strong>","level":4,"id":"**Empirical_method**_0"},{"heading":"<strong>Elbow method</strong>","level":4,"id":"**Elbow_method**_0"},{"heading":"Measure Clustering Quality","level":3,"id":"Measure_Clustering_Quality_0"},{"heading":"<strong>Extrinsic Methods</strong>","level":4,"id":"**Extrinsic_Methods**_0"},{"heading":"<strong>Intrinsic Methods</strong>","level":4,"id":"**Intrinsic_Methods**_0"}],"links":["artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157074,"modifiedTime":1737554783000,"sourceSize":10218,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/6. Evaluation of Clustering.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html":{"title":"7. Clustering","icon":"","description":"\nTrue statements Clustering algorithms are unsupervised learning methods.\nClustering is the process of grouping a set of data objects into multiple groups or clusters.&nbsp;\nA hierarchical clustering method can be classified as being either&nbsp;agglomerative or divisive. Our goal is to find 2 clusters using k-means algorithm.\nWhat are the&nbsp;cluster assignments&nbsp;&nbsp;after the first iteration given two initial cluster means (1.0, 1.0) and (5.0, 7.0)?\nTIE BREAKING RULE: if there is a point that has the same distance to both cluster means, then we can assign that point to the cluster containing (1.0, 1.0).\n(1, 2, 3), (4, 5, 6, 7) Use the Euclidean to calculate the data distance to (1,1) and (5,7) respectivly, classify the data to closer cluster. If the distance are equal, assign it to the cluster that (1,1) in. What will be the clustering result after the second iteration?\ncalculate the new mean: mean cluster1 (1,2,3) = [1.83, 2.33]\nmean cluster2 (4,5,6,7) = [4.125, 5.375]\n1. Calculate the distance between each data point and the mean values of both clusters. For data point 1: - Distance to mean cluster 1 (1.83, 2.33): sqrt((1.83 - 1.0)^2 + (2.33 - 1.0)^2) = 1.053\n- Distance to mean cluster 2 (4.125, 5.375): sqrt((4.125 - 1.0)^2 + (5.375 - 1.0)^2) = 4.657 Assign data point 1 to cluster 1 as it has a smaller distance to the mean of cluster 1. 2. Repeat the process for the remaining data points: For data point 2: - Distance to mean cluster 1: sqrt((1.83 - 1.5)^2 + (2.33 - 2.0)^2) = 0.496\n- Distance to mean cluster 2: sqrt((4.125 - 1.5)^2 + (5.375 - 2.0)^2) = 4.497 Assign data point 2 to cluster 1. For data point 3: - Distance to mean cluster 1: sqrt((1.83 - 3.0)^2 + (2.33 - 4.0)^2) = 2.080\n- Distance to mean cluster 2: sqrt((4.125 - 3.0)^2 + (5.375 - 4.0)^2) = 2.173 Assign data point 3 to cluster 2. Repeat the process for the remaining data points (4, 5, 6, 7) to assign them to the appropriate clusters based on the smaller distance to the respective mean values. True statements K-means is a non-deterministic algorithm.\nThe k-medoids method is&nbsp;more robust than k-means in the presence of noise and outliers.\nK-means is a non-deterministic algorithm because its outcome can vary depending on the initial selection of cluster centroids. The algorithm iteratively updates the cluster centroids by minimizing the sum of squared distances between data points and their assigned centroids. However, the algorithm can converge to different local optima depending on the initial placement of centroids. In the context of algorithms, deterministic refers to the property of producing the same output or result when given the same input, regardless of how many times it is executed. A deterministic algorithm will always produce the same output for a given input, making its behavior predictable and consistent. The dendogram represents the process of divisive clustering over 5 objects.\n“dendro.png” could not be found.\nWhich pair of clusters are produced in the first step of the algorithm?\n{X1, X2, X3, X4}, {X5} What will the final clustering configuration be if we cut the tree at a dissmilarity (i.e. distance) threshold of 2.7?\n(X1, X2, X3, X4), (X5) Assume that we obtain clusters as follows from some clustering algorithm (possibly AGNES). Here we have single-dimensional objects, each&nbsp; represented by the value of its single attribute. C1 = (1, 3)\nC2 = (5, 8, 12)\nC3 = (18, 25)\nC4 = (10, 15)\nC5 = (2, 20)","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157167,"modifiedTime":1737554783000,"sourceSize":8393,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/7. Clustering.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html":{"title":"1. Outliers","icon":"","description":"\nA data object that&nbsp;deviates significantly&nbsp;from the normal objects as if it were generated by a different mechanism.\nExample&nbsp;Unusual credit card purchase, Sports stars, Tax frauds\nOutliers are essentially the same as&nbsp;anomalies&nbsp;(or, at least, there is no widely accepted distinction).\nOutliers are different from noise data: Noise is random error or variance in a measured variable\nNoise should be removed before outlier detection.\nOutliers are&nbsp;interesting:&nbsp; They violate the mechanism that generates the normal data\nOutlier detection vs.&nbsp;novelty&nbsp;detection: early stage is an outlier; but later could be&nbsp;novelty&nbsp;and then merged into the model Credit card fraud detection - fraudulent transactions Telecom fraud detection -stolen phones Customer segmentation\nMedical research\nStudents at risk of failing -&gt; novelty detection? National security\nNetwork intrusion detection A data set may have multiple types of outlier.\nOne object may belong to more than one type of outlier. Object is&nbsp;Og&nbsp;if it significantly deviates from the rest of the data set\ne.g. Intrusion detection in computer networks\nIssue: Find an appropriate measurement of deviation Object is&nbsp;Oc&nbsp;if it deviates significantly based on a selected context\ne.g.&nbsp; 0 o&nbsp;C in Canberra&nbsp; outlier? (depends if it is summer or winter)\nAttributes of data objects should be divided into two groups: Contextual&nbsp;attributes: defines the context, e.g., time &amp; location\nBehavioural&nbsp;attributes:&nbsp; characteristics of the object, used in outlier determination, e.g., temperature Can be viewed as a generalization of&nbsp;local outliers—whose density significantly deviates from its local area whre the local area is defined by contextual attributes\nIssue: How to define or formulate meaningful context? Modeling normal objects and outliers properly Hard to enumerate all possible normal behaviours in an application\nThe border between normal and outlier objects is often a grey area\nCan assign a data object into class \"normal\" vs \"outlier\" , or assign an \"outlier-ness\" measure Application-specific outlier detection Choice of distance measure among objects and the model of relationship among objects are often application-dependent\ne.g., clinical data: a small deviation could be an outlier; while in marketing analysis, larger fluctuations are expected\nApplications associate different costs with detecting or missing an outlier.\nDifficult to develop generic, application independent outlier detection methods Handling noise in outlier detection Noise may distort the normal objects and blur the distinction between normal objects and outliers.&nbsp; It may help outliers to hide and reduce the effectiveness of outlier detection. It is best practice to remove noise first. Sometimes noise can be removed systematically using knowledge of the data-collection process (such as typos, sensor drift, or instrumentation errors). Interpretability Understand why these are outliers: Justification of the detection\nSpecify the degree of an outlier: the unlikelihood of the object being generated by a normal mechanism One of the really challenging problems in outlier detection is turning the human perception of \"I know one when I see it\" into an objective algorithm that knows one when it sees it. Unlike other mining problems, there are very few accepted objective quality measures of outlier-detection. Generally, the best we can do is to assume that an outlier is a good one if it ranks most highly compared to other potential outliers in the same data set, according to the measure we use to define outliers. If, subjectively, we don't like what we see, we try a different method or a different measure or perhaps decide we have no outliers.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Outlier:</strong>","level":3,"id":"**Outlier**_0"},{"heading":"<strong>Applications:</strong>","level":3,"id":"**Applications**_0"},{"heading":"<strong>Types: Global, Contextual or Collective</strong>","level":3,"id":"**Types_Global,_Contextual_or_Collective**_0"},{"heading":"<strong>Global outlier</strong>&nbsp;(or point anomaly)","level":3,"id":"**Global_outlier** (or_point_anomaly)_0"},{"heading":"<strong>Contextual outlier</strong>","level":3,"id":"**Contextual_outlier**_0"},{"heading":"<strong>Challenges in Outlier Detection</strong>","level":3,"id":"**Challenges_in_Outlier_Detection**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157174,"modifiedTime":1737554783000,"sourceSize":4836,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/1. Outliers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html":{"title":"2. Statistical Approach","icon":"","description":"Statistical methods (also known as&nbsp;model-based methods) assume that the normal data follow some statistical model (a stochastic model). The data not following the model&nbsp; (i.e. in low probability regions of the model) are deemed to be outliers.\nFor each object in region R, estimate , the probability that y fits the Gaussian distribution\nIf is very low, y is unlikely to be generated by the Gaussian model, thus an outlier The effectiveness of these&nbsp;statistical methods highly depends on whether the assumption of a statistical model holds in the real data. Analysts may need to understand something about the underlying data-generation process, perhaps a physical process,&nbsp; to select a suitable statistical model to use. The fit of the data to the model must be validated (and this is ironic because we are setting out to find what does&nbsp;not&nbsp;fit). There are many alternatives to statistical models available for this method, e.g., parametric vs. non-parametric. Assumes that the non-outlying data is generated by a parametric distribution with parameter . e.g. Gaussian as above.\nThe probability density function of the parametric distribution f(x,) gives the probability that object x is generated by the distribution. The smaller this value, the more likely x is an outlier.\nSome methods based on fitting one or more Gaussian distributions, are covered here.\nNon-parametric methods do not assume an a-priori statistical model and determine the model from the input data.\nThey are not completely parameter-free but consider the number and nature of the parameters are flexible and not fixed in advance. The parameters to be determined are bin width and bin boundaries. A simple threshold can then be applied to determine outliers as those objects falling in a low-frequency bin.&nbsp;&nbsp; Three problems occur: The method is very sensitive to the arbitrary bin boundaries. An outlier can fall outside a low-frequency class due only to the choice of starting point for binning along the x axis (and vice-versa). Too small bin size&nbsp;→&nbsp;normal objects in empty/rare bin, ie, false positive\nToo big bin size&nbsp;→&nbsp;outliers in some frequent bins, ie, false negative Kernel density estimation fits a&nbsp;smoothing&nbsp;function to estimate a probability density distribution, to achieve something like a smoothed histogram. If the density estimated by the smoothed distribution is low in some region, that is, below some threshold, then the objects in the region are considered outliers.\nThe determination of data points to become outliers is dependent on the parameters to the fitting process (parameters similar to bin size and bin boundaries for histograms), but offers a smoother boundary behaviour than the discrete histogram.\nGeneral approach to determine outliers based on only one variable\nAssume data is generated by an underlying&nbsp;Gaussian/normal&nbsp;distribution (or choose some other distribution)\nSet parameters of the distribution from input data, using e.g. maximum likelihood\nIdentify data points of low probability according to the fitted distribution as outliers\nEarlier we identified outliers for <a data-href=\"2.2 Basic Statistical Descriptions of a Single Data Variable\" href=\"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2.2 Basic Statistical Descriptions of a Single Data Variable</a>, defined as values below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. The same method for determination of outliers can be used more generally here.Outliers are &gt; 3 stddevs from the mean**Example:Consider the following data for temperature in Canberra at noon over 11 days:{24.0, 28.9, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4}.\nWe will assume a normal distribution that is defined by\nwith parameters: mean \\mu and variance \\sigma^2.\nWe will set these parameters to fit the distribution to our data by maximising likelihood.\nLikelihood is defined as\nThis can be maximised by taking derivatives of the log of the likelihood to get estimates for the maximum likelihood mean and maximum likelihood variance respectively.Using the temperatures in Canberra with n = 11, we get and Now we can determine outliers as points of low probability by a heuristic such as:\nAn outlier is any value outside 3 estimated standard deviations from the estimated mean.So outliers fall outside In our temperature data, 24.0 is the only value that falls outside this range, and therefore 24.0 is an outlier.Grubb's test, also called the maximum normed residual test is an alternative heuristic to identify data points of low probability according to the fitted normal distribution as outliers. Strictly, the statistical assurance comes when both the data follows a normal distribution and at most one outlying value exists. In this formulation, we permit the outlying values to be at either the minimum or the maximum ends of the data, and so we use the two-sided t-test, and both the minimum and maximum should be tested. In practice, all values will be tested (or more efficiently, all values starting from each end until a non-outlier from each end is found).Consider a dataset of N data points, with standard deviation s.You need to choose an \\alpha to apply this test, and by that determine the significance you want. This is not unlike having to choose the number of standard deviations in the maximising likelihood method above. A value of \\alpha = 0.05 is typical.Define the z-score for a data point x as , that is, the point's distance from the sample mean as a proportion of the standard deviation.Then, according to Grubb's test x is an outlier if<br>\nwhere is the critical value taken by a t-distribution with degrees of freedom N-2 at a significance level of . <a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Student%27s_t-distribution#Table_of_selected_values\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Student%27s_t-distribution#Table_of_selected_values\" target=\"_self\">Check the T-distribution table</a>Example\nUsing the Canberra temperature data above, we will test if the minimum value, 24.0, is an outlier by Grubb's test.Choose . From above, we have that . Then and . Then lookup using to get 3.250. Here, we approximate by taking the lower value of the nearby columns in the table to be slightly more permissive in assigning data values to be outliers. Now we have that 24.0 is an outlier if . So 24.0 is an outlier.Multivariate data refers to a data set involving two or more attributes or variables (the usual case!). This is addressed by transforming the multivariate outlier detection task into a univariate outlier detection problem.Let ō be the mean vector for a multivariate data set. Let S be the covariance matrix. Mahalaobis distance for an object o to ō is defined as\nwhere and are the operators for matrix transpose and inverse respectively.Using this transformation, we now have a univariate data set Then use the Grubb's test on this univariate data set to identify outliers.This method assumes a normal distribution.\nFor each n dimensional object o with dimension values , i = 1,...n, calculate\nwhere is the mean of the i-dimension among all objects.\nIf this is large for o, then o is an outlier.Assuming that data is generated by a normal distribution could often be overly simplified. -&gt; Assume data is generated by multiple distributions, for example two as here. For any object o in the data set, the probability that o is generated by the mixture of the two distributions is given by“parametric_mixture_distributions.png” could not be found.where and are the probability density functions of and Then we can use an expectation maximisation algorithm for probablistic model-based clustering to learn the parameters μ1, σ1, μ2, σ2 from data. Each cluster is then represented by one of the two normal distributions.\nAn object o is an outlier if it does not belong to any learned cluster, that is, the probability that it was generated by the combination of the two distributions is below some threshold.\nWe are not covering the expectation maximisation algorithm in the course but it is explained in the text. If we have&nbsp; one or two distributions and&nbsp;&nbsp;o&nbsp;in the diagram&nbsp; is a genuine outlier then the lower local density of&nbsp; the collection of objects means they, including&nbsp;o,&nbsp; are&nbsp; likely to be interpreted as non-outliers by the normal distribution-fitting.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Use Gaussian distribution to model the normal data:","level":4,"id":"Use_Gaussian_distribution_to_model_the_normal_data_0"},{"heading":"<strong>Effectiveness</strong>","level":3,"id":"**Effectiveness**_0"},{"heading":"<strong>Parametric methods</strong>","level":3,"id":"**Parametric_methods**_0"},{"heading":"<strong>Non-parametric methods</strong>","level":3,"id":"**Non-parametric_methods**_0"},{"heading":"<strong>Histogram non-parametric method</strong>","level":4,"id":"**Histogram_non-parametric_method**_0"},{"heading":"<strong>Kernel Density estimation non-parametric method</strong>","level":4,"id":"**Kernel_Density_estimation_non-parametric_method**_0"},{"heading":"Parametric Methods: Univariate Outliers from a Normal Distribution","level":3,"id":"Parametric_Methods_Univariate_Outliers_from_a_Normal_Distribution_0"},{"heading":"Outliers with <strong>only one numeric variable</strong>&nbsp;(attribute):","level":4,"id":"Outliers_with_**only_one_numeric_variable** (attribute)_0"},{"heading":"Methods","level":4,"id":"Methods_0"},{"heading":"<strong>1. Using IQR</strong>","level":5,"id":"**1._Using_IQR**_0"},{"heading":"**2. Using maximum likelihood:","level":5,"id":"**2._Using_maximum_likelihood_0"},{"heading":"<strong>3. Using Grubb's test</strong>","level":4,"id":"**3._Using_Grubb's_test**_0"},{"heading":"Parametric Methods: Multivariate Outliers","level":3,"id":"Parametric_Methods_Multivariate_Outliers_0"},{"heading":"<strong>Method 1. Compute Mahalaobis distance</strong>","level":4,"id":"**Method_1._Compute_Mahalaobis_distance**_0"},{"heading":"<strong>Method 2. Use χ2 –statistic</strong>","level":4,"id":"**Method_2._Use_χ2_–statistic**_0"},{"heading":"Parametric Methods: Mixture of Parametric Distributions","level":3,"id":"Parametric_Methods_Mixture_of_Parametric_Distributions_0"},{"heading":"<strong>Weakness</strong>","level":4,"id":"**Weakness**_0"}],"links":["artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157178,"modifiedTime":1737554783000,"sourceSize":10434,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/2. Statistical Approach.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html":{"title":"3. Proximity-Based Approaches","icon":"","description":"Based on the idea that objects that are far away from the others are outliers, proximity-based approaches&nbsp;assume&nbsp;the proximity of an outlier deviates significantly from that of most of the others in the data set.There are two types of proximity-based outlier detection methods:\nDistance-based&nbsp;outlier detection: An object&nbsp;o&nbsp;is an outlier if its neighbourhood does not have enough other points in it\nDensity-based&nbsp;outlier detection: An object&nbsp;o&nbsp;is an outlier if&nbsp; the density of objects around it is much lower than that of its neighbours\nFor a set D of data points, start with a user-defined parameter r called the distance threshold that defines a reasonable neighbourhood for each object. For each object o, examine the number of other objects in the r-neighbourhood of o. If enough of the objects in D are beyond the r-neighbourhood of o, then o should be considered an outlier. Be careful to exclude o itself when counting the objects in its near neighbourhood.Formally, let be a fraction threshold, a user-defined parameter that defines what proportion of objects in D are expected to be within the r-neighbourhood of every non-outlying object.Then an object o is a distance-based outlier DB(r,\\pi) if\nthat is, if the proportion of objects in D that are as close as r is no more than .Equivalently, one can check the distance between o and its k-nearest neighbour ok, where k is defined by compute k for nearest neighbour“distance_outlier_k_nearest.png” could not be found.In this case, o is an outlier if ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Distance-Based Outlier Detection: Nested loop method","level":3,"id":"Distance-Based_Outlier_Detection_Nested_loop_method_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157182,"modifiedTime":1737554783000,"sourceSize":6295,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/3. Proximity-Based Approaches.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html":{"title":"4. Classification-Based Approaches","icon":"","description":"If we have a labelled training set of outliers and non-outliers, then classification methods can be used. However, the training set would typically be heavily biased in favour of non-outlying data, so classification has to be sensitive to this asymmetry of classes. Learn the decision boundary of the normal class using classification methods such as a one-class variant of SVM. Using the diagram above, only those points inside the curve would be used for training, or else we assume all the training data we have is \"normal\".&nbsp; At run-time, any points that do not belong to the normal class (i.e. not within the decision boundary) are declared to be outliers. Advantage: Can easily detect new outliers that were not close to any outlier objects in the training set Extension:&nbsp; Can also have normal objects may belong to multiple classes, and can be more selective if labels available for training Strength: human knowledge can be incorporated by selection of training data Strength: Outlier detection is fast Bottleneck: Quality heavily depends on the availability and quality of the training set, but often difficult to obtain representative and high-quality training data ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>One-class model: A classifier is built to describe only the normal class</strong>","level":3,"id":"**One-class_model_A_classifier_is_built_to_describe_only_the_normal_class**_0"},{"heading":"<strong>Strengths and Weaknesses of Classification-based approaches</strong>","level":3,"id":"**Strengths_and_Weaknesses_of_Classification-based_approaches**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157185,"modifiedTime":1737554783000,"sourceSize":1399,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/4. Classification-Based Approaches.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html":{"title":"5. Clustering Based Approaches","icon":"","description":"Clustering-based approaches select outliers by examining the relationship between objects and clusters. An outlier is an object that belongs to a small and remote cluster, or belongs to no cluster.Use a density-based clustering method such as DBSCAN and consider the unclustered points (noise) to be outliers.Use&nbsp;k-means clustering to partition data points into clusters.Let be the closest cluster centre to object o. Let avdist be the average distance of all the objects in the cluster from .\nIf is large then o is considered an outlier\nAlternatively for the case of unseen data o,\nif for all training data with closest cluster centre c_o, then o is considered an outlier.&nbsp;FindCBLOF&nbsp;algorithm as follows:\nFind clusters, and sort them in decreasing size To each data point, assign a&nbsp;cluster-based local outlier factor&nbsp;(CBLOF): If obj&nbsp;p&nbsp;belongs to a large cluster, CBLOF = clustersize X similarity between&nbsp;_p&nbsp;and it's cluster\nIf&nbsp;p&nbsp;belongs to a small cluster, CBLOF = cluster size X similarity between&nbsp;p&nbsp;and the closest large cluster Data points with&nbsp;low&nbsp;CBLOF are considered outliers\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Not belonging to any cluster</strong>","level":3,"id":"**Not_belonging_to_any_cluster**_0"},{"heading":"<strong>Far from its closest cluster</strong>","level":3,"id":"**Far_from_its_closest_cluster**_0"},{"heading":"<strong>Belonging to a small, distant cluster</strong>","level":3,"id":"**Belonging_to_a_small,_distant_cluster**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157189,"modifiedTime":1737554783000,"sourceSize":1731,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/5. Clustering Based Approaches.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html":{"title":"6. Contextual and Collective Outliers","icon":"","description":"An object is a&nbsp;contextual outlier&nbsp;&nbsp;(or&nbsp;conditional outlier)&nbsp;if it deviates significantly with respect to a specific context of the object. The context is defined in the values of&nbsp; identified&nbsp;contextual attributes&nbsp;of the object, such as location, time or demographic. The remaining attributes of the object are called&nbsp;behavioural attributes.\nDetect a credit card holder with expenditure patterns matching millionaires in a context of low income\nDo not detect as an outlier a millionaire with high expenditure, given a context of high income\nDetect people with unusual spending or travelling patterns in a context of demographics and income for&nbsp; target marketing\nDetect unusual weather&nbsp; in a context of season and locality &nbsp;Use the context attributes to define groups\n&nbsp;Detect outliers in the group based on the behavioural attributes alone, using a conventional outlier detection method\nUsed when it is not so easy to choose a the significant set of contextual attributes from the data you have.\nUsing a training data set, build a predictive model for the “normal” behaviour from all the context attributes\nAn object is a contextual outlier if its behaviour attribute values significantly deviate from the values predicted by the model\nA group of data objects forms&nbsp;a&nbsp;collective outlier&nbsp;if the objects as a whole deviate from the entire data set, even though each individual is not an outlier alone.&nbsp;&nbsp;They are difficult to find because of the need to take into account the structure of the data set, ie relationships between multiple data objects.Each of these structures is inherent to its respective type of data\nFor temporal data (such as time series and sequences), we explore the structures formed by time, which occur in segments of the time series or subsequences\nFor spatial data,&nbsp; explore local areas\nFor graph and network data, we explore subgraphs\nDifference from contextual outlier detection: the structures are often not explicitly defined, and have to be discovered as part of the outlier detection process.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Contextual Outliers</strong>","level":3,"id":"**Contextual_Outliers**_0"},{"heading":"<strong>Applications:</strong>","level":4,"id":"**Applications**_0"},{"heading":"<strong>Method 1: Transform into Conventional Outlier Detection.</strong>","level":4,"id":"**Method_1_Transform_into_Conventional_Outlier_Detection.**_0"},{"heading":"<strong>Method 2: Model normal Behaviour wrt Contexts</strong>","level":4,"id":"**Method_2_Model_normal_Behaviour_wrt_Contexts**_0"},{"heading":"<strong>Collective Outliers</strong>","level":3,"id":"**Collective_Outliers**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157199,"modifiedTime":1737554783000,"sourceSize":2474,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/6. Contextual and Collective Outliers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html":{"title":"7. Outlier","icon":"","description":"\nWhat is an outlier?\nA data point that significantly deviates from the other data points in a set, to the extent that it would have been generated by some abnormal phenomenon\nThere are three kinds of outliers that require targeted techniques for search\nThere are three kinds of outliers that require targeted techniques to search for each. [Contextual] outliers can be transformed into a conventional approach by [partitioning some attributes for grouping first and using the others for prediction within the group]. This is used, for example, to find outlying people that do not conform to usual patterns of behaviour within their demographic group.[Collective] outliers can be transformed into a conventional approach by [identifying structural units of related objects and mapping these to single objects represented by conventional features ]. This is used, for example, for [mining social networks that model relations between people in a graph]. For [Global] outliers the conventional approaches are used, including [building a one‑class classifier and identifying outliers outside the class ], [fitting a probability distribution and identifying points of low probability], [using the local outlier factor to detect objects that are even more distant than others in their local neighbourhood ] and [using the nested loop method for identifying outliers that are most distant from their nearest neighbours].\nNon-parametric statistical methods do not fit a statistical distribution to the data, but instead fit a user-controllable histogram or smoothing function and look for objects in regions with unusually low frequency.\nTrue A statistical multivariate outlier detection problem can be solved as a univariate problem by Using the chi-square to compute the outlier-ness of a multidimensional object by aggregating the difference from the mean along each dimension at a time. Computing a distance function along one dimension as an aggregate function of multidimensional distances using, for example, Mahalaobis distance. The nested loop algorithm for distance-based outlier detection computes, for as many of every pair of objects in the data it needs, the distance between each pair. The distance is compared to a threshold value and an object is considered an outlier if the proportion of other objects exceeding the threshold distance away is relatively small.\nIn \"considered an outlier if the proportion of other objects exceeding the threshold distance \" the word \"exceeding\" should be more like \"inside\" or \"at a distance less than or equal to\" 'False' Outliers that are relatively distant from the clusters in which they occur Use k-means clustering and look for outliers unusually far from the cluster centroid, Outliers that do not belong to any cluster\nUse DBSCAN to build clusters and identify outliers, Outliers belong to a small cluster and are unlike others in large nearby clusters, or else they are not very similar to the others in their own cluster\nUse FindCBLOF","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157202,"modifiedTime":1737554783000,"sourceSize":3060,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/7. Outlier.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html":{"title":"0. Ensemble Methods","icon":"","description":"Ensemble methods in machine learning combine multiple models to improve overall performance and accuracy. The idea behind ensemble methods is to combine the predictions of several models instead of relying on a single model, as each model may have its own strengths and weaknesses. By combining the predictions of multiple models, ensemble methods can reduce the risk of overfitting and improve the generalization ability of the model.“ensemble.png” could not be found.There are two main types of ensemble methods: bagging and boosting. Bagging involves training multiple models independently on different subsets of the training data and averaging their predictions. Boosting, on the other hand, trains a series of weak models sequentially, with each subsequent model learning from the mistakes of its predecessors.Ensemble methods have been successfully applied in a variety of machine learning tasks, such as classification, regression, and clustering. Some popular ensemble methods include Random Forest, Gradient Boosting, and AdaBoost.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157210,"modifiedTime":1737554783000,"sourceSize":1079,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/0. Ensemble Methods.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html":{"title":"1. Bagging","icon":"","description":"An ensemble method that combines multiple models to improve the overall accuracy and stability of a single model. It works by creating multiple bootstrap samples of the original dataset and training a model on each sample.\nEach model is trained independently of the others, and the final predictions are made by aggregating the predictions of all models. Bagging is particularly useful for reducing the variance of a model, especially when the individual models are unstable or prone to overfitting. It can be used with any algorithm, and the number of models used can be adjusted to optimize performance. Bagging can also be used in conjunction with other ensemble methods, such as boosting or stacking, to further improve accuracy. However, it does not always improve performance and may not be suitable for all types of data or models. Create multiple random samples of the training data using bootstrapping, where each sample is of the same size as the original training set but with replacement.\nTrain a model on each of the bootstrapped samples. The models can be of any type, but typically they are decision trees, which are highly sensitive to variations in the training data.\nCombine the predictions of the individual models using averaging or voting. In regression problems, averaging is used, where the predicted values of the individual models are averaged. In classification problems, voting is used, where the predicted classes of the individual models are counted and the majority class is chosen as the final prediction.\nEvaluate the performance of the bagged model on a validation set or through cross-validation.\nThe benefits of bagging include reducing overfitting, improving the stability and accuracy of the model, and being less sensitive to outliers and noise in the data.we have a dataset of housing prices with various features such as number of bedrooms, square footage, and location. We want to predict the price of a house given these features. We can use bagging to improve the accuracy of our prediction. We would first create multiple random samples of the training data using bootstrapping. We would then train decision tree models on each of these samples. Finally, we would combine the predictions of the individual models using averaging to make our final prediction.Another example of bagging is in the field of image recognition. A bagged model can be trained to recognize images of a specific object or animal, such as a cat. The models are trained on different samples of images of cats, each of which is slightly different due to variations in lighting, angle, and background. When a new image of a cat is presented to the model, it is classified as a cat if the majority of the individual models classify it as such.multiple decision trees are trained independently on different subsets of the dataset, with replacement. This means that each decision tree is trained on a random subset of the data, and the same data point can be used in multiple trees.Random Forest builds on the idea of bagging by adding additional randomness to the decision tree building process. Instead of using all available features to split the data at each node of the decision tree, a random subset of features is selected. This helps to reduce the variance of the model and improve its generalization performance.The Random Forest algorithm generates a large number of decision trees, typically several hundreds or thousands, and each tree votes on the final classification or prediction. The final output is determined by the majority vote of the trees.Random Forest has several advantages over traditional decision trees, including increased accuracy, reduced variance, and the ability to handle high-dimensional datasets. It is also less prone to overfitting, as the randomness introduced in the decision tree building process helps to reduce the risk of learning spurious relationships in the data.An example of Random Forest in action is a dataset of housing prices. Random Forest can be used to predict the prices of houses based on features such as the number of bedrooms, square footage, and location. By using a large number of decision trees, Random Forest can capture complex relationships between the features and the target variable, resulting in more accurate predictions.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Bagging (Bootstrap Aggregating)","level":3,"id":"Bagging_(Bootstrap_Aggregating)_0"},{"heading":"Details:","level":3,"id":"Details_0"},{"heading":"Example:","level":3,"id":"Example_0"},{"heading":"Bagging with Random Forest","level":3,"id":"Bagging_with_Random_Forest_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157214,"modifiedTime":1737554783000,"sourceSize":4778,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/1. Bagging.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html":{"title":"2. Boosting with AdaBoost","icon":"","description":"Boosting is a machine learning ensemble method that aims to improve the accuracy of a weak model by iteratively building a strong model from a sequence of weak models. AdaBoost (Adaptive Boosting) is a popular boosting algorithm that was introduced by Freund and Schapire in 1997.\nStarts by fitting a weak learner to the training data, assigning equal weights to all training instances. The weak learner is then evaluated, and the misclassified instances are assigned higher weights than the correctly classified ones. In the next iteration, a new weak learner is trained on the same training set, but with updated weights that prioritize the misclassified instances from the previous iteration. This process is repeated for a predefined number of iterations, with each weak learner added to the ensemble weighted according to its performance on the training set. The final ensemble model is a weighted combination of all the weak models. For example, it can handle complex data distributions, reduce overfitting, and provide high accuracy even with a small number of weak learners. Additionally, AdaBoost is not restricted to any particular type of weak learner, which allows it to be applied to a wide range of classification and regression problems. Its sensitivity to noisy data and outliers, which can result in overfitting.\nAnother limitation is its tendency to focus on a small number of dominant features in the data, which can lead to bias and reduced generalization performance.\nA common example of AdaBoost application is in face detection systems. In this application, a set of weak classifiers are trained on various features, such as edge orientation or color, to detect the presence of a face. The weak classifiers are then combined using AdaBoost to create a strong classifier that can accurately detect faces in images. AdaBoost has also been used in various other applications, such as predicting customer churn in telecommunication companies, detecting credit card fraud, and predicting stock prices.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"AdaBoost algorithm","level":3,"id":"AdaBoost_algorithm_0"},{"heading":"Advantages over other ensemble methods, such as Bagging and Random Forests","level":3,"id":"Advantages_over_other_ensemble_methods,_such_as_Bagging_and_Random_Forests_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157218,"modifiedTime":1737554783000,"sourceSize":2396,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/2. Boosting with AdaBoost.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html":{"title":"3. Class-imbalanced data","icon":"","description":"In class-imbalanced data problems, where one class has only a small number of examples in the training set, simply maximizing overall accuracy may not be the best strategy as it can prioritize correct negative examples at the expense of correct positive examples. Therefore, it is necessary to evaluate classification accuracy in such cases using measures such as sensitivity, specificity, or F-score.To improve classification accuracy, oversampling or undersampling can be used to change the data distribution in the training set. Oversampling involves randomly duplicating positive examples to balance the adjusted training set, while undersampling randomly deletes negative examples from the training set.For probabilistic classifiers such as Naive Bayes and Neural networks, it is possible to lower the decision threshold for the positive class so that more examples are classified as positive, reducing the likelihood of false negatives, which can be harmful in class-imbalanced problems. ROC curves can be used to help determine an appropriate threshold.Ensemble learning methods can also be used, where each model in the ensemble differs by employing some oversampling, undersampling, or threshold-moving technique.In practice, threshold moving and ensemble methods have been shown to perform better empirically, but oversampling and undersampling are still commonly used by practitioners.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157222,"modifiedTime":1737554783000,"sourceSize":1495,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/3. Class-imbalanced data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html":{"title":"4. Data Stream Mining","icon":"","description":"\nrefers to the process of analyzing and extracting information from high-volume, fast-moving, and potentially infinite data streams in real-time. Data streams are generated by various sources, including wireless sensor networks, social media, financial transactions, and other real-time data sources. Data streams are characterized by their size, speed, and dynamic nature, which require specialized techniques to handle them effectively. Some of the characteristics of data streams are that they are continuous, fast-changing, and require real-time processing. Different from traditional data processing since data streams are typically unbounded and cannot be stored in their entirety. Random access to data in a data stream is expensive and impractical. Instead, a single-scan algorithm is used to process each record in the stream once. Moreover, only the summary of the data seen thus far is stored, which requires the use of multi-level and multi-dimensional processing methods to analyze the data effectively. telecommunication calling records, credit card transaction flows, network monitoring and traffic engineering, financial market data, industrial processes such as power supply and manufacturing. Sensor data from video streams,\nRFID's, security monitoring, web logs, web page click streams, the internet of things (IoT) are also sources of data streams.\nThe difference between data stream mining and traditional data mining is that data stream mining deals with dynamic and continuous data, while traditional data mining deals with static and stored data. Data stream mining requires specialized algorithms, techniques, and tools to handle the large volumes of data, speed, and dynamic nature of data streams.“stream_vs_traditional.png” could not be found.Data streams are continuous and large in volume, often making it impractical to scan through the entire data stream more than once, or even look at every element of a stream. This has led to the need to relax the requirement for exact answers and settle for approximate answers using synopses, which provide summaries of the data based on synopsis data structures. Random sampling, sliding windows, histograms, multi-resolution methods, and reservoir sampling are common synopsis techniques. Random sampling involves probabilistically choosing a data item for processing, which can be challenging in a data stream due to the unknown dataset size. Reservoir sampling addresses this by keeping a reservoir of s samples seen so far, from which an unbiased random sample of size up to s can be extracted at any time. Sliding windows make a decision based on some recent data, where new data that arrives will expire after a window size w. Histograms partition the data by attribute values into a set of contiguous buckets and can be used to approximate the frequency distribution of element values in a data stream, making them efficient for answering queries about data frequency.\nClassification problems on data streams can be solved using an ensemble method that allows for changes in the underlying data stream over time (called concept drift) by adding new models to the ensemble that are built from new data, and retiring old models that were built from older data.\nEnsemble learning over stream data can effectively analyze each object only once (subject to the underlying mining algorithm), store older data in summary form while newer data may be stored in detail, and adapt the overall model over time so that concept drift over time is reflected in predictions.\nThe algorithm to build an ensemble classifier involves partitioning the labelled time-sequenced data stream into k chunks and using each chunk to train a conventional probabilistic classifier (e.g., Naive Bayes) to obtain k classifiers (, ), where is the weight of (initially all equal).\nThe k classifiers can be used to classify new unlabelled data by voting for each class with their own predicted class probability multiplied by their own assigned class weight, and the highest summed-votes class is predicted.\nAfter the arrival of a fresh chunk of labeled data of that fixed size, a new classifier M is trained. Also, use that fresh chunk of data as test data for all the previous k classifiers, and discard the classifier from the ensemble with the greatest error rate over that new chunk of data. M is then also included in the ensemble, leaving k classifiers in the ensemble.\nThe weights of the k ensemble classifiers are updated with higher weight assigned for higher accuracy over the new data chunk.\nThis approach is effective for handling concept drift over time and maintaining accuracy of the classification model in a changing data stream environment.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Stream Mining","level":3,"id":"Data_Stream_Mining_0"},{"heading":"Processing of data streams","level":3,"id":"Processing_of_data_streams_0"},{"heading":"Examples of data streams","level":3,"id":"Examples_of_data_streams_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157320,"modifiedTime":1737554783000,"sourceSize":5126,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/4. Data Stream Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html":{"title":"5. Stream OLAP","icon":"","description":"Multidimensional OLAP analysis is still needed in stream data analysis.\nDue to the limited memory, disk space, and processing power, it is impossible to store the detailed level of data and compute a fully materialised cube.\nTilted time frame\nCritical layer storing\nPartial materialisation Key idea: The most recent time is registered at the finest granularity; the most distant time is registered at a coarser granularity.\nThe level of coarseness desirable depends on applications.\nTwo examples to design a tilted time frame: Time frame is structured in multiple granularities based on the natural time scale (See example in the figure (a) below).\nHeading from now back in time, we store the most recent 4 quarter-hours, followed by the last 24 hours, then 31 days, and then 12 months = 71 units of time describing a year up to the current time point. Compute frequent item sets in the last hour with the precision of a quarter of an hour or\nin the last day with the precision of an hour ...etc Time frame is structured in multiple granularities according to a logarithmic scale.\nSuppose the most recent slot holds the current quarter-hour\nThe remaining slots are for the previous quarter-hour, the half hour before that (2 quarters),&nbsp;the hour before that&nbsp;(4 quarters) , 8 quarters, and so on, with the slot time-size growing at an exponential rate backwards in time. Even with the tilted time frame model, it can still be too costly to dynamically compute and store a materialized cube.\nCompute and store only some&nbsp;mission-critical cuboids&nbsp;of the full data cube\nDynamically and incrementally compute and store two critical cuboids (or layers) The first layer, called the&nbsp;minimal interest layer, is the minimally interesting layer that an analyst would like to study.\nThe second layer, called the&nbsp;observation layer, is the layer at which an analyst (or an automated system) would like to continuously study the data.\nThese layers are determined based on their conceptual and computational importance in stream data analysis Dimensions at the&nbsp;raw data layer&nbsp;includes&nbsp;individual user, street address, and&nbsp;second.\nAt the&nbsp;minimal interest&nbsp;layer, the three dimensions are&nbsp;user group, street block, and&nbsp;minute, respectively. Any cuboids that are lower than the minimal interest layer are beyond user interest.\nWe only need to compute and store the (three-dimensional) aggregate cells for the (user group, street block, minute). At the&nbsp;observation layer, the three dimensions are&nbsp;∗ (meaning all user), city, and&nbsp;quarter, respectively.&nbsp;\nThe cuboids at the observation layer should be computed dynamically, taking the tilted time frame model into account as well.\nThis is the layer that an analyst takes as an observation to make decisions.\n“What if a user needs a layer that would be between the two critical layers?”Materialising a cube at only two critical layers leaves much room for how to compute the cuboids in between.&nbsp;These cuboids can be precomputed fully, partially, or not at all (i.e., leave everything to be computed on the fly).\nrolls up the cuboids from the minimal interest layer to the observation layer by following one popular drilling path\nmaterialises only the layers along the path, and leaves other layers to be computed only when needed.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Three key techniques for Stream OLAP","level":3,"id":"Three_key_techniques_for_Stream_OLAP_0"},{"heading":"<strong>Tilted Time Frame: Time Dimension with Compressed Time Scale</strong>","level":4,"id":"**Tilted_Time_Frame_Time_Dimension_with_Compressed_Time_Scale**_0"},{"heading":"Natural tilted time frame model","level":4,"id":"Natural_tilted_time_frame_model_0"},{"heading":"Logarithmic tilted time frame model (figure (b) below)","level":4,"id":"Logarithmic_tilted_time_frame_model_(figure_(b)_below)_0"},{"heading":"<strong>Critical Layers</strong>","level":3,"id":"**Critical_Layers**_0"},{"heading":"<strong>Partial Materialisation</strong>","level":3,"id":"**Partial_Materialisation**_0"},{"heading":"<strong>Popular path cubing:</strong>","level":4,"id":"**Popular_path_cubing**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157345,"modifiedTime":1737554783000,"sourceSize":3914,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/5. Stream OLAP.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html":{"title":"6. Frequent Pattern Mining","icon":"","description":"\nThe Lossy Counting algorithm is a frequent pattern mining algorithm that approximates the frequency of items or itemsets within a user-specified error bound. It divides the incoming data stream into buckets of fixed width and maintains a data structure that stores the frequency of elements. The algorithm prunes the data structure by deleting entries when a deletion rule is met.\nWhen a user requests a list of items with minimum support, the algorithm outputs entries in the data structure whose frequency is greater than or equal to the minimum support minus the error bound. The algorithm is useful for mining frequent patterns in data streams where scanning the whole dataset multiple times is unrealistic.\nDetails: <a data-href=\"5. Lossy Counting Algorithm\" href=\"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">5. Lossy Counting Algorithm</a>1: Set the margin of error to epsilon (0 &lt; epsilon &lt; 1)\n2: Divide the incoming stream into buckets of width w = ceil(1/epsilon) transactions\n3: Set N as the current stream length, b_c as the current bucket ID of the Nth item, and initialize f as the frequency of an element e\n4: Create an empty data structure D to store triples of the form (e, f, delta), where delta is the maximum possible error in f\n5: For each incoming element e: a. Check if an entry for e already exists in D i. If it exists, update the entry by incrementing its frequency f by one: (e, f, delta) -&gt; (e, f+1, delta) ii. If it doesn't exist, create a new entry (e, 1, b_c-1) b. Prune D by deleting some entries when N mod w = 0 i. An entry (e, f, delta) is deleted if f + delta &lt;= b_c\n6: When a user requests a list of items with minimum support sigma: a. Output all entries in D where f &gt;= (sigma - epsilon)N b. If an item e is deleted based on the deletion rule in step 5b, its frequency f will start from 1 again when it appears in the stream later on\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Lossy Counting algorithm","level":3,"id":"Lossy_Counting_algorithm_0"}],"links":["artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157350,"modifiedTime":1737554783000,"sourceSize":1933,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/6. Frequent Pattern Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html":{"title":"7. Time Series","icon":"","description":"Time series analysis involves modelling and forecasting a time series by decomposing it into four basic components, i.e., trend, cyclic, seasonal, and irregular movements. The trend represents the general direction of a time series, while cyclic movements are long-term oscillations about the trend line. Seasonal movements are systematic movements that occur due to calendar-related events, and irregular movements are sporadic motions due to random or chance events.Autocorrelation analysis can be used to identify seasonal patterns in the data. The correlation coefficient between two subsequences is computed for a given time lag. If a correlation is found, a seasonal index number is used to adjust the data for seasonal variation. The seasonal effect can be removed by dividing each value in the time series by its corresponding seasonal index value.Moving average of order n can be used to detect trends in the data. The process of replacing the time series by its moving average value eliminates unwanted fluctuations, such as the 10-day moving average. Weighted moving average of order n reduces the effect of extreme values and irregular variations, using a vector of n weights chosen with greater values in the central elements. It can be centrally-weighted or lagged.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157357,"modifiedTime":1737554783000,"sourceSize":1415,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/7. Time Series.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html":{"title":"8. Ensemble, Time series, Streams","icon":"","description":"\nSampling is called&nbsp;with replacement&nbsp;when a tuple selected at random from the population is not returned to the population, and then, the selected&nbsp; tuple cannot be selected again in a further sampling process.\nFalse\nTrue statements\nIn Bagging, each tuple in training dataset is sampled uniformly from the original dataset D with replacement.a. Bagging (Bootstrap Aggregating) involves sampling tuples from the original dataset D with replacement, which means each tuple has an equal chance of being selected for the training dataset D_i. b. Bagging does not assign different weights to each training tuple. Instead, each tuple has an equal chance of being selected during the sampling process. d. In AdaBoost, the tuples are sampled with a modified distribution. Initially, all tuples have equal weights, but the weights are adjusted based on the performance of the classifiers. Tuples that are misclassified in previous iterations are given higher weights to focus on them in subsequent iterations. The following table shows the weights of five items in dataset D after i-1 iterations of AdaBoost.\nTo train the i-th classifier, we sampled 5 tuples with replacement from the dataset D and obtained the following training dataset :We trained some classification model using and the classifier correctly classified and and misclassified .\nWhat will be the weight of after adjusting weights and normalisation based on the classification result?\n(Note that = 0.4 in this case). Write your answer rounded to two decimal places.\nWeights of x_1 and x_2 before normalisation are:\nSo to normalise the weights in D we need to divide the all new weights by (0.4/3 + 0.2/3 + 0.15+0.15+0.4) = 0.2+0.15+0,15+0.4 = 0.9For in particular,the new weight The correct answer is: 0.44 Characteristics of data stream. Random access to a data tuple is expensive. Linear or sublinear computational techniques are required to analyse the stream.\nIn general, it is not feasible to store every data tuple. To overcome the storage problem, we often store summaries of data seen so far.\nSome level of approximation is acceptable in data stream analysis. Correct statement which describes a given method for a data stream. Histogram → can be used to approximate the frequency distribution of attribute values\nRandom sampling → refers to the process of probabilistic choice of a data object to be processed or not\nSliding window → makes a decision on some recent data Multidimensional OLAP analysis is still needed in stream data analysis.&nbsp;However, due to the limited memory, disk space, and processing power, it is impossible to store the detailed level of data and compute a fully materialised cube. Several methods have been proposed to obtain an efficient OLAP with data stream.\n[tilted time frame] method compress the time dimension of the data using different granularity. This approach focuses on fact that&nbsp;1) the most recent time is registered at the finest granularity, and 2) most distance time is registered at a coarser granularity.&nbsp;For example, [natural tilted time frame model]&nbsp;structures time frames in multiple granularities based on the natural time scale, whereas [logarithmic tilted time frame model]&nbsp;structures time frames in multiple granularities according to a logarithmic scale.The above approach can only be applied to a time dimension of the data, and materialisation of all possible cubes are still too costly. [critical layers] approach tries to mitigate by&nbsp;dynamically computing and storing two critical cuboids.&nbsp;The upper layer, called the [observation layer], is the layer at which an analyst (or an automated system) would like to continuously study the data.&nbsp;The bottom layer, called the [minimal interest layer], is the minimally interesting layer that an analyst would like to study.\nLossy Counting Algorithm\nLet's say we'd like to estimate frequencies of items in a stream using the lossy counting algorithm.We set our margin of error, \\epsilon to be 0.1 and started to count frequencies.After a while, we decide to estimate the frequency of item e, so we search the corresponding item in our data structure D and find an entry of the form (e, f, \\Delta) whose value is (e, 9, 3).\nWhat will be the maximum frequency of item e?\nThe maximum frequency of item e can be determined by considering the frequency count (f) in the current entry and the maximum possible frequency count (Delta) from the previous entry for item e. In the given scenario, the entry for item e is (e, 9, 3). This means that the current frequency count for item e is 9, and the maximum possible frequency count from the previous entry is 3. -&gt; 9+3 = 12 What will be the minimum frequency of item e?\nWe know we have observed f=9 that we have counted. Choose the correct description of time series characteristic Cyclic movements or cyclic variations → Long-term oscillations about a trend line, which may or may not be periodic, Trend or long-term movements → a general direction in which a time-series is moving over a long interval of time\nSeasonal movements or seasonal variations → systematic or calendar related movements\nIrregular or random movements → sporadic motion of time series due to random or chance events, such as labor disputes or floods Given sequence&nbsp;\n2, 4, 6, 6, 4, 2, 0, 4\nWhat is the sequence of the moving average of order 4\n4.5, 5, 4.5, 3, 2.5 Given the same sequence,&nbsp;what is the sequence of the weighted moving average of order 2 with weights (2, 3)?\n3.2 5.2 6 4.8 2.8 0.8 2.4\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157365,"modifiedTime":1737554783000,"sourceSize":6604,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/8. Ensemble, Time series, Streams.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html":{"title":"1. Basic Measures for Information Retrieval","icon":"","description":"Evaluation in information retrieval involves assessing the quality of retrieved documents based on their relevance to a query. The following measures are commonly used:It is the proportion of retrieved documents that are actually relevant to the query. Precision is calculated using the formula: (relevant: actual positives)where TP (True Positives) represents the number of relevant documents correctly retrieved, and FP (False Positives) represents the number of irrelevant documents incorrectly retrieved.It is the proportion of relevant documents that were successfully retrieved by the system. Recall is calculated using the formula:where FN (False Negatives) represents the number of relevant documents that were not retrieved.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Precision (P):","level":3,"id":"Precision_(P)_0"},{"heading":"Recall (R)","level":3,"id":"Recall_(R)_0"},{"heading":"F-score (F1 score):","level":3,"id":"F-score_(F1_score)_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157415,"modifiedTime":1737554783000,"sourceSize":1813,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/1. Basic Measures for Information Retrieval.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html":{"title":"2. Informaiton Retrieval Techniques","icon":"","description":"Text processing involves working with a sequence of words or a bag of words, where words are referred to as terms. These terms are aggregated into documents, and a collection or corpus is formed by aggregating multiple documents. A document can be described using representative keywords known as index terms.Index terms serve as attributes in Information Retrieval and can be binary-valued, indicating their absence or presence.However, not all index terms have the same relevance in describing document contents. This is addressed by assigning numerical weights to each index term, such as frequency or TF-IDF, which act as attribute values.\nSelect index terms.\nBuild an index, which involves creating high-dimensional term and document frequency matrices.\nMatch the query to the index to retrieve optimal answers using approaches like the Boolean model, Vector space model, or Probabilistic model (where categories are modeled using probability distributions to determine the likelihood of a document belonging to a specific category, similar to Bayesian classification). Tokenising: The document is divided into individual words or tokens by separating them based on spaces and other punctuation marks. Punctuation is also removed.\nNormalisation and Stemming: The words are transformed to a canonical or standard form to eliminate variations in syntax. Stemming involves cutting off suffixes (and sometimes prefixes) that serve grammatical purposes but do not contribute to the word's meaning. For example, \"skipping\" is stemmed to \"skip.\" Normalisation maps a term to another term that has the same meaning. For instance, \"Practise\" is normalised to \"practice.\"\nRemove Stop Words: Common words that carry little meaning, such as articles (e.g., \"the,\" \"of,\" \"for,\" \"to,\" \"with\"), are removed from the index. Stop words can also be chosen based on the specific problem domain. For example, in a legal database, the term \"law\" might be considered a stop word.\n“text_mining.png” could not be found.Indexes need to link the (preprocessed) words in the document collection to the documents in which they occur. This is typically a&nbsp;term-document matrix&nbsp; or similar&nbsp;inverted index. A&nbsp;signature file&nbsp;is an alternative approach.An inverted index is well-suited to parallel computation using methods like MapReduce over distributed file systems.“text_indexing.png” could not be found.Assessing the similarity between a document and a query, which consists of a set of keywords. Different similarity metrics are used to measure the closeness of a document to the query.Boolean queries: In Boolean queries, such as those used in a library catalogue, the query is constructed using index terms connected by logical operators (not, and, or). The index can directly retrieve matching documents based on these queries, sometimes after preprocessing to remove stop words.Vector space model: The vector space model and document clustering approaches require a more sophisticated notion of document similarity. In this model, the query itself is treated as a document and is compared to other documents based on similarity. Ranking of matching documents is done by utilizing dot product or cosine similarity with weighted term vectors.\nDot Product similarity: This measures the similarity between two documents by taking the dot product of their weighted term vectors. Normalized Dot Product (cosine similarity): This is a variation of dot product similarity that normalizes the vectors, resulting in the cosine of the angle between them. If the vectors are orthogonal (angle of 90°), the similarity is 0, while if they are co-linear (angle of 0°), the similarity is 1.\nSophisticated weighting schemes, beyond simple boolean weights or term frequency counts, have proven to be beneficial in capturing the true semantic relationships between documents and queries.\nHere are three weighting heuristics based on term frequency and document frequency, but many other variants are used in practice. TF-IDF is a very common choice.\nTF (Term Frequency):\nMore frequent within a document -&gt; more relevant to the semantics of the document as a whole. Example: \"classification\" versus \"SVM\" Raw TF (Term Frequency) = tf(t, d) from the term-document matrix, which counts how many times term t appears in document d. However, document length varies, so the relative frequency within the document is preferred to avoid bias against short documents. Relevance is not linearly proportional to the term frequency, so normalization or scaling is performed. There are various ways to achieve this. One example is the Logarithmic Term Frequency:\nTF(t, d) = {\n0, if tf(t, d) = 0,\n1 + log10(tf(t, d)), otherwise. } IDF (Inverse Document Frequency):\nTerms that are less frequent among documents in the collection are more discriminative and hence more useful. Example: \"algebra\" versus \"science\" To assign a higher weight to rare terms than frequent terms, IDF (Inverse Document Frequency) is used. , where: n = total number of documents k = number of documents with term t appearing in them at least once (also called DF(t)) Example: Inverse document frequency, Kerry.\nTF-IDF (Term Frequency - Inverse Document Frequency):\nn&nbsp;= total number of documents\nk&nbsp;= number of documents with term&nbsp;t&nbsp;appearing&nbsp; in them at least once (also called&nbsp;DF(t))TF and IDF may be combined to form the TF-IDF measure.Terms that are frequent within a document tend to have a high TF, which in turn leads to a high TF-IDF weight. Terms that are selective among documents tend to have a high IDF, which also contributes to a high TF-IDF weight.\nDocuments: Weight document terms by TF.\nQuery: Weight query terms as TF-IDF.\nCompute relevance (Q, ) = cosine(Q, ).\nThat is, the relevance of document , expressed as a TF-weighted vector, to the query Q, expressed as a TF-IDF-weighted vector, is given by the cosine similarity of the vectors.In the above, sometimes the Query and Document vectors are normalized in steps 1 and 2, respectively, by dividing each element of the vector by the length of the vector (also called the vector norm or Euclidean norm). This computation involves taking the square root of the sum of the squares of each element (similar to the Pythagorean theorem). Normalizing ensures that each element in the vector is within the interval [0,1]. However, it is worth noting that in step 3, when applying the cosine formula, the divisor is the product of the lengths of the two vectors. Therefore, normalizing the vectors beforehand is not strictly necessary to achieve the same result.Example: The visualization below depicts the query and three documents (D1, D2, and D3) projected onto a 2-dimensional vector space (or where |v| = 2, representing two dimensions). Among these, D3 is the most relevant because the angle between the Query and D3 is the smallest. Similarly, the ranking order based on relevance will be D3, D2, D1.“cosine_90.png” could not be found.Evaluate relevance ranking\nCarry out experiments using Precision, Recall or F-score, where&nbsp; you have a \"gold standard\" set of validation queries with the \"right\" answers already selected by people.\nOn-line learning from user behaviour and feedback can be used to improve performance:&nbsp;relevance feedback\nThis is the underlying basis of modern web search, but there are&nbsp;many&nbsp;more things done, too.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Basic Process:</strong>","level":3,"id":"**Basic_Process**_0"},{"heading":"Selecting index terms","level":3,"id":"Selecting_index_terms_0"},{"heading":"Building an Index","level":4,"id":"Building_an_Index_0"},{"heading":"Matching the query to the index","level":3,"id":"Matching_the_query_to_the_index_0"},{"heading":"Assign weights to term occurrences:","level":3,"id":"Assign_weights_to_term_occurrences_0"},{"heading":"Ranking in the vector space model","level":4,"id":"Ranking_in_the_vector_space_model_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157427,"modifiedTime":1737554783000,"sourceSize":8082,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/2. Informaiton Retrieval Techniques.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html":{"title":"3. Text mining problems","icon":"","description":"• Collect sets of keywords or terms that occur frequently together and then find the association or correlation relationships among them\nPre-process the text data by parsing, stemming, removing stop words, etc.\nInvoke&nbsp;association mining&nbsp;algorithms\n• Consider each&nbsp;document&nbsp;as a&nbsp;transaction\n• View a set of words in the document as a set of items in the transaction\nTerm level&nbsp;association mining Can extract compound associations as entities or domain concepts (e.g. \"New South Wales\", or \"big data\").\nCan replace human effort for tagging documents in databases.\nThe number of meaningless results and the execution time is greatly reduced over word-based search or mining Motivation\n• Automatic classification for the large number of on-line text documents\n(Web pages, e-mails, corporate intranet documents, etc.)Classification process\n• Data pre-processing\n• Definition of training set and test sets\n• Creation of the classification model using the selected classification algorithm\n• Classification model validation\n• Classification of new/unknown text documents ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Keyword based association analysis","level":3,"id":"Keyword_based_association_analysis_0"},{"heading":"<strong>Motivation</strong>","level":4,"id":"**Motivation**_0"},{"heading":"<strong>Association analysis process</strong>","level":4,"id":"**Association_analysis_process**_0"},{"heading":"Text Classification","level":3,"id":"Text_Classification_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157430,"modifiedTime":1737554783000,"sourceSize":2856,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/3. Text mining problems.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html":{"title":"4. Web mining","icon":"","description":"• The biggest source of information\n• Distributed, dynamic, linked, new data types\n• Web pages contain semi-structured data (HTML and XML), as well as free format text, images, videos, sounds, etc.\n• Many Web pages are dynamically created, often by accessing databases (for example online stores, information directories, search engines)\n• Web pages are linked\n• Some parts of the Web are only accessible to certain people (logins required) • Mining the Web page layout structure\n• Mining the Web's link structure\n• Mining multimedia data on the Web\n• Automatic classification of Web documents\n• Weblog mining\n• Linked data mining\nMining Web log records to discover user access patterns of Web pages For example: “after looking at a&nbsp; digital camera pages, 70% of users will look at memory card pages” Web log entry: URL requested, source IP address, time stamp, browser details, cookies, etc.Apply association and frequent pattern mining, and trend analysis Low level details, need to be cleaned, condensed, and transformed\nUse data stream mining techniques Applications: e-Commerce, improve Web system design (navigation and caching), Web page pre-fetching, adaptive Web sites (that depend upon user's history)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>The Web as a data source</strong>","level":3,"id":"**The_Web_as_a_data_source**_0"},{"heading":"<strong>Types of Web mining</strong>","level":4,"id":"**Types_of_Web_mining**_0"},{"heading":"Web Usage Mining","level":4,"id":"Web_Usage_Mining_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157434,"modifiedTime":1737554783000,"sourceSize":1434,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/4. Web mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html":{"title":"5. Word meaning by embedding","icon":"","description":"Recently, the&nbsp;Word2Vec&nbsp;models have become very popular for their ability to&nbsp; represent word meaning as a vector of probabilities of association with other&nbsp;context&nbsp;words, partially replacing earlier approaches such as&nbsp;latent semantic analysis.One important training algorithm is&nbsp;Continuous Bag of Words&nbsp;whereby neural net classifiers are trained on a&nbsp;window&nbsp;of the words surrounding the target word in the corpus to predict the missing word. The other algorithm,&nbsp;Continuous Skip-gram&nbsp;does the reverse and predicts the surrounding words based on a single focus word.&nbsp;It turns out that this causes the neural net to represent&nbsp;meaning&nbsp;in the sense that the vectors in the learnt classifiers for words can be arithmetically manipulated to derive the representations for similar words. Famously,vector(”King”) - vector(”Man”) + vector(”Woman”) = vector(\"Queen\")The success of word2vec initiated a furious interest in building language models that aim to build a domain-independent representation of words (that is, language) that can be used in many different language tasks.&nbsp; These language models typically comprise both a trained neural network and&nbsp;embeddings,&nbsp;where embeddings are vectors of the form produced by word2vec, that is, for each word, a vector of values that represent the meaning of a word via its relationship to other words in training text.&nbsp; The new representation (e.g. trained neural networks and embeddings) has some features that make it easy to feed in to ML pipelines. In particular,&nbsp; labelled training data is not required. A language model predicts what is the next word given a sequence of words.&nbsp;&nbsp; Input: A sequence of words , …, Output: Predict P(|,…,) Completing search queries&nbsp;\nSuggesting the next word on keyboards\nChatbots ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Language Models","level":3,"id":"Language_Models_0"},{"heading":"The applications of language models&nbsp;are","level":4,"id":"The_applications_of_language_models are_0"},{"heading":"How the language models are built","level":4,"id":"How_the_language_models_are_built_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157438,"modifiedTime":1737554783000,"sourceSize":3252,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/5. Word meaning by embedding.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html":{"title":"6. Text & Web Mining","icon":"","description":"\nText mining is founded in Information Retrieval research, which also developed Web Search. How does Information Retrieval relate to Database Systems?\nDatabase systems are designed for some things that IR systems do not offer, including\ntransaction management, structured objects, specialist data types like spatial and temporal, updateIR systems work well for some things that Database systems do not, such as\nunstructured and semi-structured documents, approximate queries, best answers amongst many alternatives\nIt is usual for any text retrieval or text mining application to pre-process collection data as follows. Tokenise&nbsp;to split&nbsp;text into words and remove punctuation,&nbsp; Normalise&nbsp;to translate each token to a form that will be considered equivalent to the original form,. example (convert \"run,\" \"runs,\" and \"running\" into the base form \"run.\") Remove&nbsp;stop words,&nbsp; Stop words&nbsp;could be customised to the domain of application so that overly frequent, non-selective words in the domain are removed Which of the following terms has been stemmed as a stage of text preprocessing?\nRun\n“web_mining.png” could not be found.\nThe table here shows a term-document matrix (TDM). Which of the following are true statements about the TDM? If we update the TDM for Doc 4 which says \"Elephants roam freely in Melbourne.\" then the Total count for term \"melbourne\" will increase by 1 to 5\nAllowing for stop words and normalisation, it is plausible that Doc2 is the sentence \"Zoo animals eat their food very slowly.\" Consider the&nbsp; query \"Who is crazy enough to eat food in Melbourne\"?Referring to the DTM above, what is the feature vector for the query?\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"6. Text &amp; Web Mining","level":1,"id":"6._Text_&_Web_Mining_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084157579,"modifiedTime":1737554783000,"sourceSize":5887,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/6. Text & Web Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html":{"title":"1. Semantic Web","icon":"","description":"•Web of data where web content is processed by machines, without direct human readers, but not a separate Web.•The web as a huge, dynamic, evolving database of facts, rather than pages, that can be interpreted and presented in many ways (mashups).•Fundamental importance of&nbsp;ontologies&nbsp;to describe the fact that is the data. RDF(S)&nbsp; emphasises labelled links as the source of meaning: essentially a graph model either stored locally in a triple store or distributed as linked open data. A label (that is a URI) uniquely identifies a concept and reuse of the label implies the same meaning. OWL adds emphasis on inference as the source of meaning: a label also refers to a package of logical axioms with a proof theory over a model-theoretic semantics. Mostly, the two notions of meaning coincide. Identity: things be referred to Relationships: things be connected to/ described by other things. This is where the notion of a&nbsp;graph&nbsp;comes from. Inference: general statements about things can be applied to particular things and don’t need to be restated every time. This is&nbsp; how&nbsp;knowledge&nbsp;arises. Encoding: serialisation in RDF 1.1: Turtle, RDF/XML, N-Triples, N-Quads, TriG, JSON-LD Open World Assumption: All information to hand is always assumed to be only partial, i.e. from the absence of a statement alone, a deductive reasoner cannot (and must not) infer that the statement is false. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Linked Open data","level":3,"id":"Linked_Open_data_0"},{"heading":"Semantics","level":4,"id":"Semantics_0"},{"heading":"Components","level":4,"id":"Components_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156530,"modifiedTime":1737554783000,"sourceSize":1543,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/1. Semantic Web.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html":{"title":"2. Foundation Technologies","icon":"","description":"c\nThe World Wide Web Consortium (W3C), is the governing standards body for the Web, including the Semantic Web. The Semantic Web is often depicted graphically as a \"Layer Cake\".“SWlayer.gif” could not be found.Here, we address the lower parts of the cake, i.e.&nbsp;URIs, RDF, RDFS, SPARQL and OWL. \"Rules\" are now well-developed in the semantic web standards stack;&nbsp; but the notions of&nbsp; \"Crypto\", \"Unifying Logic\" , \"Proof\" and \"Trust\" remain active research in Semantic Web as well as the broader research community and Web standards development.URIs are the identifiers of the Semantic Web.&nbsp; They leverage the well-developed URLs of the Web, but also permit URNs that are not URLs. Technically, we should be speaking of \"IRIs\" that use an internationalised chacter set, but the term \"URI\" is popularly used.Tim Berners-Lee’s&nbsp; “four rules”: Use URIs as names for things Use HTTP URIs so that people can look up those names When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL) Include links to other URIs so that they can discover more things. \"Things” in the semantic web, even very abstract ones, are uniquely named by URIs.\nA URI is an identifier, not necessarily a locator as is a URL.\nIRIs (International Resource Identifiers) allow extended characters and can be used in place of URIs.\n“URI.png” could not be found.&nbsp; &lt;scheme&gt; : &lt;path&gt; [? &lt;query&gt;] [# &lt;fragment&gt;]\n&nbsp;ANU website: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.anu.edu.au/\" target=\"_self\">http://www.anu.edu.au/</a>. This is a URL but is reused as a URI referring to the thing that is the ANU website. <br>The actual ANU itself:&nbsp; <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://anu.edu.au/resource#id\" target=\"_self\">http://anu.edu.au/resource#id</a>. This is a URI that refers to the thing that is the Australian National University. It also happens to be a URL. URIs include URNs (e.g., urn:oid:2.16.840&nbsp; e.g. urn:doi:10.1109/MIS.2013.102)\nRDF&nbsp;refers to the&nbsp;Resource Description Framework&nbsp;(but it is always called by its acronym). It was originally developed for representation of&nbsp;Metadata&nbsp;(i.e data about resources e.g.&nbsp; in Dublin Core), and is widely used that way.&nbsp; But it is now designed for representing data itself, i.e, as a&nbsp; general data model. According to the standard, it must be encoded in XML (as \"RDF/XML\"), but other serialisations&nbsp; are much more popular, including \"Turtle\" and \"JSON-LD\" which is a JSON encoding. RDFS&nbsp;refers to the&nbsp;Resource Description Framework Schema.&nbsp;&nbsp;RDFS is both a name-space and some formal modelling primitives and semantics that extend the descriptive power of RDF.&nbsp; For simplicity of explanation, we do not present RDFS separately here, but instead wrap some of it into our presentation of RDF first and, later, some into our presentation of OWL. RDF uses a flexible graph data model which doesn’t require much systematic design in advance, is easily extensible and seems to fit with the&nbsp; foundational Web idea of decentralised creation and rich interconnections RDF graphs are defined&nbsp; by nodes (usually labelled), which are connected by directed (one-way), labelled arcs. This is a&nbsp;triple&nbsp;&nbsp;that asserts a fact. The triple has three components, a node that is the&nbsp;Subject,&nbsp;a directed arc that is the&nbsp;Predicate, and a node that is the&nbsp;Object.&nbsp; The&nbsp;Predicate&nbsp;is also somewhat loosely called the&nbsp;Property&nbsp;and more formally is a binary relation.“RDF_example.png” could not be found.&nbsp;In the serialisation syntax,&nbsp;Turtle, it is written as a statement terminated by a full stop, \".\", as follows. URIs are enclosed by \"&lt;&nbsp;&nbsp;&nbsp; &gt;\" angle brackets. The property is also a URI but is written in an abbreviated form (without angle brackets) here.“serialisation_turtle.png” could not be found.Here, the name of the crcpress has a string value, “CRC Press”, that is not a URI, and is depicted in a rectangular box.&nbsp; Unlike the subject and predicate in this graph, the object is a literal, not a URI.\n“CRC.png” could not be found.<br>\nIn turtle, this is written as follows. Note the trailing \".\" that terminates the triple statement. &lt; <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://crcpress.com/uri\" target=\"_self\">http://crcpress.com/uri</a> &gt;&nbsp; &lt; <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://example.org/name\" target=\"_self\">http://example.org/name</a> &gt;&nbsp; “CRC Press” .Reading and writing all those&nbsp; long URIs is painful.&nbsp; RDF re-uses the XML&nbsp;namespace&nbsp;idea to abbreviate URIs.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Universal Resource Identifiers (URIs)","level":4,"id":"Universal_Resource_Identifiers_(URIs)_0"},{"heading":"URI structure","level":4,"id":"URI_structure_0"},{"heading":"URI syntax:","level":3,"id":"URI_syntax_0"},{"heading":"Example:","level":3,"id":"Example_0"},{"heading":"RDF/RDFs","level":3,"id":"RDF/RDFs_0"},{"heading":"RDF for relations","level":3,"id":"RDF_for_relations_0"},{"heading":"Example","level":3,"id":"Example_1"},{"heading":"Encode Literals that are not things","level":3,"id":"Encode_Literals_that_are_not_things_0"},{"heading":"Namespace abbreviations","level":3,"id":"Namespace_abbreviations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156534,"modifiedTime":1737554783000,"sourceSize":9571,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/2. Foundation Technologies.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html":{"title":"3. The Web Ontology Language","icon":"","description":"The Web Ontology Language (OWL) is a modellng language (like&nbsp;UML)&nbsp; that is&nbsp;logical,&nbsp;not specifically diagrammatic, and is expressed as logical axioms over which deductive&nbsp;inference&nbsp;is supported. It is a very expressive data modelling language with a formal semantics (c.f. UML). Is fundamentally a logical language: a&nbsp;Description Logic: a decidable fragment of First-Order Predicate Calculus with only unary and binary predicates permitted. Is best used with a&nbsp;reasoner&nbsp;for logical inference. There are many OWL docs out there which do not say what they claim to say and which do not respect the semantics because someone just looked at the syntax and went from there Especially happens&nbsp; with property owl:sameAs, used widely for linked data over distributed Web endpoints OWL 2 standardised October 2009 (supersedes OWL 2004). Minor update December 2012 (for XML types). Spec:&nbsp;<a data-tooltip-position=\"top\" aria-label=\"http://www.w3.org/standards/techs/owl\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.w3.org/standards/techs/owl\" target=\"_self\">http://www.w3.org/standards/techs/owl#w3c_all</a> Comes with several syntaxes:&nbsp;RDF/XML&nbsp;is normative;&nbsp;Functional-Style&nbsp;syntax is used in the specs; OWL/XML is an XML encoding of that;&nbsp;Manchester Syntax&nbsp;is readable (e.g.&nbsp; in Protégé ontology editor),&nbsp;Turtle&nbsp;is popular Is meant to be built on top of RDFS, but they are not quite comfortable co-habitating. Advise to avoid OWL2 Full = OWL2 DL + RDF(S). Instead use OWL 2 DL or a sublanguage (OWL EL, OWL QL, OWL RL): think of it as a new language that reuses some RDF(S)&nbsp; vocabulary Prefix: ex: &lt;http://example.org/ontology#&gt;\nPrefix: rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPrefix: rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nPrefix: owl: &lt;http://www.w3.org/2002/07/owl#&gt; Ontology: ex:myOntology Class: ex:Person SubClassOf: owl:Thing Class: ex:Student SubClassOf: ex:Person Class: ex:Teacher SubClassOf: ex:Person Property: ex:hasAge Domain: ex:Person Range: xsd:integer Individual: ex:John Types: ex:Person Facts: ex:hasAge 25 Individual: ex:Mary Types: ex:Person Facts: ex:hasAge 30 Open World Assumption&nbsp;means Can assert negative things, but cannot infer it from the absence of the positive thing (beware, database modellers!)\nNo&nbsp;unique names assumption: i.e cannot infer that two things are not the same because they have different names (URIs) OWL is strongly typed Disjoint domains of&nbsp;individuals,&nbsp;classes,&nbsp;object properties,&nbsp;datatype properties,&nbsp;datatypes,&nbsp;&nbsp;annotation properties.\nWhere the same name is used for different ones, they are different things (punning). OWL modelling is about defining statements about the relationships between and amongst various classes, properties and individuals. There are two predefined classes in OWL: \"owl:Thing\" and \"owl:Nothing\". \"owl:Thing\" represents the most general class and serves as the root of the class hierarchy, while \"owl:Nothing\" represents the empty class with no instances.\nThere are two predefined object properties in OWL: \"owl:topObjectProperty\" and \"owl:bottomObjectProperty\".\n\"owl:topObjectProperty\" represents the most general object property and \"owl:bottomObjectProperty\" represents the empty object property.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"OWL preliminaries","level":3,"id":"OWL_preliminaries_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156538,"modifiedTime":1737554783000,"sourceSize":9197,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/3. The Web Ontology Language.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html":{"title":"4. Semantic Web Mining","icon":"","description":"Having put in place the basic principles, data models and standards of the semantic Web, now we turn to the question of mining all that Semantic Web data.The approach is called&nbsp;OWL-Miner.&nbsp;It is a rule-learner that explores the space of (OWL) concept descriptions to find one that best describes the training data.&nbsp; It has access to an OWL ontology describing the application domain from which the training data is drawn, and it uses this to find very readable descriptions of potentially&nbsp; complex relationships among things, for example, relationships amongst the structural components of an object.OWL-miner can be used for a range of learning problems, but for this purpose we describe it as a&nbsp;binary classification&nbsp;algorithm.The problem can be stated as follows: Given a knowledge base comprising An OWL ontology&nbsp; of axioms introducing the classes and relationships of the problem domain, including an identified target class\nAn hypothesis language &nbsp;defined as a choice of class constuctors (such as union,&nbsp; intersection, negation, cardinality, etc). RDF triples giving&nbsp;positive&nbsp;example instances of the target class,&nbsp; and&nbsp;negative&nbsp;example instances that are not in the target class,\nRDF triples that further describe the positive and negative examples by features expressed as class and property instances of the ontology;\nA quality function to assess hypotheses (such as&nbsp;accuracy&nbsp;or&nbsp;f-measure) Find an OWL class expression in&nbsp;&nbsp;that describes the positive examples but does not describe the negative examples to optimally satisfy a bounded quality function (such as&nbsp;accuracy &gt; 95%). OWL-miner&nbsp; is a top-down refinement-based learner&nbsp;that searches for useful class descriptions by (in principle) enumerating&nbsp;every&nbsp;possible expression and testing them for quality against the training data. It starts with a very simple expression and gradually builds a tree of more complex expressions from there, checking them as it goes.“OWL learning.png” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"OWL learning: Example","level":3,"id":"OWL_learning_Example_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156542,"modifiedTime":1737554783000,"sourceSize":3379,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/4. Semantic Web Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html":{"title":"5. Semantic Web","icon":"","description":"Sir Tim Berners-Lee, the inventor of the Web and 2016 Turing Award winner,&nbsp; has said that he always wanted the Web to be something beyond human-readable documents.&nbsp; In his 2009 TED talk&nbsp;[The Next Web],&nbsp;he asks people to put data on the Web. He describes&nbsp;linked data&nbsp;as using http names no longer just for documents but also for&nbsp;real world things&nbsp; The technical name for these&nbsp;http names&nbsp;to which he refers is (abbreviated) URI.&nbsp; When you resolve one of those http names you get&nbsp; important information back.&nbsp; He emphasises the power of explicit&nbsp;relationships&nbsp;allowing computational follow-the-links, like in old-fashioned hypertext. In the semantic web standards stack, relationships are modelled by&nbsp;predicates.\nWhat is the&nbsp;open world&nbsp;assumption?\nThat you cannot infer the negative of some statement from the absence of the positive form. What does the following RDF say? Check each statement that the Turtle expression includes.\n@prefix rdf: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" target=\"_self\">http://www.w3.org/1999/02/22-rdf-syntax-ns#</a> .<br>\n@prefix rdfs: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.w3.org/2000/01/rdf-schema#\" target=\"_self\">http://www.w3.org/2000/01/rdf-schema#</a> .<br>\n@prefix : <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://example.org/mine#\" target=\"_self\">http://example.org/mine#</a> .<br>\n@prefix xsd: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.w3.org/2001/XMLSchema#\" target=\"_self\">http://www.w3.org/2001/XMLSchema#</a> .\n:hexagon a Shape ;\nrdfs:comment “six sided polygon” ;\n:sides “7”^^xsd:integer ;\n:canBeDrawnWith [ :has :pencil ] . The hexagon is a Shape\nThe hexagon has the comment&nbsp; \"six sided polygon\", The hexagon has 7 sides, The hexagon can be drawn with something and that thing has a pencil, (In Turtle: ) :hexagon rdf:type :Shape\nConsider the following OWL ontology of 3 classes and one object property. [N.B. To interpret the&nbsp; compact notation below, consider, for example, that the first line below could be written as three Turtle triples after any prefix declarations as &nbsp; \":Alex&nbsp; a :Person .&nbsp;&nbsp;&nbsp;:Sandy&nbsp; a :Person .&nbsp;:Russell&nbsp; a :Person .&nbsp;\"] Person: { Alex, Sandy, Russell }&nbsp; Student: { Alex, Sandy, Peta }Subject: { AI, DB, CM, SE }Likes: { (Alex, AI), (Sandy, SE) }\nEnumerate all the&nbsp;Individuals&nbsp;in this ontology.\nAlex, Sandy, Russell, Peta, AI, DB, CM, SE Which individuals are necessarily in the domain and range of the Likes property?\ndomain: {Alex, Sandy}, range: { AI, SE } What are some distinctive features of WikiData&nbsp; as a knowledge graph exemplar, as compared to&nbsp; relational database technology? It is designed for data on the Web, first., It uses a graph data model, emphasising&nbsp; relations as links&nbsp; between entities, Entities in a Knowledge Graph are subjects or objects of an RDF triple, with a&nbsp; named predicate (or property) relating them, Knowledge Graphs can be readily inter-connected with other independently-developed Knowledge Graphs, Wkidata contains more than 80 million nodes connected by more than a billion edges\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html","pathToRoot":"../../../../..","attachments":[],"createdTime":1741084156545,"modifiedTime":1737554783000,"sourceSize":2909,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/5. Semantic Web.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html":{"title":"Data Mining Links","icon":"","description":"\nExplosive Growth of Data: Collection and Availability: Automated tools\nSources: Business, Science, Society and everyone Interesting patterns and knowledge&nbsp;\nfrom data (**non-trivial,&nbsp;implicit,&nbsp;previously unknown&nbsp;and&nbsp;potentially useful)\nBuilding models\nevaluating the models&nbsp;for usefulness\nKDD\nData cleaning -&gt; Data integration -&gt; Data selection -&gt; Data transformation -&gt; Data mining -&gt; Pattern evaluation -&gt; Knowledge representation -&gt; Deployment Interestingness: Easily understood Valid&nbsp;(true), with some required degree of&nbsp;certainty, on sample data Potentially useful or actionable\nNovel Objective: Based on pattern structure and statistical properties\nRequire a user-selected threshold to be considered interesting\nExamples include support, confidence, coverage, accuracy, and simplicity\nMeasures may be specific to a particular mining method and cannot be compared across methods Subjective: Based on user beliefs about the data\nDomain-dependent and require judgment by the data miner\nResults need to be communicated and shared with domain experts\nMay include unexpected findings, actionable results, or confirming a previously held belief Data Mining for Generalisation Data warehousing\nMultidimensional class or concept description: Characterization and discrimination\nGeneralize, summarize, and contrast data characteristics\nOLAP operations can be used to summarise data along user-specified dimensions Frequent Patterns, Association and Correlation Analysis Frequent patterns\nAssociation, correlation vs. causality Classification and Regression for Prediction Classification\nRegression\nTypical methods and applications Clustering/ Cluster Analysis Unsupervised learning\nGroup data to form new categories\nPrinciple: Maximizing intra-cluster similarity &amp; minimizing inter-cluster similarity Outlier Analysis Outlier: A data object that does not comply with the general behavior of the data\nMethods: by-product of clustering or regression analysis\nUseful in fraud detection, rare events analysis Others Sequence, trend, and evolution analysis\nStructure, network and text analysis\nInductive logic programming <a data-href=\"1.5 Challenges in Data Mining\" href=\"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1.5 Challenges in Data Mining</a> Basic Competency Selecting the right tool for the job\nEvaluating the output of the tool\nInterpreting the results in the context of the question Mining Methodology Mining various and new kinds of knowledge\nMining knowledge in multi-dimensional space\nInterdisciplinary skills\nHandling noise, uncertainty, and incompleteness of data\nPattern evaluation and pattern- or constraint-guided mining Leveraging Human Knowledge Goal-directed mining\nInteractive mining\nIncorporation of background knowledge\nPresentation and visualization of data mining results Efficiency and Scalability Data reduction\nEfficiency and scalability of data mining algorithms\nParallel, distributed, stream, and incremental mining methods Diversity of Data Types Handling complex types of data\nMining dynamic, networked, and global data repositories Data Mining and Society Social impacts of data mining\nPrivacy-preserving data mining\nBias and algorithmic transparency Personal data used in data mining raises concerns about privacy\nOECD principles for fair use of personal data include limiting collection, ensuring quality and security, specifying purposes, limiting use, and enabling individual participation and accountability\nTo protect privacy when data mining, data security enhancing techniques can include multilevel security models, encryption, intrusion detection, and physical access control\nPrivacy-preserving data mining techniques: randomization methods, k-anonymity methods, L-diversity methods, federated analytics, downgrading results, differential privacy, and statistical disclosure control. Data is structured as objects with properties or attributes and may include relationships to other objects.\nObjects may also be called samples, instances, data points, observations, rows, tuples, records, or vectors, while attributes may be called fields, variables, dimensions, properties, or features.\nThe type of an attribute defines how its values are represented, how it can be manipulated, and appropriate methods for mining.\nAttribute types include nominal (categories, states, classes), binary (nominal with only two values), ordinal (values have a meaningful order), numeric interval-scaled (measured on a scale of equal-sized units), and numeric ratio-scaled (inherent zero-point, permitting reasoning over multiples of values).\nAnother classification of attributes is discrete (finite or countably infinite set of values) and continuous (real numbers as values).\nDiscrete attributes may include binary, nominal, and ordinal attributes as a special case.\nContinuous attributes are typically represented as floating-point variables. Central tendency measures in statistics represent the typical or central value of a dataset.\nThe three most common measures of central tendency are mean (average value), median (middle value), and mode (most common value).\nThe mean is the sum of all values in a dataset divided by the number of values, but it is sensitive to outliers.\nThe median is the middle value of a dataset when the values are arranged in order, and it is less sensitive to outliers than the mean.\nThe mode is the value that appears most frequently in a dataset and is useful for dealing with categorical or discrete data.\nThe arithmetic mean is the same as the regular mean but takes into account the weights of the values in the dataset.\nThe weighted mean is a variation of the arithmetic mean that multiplies each value by a weight that reflects its importance in the dataset.\nThe midrange is the average of the largest and smallest values and is a cheap indication of central tendency.\nSkewness is a measure of asymmetry in data and can be indicated by the difference between the mean and mode. The range is the difference between the maximum and minimum values in a dataset.\nQuartiles divide a dataset into four equal parts. The first quartile (Q1) is the 25th percentile, the second quartile (Q2) is the median, and the third quartile (Q3) is the 75th percentile.\nThe Interquartile Range (IQR) is the difference between the third quartile and the first quartile.\nVariance measures how much the data deviate from the mean.\nStandard Deviation is the square root of the variance and measures the amount of variation or dispersion of a set of values.\nChebyshev's inequality states that at least (1 - 1/k^2) x 100% of the observations are no more than k standard deviations from the mean.\nThe five-number summary consists of the minimum value, Q1, median, Q3, and maximum value of a dataset.\nBoxplots can be used to visually compare sets of compatible data, with the box representing the quartiles and the whiskers extending to the min and max values or to 1.5 x IQR if there are outliers.\nAn outlier is a value below Q1 - 1.5 IQR or above Q3 + 1.5 IQR, and they are often plotted individually in a boxplot. In statistics, computational cost of all processing stages should be considered when dealing with big data.\nSome measures of data character, data summarization, and data pattern interestingness are computationally more difficult to compute than others.\nMeasure functions can be classified as distributive, algebraic, or holistic based on their computational complexity.\nDistributive functions are applied to each partition of the data and then combined to obtain the result.\nAlgebraic functions can be computed by a function with a bounded number of arguments, each of which is obtained by applying a distributive function.\nHolistic functions are difficult to compute efficiently and often require approximations.\nMean, median, mode, midrange, quartiles, variance, standard deviation, and interquartile range are commonly used measures of central tendency, variability, and distribution in data.\nBoxplots can be used to summarize and compare data, and to identify outliers. Measures of correlation are used to assess the relationship between two variables. Pearson's correlation coefficient measures the strength and direction of the linear relationship between two continuous variables. The coefficient ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation. Spearman's rank correlation coefficient and Kendall's tau coefficient are nonparametric measures of correlation that do not require the assumption of a linear relationship between the variables. Spearman's rank correlation coefficient is based on the ranks of the observations, while Kendall's tau coefficient is based on the signs of the differences between the observations. These measures of correlation can be calculated using various statistical software packages or programming languages. Similarity and dissimilarity measures are used to assess how alike or unlike objects are to each other.\nSimilarity is a numerical measure of how alike two data objects are, often falling in the range of [0, 1].\nDissimilarity, also known as distance, is a numerical measure of how different two data objects are, with the minimum dissimilarity being 0 and upper limit varying.\nA dissimilarity matrix is a common data structure used to represent dissimilarity among objects, with an n x n matrix data structure showing dissimilarity or distance d(i,j) for all pairs of n objects i and j.\nThe matrix is symmetric or triangular with zeros along the diagonal and duplicates in the upper right triangle.\nSimilarity can be expressed as a function of dissimilarity, such as sim(i,j) = 1 - d(i,j), when d(i,j) is normalized to [0,1].\nDistances can be calculated for each attribute type, and then combined to give an overall object distance for objects with heterogeneous types of attributes. Simple matching method: Calculate the number of matches (m) between two objects for all their attributes and divide it by the total number of attributes (p) to obtain the dissimilarity measure (d). The similarity measure (sim) can be obtained by subtracting d from 1. This method is based on the idea that objects with a higher number of matches are more similar.\n2. Mapping to binary attributes method: Create a new binary attribute for each state of the nominal attribute and map each object to a corresponding object with binary attributes. Then, use any method for proximity of binary attributes to calculate the similarity or dissimilarity. This method is also known as one-hot encoding. Binary attributes have two values: 0 or 1. There are two types of binary attributes: symmetric and asymmetric. Dissimilarity for symmetric binary attributes : d(i,j) = (r + s) / (q + r + s + t) Dissimilarity for asymmetric binary attributes: d(i,j) = (r + s ) / (q + r + s )\nsim(i,j) = 1 - d(i,j) Treating binary attributes as numeric can be misleading. The contingency table is used to count the number of matching and non-matching attributes for two objects. For symmetric binary dissimilarity, the dissimilarity is defined as the proportion of non-matching attributes amongst all attributes. For asymmetric binary dissimilarity, the dissimilarity is defined as the proportion of non-matching attributes among the matching attributes, and the corresponding similarity is called the Jaccard coefficient. Alternatively, the Jaccard similarity between two objects can be thought of as the size of the intersection of the two sets divided by the size of the union of the sets. There are different methods of normalization, such as:\nMin-Max Scaling: This is the most commonly used normalization technique, which rescales the data to a fixed range between 0 and 1. The formula for min-max scaling is:\nwhere is the original data and is the normalized data.\nZ-Score Normalization: This technique standardizes the data by transforming it to have a mean of 0 and a standard deviation of 1. The formula for z-score normalization is:\nwhere is the original data, is the mean of the data, is the standard deviation of the data, and is the normalized data.\nDecimal Scaling: This technique involves dividing each observation by a power of 10 to move the decimal point. The number of decimal places is chosen based on the maximum absolute value of the data. For example, if the maximum absolute value of the data is 1000, we can divide each observation by 1000 to move the decimal point three places to the left. A data warehouse is a decision support database that is maintained separately from the operational database and supports information processing by providing a solid platform of consolidated, historical data for analysis.\nA data warehouse is subject-oriented, integrated, time-variant, and nonvolatile.\nData warehousing is the process of constructing and using data warehouses.\nData warehouses are organised around major subjects and provide a simple and concise view around particular subject issues by excluding data that are not useful in the decision support process.\nData warehouses are constructed by integrating multiple, heterogeneous data sources, and data cleaning and data integration techniques are applied.\nThe time horizon for the data warehouse is significantly longer than that of operational systems, and every key structure in the data warehouse contains an element of time, explicitly or implicitly.\nData warehouses are nonvolatile, meaning that they are a physically separate store of data transformed from the operational environment, and operational update of data does not occur in the data warehouse environment.\nData warehouses are built specifically for analytics to support decision making, that is, online analytical processing (OLAP), while operational databases are used for day-to-day operations, that is, online transaction processing (OLTP).\nThe formula for calculating the number of cells in an n-dimensional data cube with m levels for each dimension is C = (m+1)^n - 1, where C is the total number of cells in the cube, n is the number of dimensions in the cube, and m is the number of levels for each dimension. A data warehouse uses a multidimensional data model called a data cube.\nA data cube allows data to be viewed and analyzed in multiple dimensions.\nThe cube axes represent dimensions such as item, time, or location.\nThe cube cells hold measures such as dollars sold, quantity sold, and quantity returned.\nA cuboid is a component of a data cube and can be used to aggregate measures along one or more dimensions of the base cuboid.\nThe top-most 0-D cuboid is called the apex cuboid, which holds the highest level of summarization in a single cell.\nThe lattice of cuboids forms a data cube.\nThe bottom-most base cuboid contains cells for every possible combination.\nData cubes are commonly used in data warehousing and business intelligence to analyze and report on large amounts of data. A concept hierarchy provides mappings from low-level concepts to higher-level, more general concepts.\nThey allow for the modeling of data at different levels of abstraction.\nExamples include a geography hierarchy and a sales organization hierarchy.\nIn a geography hierarchy, data is aggregated by country at the top level and becomes more granular as you move down to store location.\nIn a sales organization hierarchy, data is aggregated by year at the top level and becomes more granular as you move down to the day level. Dimension tables, such as item and time, and a fact table containing measures are necessary for data warehousing.\nStar schema is a structure where a fact table in the middle is connected to a set of dimension tables.\nIn the star schema, the primary key for the fact table is composed of a key for each dimension and the remaining attributes are the measures.\nSnowflake schema is a refinement of star schema where some dimensional hierarchy is normalized into a set of smaller dimension tables.\nFact constellation is a structure where multiple fact tables share dimension tables, viewed as a collection of stars or a galaxy schema.\nSnowflake and Constellation schemes are extensions of the basic Star schema to more complex data. There are three kinds of data warehouse applications: information processing, analytical processing (OLAP), and multidimensional data mining (OLAM).\nInformation processing supports querying, basic statistical analysis, and reporting using crosstabs, tables, charts and graphs.\nAnalytical processing (OLAP) is multidimensional analysis of data warehouse data that supports basic OLAP operations, slice-dice, drilling, pivoting and derives information summarised at multiple granularities from user-specified subsets.\nMultidimensional data mining (OLAM) is knowledge discovery from hidden patterns and supports finding associations, constructing analytical models, performing classification and prediction, and presenting the mining results using visualization tools.\nOLAP operations are typically implemented in OLAP servers, while SQL standard also defines some OLAP operators but these are generally implemented inconsistently in relational databases.\nA common architecture for multidimensional cube operations is ROLAP (Relational OLAP) server, which extends and optimizes relational architecture to form an OLAP server and relies on a star schema (+snowflake, fact constellation) database structure.\nAn MOLAP (Multidimensional OLAP) server uses a column-oriented data storage architecture, particularly well-suited for optimizing rapid access to aggregate data and storage of sparse cubes.\nA hybrid architecture HOLAP (Hybrid OLAP) server combines ROLAP and MOLAP, with detailed data in a relational database and aggregations in a MOLAP store, and has the performance advantages of each.\nRollup (also called drill-up) summarizes data in one of two ways:\nBy dimension reduction\nBy climbing up the concept hierarchy\nDrill-down (also called roll down) is the reverse of rollup, navigating from less detailed data to more detailed data.Slice and Dice\nSlice cuts off one dimension of the cube, not by aggregating but by selecting only one fixed value along one dimension.\nDice cuts out a sub-cube, not by aggregating but by selecting multiple fixed values for each of multiple dimensions.\nPivot (also called rotate) is a visualization operator and does not change the data. It changes the data axes in view to an alternative presentation.Other OLAP Operations offered by OLAP servers:\nDrill-across: involving (across) more than one fact table\nDrill-through: through the bottom level of the cube to its back-end relational tables (using SQL)\nRanking: top-k or bottom-k items in a list ordered by some measure\nMoving averages: over time\nStatistical functions\nDomain specific operators: growth rates, interest, internal rate of return, depreciation, currency conversion. Data warehouses contain large volumes of data but aim to answer queries in interactive query time-frames.\nPre-computing all aggregate measures required in a data cube can deliver fast response times, but this is expensive in storage.\nData warehouses must support efficient cube computation, access methods, and query processing techniques.\nA data cube is a lattice of cuboids where each cuboid represents a choice of group-by attributes.\nThe total number of cuboids in a data cube with n dimensions and m levels for each dimension is (m+1)^n - 1.\nOLAP servers can materialize every, none, or some cuboids to reduce run-time query processing costs.\nProcessing OLAP queries involves determining which operations to perform on available cuboids and selecting the materialized cuboid(s) with the least estimated query processing cost.\nEfficient cube materialization strategies include full cube materialization, cube shell, and iceberg cube.\nThe three OLAP server architectures are ROLAP, MOLAP, and HOLAP. Frequent pattern mining discovers associations and correlations in itemsets in databases.\nTechniques were initially developed for commercial market basket analysis in the 90s.\nThe language of frequent pattern mining assumes data items are grouped into transactions.\nThe goal is to find patterns of items that occur in a high proportion of transactions.\nExamples include supermarket shopping, pharmaceutical benefits, cross-marketing, and DNA sequence analysis.\nFrequent pattern mining is used in many other applications such as catalogue design, sale campaign analysis, and web clickstream analysis.\nThe focus is on association mining to learn patterns of association rules. Item: a single piece of nominal data, like \"apple,\" \"orange,\" or \"banana.\"\nItemset: a set of one or more items, denoted by , for example .\nk-itemset: an itemset containing k items, denoted by , for example, is a 2-itemset.\nTransaction: a non-empty itemset identified by a unique identifier in a database. Denoted by , where is a set of task-relevant transactions, for example, , , .\nSupport Count: the number of transactions that contain an itemset, denoted by . Also known as occurrence frequency, frequency, absolute support, and count.\nSupport: the proportion of transactions that contain an itemset, denoted by . Also called relative support and sometimes frequency.\nFrequent Itemset: an itemset with support greater than or equal to a minimum support threshold.\nAssociation Rule: an implication of the form , where , , , , and .\nRule Support: the proportion of transactions that contain all of both 's and 's items. Denoted by , it measures the significance of the association rule in the dataset.\nConfidence: the conditional probability of given , denoted by . Also called the support ratio, it is the support count of in as a proportion of the support count of in .\nStrong Rule: an association rule is strong when is frequent. Lift: a measure of dependence or correlation between two itemsets and . If lift = 1, and are independent; if lift &gt; 1, they are positively correlated; if lift &lt; 1, they are negatively correlated. Chi-square test: a hypothesis test used to determine if two nominal variables are related. The chi-square statistic is calculated by comparing observed and expected frequencies in a contingency table, and degrees of freedom are (r-1)*(c-1), where r and c are the number of rows and columns in the table, respectively. If the chi-square value is greater than the critical value at a given level of significance and the degrees of freedom, we reject the null hypothesis and conclude that there is a relationship between the two variables. Chi-square test for non-singleton itemsets: a hypothesis test used to determine if there is a significant association between two or more items in a transaction. The chi-square value is calculated by comparing observed and expected frequencies in a contingency table, and the expected frequency is calculated as (row total * column total) / grand total. Find all frequent itemsets--i.e. all the itemsets that occur at least times in the dataset. Use the frequent itemsets to generate strong association rules that satisfy . Start by scanning the database of transactions to determine the support of each item, i.e., the number of transactions in which it occurs.\nGenerate a list of frequent 1-itemsets by selecting those items whose support is greater than or equal to a predefined minimum support threshold.\nUse the frequent 1-itemsets to generate candidate 2-itemsets, i.e., pairs of items, by joining each frequent itemset with itself.\nScan the database again to determine the support of each candidate 2-itemset and generate a list of frequent 2-itemsets by selecting those whose support is greater than or equal to the minimum support threshold.\nUse the frequent k-itemsets to generate candidate (k+1) itemsets by joining each frequent itemset with itself.\nRepeat steps 4 and 5 until no new frequent itemsets are generated, i.e., until the list of frequent k-itemsets is empty.\nUse the frequent itemsets to generate association rules, i.e., rules of the form A→B, where A and B are disjoint itemsets and the support and confidence of the rule are greater than or equal to predefined minimum thresholds. All frequent itemsets and their non-empty subsets meet the minimum support requirement.\nSo, only the confidence of the rule needs to be checked.\nTo create association rules using Apriori algorithm:\nFor each frequent itemset, generate non-empty proper subsets.\nCheck if the confidence of the rule is greater than or equal to the minimum confidence threshold.\nOutput the rule if it meets the threshold. An important optimization in frequent itemset mining algorithms based on the fact that if an itemset is frequent, then all of its non-empty subsets are also frequent. This allows for significant memory-saving in algorithms like Apriori. Closed frequent itemsets and maximal frequent itemsets are used to efficiently mine frequent itemsets without losing important information.\nAn itemset X is considered closed in a dataset D if there is no other itemset Y in D that is a superset of X and has the same support count as X.\nA closed frequent itemset is both closed and frequent, meaning it occurs in the dataset with a frequency that is at least the minimum support threshold.\nA maximal frequent itemset is an itemset that is frequent in the dataset and has no superset that is also frequent, satisfying the minimum support threshold.\nClosed frequent itemsets with their support counts represent all frequent itemsets in the dataset, including those that are not closed.\nMaximal frequent itemsets represent all the frequent itemsets but may not give the exact counts for all of them.\nMining closed and maximal frequent itemsets can efficiently discover all frequent itemsets in a large dataset without generating redundant or unnecessary itemsets.\nThis approach significantly reduces the computational cost of frequent itemset mining while preserving the important information about the frequency of itemsets in the dataset. Algorithms other than Apriori have been developed to reduce database scans or reduce main memory usage, which may scale better over large datasets.\nSome of the most well-known alternative algorithms are FP-growth and mining closed and max-patterns.\nSome algorithms rely on representing transactions in a vertical form as &lt;item, set of TIDs&gt; instead of the horizontal form &lt;TID, set of items&gt;.\nAdvanced pattern mining methods have been developed to apply more structure over the input data and the rules produced, such as: Multilevel association rules: employing a concept hierarchy to discover rules at different levels of abstraction.\nMultidimensional associations: replacing items in itemsets by instantiated relational predicates to allow mined rules to become more expressive.\nRare patterns and negative patterns: looking for patterns that are surprisingly infrequent or strongly negatively correlated by lift.\nCompressed or approximate patterns: looking for the top-k most frequent patterns or clustering patterns to stand for all the patterns in the cluster more efficiently.\nSequential patterns: where the order of items is significant, for example in text or DNA sequences.\nGraph patterns: where frequent sub-graphs may be discovered in graph network structures. Extended data types, such as multidimensional patterns over nominal data, ordinal data, and quantitative (continuous) data, require different mining approaches and interpretation of rules.\nA simple solution to distinguish the attributes for single-dimensional algorithms like Apriori is to transform the values of nominal attributes to explicit attribute-value pairs.\nDynamic discretization is a more sophisticated approach that interacts with the mining algorithm to choose good value ranges.\nInterpreting rules over transformed data requires being aware of the transformation and its impact on the evaluation metrics for the rules.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<a data-href=\"1.1 Why Data Mining\" href=\"1.1 Why Data Mining\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">1.1 Why Data Mining</a>","level":3,"id":"[[1.1_Why_Data_Mining]]_0"},{"heading":"<a data-href=\"1.2 What is Data Mining\" href=\"1.2 What is Data Mining\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">1.2 What is Data Mining</a>","level":3,"id":"[[1.2_What_is_Data_Mining]]_0"},{"heading":"<a data-href=\"1.3. What makes a pattern useful\" href=\"1.3. What makes a pattern useful\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">1.3. What makes a pattern useful</a>","level":3,"id":"[[1.3._What_makes_a_pattern_useful]]_0"},{"heading":"<a data-href=\"1.4 What kind of patterns can be mined\" href=\"1.4 What kind of patterns can be mined\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">1.4 What kind of patterns can be mined</a>","level":3,"id":"[[1.4_What_kind_of_patterns_can_be_mined]]_0"},{"heading":"<a data-href=\"1.6 Privacy Implications of Data Mining\" href=\"1.6 Privacy Implications of Data Mining\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">1.6 Privacy Implications of Data Mining</a>","level":3,"id":"[[1.6_Privacy_Implications_of_Data_Mining]]_0"},{"heading":"<a data-href=\"2.1 Data Types and Representations\" href=\"2.1 Data Types and Representations\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.1 Data Types and Representations</a>","level":3,"id":"[[2.1_Data_Types_and_Representations]]_0"},{"heading":"<a data-href=\"2.2 Basic Statistical Descriptions of a Single Data Variable\" href=\"2.2 Basic Statistical Descriptions of a Single Data Variable\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.2 Basic Statistical Descriptions of a Single Data Variable</a>","level":3,"id":"[[2.2_Basic_Statistical_Descriptions_of_a_Single_Data_Variable]]_0"},{"heading":"<a data-href=\"2.3 Measuring the Dispersion of Data\" href=\"2.3 Measuring the Dispersion of Data\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.3 Measuring the Dispersion of Data</a>","level":3,"id":"[[2.3_Measuring_the_Dispersion_of_Data]]_0"},{"heading":"<a data-href=\"2.4 Computation of Measures\" href=\"2.4 Computation of Measures\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.4 Computation of Measures</a>","level":3,"id":"[[2.4_Computation_of_Measures]]_0"},{"heading":"<a data-href=\"2.5 Measuring Correlation amongst Two Variables\" href=\"2.5 Measuring Correlation amongst Two Variables\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.5 Measuring Correlation amongst Two Variables</a>","level":3,"id":"[[2.5_Measuring_Correlation_amongst_Two_Variables]]_0"},{"heading":"<a data-href=\"2.6 Measuring Similarity and Dissimilarity of Multivariate Data\" href=\"2.6 Measuring Similarity and Dissimilarity of Multivariate Data\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.6 Measuring Similarity and Dissimilarity of Multivariate Data</a>","level":3,"id":"[[2.6_Measuring_Similarity_and_Dissimilarity_of_Multivariate_Data]]_0"},{"heading":"<a data-href=\"2.7 Proximity Measures for Nominal Attributes\" href=\"2.7 Proximity Measures for Nominal Attributes\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.7 Proximity Measures for Nominal Attributes</a>","level":3,"id":"[[2.7_Proximity_Measures_for_Nominal_Attributes]]_0"},{"heading":"<a data-href=\"2.8  Proximity Measures for Binary Attributes\" href=\"2.8  Proximity Measures for Binary Attributes\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.8  Proximity Measures for Binary Attributes</a>","level":3,"id":"[[2.8_Proximity_Measures_for_Binary_Attributes]]_0"},{"heading":"<a data-href=\"2.9 Normalisation of numeric data\" href=\"2.9 Normalisation of numeric data\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.9 Normalisation of numeric data</a>","level":3,"id":"[[2.9_Normalisation_of_numeric_data]]_0"},{"heading":"<a data-href=\"2.10 Minkowski Distance\" href=\"2.10 Minkowski Distance\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2.10 Minkowski Distance</a>","level":3,"id":"[[2.10_Minkowski_Distance]]_0"},{"heading":"<a data-href=\"0. Basic concepts\" href=\"0. Basic concepts\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">0. Basic concepts</a>","level":3,"id":"[[0._Basic_concepts]]_0"},{"heading":"<a data-href=\"1. Multi-dimensional Data Cubes\" href=\"1. Multi-dimensional Data Cubes\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">1. Multi-dimensional Data Cubes</a>","level":3,"id":"[[1._Multi-dimensional_Data_Cubes]]_0"},{"heading":"<a data-href=\"2. Concept Hierarchies\" href=\"2. Concept Hierarchies\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">2. Concept Hierarchies</a>","level":3,"id":"[[2._Concept_Hierarchies]]_0"},{"heading":"<a data-href=\"3. Modelling the Cube in a Relational Database\" href=\"3. Modelling the Cube in a Relational Database\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">3. Modelling the Cube in a Relational Database</a>","level":3,"id":"[[3._Modelling_the_Cube_in_a_Relational_Database]]_0"},{"heading":"<a data-href=\"4. Data Mining in Data Warehouses\" href=\"4. Data Mining in Data Warehouses\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4. Data Mining in Data Warehouses</a>","level":3,"id":"[[4._Data_Mining_in_Data_Warehouses]]_0"},{"heading":"<a data-href=\"5. Typical OLAP Operations\" href=\"5. Typical OLAP Operations\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">5. Typical OLAP Operations</a>","level":3,"id":"[[5._Typical_OLAP_Operations]]_0"},{"heading":"<a data-href=\"6. Processing OLAP queries\" href=\"6. Processing OLAP queries\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">6. Processing OLAP queries</a>","level":3,"id":"[[6._Processing_OLAP_queries]]_0"},{"heading":"<a data-href=\"4.1 Motivation\" href=\"4.1 Motivation\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.1 Motivation</a>","level":3,"id":"[[4.1_Motivation]]_0"},{"heading":"<a data-href=\"4.2 Basic Concepts\" href=\"4.2 Basic Concepts\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.2 Basic Concepts</a>","level":3,"id":"[[4.2_Basic_Concepts]]_0"},{"heading":"<a data-href=\"4.3 Interesting pattern\" href=\"4.3 Interesting pattern\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.3 Interesting pattern</a>","level":3,"id":"[[4.3_Interesting_pattern]]_0"},{"heading":"<a data-href=\"4.4 Frequent Itemset Mining\" href=\"4.4 Frequent Itemset Mining\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.4 Frequent Itemset Mining</a>","level":3,"id":"[[4.4_Frequent_Itemset_Mining]]_0"},{"heading":"<a data-href=\"4.5 Apriori algorithm\" href=\"4.5 Apriori algorithm\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.5 Apriori algorithm</a>","level":3,"id":"[[4.5_Apriori_algorithm]]_0"},{"heading":"<a data-href=\"4.6 Generating Association rules\" href=\"4.6 Generating Association rules\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.6 Generating Association rules</a>","level":3,"id":"[[4.6_Generating_Association_rules]]_0"},{"heading":"<a data-href=\"4.7 Efficient frequent Data Mining\" href=\"4.7 Efficient frequent Data Mining\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.7 Efficient frequent Data Mining</a>","level":3,"id":"[[4.7_Efficient_frequent_Data_Mining]]_0"},{"heading":"<a data-href=\"4.8 Closed and Maximal Frequent\" href=\"4.8 Closed and Maximal Frequent\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.8 Closed and Maximal Frequent</a>","level":3,"id":"[[4.8_Closed_and_Maximal_Frequent]]_0"},{"heading":"<a data-href=\"4.9 Adavanced pattern Mining\" href=\"4.9 Adavanced pattern Mining\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">4.9 Adavanced pattern Mining</a>","level":3,"id":"[[4.9_Adavanced_pattern_Mining]]_0"}],"links":["artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html#_0","artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084156467,"modifiedTime":1737554783000,"sourceSize":35895,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/Data Mining Links.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html":{"title":"0. Data Types","icon":"","description":"Data can be of different types such as numerical, categorical, ordinal, and binary. Understanding the type of data is important as it determines the type of analysis that can be performed.\nNumerical data: Data that is expressed in numbers, such as age, income, and height. Numerical data can be further classified as discrete or continuous. Discrete data is countable, while continuous data can take on any value within a range.\nCategorical data: Data that is expressed in categories, such as gender, race, and education level. Categorical data can be further classified as nominal or ordinal. Nominal data has no inherent order, while ordinal data has a natural order.\nBinary data: Data that can take on one of two possible values, such as true/false or yes/no.\nText data: Text data consists of unstructured textual information such as email messages, social media posts, and news articles. Analyzing text data involves techniques such as natural language processing (NLP), sentiment analysis, and topic modeling.\nTemporal data: Temporal data is time-stamped data that captures events or observations over time. Examples of temporal data include stock prices, weather data, and website traffic.\nSpatial data: Spatial data is data that has a geographic component, such as location coordinates or addresses. Examples of spatial data include maps, satellite images, and GPS data.\nMulti-dimensional data: Multi-dimensional data is data that has multiple attributes or features. Examples of multi-dimensional data include images, videos, and sensor data.\nGraph data: Graph data is data that is represented as a set of nodes and edges, where the nodes represent entities and the edges represent relationships between them. Examples of graph data include social networks, protein interaction networks, and transportation networks.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157588,"modifiedTime":1737554783000,"sourceSize":1922,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/0. Data Types.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html":{"title":"1. Data Preprocessing","icon":"","description":"Data preprocessing is an important step in data mining. It involves cleaning, transforming, and integrating data to prepare it for analysis.\nData cleaning: Data cleaning involves identifying and correcting errors or inconsistencies in the data, such as missing values or outliers.\nData transformation: Data transformation involves converting the data into a format that can be easily analyzed. This may involve scaling or normalizing the data, or converting categorical data into numerical form.\nData integration: Data integration involves combining data from multiple sources into a single dataset for analysis.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157593,"modifiedTime":1737554783000,"sourceSize":637,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/1. Data Preprocessing.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html":{"title":"2. Exploratory Data Analysis","icon":"","description":"Exploratory Data Analysis (EDA) is the process of exploring and summarizing a dataset to gain insights into its underlying structure, features, and patterns. EDA is an important step in data mining as it helps to identify relationships between variables, detect outliers, and understand the distribution of the data.The main goals of EDA are:\nTo summarize the main features of the data: This includes computing summary statistics such as mean, median, and standard deviation, and visualizing the data using histograms, box plots, and scatter plots.\nTo detect patterns and relationships in the data: This involves exploring the relationships between variables using correlation coefficients and visualizing the data using heatmaps, cluster analysis, and principal component analysis (PCA).\nTo identify potential outliers or anomalies: This involves using techniques such as scatter plots, box plots, and kernel density estimates to identify data points that are far from the central tendency of the data.\nTo check assumptions: EDA is used to verify assumptions about the data, such as normality, linearity, and independence.\nEDA is an iterative process that involves multiple rounds of analysis and visualization. It helps to generate hypotheses about the data and identify potential areas for further investigation.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157596,"modifiedTime":1737554783000,"sourceSize":1371,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/2. Exploratory Data Analysis.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html":{"title":"3. Data Mining Techniques","icon":"","description":"Data mining techniques are used to extract useful patterns and insights from large datasets. There are many different techniques that can be used depending on the type of data and the goals of the analysis. Some common data mining techniques include:\nClassification: Classification is the process of categorizing data into pre-defined classes or categories. It involves training a model on a set of labeled data and then using that model to predict the class of new, unlabeled data. Examples of classification problems include spam detection, fraud detection, and image recognition.\nRegression: Regression is the process of predicting a continuous output variable based on one or more input variables. It involves modeling the relationship between the input variables and the output variable using a mathematical function. Examples of regression problems include predicting sales revenue, housing prices, and stock prices.\nClustering: Clustering is the process of grouping similar data points together based on their characteristics. It involves partitioning a dataset into groups or clusters, where each cluster consists of data points that are similar to each other and dissimilar to data points in other clusters. Examples of clustering problems include customer segmentation, image segmentation, and anomaly detection.\nAssociation rule mining: Association rule mining is the process of discovering interesting relationships between variables in a dataset. It involves identifying sets of items that frequently co-occur in transactions. Examples of association rule mining problems include market basket analysis, where the goal is to identify items that are often purchased together, and recommendation systems, where the goal is to recommend items based on a user's past purchases.\nText mining: Text mining is the process of extracting useful information from unstructured text data, such as email messages, social media posts, and news articles. It involves techniques such as natural language processing (NLP), sentiment analysis, and topic modeling.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157600,"modifiedTime":1737554783000,"sourceSize":2120,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/3. Data Mining Techniques.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html":{"title":"4. Model Evaluation","icon":"","description":"Model evaluation is the process of assessing the performance of a predictive model on new, unseen data. It is an important step in data mining as it helps to determine whether the model is accurate and reliable, and whether it can be used to make predictions on new data.The main goals of model evaluation are:\nTo estimate the accuracy of the model: This involves comparing the predicted values of the model to the actual values of the target variable. Common metrics for evaluating model accuracy include mean squared error (MSE), mean absolute error (MAE), and root mean squared error (RMSE).\nTo identify sources of error: This involves examining the residuals or errors of the model to identify patterns or trends that may indicate sources of error. For example, if the residuals are systematically higher or lower than the actual values, this may indicate a problem with the model.\nTo test the generalizability of the model: This involves evaluating the performance of the model on new, unseen data to test its ability to generalize to new situations. This is typically done by dividing the data into training and testing sets, and evaluating the model on the testing set.\nThere are several techniques for evaluating the performance of a model, including:\nCross-validation: Cross-validation is a technique for evaluating the performance of a model by splitting the data into multiple subsets, or folds, and training the model on one fold and testing it on the remaining folds. This helps to ensure that the model is not overfitting to the training data.\nReceiver Operating Characteristic (ROC) curve analysis: ROC curve analysis is a technique for evaluating the performance of a binary classification model by plotting the true positive rate against the false positive rate at various threshold values. The area under the ROC curve (AUC) is a common metric for evaluating the performance of the model.\nConfusion matrix analysis: Confusion matrix analysis is a technique for evaluating the performance of a classification model by calculating various metrics such as accuracy, precision, recall, and F1-score based on the number of true positives, true negatives, false positives, and false negatives.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157606,"modifiedTime":1737554783000,"sourceSize":2284,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/4. Model Evaluation.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html":{"title":"5. Feature Selection","icon":"","description":"Feature selection is the process of selecting a subset of relevant features from a larger set of features in a dataset. It is an important step in data mining as it helps to improve the accuracy and efficiency of predictive models by reducing the dimensionality of the data.The main goals of feature selection are:\nTo improve the accuracy of the model: By selecting only the most relevant features, the model can focus on the most important information in the data and reduce the impact of noise or irrelevant features.\nTo reduce the computational complexity of the model: By reducing the number of features, the model can be trained more efficiently and can make predictions more quickly.\nThere are several techniques for feature selection, including:\nFilter methods: Filter methods involve evaluating the relevance of each feature based on a statistical measure such as correlation, mutual information, or chi-squared test. Features are ranked based on their relevance, and a subset of the top-ranked features is selected.\nWrapper methods: Wrapper methods involve evaluating the performance of a model using a subset of features and selecting the subset that provides the best performance. This involves training and testing the model multiple times on different subsets of features.\nEmbedded methods: Embedded methods involve selecting features as part of the model training process. For example, regularization techniques such as Lasso or Ridge regression can be used to penalize the coefficients of less important features, effectively selecting only the most important features.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157657,"modifiedTime":1737554783000,"sourceSize":1649,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/5. Feature Selection.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html":{"title":"6. Model Deployment","icon":"","description":"Model deployment is the process of integrating a predictive model into a larger system or application for real-world use. It is an important step in data mining as it allows the insights and predictions generated by the model to be put into practice.The main goals of model deployment are:\nTo make the model available for use: The model needs to be integrated into a larger system or application so that it can be accessed by end-users.\nTo ensure the model is reliable and scalable: The model needs to be tested and optimized to ensure it can handle large volumes of data and make accurate predictions in real-time.\nTo monitor the performance of the model: The model needs to be monitored to ensure it continues to perform accurately over time, and to identify any issues that may arise.\nThere are several steps involved in model deployment, including:\nConverting the model into a production-ready format: The model needs to be optimized and converted into a format that can be easily integrated into a larger system or application.\nBuilding an API: An API (Application Programming Interface) is a set of protocols that allow different software components to communicate with each other. Building an API for the model allows it to be accessed by end-users.\nTesting and optimizing the model: The model needs to be tested and optimized to ensure it can handle large volumes of data and make accurate predictions in real-time.\nMonitoring the performance of the model: The model needs to be monitored to ensure it continues to perform accurately over time, and to identify any issues that may arise.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157663,"modifiedTime":1737554783000,"sourceSize":1687,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/6. Model Deployment.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html":{"title":"0. Supervised learning algorithms","icon":"","description":"\nLinear regression: Linear regression is a type of regression analysis used to predict a continuous output variable based on one or more input variables. The goal is to find the line of best fit that minimizes the sum of the squared differences between the predicted and actual values. <a data-href=\"1. Linear Regression\" href=\"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Linear Regression</a>\n<br>Logistic regression: Logistic regression is a type of regression analysis used to predict a binary output variable based on one or more input variables. The goal is to find the best decision boundary that separates the two classes. <a data-href=\"2. Logistic Regression\" href=\"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. Logistic Regression</a>\nDecision trees: Decision trees are a type of algorithm that can be used for both regression and classification problems. They partition the data into subsets based on the values of input variables, and recursively split the subsets until the output variable is sufficiently well-defined.\nRandom forests: Random forests are an ensemble learning method that combines multiple decision trees to improve performance and reduce overfitting. They randomly sample the input variables and data points to create a collection of decision trees, and use them to make predictions by averaging their outputs.\n<br>Support vector machines (SVM): SVMs are a type of algorithm that can be used for both regression and classification problems. They find the best hyperplane that separates the classes, and maximize the margin between the hyperplane and the closest data points. <a data-href=\"5. Support Vector Machine\" href=\"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">5. Support Vector Machine</a>\nNeural networks: Neural networks are a type of algorithm that are inspired by the structure and function of the human brain. They consist of layers of interconnected nodes that perform computations on the input data, and are commonly used for classification and regression problems.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html#_0","artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html#_0","artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157671,"modifiedTime":1737554783000,"sourceSize":1811,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/0. Supervised learning algorithms.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html":{"title":"1. Unsupervised learning algorithms","icon":"","description":"Unsupervised learning algorithms are a type of machine learning algorithm that are used to identify patterns or structure in data that is not labeled or classified. The goal of unsupervised learning is to discover hidden relationships or structures in the data, without the need for prior knowledge or supervision.\nClustering: Clustering is a technique used to group similar data points together based on their characteristics or features. The goal is to partition the data into subsets, or clusters, such that data points within each cluster are more similar to each other than to those in other clusters.\nDimensionality reduction: Dimensionality reduction is a technique used to reduce the number of features or variables in a dataset while retaining as much of the original information as possible. This can be helpful for visualizing data, speeding up other algorithms, or identifying the most important features.\nAnomaly detection: Anomaly detection is a technique used to identify data points that are significantly different from the majority of the data. This can be useful for detecting fraud or other unusual events.\nAssociation rule mining: Association rule mining is a technique used to identify relationships between different features or variables in a dataset. The goal is to find patterns in the data such as \"if X, then Y\".\nPrincipal component analysis (PCA): PCA is a specific type of dimensionality reduction technique that finds the most important linear combinations of the original variables that capture the most variance in the data.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157676,"modifiedTime":1737554783000,"sourceSize":1618,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/1. Unsupervised learning algorithms.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html":{"title":"2. Association rule mining","icon":"","description":"\nAssociation rule mining is an unsupervised learning technique used to identify patterns or relationships between different items or variables in a dataset.\nIt is commonly used in market basket analysis to identify groups of products that are frequently purchased together.\nThe two main metrics used in association rule mining are support and confidence, which can be used to filter out rules that are not meaningful or interesting.\nThe Apriori algorithm is a popular algorithm for association rule mining that uses a \"bottom-up\" approach to find frequent itemsets and generate association rules.\nOther algorithms, such as FP-Growth and Eclat, have also been developed to improve the efficiency of association rule mining on large datasets.\nAssociation rule mining can be used in a variety of applications beyond market basket analysis, such as identifying patterns in healthcare data, customer behavior, and social networks.\nIt is important to be aware of potential issues with correlation versus causation and to carefully interpret the results of association rule mining to avoid drawing incorrect conclusions.\n<a data-href=\"4.5 Apriori algorithm\" href=\"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">4.5 Apriori algorithm</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157679,"modifiedTime":1737554783000,"sourceSize":1197,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/2. Association rule mining.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html":{"title":"3. Time series analysis","icon":"","description":"Time series analysis is a statistical technique used to analyze and model data that varies over time. It is a useful tool in many fields, including finance, economics, engineering, and environmental science, among others. Here are some key points on time series analysis:\nTime series data is a sequence of observations or measurements taken at regular intervals over time. The data can be univariate (i.e., a single variable is measured) or multivariate (i.e., multiple variables are measured over time).\nTime series analysis involves several steps, including visualization of the data, identification of trends and seasonality, and fitting models to the data.\nTrend analysis involves identifying the overall pattern of the data over time, whether it is increasing, decreasing, or staying relatively constant. Seasonality analysis involves identifying periodic patterns or cycles in the data, such as daily, weekly, or annual cycles.\nSeveral methods can be used for time series modeling, including ARIMA (Autoregressive Integrated Moving Average), exponential smoothing, and machine learning algorithms like neural networks.\nTime series analysis can be used for a wide variety of applications, including forecasting future values, anomaly detection, and signal processing.\nIt is important to evaluate the accuracy of time series models by comparing their predictions with actual data using metrics like mean absolute error (MAE) or mean squared error (MSE).\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157683,"modifiedTime":1737554783000,"sourceSize":1507,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/3. Time series analysis.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html":{"title":"4. Text Mining","icon":"","description":"Text mining is a process of extracting useful information from unstructured textual data. It involves a range of techniques from natural language processing (NLP), machine learning, and data mining to analyze text and derive meaningful insights from it. Here are some key points on text mining:\nText mining involves several steps, including data acquisition, data preprocessing, feature extraction, and modeling.\nData acquisition involves gathering large amounts of unstructured text data from a variety of sources, such as social media, news articles, customer reviews, and emails.\nData preprocessing involves cleaning and transforming the raw text data into a format that can be used for analysis. This includes tasks like tokenization (breaking the text into individual words or phrases), stop-word removal (removing common words that do not carry much meaning), and stemming (reducing words to their root form).\nFeature extraction involves selecting the most relevant features (words, phrases, or concepts) from the text data for use in the modeling phase.\nSeveral text mining techniques can be used for modeling, including sentiment analysis (determining the sentiment or emotion expressed in text), topic modeling (identifying the main topics or themes in the text data), and text classification (categorizing text into predefined categories or labels).\nText mining has a wide range of applications, such as in marketing, customer service, healthcare, and social media analysis.\nIt is important to evaluate the accuracy and effectiveness of text mining models using metrics like precision, recall, and F1-score.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157688,"modifiedTime":1737554783000,"sourceSize":1676,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/4. Text Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html":{"title":"5. Lossy Counting Algorithm","icon":"","description":"The Lossy Counting algorithm is a streaming algorithm used for approximate frequency counting of items in a data stream. It allows for efficient processing of large-scale data streams while maintaining a certain level of error tolerance.Algorithm Steps: Initialize the parameters: Support threshold (ε) - determines the minimum frequency threshold for an item to be considered frequent.\nError tolerance (δ) - controls the maximum allowable error in frequency estimation.\nCreate a data structure to store the frequent items and their counts. Process each item in the data stream sequentially: Update the count of the item in the data structure.\nCheck if the count of any item falls below the threshold (ε - δ) and remove it from the data structure.\nIncrement the total number of items processed. After processing all items in the data stream, the remaining items in the data structure are considered frequent items with an estimated frequency within the desired error bounds. Advantages:\nMemory efficient: The Lossy Counting algorithm maintains a compact data structure, making it suitable for processing large-scale data streams with limited memory.\nError tolerance: By allowing a certain level of error in frequency estimation, the algorithm provides approximate results while reducing computational complexity.\nLimitations:\nLimited accuracy: The Lossy Counting algorithm provides approximate frequency counts with controlled error bounds, which means the estimated counts may not be exact.\nRequires predefined threshold: The algorithm requires setting a support threshold (ε) to determine the minimum frequency for an item to be considered frequent.\nUse Cases:\nFrequent item mining: The Lossy Counting algorithm is commonly used in scenarios where identifying frequent items in large-scale data streams is required, such as market basket analysis, web clickstream analysis, and network traffic monitoring.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157706,"modifiedTime":1737554783000,"sourceSize":2104,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/5. Lossy Counting Algorithm.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/0.-concepts.html":{"title":"0. Concepts","icon":"","description":"Data wrangling, also known as data munging, is the process of transforming raw data into a more digestible, usable format. In the real world, data is often messy and complex. It is hardly ever ready to use straightaway for meaningful analysis or machine learning models. Data wrangling is therefore a critical step to clean, structure, and enrich these raw datasets to make them suitable for further processing or analysis.\nData extraction Data quality assessment Data profiling (exploration, summarisation, and visualisation) Data cleaning (transformation, reshaping, aggregation, reduction, imputation, parsing, standardisation) Data integration (schema matching and mapping, data matching, record linkage, deduplication, data fusion) Understanding and characterisation of the quality aspects of a given data set or database Cleaned, standardised, consistent and integrated data in a format that is suitable for the further processing and/or analytics tasks at hand Documentation of the data quality assessment, profiling, exploration and cleaning conducted\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Main Data Wrangling","level":3,"id":"Main_Data_Wrangling_0"},{"heading":"Outcomes of Data Wrangling","level":3,"id":"Outcomes_of_Data_Wrangling_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/0.-concepts.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157715,"modifiedTime":1737554783000,"sourceSize":1181,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/0. Concepts.md","exportPath":"artificial-intelligence/data-science/data-wrangling/0.-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/1.-process.html":{"title":"1. Process","icon":"","description":" Data Extraction: Identify and gather data from various sources such as databases, files, web services, APIs, or through web scraping.\nUnderstand the structure and format of the extracted data: CSV, Excel, SQL, JSON, XML, etc. This also includes understanding the semantics of the data fields. Data Quality Assessment: Examine the data quality by checking for completeness, consistency, uniqueness, validity, accuracy, and timeliness. This step helps to ascertain whether the data is suitable for the intended analysis or not.\nLook for issues like missing values, duplicate records, inconsistent formats, or incorrect data. Each of these issues may require different handling strategies. Data Profiling: Examining, exploring, visualising and computing statistics\nExploration: This involves a deep dive into the data to understand its characteristics and patterns. You can use techniques like data querying and data mining to explore the data.\nSummarisation: This involves presenting the data in a condensed form like statistical summaries or data distribution to get an overview of the data.\nVisualisation: Use graphical representations such as plots, histograms, or heat maps to understand and communicate the data distribution and patterns. Data Cleaning: Imputation: Handle missing data by substituting them with statistical estimates or other logical values.\nTransformation: Convert the data into suitable formats for analysis. This could include encoding, normalisation, or other mathematical transformations.\nAggregation: Combine data in a way that summarises the information or provides a different perspective.\nStandardisation: Ensure that data follows a common format or scale, making it easier for further analysis. Data Integration: Schema Mapping: Align the schema of different datasets to ensure consistency. This could include renaming columns, changing data types, etc.\nData Matching: Identify and match records that refer to the same entity across different datasets. This is also known as record linkage or deduplication.\nData Fusion: Merge data from multiple sources into a single, consistent dataset. This often requires handling conflicts between different data sources. Data Export: Determine the appropriate format for data export based on the subsequent usage, such as CSV, JSON, XML, SQL, etc. Different formats are suitable for different kinds of applications.\nValidate the exported data to ensure it matches with the final version of the cleaned and processed data. This ensures that no errors were introduced during the export process.\nAutomate the data export process if it needs to be done on a regular basis. Automation can save a lot of time and effort in repeated tasks. Understanding: After going through the data wrangling process, you should have a comprehensive understanding of your dataset. This includes knowledge about the data structure, the meaning of different features, their distributions, relationships between features, potential issues, and how to handle them. This understanding is crucial for further data analysis or modelling work. Cleaned, Standardised, Consistent, and Integrated Data: The wrangled data is now cleaned, which means it is free of inaccuracies, inconsistencies, and missing values.\nIt's standardised, meaning that the data follows a common format or scale, enabling easier comparison and analysis.\nThe data is consistent, ensuring that similar entities have the same representation across the dataset.\nIf data from multiple sources was used, the result is an integrated dataset where data from different sources has been harmoniously combined. Documentation of Data: As part of the data wrangling process, you should also have created detailed documentation about the data. This includes descriptions of the source of data, meaning of different features, decisions made during cleaning and transformation, known issues and their resolutions, and other details about the data.\nThis documentation is extremely valuable for anyone who might use the dataset in the future. It provides context for the data, ensures reproducibility of the wrangling process, and saves a lot of time and effort in understanding the data. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Outcome","level":3,"id":"Outcome_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/1.-process.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157719,"modifiedTime":1737554783000,"sourceSize":5153,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/1. Process.md","exportPath":"artificial-intelligence/data-science/data-wrangling/1.-process.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html":{"title":"2. Source of Data","icon":"","description":"Transactional data, mostly normalised into many tables, with keys between them, continuous and frequent updates on (single) records Decision support data, processed and cleaned, historical data, aggregated, updated at certain intervals Click-stream data, log files, Web pages (HTML, XML), blogs, e-mails, posts, multi-media data(images, videos, audio, etc.)Portable text (like comma separated, tabulator, fixed column) or nonportable proprietary binary files Astronomy, genomics, seismology, physics, chemistry, etc. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Relational databases","level":3,"id":"Relational_databases_0"},{"heading":"Data warehouses","level":3,"id":"Data_warehouses_0"},{"heading":"Internet","level":3,"id":"Internet_0"},{"heading":"Files","level":3,"id":"Files_0"},{"heading":"Scientific instruments, experiments and simulations","level":3,"id":"Scientific_instruments,_experiments_and_simulations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157730,"modifiedTime":1737554783000,"sourceSize":3025,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/2. Source of Data.md","exportPath":"artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html":{"title":"3. Types and Measurements of Data","icon":"","description":"\nInteger: Whole numbers (e.g. 1, 2, 3) Floating-point: Decimal numbers (e.g. 1.23, 3.14) Binary: Data represented in two states, 0 and 1 Interval: Data that represents differences between values Ratio: Data with a defined zero point Velocity: Composed of speed and direction Location: Defined by latitude and longitude A. Nominal: Used for naming or labeling entities (e.g. personal names) B. Categorical: Used for grouping entities (e.g. postcodes, university course codes) C. Ordinal: Used for ordering entities (e.g. wine tasting scores, movie ratings)\nA. Characteristics Ordering is an important feature One attribute must always be monotonic, meaning that it is always increasing or decreasing\nB. Types Most common type is time series data Relational database tables Integrated data warehouses Multimedia formats Images Video Audio (can be compressed) XML HTML E-mails SMS Log files Mainly free-format text represented in ASCII or Unicode\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Numerical Data</strong>","level":3,"id":"**Numerical_Data**_0"},{"heading":"A. Scalar Types","level":4,"id":"A._Scalar_Types_0"},{"heading":"B. Non-scalar Types","level":4,"id":"B._Non-scalar_Types_0"},{"heading":"<strong>Non-numerical Data</strong>","level":3,"id":"**Non-numerical_Data**_0"},{"heading":"<strong>Series Data</strong>","level":3,"id":"**Series_Data**_0"},{"heading":"<strong>Formats of Data</strong>","level":3,"id":"**Formats_of_Data**_0"},{"heading":"Structured Data","level":4,"id":"Structured_Data_0"},{"heading":"Semi-Structured Data","level":3,"id":"Semi-Structured_Data_0"},{"heading":"Free-Format Data","level":4,"id":"Free-Format_Data_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157735,"modifiedTime":1737554783000,"sourceSize":1233,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/3. Types and Measurements of Data.md","exportPath":"artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html":{"title":"4. Record Linkage Process","icon":"","description":"Record linkage is the process of finding and linking records about the same entity across different data sources. It's crucial for integrating data sets, de-duplicating records, and creating a comprehensive view of entities. The process can be broken down into several steps: Data Preprocessing: Standardization: Ensure that data from different sources adhere to common formats. For instance, date formats, addresses, and names should be consistent.\nCleaning: Rectify any inconsistencies or errors in the data. This includes correcting typographical errors, handling missing values, and so forth.\nTransformation: Convert data into formats or structures that are amenable for linkage. For example, segmenting full names into first names and last names. Blocking or Indexing: This step reduces the number of record pairs to be compared by grouping similar records based on specific attributes (e.g., birth date or zip code).\nIt ensures that only potential matches are compared, thereby increasing efficiency. Record Pair Comparison: For every record in one dataset, compare it to records in another dataset.\nUtilize algorithms or techniques to determine the similarity between two records. Commonly used metrics include Jaccard similarity, cosine similarity, or edit distance for textual data. Decision Making: Based on the comparison, decide whether a pair of records refers to the same entity.\nThis can be a deterministic approach where rules are set (e.g., if two records have the same name and birthdate, they're considered a match).\nAlternatively, probabilistic linkage can be used, where weights are assigned to matches and mismatches for various fields, and records are linked based on a threshold score. Evaluation: Assess the quality of the linkage using metrics like precision, recall, and F1-score.\nThis step often requires a manually curated set of true matches (ground truth) to compare against. Post-Linkage Processing: Clustering: Group linked records to ensure that all records in a group refer to the same entity.\nConsolidation: In situations with multiple linked records for the same entity, consolidate them into a single, 'golden' record. This can involve averaging, voting, or choosing the most recent value among linked records. Continuous Update &amp; Review: As new data becomes available or entities change (e.g., someone changes their last name), the linkage process may need to be periodically reviewed and updated. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157771,"modifiedTime":1737554783000,"sourceSize":2809,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/4. Record Linkage Process.md","exportPath":"artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/5.-bins.html":{"title":"5. Bins","icon":"","description":" Equal Width Binning :&nbsp;bins have equal width with a range of each bin&nbsp;are defined as [min + w], [min + 2w] …. [min + nw] where w = (max – min) / (no of bins). Equal depth (or frequency) binning :&nbsp;In equal-frequency binning we divide the range [A, B] of the variable into intervals that contain (approximately) equal number of points; equal frequency may not be possible due to repeated values. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/5.-bins.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157775,"modifiedTime":1737554783000,"sourceSize":412,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/5. Bins.md","exportPath":"artificial-intelligence/data-science/data-wrangling/5.-bins.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html":{"title":"6. Record Comparison","icon":"","description":"\nRecord pairs are compared based on their common available attributes (fields) Names, addresses, dates, etc. Contain errors Exact comparison will not provide good results Approximate comparison functions are required Numerical comparison Using absolute maximum difference and maximum percentage difference Date and time comparison Dates and times can be converted into numbers by counting the number of days/hours/minutes since a certain fixed date/time, then calculate numerical similarity between these numbers String comparison Q-gram based functions such as Jaccard and Dice similarity\nEdit distance – number of edit operations needed to go from one string to another\nBag distance – fast approximation of edit distance\nJaro and Jaro-Winkler comparison Jaccard similarity: Dice similarity: Bag distance: Where Qx is the set of q-grams extracted from string sx. The Bag distance here using multi-set instead of set, and the |s1-s2| is the cardinality after a set minus.\nJaccard and Dice q-gram sets of the two strings which can be done with a linear scan with complexity O((l1 – q + 1) + (l2 – q + 1) ) -&gt; O(l1 + l2) Two sets containing n and m elements respectively The complexity for obtaining the set union is O(n + m) The complexity for obtaining the set intersection is O(min(n, m)) Note that maximum n = l1 – q + 1 and maximum m = l2 – q + 1\nTherefore, O(n + m) -&gt; O(l1 + l2) and O(min(n, m)) -&gt; O(min(l1, l2)) s1 = \"arnab\"\ns2 = \"aranby\"\nd = 2 (half the distance of the longer string -&gt; max(|s1|, |s2|) / 2 - 1)\nc = 5 (number of characters that agree within the distance d)\nt = 1 (number of transpositions within the distance d)\njaro_sim(s1, s2) = 1/3 (c/|s1| + c/|s2| + (c-t)/c) = 0.8778Jaro-Winkler(s1,s2)=Jaro(s1,s2)+(l⋅p⋅(1−Jaro(s1,s2)))\nJaro(s1, s2): Jaro(s1, s2) represents the Jaro similarity between strings s1 and s2.\nl: l is the length of the common prefix between s1 and s2, up to a maximum prefix length of 4.\np: p is a constant scaling factor (typically set to a small value like 0.1) that controls how much additional weight is given to the common prefix. pd = 100* abs(n1 – n2) / max(n1, n2) sim = 1.0 – (pd / pd_max) if pd &lt; pd_max sim = 0.0 else. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Comparison functions","level":3,"id":"Comparison_functions_0"},{"heading":"Formula","level":3,"id":"Formula_0"},{"heading":"Complexity","level":3,"id":"Complexity_0"},{"heading":"Jaro similarity","level":3,"id":"Jaro_similarity_0"},{"heading":"Jaro-Winkler","level":4,"id":"Jaro-Winkler_0"},{"heading":"Numerical with maximum percentage difference","level":3,"id":"Numerical_with_maximum_percentage_difference_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157779,"modifiedTime":1737554783000,"sourceSize":2792,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/6. Record Comparison.md","exportPath":"artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/7.-record-classification.html":{"title":"7. Record classification","icon":"","description":"\nMatches: These are record pairs that are identified as referring to the same entity or representing the same real-world object. For example, two records with similar names and addresses might be considered a match. Non-Matches: Non-match pairs are clearly distinct and do not represent the same entity. These pairs are unrelated and should be treated as separate entities. Potential Matches: Record pairs that are not definitively classified as matches or non-matches based on initial similarity metrics. These pairs require further investigation or more advanced classification techniques to determine their true status.\nRecord pair classification is conducted using their calculated similarity vectors. These vectors capture the similarity between records across various attributes, such as names, addresses, or dates of birth. Threshold based classification and weighted threshold based classification A user-defined threshold(s) are used to classify record pairs into different classes\nMore weights are given to attributes that contain more information Probabilistic classification Based on matching and non-matching probabilities for individual attributes (assuming conditional independence)\nRequire ground truth to calculate the probabilities Cost-based classification Based on costs associated with true positives, true negatives, false positives, and false negatives\nAim to minimize the total cost for classifying all record pairs Rule-based classification A set of rules is used to classify a record pair as a match or non-match\nRules are applied to the calculated attribute similarities Machine learning-based classification Supervised machine learning: Requires training data in the form of ground truth\nUnsupervised machine learning: Involves clustering similar data points\nActive learning: Prioritizes record pair classification that is most important for the model and may involve interaction with domain experts to make better decisions ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Different classification techniques","level":3,"id":"Different_classification_techniques_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/7.-record-classification.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157782,"modifiedTime":1737554783000,"sourceSize":2128,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/7. Record classification.md","exportPath":"artificial-intelligence/data-science/data-wrangling/7.-record-classification.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html":{"title":"8. Measure Linkage","icon":"","description":"Measures by how much a blocking technique is able to reduce the comparison space Compared to the full pair-wise comparison of all record pairs rr = 1 – (sM + sN) / (nM + nN) where: sM and sN are the number of true matching and non-matching candidate record pairs generated by a blocking technique nM and nN are the total number of true matching and non-matching record pairs (in the pair-wise comparison space) Measures how many true matches 'pass' through a blocking process\nIt corresponds to the recall of blocking It requires the true match status of all record pairs (as with the linkage quality measures) Measures how many candidate record pairs generated by blocking are true matches It corresponds to the precision of blocking pq = sM / (sM + sN) It requires the true match status of all record pairs (as with the linkage quality measures)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Reduction ratio","level":3,"id":"Reduction_ratio_0"},{"heading":"Pairs completeness","level":3,"id":"Pairs_completeness_0"},{"heading":"Pairs quality","level":3,"id":"Pairs_quality_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157786,"modifiedTime":1737554783000,"sourceSize":954,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/8. Measure Linkage.md","exportPath":"artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html":{"title":"9. Blocking Process Evaluation Metrics","icon":"","description":"When performing record linkage or deduplication, the blocking process reduces the number of candidate pairs to be compared. Evaluating its effectiveness involves several metrics:\nRepresents the effectiveness of a blocking scheme in reducing candidate pairs.\nFormula: A higher RR means better efficiency in reducing comparisons. Measures the percentage of true matches retained post-blocking.\nFormula: A high PC means most true matches are retained, ensuring effectiveness. But a very high PC with a low RR might indicate inefficient blocking. Evaluates the proportion of true matches among retained candidate pairs.\nFormula: A higher PQ indicates that a larger proportion of the retained pairs are true matches, suggesting efficient filtering.\nIdeally, you'd aim for an RR close to 1, and both PC and PQ also close to 1. However, there's often a trade-off between efficiency (RR) and effectiveness (PC and PQ). The desired balance depends on the specifics of your use case, considering the costs of missed true matches versus the computational costs of extra comparisons.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Reduction Ratio (RR)","level":3,"id":"1._**Reduction_Ratio_(RR)**_0"},{"heading":"2. Pairs Completeness (PC)","level":3,"id":"2._**Pairs_Completeness_(PC)**_0"},{"heading":"3. Pairs Quality (PQ)","level":3,"id":"3._**Pairs_Quality_(PQ)**_0"},{"heading":"Example","level":3,"id":"Example_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157790,"modifiedTime":1737554783000,"sourceSize":2724,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/9. Blocking Process Evaluation Metrics.md","exportPath":"artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html":{"title":"9.1 Blocking Methods","icon":"","description":"Simple blocking is the most straightforward method where records are grouped together based on the exact match of an attribute or a combination of attributes. For example:\nName-based blocking: All records with the exact name \"John Doe\" are grouped together.\nBirth year blocking: All records with the birth year \"1990\" are grouped together.\nThis method is efficient but can miss potential matches due to slight variations or errors in the blocking attribute.This method is designed to overcome variations in name spellings by grouping together names that sound alike. Soundex is one of the most popular phonetic algorithms.\nIt converts a name into a character code. The first character is the first letter of the name, and the next three characters are numbers that represent the phonetics of the name.\nNames that have the same Soundex code are considered phonetically similar.\nFor example:\nBoth \"Smith\" and \"Smythe\" might be represented by the same Soundex code.\nSoundex blocking can be particularly useful for datasets with name variations due to misspellings, transcription errors, or different transliterations.SLK-581 is a specific method used primarily in health data linkage in Australia. It creates a linkage key based on specific attributes: Gender: 1 character (male, female, or unknown).\nDate of Birth: 8 characters (day, month, year). If a part of the DOB is missing or invalid, it gets replaced with a wildcard character.\nGiven name and family name: First 2 characters from each. If the name has fewer than 2 characters, it's supplemented with wildcard characters.\nSo, an SLK-581 key looks something like this: M31011989JOEBSM\nWhere \"M\" represents male, \"31011989\" is the date of birth (31 Jan 1989), \"JO\" is the first two letters of the given name \"Joe\", and \"BS\" is the first two letters of the family name \"Bishop\".\nRecords with the same SLK-581 key are grouped together. This method has been designed to be privacy-preserving and efficient for health data, but like all blocking methods, there's a trade-off between reducing the number of comparisons and ensuring all potential matches are retained.In summary:\nSimple Blocking relies on exact attribute matches.\nSoundex groups together names that sound similar.\nSLK-581 is a specific, structured blocking method designed primarily for health data linkage.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Simple Blocking","level":3,"id":"1._Simple_Blocking_0"},{"heading":"2. Phonetic (Soundex) Based Blocking","level":3,"id":"2._Phonetic_(Soundex)_Based_Blocking_0"},{"heading":"3. Statistical Linkage Key (SLK-581) Based Blocking","level":3,"id":"3._Statistical_Linkage_Key_(SLK-581)_Based_Blocking_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157794,"modifiedTime":1737554783000,"sourceSize":2709,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/9.1 Blocking Methods.md","exportPath":"artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html":{"title":"10. Classification Techniques","icon":"","description":"This classification method is based on a similarity measure. For a new sample to be classified, its similarity with existing samples in the database is calculated. If the similarity exceeds a preset threshold, then the sample is classified into the same class as the most similar sample.Formula:\nWhere: is the new sample to be classified. is the most similar sample in the database. is the class of the sample . is the preset similarity threshold.\nIt's a variation of the above method. Instead of classifying the new sample based on the highest similarity, the sample is classified into a certain class if its similarity with any sample from that class exceeds a preset minimum threshold.Formula:\nWhere:\n( C ) represents all samples of a certain class. is the minimum similarity between and any sample in .\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Similarity Threshold Based Classification:","level":3,"id":"1._Similarity_Threshold_Based_Classification_0"},{"heading":"2. Minimum Similarity Threshold Based Classification:","level":3,"id":"2._Minimum_Similarity_Threshold_Based_Classification_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157723,"modifiedTime":1737554783000,"sourceSize":2573,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/10. Classification Techniques.md","exportPath":"artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html":{"title":"11. Ontology Matching","icon":"","description":"Ontology matching, sometimes also called ontology alignment, refers to the process of determining correspondences between concepts in different ontologies. Ontologies are formal, explicit specifications of a conceptualization, typically used to represent knowledge about some domain. When two different ontologies cover similar or overlapping domains, there is often a need to find out how concepts in one ontology relate to concepts in another. This is where ontology matching comes into play.Here are some key points related to ontology matching: Purpose: The main goal is to allow for meaningful interoperability between systems that use different ontologies. This is especially relevant in areas like the Semantic Web, where different parties might model similar domains differently. Types of Matches: Exact Match: Two concepts are semantically equivalent.\nSubsumption Match: One concept is more general than another (e.g., a superclass-subclass relationship).\nOverlap Match: Two concepts overlap in meaning but aren't equivalent.\nMismatch: Two concepts are semantically different. Matching Techniques: Linguistic Matching: Uses lexical similarities, synonyms, and other linguistic techniques.\nStructural Matching: Considers the structure of the ontology, such as hierarchies or relationships among concepts.\nInstance-based Matching: Uses instances (or data) that the ontology describes to infer alignments.\nLogical Matching: Employs logical reasoning to infer relationships between concepts.\nConstraint-based Matching: Uses domain-specific constraints or rules to guide the matching process. Challenges: Ontologies might use different languages or terminologies to describe similar things.\nThe level of granularity might differ between ontologies.\nContextual differences: The same term might be used differently in different contexts. Tools and Frameworks: Various tools and frameworks have been developed to automate ontology matching, such as the Alignment API, AgreementMaker, and others. Many of these provide graphical interfaces to visualize and adjust the proposed matches. Evaluation: The quality of ontology matching results is typically evaluated in terms of precision, recall, and F1-score, comparing the proposed alignments to a reference alignment (ground truth). In summary, ontology matching is a crucial process in knowledge engineering and the Semantic Web, allowing for data integration, query translation, and other tasks that involve dealing with multiple, diverse ontologies.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084157727,"modifiedTime":1737554783000,"sourceSize":2723,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/11. Ontology Matching.md","exportPath":"artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html":{"title":"0. Information Retrieval","icon":"","description":"Introduction: Information Retrieval (IR) is a field of study that focuses on the effective and efficient retrieval of relevant information from large collections of unstructured or semi-structured data. The goal of IR is to provide users with the most relevant documents or information in response to their information needs, expressed as queries.Key Components of Information Retrieval:\nDocument Collection: IR systems typically operate on a large collection of documents, such as web pages, articles, books, emails, or any unstructured textual data.\nQuery Processing: Users express their information needs through queries, which are typically short strings of text. Query processing involves understanding and analyzing the query to identify relevant documents from the collection.\nIndexing: To efficiently retrieve relevant documents, IR systems create indexes that map terms or words in the documents to the documents themselves. These indexes speed up the retrieval process by reducing the need for exhaustive document scans.\nRanking: When multiple documents are relevant to a query, ranking algorithms are employed to determine the order in which they should be presented to the user. The goal is to rank the most relevant documents higher in the search results.\nRelevance Feedback: Some IR systems incorporate user feedback to improve the relevance of search results. Users can indicate which documents are relevant or irrelevant, and the system adapts its ranking accordingly.\nIR Techniques and Models:\nBoolean Retrieval Model: A simple IR model that uses Boolean operators (AND, OR, NOT) to combine terms in a query and retrieve documents containing those terms.\nVector Space Model (VSM): A widely used IR model that represents documents and queries as vectors in a high-dimensional space. Similarity measures (e.g., cosine similarity) are used to rank documents based on their proximity to the query vector.\nProbabilistic Retrieval Model: IR models based on probability theory, such as the Okapi BM25 model, which uses document and query term statistics to rank documents.\nLanguage Models: IR models that treat documents and queries as statistical language models. The relevance of a document is determined based on its similarity to the query language model.\nNeural IR Models: Recent advancements in deep learning have led to the development of neural network-based IR models, such as BERT for document ranking and learning to rank methods.\nEvaluation Metrics:To assess the effectiveness of IR systems, various evaluation metrics are used, including:\nPrecision: The proportion of retrieved documents that are relevant to the query.\nRecall: The proportion of relevant documents that are retrieved by the system.\nF1-score: The harmonic mean of precision and recall, which provides a balance between the two metrics.\nMean Average Precision (MAP): The average precision across multiple queries, useful for evaluating the overall performance of an IR system.\nApplications of Information Retrieval:\nWeb Search Engines: To retrieve relevant web pages in response to user queries.\nDocument Retrieval: For organizing and searching large document repositories.\nInformation Filtering: To deliver relevant news or content to users based on their interests.\nQuestion Answering Systems: To find answers to user questions in a knowledge base.\nPersonal Assistants: To provide relevant information or suggestions to users in real-time.\nBook:\n<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://nlp.stanford.edu/IR-book/\" target=\"_self\">https://nlp.stanford.edu/IR-book/</a>Information Retrieval (IR):\nComputational Approaches: Information retrieval focuses on computational methods and algorithms to retrieve relevant information from large collections of unstructured or semi-structured data, such as documents or web pages.\nStatistical (Shallow) Understanding of Language: In IR, the primary focus is on statistical or shallow understanding of language. IR models typically use bag-of-words representations and simple linguistic features to match queries with documents.\nHandle Large-Scale Problems: IR systems are designed to handle large-scale problems, such as web search engines that need to process billions of web pages to retrieve relevant results quickly.\nNatural Language Processing (NLP):\nCognitive, Symbolic, and Computational Approaches: NLP employs a more diverse range of approaches, including cognitive models, symbolic methods, and computational techniques. NLP aims to understand and process natural language in a more comprehensive way.\nSemantic (Deep) Understanding of Language: Unlike IR, NLP aims to achieve semantic or deep understanding of language. It involves modeling syntax, semantics, and pragmatics to comprehend the meaning and context of natural language text.\n(Often) Smaller Scale Problems: NLP often deals with smaller scale problems compared to IR. NLP tasks include language translation, sentiment analysis, question answering, and chatbot development, which typically involve a more targeted and focused analysis of language.\nKey Differences:\nScope: IR is primarily concerned with retrieving relevant documents or information from large collections, whereas NLP aims to understand, process, and generate natural language text in a more comprehensive manner.\nDepth of Understanding: IR's focus on statistical and shallow understanding means it is well-suited for tasks that rely on keyword matching and retrieval, while NLP's emphasis on semantic understanding allows it to tackle more complex language processing tasks.\nProblem Scale: IR is optimized to handle large-scale problems, such as web search, where efficiency and speed are crucial, while NLP typically deals with smaller scale tasks that require more detailed language analysis.\nApplications: IR is commonly used in web search engines, document retrieval systems, and information filtering. On the other hand, NLP is applied in machine translation, sentiment analysis, speech recognition, chatbots, and language understanding tasks.\nConclusion: Information Retrieval and Natural Language Processing are related fields with different focuses. While IR is specialized in efficiently retrieving relevant information from large collections, NLP delves deeper into understanding language semantics and handling more complex language processing tasks. Both areas play essential roles in various applications, ranging from web search engines to sophisticated language-based AI systems.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"IR vs NLP","level":3,"id":"IR_vs_NLP_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157805,"modifiedTime":1737554783000,"sourceSize":7271,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/0. Information Retrieval.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html":{"title":"1. Classic Search Model","icon":"","description":"Performing information retrieval in the classic search model involves several steps, from understanding the user's task and information needs to refining the initial query based on the search results. User Task: Identify the user's task or goal, which involves determining the purpose of the search. Users may have various tasks, such as seeking information, finding relevant documents, answering a specific question, or exploring a topic.\nInformation Need: Understand the user's information need by analyzing their query or request. Information needs can be explicit (specifically stated by the user) or implicit (inferred from the context of the search).\nQuery: Formulate a query based on the user's information need. The query is a string of keywords or phrases that represent the user's search request. It should be constructed to capture the most relevant aspects of the user's information need.\nSearch Engine: Submit the query to a search engine. A search engine is a specialized software that retrieves relevant documents from a large collection based on the query. The search engine uses indexing and ranking algorithms to find and present the most relevant documents to the user.\nResults: The search engine returns a set of search results, which are documents or web pages that are considered relevant to the user's query. The search results are typically ranked based on their perceived relevance to the query.\nQuery Refinement: Review the search results to assess their relevance to the user's information need. If the initial search results are not satisfactory, the user may refine their query by modifying the keywords, adding more context, or using advanced search operators.\nIterative Process: Information retrieval is often an iterative process. If the user's information need is not fully satisfied with the initial search results, they may iterate through the query refinement step multiple times until they find the desired information.\nUser Interaction and Feedback: Throughout the process, user interaction and feedback play a crucial role in improving the relevance of search results. Users can provide explicit feedback (e.g., relevance judgments) or implicit feedback (e.g., click-through behavior) to help the search engine better understand their preferences and information needs.Continual Improvement: Search engines continuously learn and improve by analyzing user interactions, click-through rates, and user feedback. This data is used to update search algorithms, refine rankings, and enhance the overall search experience.Performing information retrieval in the classic search model involves a combination of user understanding, query formulation, search engine utilization, and iterative refinement, with the ultimate goal of providing relevant and accurate information to meet the user's information needs.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157810,"modifiedTime":1737554783000,"sourceSize":2982,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/1. Classic Search Model.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html":{"title":"2. Boolean Retrieval","icon":"","description":"Boolean Retrieval is a simple and fundamental information retrieval model based on Boolean algebra. In this model, queries are expressed using Boolean operators (AND, OR, NOT) to combine terms, allowing users to specify precise criteria for document retrieval. The model is widely used in databases and early search systems, and it forms the basis for more complex retrieval models. Boolean Operators: AND: Represents the intersection of two or more terms. Only documents containing all the specified terms are retrieved.\nOR: Represents the union of two or more terms. Documents containing any of the specified terms are retrieved.\nNOT: Excludes documents containing a specific term. It retrieves documents that do not contain the specified term. Query Formulation: Users construct queries using terms and Boolean operators to define their information needs. For example, a query \"cat AND dog NOT mouse\" retrieves documents containing both \"cat\" and \"dog\" but excluding those containing \"mouse.\" Term-based Retrieval: Boolean Retrieval treats each term in the query as an individual entity. The retrieval model doesn't consider the order of terms or their frequency of occurrence in documents. Binary Representation: Documents and queries are typically represented as binary vectors. Each position in the vector corresponds to a term, and the value is 1 if the term is present in the document or query and 0 if it is absent. Advantages of Boolean Retrieval:\nPrecision and Specificity: Boolean Retrieval allows users to specify precise and specific queries, which can lead to highly relevant results.\nExact Matches: Boolean Retrieval guarantees that retrieved documents match the exact criteria specified in the query.\nEfficiency: The simplicity of Boolean Retrieval makes it computationally efficient, especially for small document collections.\nLimitations of Boolean Retrieval:\nDocuments either match or don’t Good for expert users with precise understanding of their needs and the document collection Not good for the majority of users Most users are incapable of writing Boolean queries Or they find it takes too much effort Boolean queries often result in either too few or too many results\nUse Cases:\nBoolean Retrieval is commonly used in databases and early information retrieval systems where precise queries are necessary.\nIndexing: Indexing is the process of creating a data structure that maps terms or words to the documents that contain them. In the context of information retrieval, a dictionary (also known as an inverted index) is created, which stores this mapping from terms to documents. The dictionary allows efficient and quick access to documents containing specific terms, making the retrieval process faster.Querying: Querying is the process of searching for documents that match a user's search request (query). When a user submits a query, the information retrieval system looks up the terms in the query within the index and returns the relevant documents.Boolean Query: A Boolean query is a type of query that uses Boolean operators (AND, OR, NOT) to combine terms and specify precise criteria for document retrieval. In the provided example, the Boolean query is \"canberra AND healthcare NOT covid.\"Boolean Retrieval Procedures: In the context of Boolean retrieval, the following procedures are followed: Lookup Query Term in the Dictionary: When the query is submitted, the system first looks up each term in the dictionary to find the corresponding posting lists. The posting list contains information about the documents that contain the term. Retrieve the List of Relevant Documents (Posting Lists): For each term in the query, the system retrieves its posting list, which is a list of document identifiers (IDs) or pointers to the documents containing the term. Boolean Operations: After retrieving the posting lists, the system performs the following Boolean operations based on the Boolean query: AND: The system intersects the posting lists of the terms in the AND query. This results in a new posting list containing only the document IDs that appear in all the individual posting lists. OR: The system unions the posting lists of the terms in the OR query. This creates a new posting list with document IDs that appear in at least one of the individual posting lists. NOT: For the NOT query, the system takes the difference of the posting lists. It removes the document IDs from the first posting list that also appear in the second posting list. Example: Suppose we have the following posting lists for the terms \"canberra,\" \"healthcare,\" and \"covid\":\n\"canberra\" -&gt; [doc1, doc3, doc5]\n\"healthcare\" -&gt; [doc1, doc2, doc4]\n\"covid\" -&gt; [doc3, doc4, doc6]\nFor the Boolean query \"canberra AND healthcare NOT covid,\" the system performs the following operations:\nIntersect \"canberra\" and \"healthcare\": [doc1]\nTake the difference with \"covid\": [doc1]\nThe result of the query is document \"doc1.\"Refer to the inclusion of field information along with the term and its associated posting in an inverted index. An inverted index is a data structure used to map terms (words or phrases) to the documents in which they occur. Each term is associated with a posting list, which contains information about the documents that contain the term.Typically, a basic inverted index contains only the term and its associated list of document identifiers (IDs) or pointers to the documents. However, in some cases, it can be advantageous to include additional metadata or field information for each term occurrence in the posting list.Example: Suppose we have a document collection with two fields: \"Title\" and \"Content,\" and we want to build an inverted index for keyword search. Without \"field in posting\": Term: \"Introduction\"\nPosting List: [Doc1, Doc3] With \"field in posting\": Term: \"Introduction\"\nPosting List: [Doc1 (Title), Doc3 (Content)] The evolution of information retrieval has led to modern retrieval procedures that go beyond the simple Boolean model. These procedures include ranking candidates based on relevance, which has become a crucial aspect of modern IR systems. Here's an explanation of these modern IR retrieval procedures:Boolean Model Provides All the Ranking Candidates: In the Boolean model of IR, the retrieval process starts by locating documents that satisfy the Boolean conditions specified in the query. For example, if the user's query is \"travel insurance,\" the Boolean model will translate it into \"travel AND insurance\" to find documents that contain both \"travel\" and \"insurance.\"Locate Documents Satisfying Boolean Condition: The Boolean model retrieves documents based on whether they match the Boolean query conditions. It uses Boolean operators like AND, OR, and NOT to determine the intersection, union, and exclusion of documents, respectively. The result is a set of documents that satisfy the Boolean condition specified in the query.Rank Candidates by Relevance: Once the Boolean model provides the candidate documents, modern IR systems go a step further by ranking these candidates based on their relevance to the user's information need. Relevance is a key concept in IR, representing how well a document meets the user's query intent and how useful it is to the user.Important: The Notion of Relevance: Relevance is a subjective concept and depends on the user's perspective. A document can be relevant to one user's query but not to another's. Modern IR systems use various ranking algorithms and relevance models to estimate the relevance of documents to a specific query.Efficiency Consideration: As the volume of documents and queries in modern IR systems increases, efficiency becomes crucial. Top-k retrieval is a common technique to address this efficiency concern. Instead of retrieving all relevant documents, the system focuses on retrieving the top-k most relevant documents based on the ranking score. For example, a web search engine like Google typically retrieves the top few (k) most relevant web pages to present to the user.Top-k Retrieval (e.g., Google): Top-k retrieval optimizes the retrieval process to quickly provide the most relevant results to the user. This is particularly important in scenarios with a large number of documents and real-time retrieval requirements. For example, when a user enters a query in a search engine like Google, the search engine performs top-k retrieval to present a ranked list of web pages that are most likely to satisfy the user's query.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Basic Concepts:","level":3,"id":"Basic_Concepts_0"},{"heading":"Indexing, Querying &amp; Retrieval Procedures","level":3,"id":"Indexing,_Querying_&_Retrieval_Procedures_0"},{"heading":"Field in posting","level":3,"id":"Field_in_posting_0"},{"heading":"Modern Information Retrieval (IR) Retrieval Procedures","level":3,"id":"Modern_Information_Retrieval_(IR)_Retrieval_Procedures_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157825,"modifiedTime":1737554783000,"sourceSize":9878,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/2. Boolean Retrieval.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html":{"title":"3. Term-Document Incidence Matrix","icon":"","description":"A Term-Document Incidence Matrix (also known as a Document-Term Matrix) is a fundamental data structure used in information retrieval and natural language processing to represent the relationship between terms (words) and documents in a corpus. It is a sparse matrix where each row corresponds to a unique term, each column corresponds to a document, and the cells represent the presence or absence of a term in a particular document.Representation: Let's consider a simple example with four documents (D1, D2, D3, D4) and six unique terms (T1, T2, T3, T4, T5, T6). The Term-Document Incidence Matrix might look like this: D1 D2 D3 D4\nT1 1 1 0 0\nT2 0 1 1 0\nT3 0 0 1 1\nT4 1 0 1 0\nT5 0 1 0 1\nT6 1 1 0 0\nInterpretation:\nThe \"1\" in the cell indicates that the corresponding term exists in the corresponding document.\nThe \"0\" in the cell indicates that the term does not exist in the corresponding document.\nExample Interpretation:\nTerm T1 exists in documents D1 and D2, but not in D3 and D4.\nTerm T2 exists in documents D2 and D3, but not in D1 and D4.\nTerm T3 exists in documents D3 and D4, but not in D1 and D2.\nTerm T4 exists in documents D1 and D3, but not in D2 and D4.\nTerm T5 exists in documents D2 and D4, but not in D1 and D3.\nTerm T6 exists in documents D1 and D2, but not in D3 and D4.\nUse in Information Retrieval: In information retrieval tasks, the Term-Document Incidence Matrix serves as the basis for building more advanced models such as the Vector Space Model. It allows efficient calculation of term frequencies, inverse document frequencies, and similarity measures between documents and queries. The matrix also facilitates the process of ranking documents based on relevance to a given query.Use in Natural Language Processing: In natural language processing, the Term-Document Incidence Matrix is used for text classification, clustering, topic modeling, and other text analysis tasks. It helps identify the most relevant terms for a given document and supports various feature engineering techniques for machine learning models.Efficiency is a critical consideration in information retrieval systems, especially when dealing with large collections of documents. The example provided illustrates the efficiency challenges associated with processing and storing vast amounts of textual data while maintaining the ability to retrieve relevant information quickly.Example Scenario:\nBigger Collections: The example assumes a collection of 1 million documents, with each document being 1,000 words long on average. Considering that each word, including spaces and punctuation, takes an average of 6 bytes, the total data size of all the documents is approximately 6GB.\nDistinct Terms: Among the 1 million documents, there are around 500,000 distinct terms (words).\nChallenges and Solution:\nSparse Matrix: The term-document incidence matrix in this scenario would be enormous, with 500 billion potential entries, as each term could potentially appear in each document. However, it is important to note that in practice, most terms appear in only a few documents. As a result, the term-document incidence matrix is extremely sparse, with the vast majority of its entries being zeros.\nStorage Efficiency: Due to the sparsity of the matrix, it is not efficient to store it explicitly as a dense matrix, as the vast majority of the entries would be zeros. Instead, efficient sparse data structures (e.g., compressed sparse row (CSR) format or hash tables) are employed to store only the non-zero entries and their corresponding row and column indices. This significantly reduces memory usage and storage requirements.\nQuery Efficiency: Efficient data structures and algorithms are also crucial for querying this large and sparse matrix. Information retrieval systems use indexing and inverted index structures to speed up the search process and retrieve relevant documents based on user queries.\nAdvantages of Sparsity:The example's high level of sparsity (only one billion 1's among 500 billion potential entries) offers several advantages:\nReduced Storage Requirements: Storing and processing the sparse matrix becomes much more efficient compared to dense representations, as it avoids wasting space on zero entries. Faster Query Processing: The sparsity allows for quicker retrieval of relevant documents during user queries, as the system can focus only on the non-zero entries, which are the ones representing term occurrences.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Efficiency","level":3,"id":"Efficiency_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157873,"modifiedTime":1737554783000,"sourceSize":4973,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/3. Term-Document Incidence Matrix.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html":{"title":"4. Inverted Index","icon":"","description":"An inverted index is a data structure used in information retrieval systems to efficiently map terms (words) to the documents that contain them. It allows for quick and effective searching of documents based on specific terms, enabling faster retrieval of relevant information from a large collection of documents.An inverted index is typically implemented as a dictionary (or hash table) where each entry corresponds to a unique term. For each term, the entry contains a list of document identifiers (IDs) or pointers to the documents that contain that term (sorted ascending order).Example:Let's consider a small example with three documents (D1, D2, and D3) and six unique terms (T1, T2, T3, T4, T5, and T6). The inverted index might look like this: Term Documents T1 D1, D2 T2 D1, D3 T3 D2, D3 T4 D1, D2, D3 T5 D2 T6 D3\nInterpretation:\nTerm T1 appears in documents D1 and D2.\nTerm T2 appears in documents D1 and D3.\nTerm T3 appears in documents D2 and D3.\nTerm T4 appears in documents D1, D2, and D3.\nTerm T5 appears in document D2.\nTerm T6 appears in document D3.\nUse in Information Retrieval:During the search process, when a user submits a query containing terms, the information retrieval system refers to the inverted index to quickly identify the documents that contain those terms. It then retrieves and ranks the relevant documents based on their relevance to the query.Benefits of Inverted Index:\nFast Retrieval: The inverted index allows for fast retrieval of documents containing specific terms. Instead of scanning the entire document collection, the system can directly access the relevant documents based on the terms in the query.\nReduced Storage Requirements: Inverted indexes are space-efficient, particularly for large collections, as they store only the term-document mappings, avoiding duplication of terms across documents.\nScalability: Inverted indexes are scalable and can handle large document collections effectively, making them suitable for modern information retrieval systems and web search engines.\nInverted Index Construction:Building an inverted index involves processing the entire document collection to extract terms and their occurrences in each document. The process typically includes tokenization, stemming (reducing words to their root form), and stop word removal (excluding common words like \"the,\" \"and,\" etc.). The resulting terms are then mapped to their corresponding documents in the inverted index.In an inverted index, each term (word) in the document collection is associated with a posting list that contains information about the documents in which the term appears. The posting list records the identifiers of these documents, commonly referred to as docIDs or document serial numbers.Storing Lists of Documents: For each term \"t\", the inverted index must store a list of all documents that contain \"t\". This is accomplished by creating a posting list that maintains the docIDs of the relevant documents. The posting list acts as a link between the term and the documents in which it occurs.Variable-Size Postings Lists: The size of the posting list for each term can vary significantly, depending on factors such as the term's frequency and the number of documents containing the term. Some terms may appear in many documents, resulting in larger posting lists, while others may be less common and have smaller posting lists.On Disk Storage: When storing the inverted index on disk, it is beneficial to keep the posting lists contiguous. A contiguous run of postings is considered the norm and is generally the most efficient way to store data on disk. Contiguous storage minimizes disk seek times and allows for efficient block-level reads, enhancing overall retrieval performance.In Memory Storage: In memory, the storage considerations are different. While contiguous runs are optimal on disk, they might not be the most efficient choice in memory due to variable-size posting lists. In memory, data structures such as linked lists or variable-length arrays can be used to represent the posting lists for each term.\nLinked Lists: Each node in the linked list represents a document containing the term, and the nodes are connected through pointers. Linked lists allow for flexible memory allocation and are suitable when the size of the posting list is not known in advance. Variable Length Arrays: Variable-length arrays are data structures that can dynamically resize to accommodate the number of documents in the posting list. This enables efficient memory usage while allowing for fast access to elements in the posting list.\nBalancing Disk and Memory Efficiency: In designing an information retrieval system, a trade-off exists between optimizing disk storage and memory efficiency. For disk storage, contiguous runs of postings are preferred, while in memory, data structures that efficiently manage variable-size posting lists are employed.The indexer is a crucial component in an information retrieval system responsible for building the inverted index, which efficiently maps terms to the documents that contain them. The indexing process involves several steps, and here is an explanation of each step:1. Token Sequence: The first step of the indexer is to tokenize the documents in the collection. Tokenization involves breaking the text into individual units called tokens, which can be words, phrases, or other meaningful elements. The token sequence represents the extracted terms from the documents.2. Sort Tuples by Terms (and then DocID): After tokenization, the indexer creates tuples (term, docID) for each occurrence of a term in a document. Each tuple consists of the term and the identifier (ID) of the document where the term appears. The tuples are then sorted primarily by the terms and, if necessary, by the docID, ensuring that all occurrences of the same term are grouped together.3. Merge Multiple Term Entries in a Single Document: In some cases, a single document may contain multiple occurrences of the same term. To save space and improve efficiency, the indexer merges the multiple tuples for the same term within a document into a single entry. The merged entry typically contains the term and a list of docIDs where the term appears in that document.4. Split into Dictionary and Postings: The sorted and merged tuples are then split into two components: the dictionary and the postings.\nDictionary: The dictionary is a data structure that contains the unique terms from the collection. Each term in the dictionary is associated with a pointer or offset that leads to the corresponding posting list for that term. Postings: The postings store the information about which documents contain each term. For each term, the postings list includes the docIDs where the term appears. The postings can be represented using various data structures, such as arrays, linked lists, or compressed formats, to optimize memory usage and retrieval efficiency. 5. Doc Frequency Information is Added: The document frequency (df) information is added to each entry in the postings list. Document frequency indicates the number of documents in the collection that contain a specific term. This information is essential for calculating term frequencies, inverse document frequencies, and relevance scores during the information retrieval process.Let's consider a small example to illustrate the steps of the indexer in building the inverted index. We have a collection of four documents (D1, D2, D3, and D4) with the following token sequences:D1: \"apple orange apple\"\nD2: \"orange banana\"\nD3: \"apple grape\"\nD4: \"banana grape\"\nStep 1: Token Sequence: The token sequences represent the terms extracted from each document after tokenization:D1: \"apple\", \"orange\", \"apple\"\nD2: \"orange\", \"banana\"\nD3: \"apple\", \"grape\"\nD4: \"banana\", \"grape\"\nStep 2: Sort Tuples by Terms (and then DocID): Sorting the tuples by terms and then by docID gives us:(\"apple\", D1)\n(\"apple\", D3)\n(\"banana\", D2)\n(\"banana\", D4)\n(\"grape\", D3)\n(\"grape\", D4)\n(\"orange\", D1)\n(\"orange\", D2)\nStep 3: Merge Multiple Term Entries in a Single Document: We merge multiple tuples with the same term within a document:(\"apple\", D1, D3)\n(\"banana\", D2, D4)\n(\"grape\", D3, D4)\n(\"orange\", D1, D2)\nStep 4: Split into Dictionary and Postings: The dictionary contains unique terms and their pointers to the corresponding postings lists:Dictionary:\n\"apple\" -&gt; pointer to postings list\n\"banana\" -&gt; pointer to postings list\n\"grape\" -&gt; pointer to postings list\n\"orange\" -&gt; pointer to postings list\nThe postings lists contain the docIDs where each term appears:Postings Lists:\n\"apple\": D1, D3\n\"banana\": D2, D4\n\"grape\": D3, D4\n\"orange\": D1, D2\nStep 5: Doc Frequency Information is Added: Document frequency (df) information is added to each entry in the postings list:Postings Lists with df:\n\"apple\": df=2, postings=D1, D3\n\"banana\": df=2, postings=D2, D4\n\"grape\": df=2, postings=D3, D4\n\"orange\": df=2, postings=D1, D2\nFinal Inverted Index: The final inverted index is represented using the dictionary and postings lists:Inverted Index:\nDictionary:\n\"apple\" -&gt; postings=D1, D3, df=2\n\"banana\" -&gt; postings=D2, D4, df=2\n\"grape\" -&gt; postings=D3, D4, df=2\n\"orange\" -&gt; postings=D1, D2, df=2\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Structure of an Inverted Index:","level":3,"id":"Structure_of_an_Inverted_Index_0"},{"heading":"Storation","level":3,"id":"Storation_0"},{"heading":"Indexer Step","level":3,"id":"Indexer_Step_0"},{"heading":"Example","level":4,"id":"Example_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157877,"modifiedTime":1737554783000,"sourceSize":10605,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/4. Inverted Index.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html":{"title":"5. Merge Intersection Algorithm (p1, p2)","icon":"","description":"This algorithm performs an efficient intersection of two sorted lists, p1 and p2, to find their common elements. The algorithm runs in linear time, which means its time complexity is proportional to the total number of elements in both lists.Input:\np1: A sorted list (e.g., array, linked list) containing elements in ascending order.\np2: Another sorted list (e.g., array, linked list) containing elements in ascending order.\nOutput:\nintersection_result: A list containing the common elements present in both p1 and p2.\nAlgorithm Steps:\nInitialize two pointers, i and j, to the beginning of p1 and p2, respectively.\nCreate an empty list, intersection_result, to store the common elements.\nWhile both pointers are within the bounds of their respective lists, repeat steps 4 to 6.\nIf the element at p1[i] is equal to the element at p2[j], add it to the intersection_result, and move both pointers (i and j) to the next element in their respective lists.\nIf the element at p1[i] is smaller than the element at p2[j], move pointer i to the next element in p1.\nIf the element at p1[i] is greater than the element at p2[j], move pointer j to the next element in p2.\nIf either pointer reaches the end of its list, exit the loop.\nReturn the intersection_result containing the common elements.\nPseudocode:function intersection(p1, p2): i = 0 j = 0 intersection_result = [] while i &lt; len(p1) and j &lt; len(p2): if p1[i] == p2[j]: intersection_result.append(p1[i]) i += 1 j += 1 elif p1[i] &lt; p2[j]: i += 1 else: j += 1 return intersection_result\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157881,"modifiedTime":1737554783000,"sourceSize":1795,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/5. Merge Intersection Algorithm (p1, p2).md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html":{"title":"6. Text Preprocessing","icon":"","description":"Text preprocessing is a crucial step in natural language processing (NLP) that involves transforming raw text data into a clean and structured format suitable for further analysis and machine learning tasks. Preprocessing helps to remove noise, inconsistencies, and irrelevant information from the text, making it easier for NLP models to understand and extract meaningful insights. Check <a data-href=\"1. Pipeline\" href=\"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Pipeline</a>The text preprocessing pipeline typically includes the following steps:1. Tokenization: Tokenization is the process of breaking the text into individual words or tokens. It involves splitting the text based on spaces or punctuation marks to create a sequence of meaningful units for analysis.2. Lowercasing: Converting all text to lowercase helps in standardizing the text and reducing the vocabulary size. This ensures that words with the same meaning but different cases (e.g., \"apple\" and \"Apple\") are treated as the same token.3. Removing Punctuation: Punctuation marks (e.g., periods, commas, exclamation marks) are often unnecessary for many NLP tasks. Removing punctuation simplifies the text and reduces the vocabulary size without sacrificing meaningful information.4. Removing Stopwords: Stopwords are common words that occur frequently in a language (e.g., \"the,\" \"and,\" \"is\"). These words often do not contribute much to the meaning of the text and can be removed to reduce noise and improve processing speed.5. Stemming or Lemmatization: Stemming and lemmatization are techniques used to reduce words to their base or root form. Stemming chops off word suffixes (e.g., \"running\" to \"run\"), while lemmatization transforms words to their dictionary form (e.g., \"better\" to \"good\"). These techniques help to reduce inflectional forms and consolidate related words.6. Removing Numbers and Special Characters: In some cases, numbers and special characters may not be relevant for NLP tasks, so they can be removed to simplify the text further.7. Handling Contractions and Abbreviations: Contractions (e.g., \"don't\" for \"do not\") and abbreviations (e.g., \"USA\" for \"United States of America\") may be expanded to their full forms for better understanding.8. Spell Checking and Correction: In some cases, spell checking and correction can be applied to fix common spelling errors and typos.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/natural-language-processing/basics/1.-pipeline.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157885,"modifiedTime":1737554783000,"sourceSize":2953,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/6. Text Preprocessing.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html":{"title":"7. Ranked Retrieval","icon":"","description":"Ranked retrieval is a technique used in information retrieval systems to retrieve and present search results in order of relevance, where the most relevant documents are ranked higher than less relevant ones. The goal of ranked retrieval is to provide users with the most pertinent information quickly and efficiently, especially when dealing with large collections of documents.Key Concepts of Ranked Retrieval: Scoring Function: In ranked retrieval, each document is assigned a score based on its relevance to the user's query. The scoring function calculates the relevance score by considering various factors, such as term frequency (TF), inverse document frequency (IDF), document length normalization, and other relevance metrics.\nThe scoring function aims to measure how well the document matches the query and how important the terms in the document are within the context of the entire document collection. Ranking Algorithm: A ranking algorithm is used to sort the documents based on their relevance scores in descending order. The most relevant documents are ranked higher and presented at the top of the search results.\nCommon ranking algorithms include the Vector Space Model (VSM), Okapi BM25, and language models. Query Processing: When a user submits a query, the information retrieval system processes the query and identifies the relevant documents using the scoring function.\nThe system then ranks the documents based on their relevance scores to present the most relevant results first. User Interaction: Ranked retrieval allows users to interact with the search results by exploring documents at different ranks. Users can refine their queries, view additional results, or adjust the relevance criteria based on the presented rankings. A technique used in information retrieval systems to rank and score documents based on their relevance to a user's query while considering different fields (attributes) of the documents. Unlike traditional ranked retrieval, where a single relevance score is computed for each document, weighted field scoring assigns different weights to each field based on its importance in the query and relevance evaluation.Key Concepts of Weighted Field Scoring: Query with Multiple Fields: In weighted field scoring, a user's query can include multiple search fields, each targeting a specific attribute of the documents. For example, a search query might have separate fields for title, content, author, and date. Field Weights: Each search field is assigned a weight that represents its importance in the query. Fields with higher weights contribute more to the overall relevance score of a document.\nField weights can be assigned manually based on domain knowledge or learned automatically from training data using machine learning algorithms. Scoring Function: The scoring function in weighted field scoring calculates the relevance score of each document for each search field individually.\nThe relevance score for each field is then multiplied by its corresponding field weight to reflect its importance in the overall document ranking. Aggregation of Field Scores: The relevance scores obtained for each field are aggregated to compute the final document score. This can be achieved through various aggregation methods, such as weighted summation or other combination techniques. Example:Suppose we have a document collection with fields such as \"Title,\" \"Content,\" and \"Author.\" A user submits a query with different weights assigned to each field. For example:\nQuery: \"machine learning\"\nField Weights: Title (0.6), Content (0.8), Author (0.4)\nThe scoring function computes the relevance scores for each document in each field based on the query terms and assigns the corresponding weights. The final document score is obtained by aggregating the weighted scores from all the fields.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Weighted field scoring","level":3,"id":"Weighted_field_scoring_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157889,"modifiedTime":1737554783000,"sourceSize":4511,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/7. Ranked Retrieval.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html":{"title":"8. TF & IDF","icon":"","description":"Term Frequency (TF) and Inverse Document Frequency (IDF) are two important concepts in information retrieval and text mining. They play a crucial role in ranking and scoring documents for relevance in search engines and other natural language processing tasks. Term Frequency (TF): Term Frequency measures the frequency of a term (word) in a document. It indicates how many times a specific term occurs in a document relative to the total number of terms in that document.\nTF provides information about the local importance of a term within a specific document.\nTF is computed using the following formula: TF(term, document) = (Number of occurrences of the term in the document) / (Total number of terms in the document) Inverse Document Frequency (IDF): Inverse Document Frequency measures the importance of a term in the entire document collection. It is designed to downweight terms that occur frequently across many documents, as they are likely to be less informative for distinguishing between documents.\nThe logarithm is used to dampen the effect of very high document frequencies and prevent overly dominant terms from affecting the score.\nIDF is computed using the following formula: IDF(term) = log((Total number of documents) / (Number of documents containing the term))\nTF-IDF:\nThe combination of Term Frequency (TF) and Inverse Document Frequency (IDF) is known as TF-IDF. It is a popular weighting scheme used to represent the importance of a term in a document relative to the entire document collection.\nTF-IDF gives higher scores to terms that appear frequently in a particular document (high TF) and are rare across the entire collection (high IDF).\nThe TF-IDF score for a term in a document is calculated as follows:\nUse Cases:\nTF-IDF is widely used in information retrieval systems for document ranking, where documents containing more relevant and rare terms receive higher scores.\nIt is used in text classification, sentiment analysis, and clustering tasks to identify important features (terms) in a document or set of documents.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"8. TF &amp; IDF","level":1,"id":"8._TF_&_IDF_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157893,"modifiedTime":1737554783000,"sourceSize":4980,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/8. TF & IDF.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html":{"title":"9. Vector Space Model","icon":"","description":"The Vector Space Model (VSM) is a widely used mathematical model in information retrieval and natural language processing for representing text documents as vectors in a high-dimensional space. The VSM transforms documents and queries into numerical vectors, allowing the application of various mathematical operations to measure similarity and relevance between documents and queries.Key Concepts of the Vector Space Model: Document-Term Matrix: In the VSM, a document collection is represented as a matrix where each row corresponds to a document, and each column represents a unique term (word) in the entire collection.\nThe matrix is sparse, as most documents only contain a subset of the available terms. Term Frequency (TF): The term frequency measures the number of occurrences of a term in a document. It reflects the importance of a term in a document relative to the other terms in that document.\nTF values are used to populate the document-term matrix. Inverse Document Frequency (IDF): The inverse document frequency represents the inverse of the proportion of documents that contain a specific term.\nIt helps in downweighting terms that occur in many documents and thus might be less informative for distinguishing between documents.\nIDF values are typically computed based on the entire document collection. TF-IDF Weighting: The VSM incorporates the TF-IDF weighting scheme, where each term's TF value is multiplied by its IDF value to obtain the final term weight for each document.\nThe TF-IDF weighting balances the importance of terms within a document and across the entire collection. Cosine Similarity: To compare the similarity between two vectors (documents or queries) in the vector space, the cosine similarity metric is commonly used.\nCosine similarity measures the cosine of the angle between two vectors, indicating their direction and similarity. A cosine similarity of 1 indicates perfect similarity, while 0 means no similarity. Example:Suppose we have a document collection with three documents:\nDocument 1: \"Introduction to Machine Learning\"\nDocument 2: \"Python Programming for Beginners\"\nDocument 3: \"Machine Learning in Practice\"\nAfter preprocessing (removing stop words, stemming, etc.), we obtain a list of unique terms:\nTerms: [\"Introduction\", \"Machine\", \"Learning\", \"Python\", \"Programming\", \"Beginners\", \"Practice\"]\nUsing TF-IDF, we calculate the term weights for each document based on their term frequencies and inverse document frequencies.Document-Term Matrix (simplified):","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157897,"modifiedTime":1737554783000,"sourceSize":4052,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/9. Vector Space Model.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html":{"title":"10. Evaluation","icon":"","description":"The evaluation of an Information Retrieval (IR) system is crucial to assess its effectiveness and performance. It involves comparing the system's retrieved results against a known set of relevant documents, typically provided by human assessors. The evaluation process allows us to measure various metrics that reflect the system's retrieval accuracy, relevance, and user satisfaction. Relevance: Relevance refers to the degree to which a retrieved document meets the information needs of a user with respect to their query.\nDocuments that are highly relevant are considered as true positives, while irrelevant documents are considered false positives. Precision and Recall: Precision measures the proportion of retrieved documents that are relevant out of all retrieved documents. Recall measures the proportion of relevant documents that were successfully retrieved out of all relevant documents in the collection. Precision and Recall are commonly used together to evaluate an IR system's retrieval effectiveness. F1 Score: The F1 score is the harmonic mean of Precision and Recall and provides a single metric to evaluate the trade-off between the two.\nIt is especially useful when the focus is on balancing precision and recall. Mean Average Precision (MAP): MAP is a popular metric for evaluating IR systems when there are multiple queries.\nIt calculates the average precision across all queries and provides a comprehensive measure of the system's performance. Mean Reciprocal Rank (MRR) a metric used to evaluate the performance of a ranked retrieval system in Information Retrieval (IR) tasks. MRR is particularly useful when dealing with ranked lists of documents, such as in the evaluation of search engine results.\nFor each query in the evaluation set, the ranked list of retrieved documents is generated.\nThe reciprocal rank (RR) for each query is calculated as the inverse of the rank of the first relevant document in the list. If no relevant documents are retrieved for a query, the RR is 0.\nThe Mean Reciprocal Rank (MRR) is computed as the average of the reciprocal ranks across all queries in the evaluation set. Normalized Discounted Cumulative Gain (nDCG): nDCG is a metric that considers the ranking order of retrieved documents in addition to their relevance.\nIt takes into account the graded relevance of documents and penalizes the system for suboptimal ranking. Evaluation Methodologies: Offline Evaluation: In offline evaluation, the evaluation is performed using a pre-defined set of queries and known relevant documents.\nThe system's retrieval results are compared to the relevance judgments to compute metrics like precision, recall, F1 score, MAP, etc. Online Evaluation: Online evaluation involves conducting experiments with real users in a live environment.\nUser interactions, click-through rates, and other user behavior are analyzed to assess the system's performance in a real-world setting. In IR evaluation, a test collection is a predefined set of queries and corresponding relevant documents used for testing and benchmarking the IR system's performance.Test Collections in Information Retrieval:Definition: A test collection is a carefully curated dataset used for evaluating and benchmarking the performance of Information Retrieval (IR) systems. It consists of a set of queries, a corresponding set of relevant documents for each query, and sometimes additional relevance judgments provided by human assessors.Purpose of Test Collections: The primary purpose of test collections is to provide a standardized and reproducible way to evaluate the effectiveness of IR systems. They serve as a reference point for comparing different retrieval algorithms, configurations, and improvements in a fair and controlled manner.Components of Test Collections: Queries: Queries are the information needs or search requests provided to the IR system for retrieval.\nThey can be specific keywords or phrases, or more complex natural language queries. Relevant Documents: Relevant documents are the documents considered to be useful and related to a particular query's information need.\nHuman assessors typically provide relevance judgments for each query-document pair. Additional Metadata: Some test collections may include additional metadata about the documents, such as author names, publication dates, or document categories, to support more sophisticated evaluation. Creating Test Collections: Query Selection: Test collections often include a diverse set of queries to cover various user information needs and scenarios.\nQueries can be derived from real user search logs, topic lists, or common information needs identified by domain experts. Relevance Judgments: Relevance judgments are essential for evaluating the performance of IR systems accurately.\nHuman assessors manually judge the relevance of retrieved documents for each query (ground truth).\nRelevance judgments are typically binary (relevant/not relevant) or graded (e.g., on a scale of 0 to 3). Document Selection: The document collection used in a test collection should be representative of the target domain or the application area of the IR system being evaluated.\nThe size of the document collection can vary based on the evaluation needs and the available resources. Definition: In the ranked evaluation setting, the retrieved documents are ranked based on their predicted relevance scores, usually obtained from an IR system. The highest-ranked documents are considered more relevant to the query than lower-ranked ones. The primary goal of ranked evaluation is to assess the effectiveness of an IR system in returning the most relevant documents at the top of the ranked list.Process:\nRetrieval and Ranking: The IR system retrieves a set of documents for a given query and ranks them based on their relevance scores. The higher the relevance score, the higher the document's rank.\nRelevance Judgments: For each query, human assessors provide relevance judgments for a subset of the retrieved documents, ----indicating whether they are relevant or not.\nEvaluation Metrics: Common evaluation metrics in ranked evaluation include Precision-Recall curves, Average Precision (AP), Mean Average Precision (MAP), and Discounted Cumulative Gain (DCG).\nDefinition: In the unranked evaluation setting, retrieved documents are considered equally relevant to the query, and their order is not taken into account. The primary goal of unranked evaluation is to measure the presence or absence of relevant documents without considering their specific rankings.Process:\nRetrieval: The IR system retrieves a set of documents for a given query.\nRelevance Judgments: For each query, human assessors provide binary relevance judgments (relevant or not relevant) for the retrieved documents.\nEvaluation Metrics: Common evaluation metrics in unranked evaluation include Precision, Recall, F1 score, and Accuracy.\nTo construct the Precision-Recall curve, the IR system retrieves documents for a given query and ranks them based on their predicted relevance scores. Then, the Precision and Recall values are computed for each possible cut-off point, where the system retrieves only the top k documents (where k varies from 1 to the total number of retrieved documents).Plotting the Precision-Recall Curve:\nCompute Precision and Recall for each cut-off point (k) as the system retrieves top k documents.\nPlot the Precision-Recall pairs on the graph, with Recall on the x-axis and Precision on the y-axis.\nConnect the points to create the Precision-Recall curve.\nInterpretation of the Curve:\nThe Precision-Recall curve shows the trade-off between Precision and Recall as the system retrieves more or fewer documents.\nA higher Precision value indicates that the retrieved documents are highly relevant, while a higher Recall value indicates that more relevant documents are retrieved, regardless of non-relevant ones.\nThe ideal system would have both high Precision and high Recall, achieving a curve that is close to the top-right corner of the graph\nExample:\nSuppose an IR system retrieves documents for a query, and the results are as follows:Precision-Recall Curve:\nAt k=1: Precision = 1/1=1.01/1=1.0, Recall = 1/3≈0.3331/3≈0.333\nAt k=2: Precision = 1/2=0.51/2=0.5, Recall = 1/3≈0.3331/3≈0.333\nAt k=3: Precision = 2/3≈0.6672/3≈0.667, Recall = 2/3≈0.6672/3≈0.667\nAt k=4: Precision = 3/4=0.753/4=0.75, Recall = 3/3=1.03/3=1.0\nAt 5k=5: Precision = 3/5=0.63/5=0.6, Recall = 3/3=1.03/3=1.0\nAt k=6: Precision = 3/6=0.53/6=0.5, Recall = 3/3=1.03/3=1.0\nInterpolated Precision\nis a method used to compute precision values at specific recall levels on a Precision-Recall curve. It is particularly helpful when dealing with sparse Precision-Recall curves with few or no precision values at certain recall levels. The interpolated precision assigns the highest precision value seen at or after each recall level to ensure a monotonically decreasing precision curve.Calculating Interpolated Precision:\nStart with the Precision-Recall curve, where precision values are calculated for various recall levels (usually at each relevant document retrieved).\nFor each recall level r, find the highest precision value ′p′ that occurs at or after r. In other words, find the maximum precision value from the precision values at recall levels greater than or equal to r.\nAssign the �′p′ value to the interpolated precision at recall level �r.\nRepeat this process for all recall levels.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Key Concepts in IR Evaluation:</strong>","level":3,"id":"**Key_Concepts_in_IR_Evaluation**_0"},{"heading":"<strong>Test Collections:</strong>","level":3,"id":"**Test_Collections**_0"},{"heading":"Two evaluation settings","level":3,"id":"Two_evaluation_settings_0"},{"heading":"<strong>Ranked Evaluation Setting:</strong>","level":4,"id":"**Ranked_Evaluation_Setting**_0"},{"heading":"<strong>Unranked Evaluation Setting:</strong>","level":4,"id":"**Unranked_Evaluation_Setting**_0"},{"heading":"<strong>Precision-Recall Curve:</strong>","level":3,"id":"**Precision-Recall_Curve**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157814,"modifiedTime":1737554783000,"sourceSize":10785,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/10. Evaluation.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html":{"title":"11. Web Search","icon":"","description":"\nNodes: These represent entities such as documents, people, or organizations. Edges: Represent relationships between nodes. Examples include hyperlinks and co-authorship relations. Direction of Edges: Edges can be directional, implying they move from one node to another in a specified direction. Degree: Represents the number of edges connected to a node. In-degree: Denotes the number of incoming edges to a node. Out-degree: Denotes the number of outgoing edges from a node. Overview: Link analysis utilizes the web graph's structure to enhance search functionality. Significance: It's one of the revolutionary advancements in web search. Google's initial success is attributed in part to its innovative link analysis techniques. Concept: Many documents feature bibliographies that cite earlier publications. When these citations are viewed as edges, a collection of documents can form a graph. Benefits: The graph's structure can yield valuable insights. For instance, it can highlight document similarities without focusing on the document's actual content. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Web Basics","level":3,"id":"Web_Basics_0"},{"heading":"Degree of a Node","level":4,"id":"Degree_of_a_Node_0"},{"heading":"Link Analysis","level":3,"id":"Link_Analysis_0"},{"heading":"Citation Analysis","level":3,"id":"Citation_Analysis_0"},{"heading":"Bibliometrics","level":4,"id":"Bibliometrics_0"},{"heading":"Bibliographic Coupling","level":4,"id":"Bibliographic_Coupling_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157817,"modifiedTime":1737554783000,"sourceSize":4362,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/11. Web Search.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html":{"title":"12. HITS Algorithm","icon":"","description":"\nIntroduced by Jon Kleinberg in 1999.\nDetermines two measures for webpages: Authority and Hub values. Authorities: Pages with quality content frequently referred to.\nHubs: Pages that link out to many other pages. Start with a query to obtain a focused subgraph of the web (the \"base set\"). Contains pages with query terms.\nContains pages linking to/from the pages with the query terms. Every page gets two scores: Hub score (h): Initially 1.\nAuthority score (a): Initially 1. For each page p: (Sum of hub scores of all pages q pointing to p.)","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":2,"id":"Introduction_0"},{"heading":"Process","level":2,"id":"Process_0"},{"heading":"Preliminaries","level":3,"id":"Preliminaries_0"},{"heading":"Initialization","level":3,"id":"Initialization_0"},{"heading":"Iterative Algorithm","level":3,"id":"Iterative_Algorithm_0"},{"heading":"Authority Update Step","level":4,"id":"Authority_Update_Step_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157821,"modifiedTime":1737554783000,"sourceSize":1529,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/12. HITS Algorithm.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html":{"title":"Review","icon":"","description":"Res:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Querying","level":3,"id":"Querying_0"},{"heading":"Calculate the tf-idf score","level":4,"id":"Calculate_the_tf-idf_score_0"},{"heading":"Cosine similarity","level":4,"id":"Cosine_similarity_0"},{"heading":"Suppose the query \"quick fox\"","level":5,"id":"Suppose_the_query_\"quick_fox\"_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157901,"modifiedTime":1737554783000,"sourceSize":3229,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/Review.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html":{"title":"0. Representation","icon":"","description":"The Bag-of-Words (BoW) model is a popular and essential technique in natural language processing (NLP) that represents text data in a simple and efficient numerical format. The BoW model treats a text document as an unordered collection or \"bag\" of words, focusing solely on the frequency of words in the document while disregarding grammar, word order, and word relationships. It is widely used in various NLP tasks, such as text classification, sentiment analysis, information retrieval, and topic modeling.Key Concepts of the Bag-of-Words Model: Tokenization: The first step in creating the BoW model is tokenization, where the text data is divided into individual words or tokens. Each token represents a single word or a group of words (n-grams) based on the chosen language processing technique.\nTokenization can be performed using simple techniques like splitting the text on whitespace or more sophisticated methods that consider punctuation and special characters. Vocabulary Creation: The BoW model builds a vocabulary by collecting all unique tokens across the entire dataset. Each token in the vocabulary corresponds to a unique dimension in the feature vector that will represent the text documents.\nThe size of the vocabulary depends on the number of unique tokens present in the text corpus. Document Representation: Each document in the dataset is represented as a feature vector, with the length of the vector equal to the size of the vocabulary. Each dimension in the vector corresponds to a token (word) from the vocabulary.\nThe value in each dimension of the vector represents the frequency of the corresponding token (word) in the document. If a word occurs multiple times in a document, its frequency value will be greater than one. Term Frequency (TF): Term Frequency (TF) is the count of how often a word appears in a document. It is calculated by counting the occurrences of each word in the document.\nTF values represent the local importance of words within individual documents. Document-Term Matrix (DTM): The BoW model creates a Document-Term Matrix (DTM) to store the representation of the entire dataset. The DTM is a 2D matrix where rows represent documents, columns represent unique words from the vocabulary, and the matrix entries represent the term frequencies (TF) of each word in each document.\nEach row in the DTM corresponds to a document, and each column corresponds to a unique word in the vocabulary. Example:Let's consider the following two sentences:\n\"The cat sat on the mat.\"\n\"The mat was sat on by the cat.\"\nDespite the different word orders, both sentences convey a similar idea: a cat sitting on a mat. In a BoW representation, both sentences would have very similar vectors.Steps to compute BoW:\nList all unique words from both sentences without repetition.\nCount the frequency of each word in every sentence.\nUnique words: The\ncat\nsat\non\nthe\nmat\nwas\nby\nUsing these words, our BoW representation becomes:Vectors:Sentence 1: [1, 1, 1, 1, 1, 1, 0, 0]\nSentence 2: [1, 1, 1, 1, 0, 1, 1, 1]Sparse representation refers to a representation of data where most of the values are zero (or another \"default\" value), and only a few positions have meaningful or non-default values. Sparse representations are commonly used in various fields, especially in contexts where storing or processing a dense representation would be inefficient.Let's consider a large document corpus containing 10,000 unique words. When representing documents using the Bag of Words (BoW) model, each document is represented as a vector of length 10,000, where each position in the vector corresponds to a unique word in the corpus and its value indicates the frequency of the word in the document.Imagine a short document:\n\"The cat sat on the mat.\"\nUsing BoW, the vector representation of this document might look something like:[1, 1, 1, 1, 1, 0, 0, 0, ... , 0]\n(1 for 'The', 1 for 'cat', 1 for 'sat', 1 for 'on', 1 for 'the mat', and zeros for all other 9,995 words in our vocabulary)This is a dense representation because it has an entry for every word in the vocabulary.In contrast, a sparse representation might record only the non-zero entries and their positions:{(0: 1), (1: 1), (2: 1), (3: 1), (4: 1)}\nIn the sparse representation, the keys represent the positions of the non-zero entries, and the values represent the non-zero entries themselves.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Example","level":3,"id":"Example_0"},{"heading":"<strong>BoW Representation (Bag of Words)</strong>","level":4,"id":"**BoW_Representation_(Bag_of_Words)**_0"},{"heading":"<strong>Sparse Representation</strong>","level":4,"id":"**Sparse_Representation**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157909,"modifiedTime":1737554783000,"sourceSize":8727,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/0. Representation.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html":{"title":"1.  Word2Vec","icon":"","description":"Check: <a data-href=\"Word2Vec\" href=\"artificial-intelligence/natural-language-processing/models-&amp;-algorithm/word2vec.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Word2Vec</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Word2Vec","level":1,"id":"1._Word2Vec_0"}],"links":["artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157913,"modifiedTime":1737554783000,"sourceSize":20,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/1.  Word2Vec.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html":{"title":"2. Transformer","icon":"","description":"Check:\n<a data-href=\"0. Self-Attention\" href=\"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">0. Self-Attention</a>\n<br><a data-href=\"1. Encoder\" href=\"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Encoder</a>\n<br><a data-href=\"2. Decorder\" href=\"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. Decorder</a>\n<br><a data-href=\"3. BERT\" href=\"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3. BERT</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html#_0","artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html#_0","artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html#_0","artificial-intelligence/deep-learning/5.-transformer/3.-bert.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157917,"modifiedTime":1737554783000,"sourceSize":84,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/2. Transformer.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html":{"title":"3. Neural Network Language Models","icon":"","description":"Neural Network Language Models, often abbreviated as NNLMs, represent a shift from traditional statistical methods of language modeling like n-gram models. Instead of counting word frequencies, NNLMs learn continuous representations of words and leverage these representations to predict the likelihood of word sequences.Functions of a Neural Network Language Model: Text Generation: One of the most popular applications of NNLMs is in generating text. Given a seed phrase or starting sequence, the model can produce a continuation of that sequence, attempting to mimic the style and coherence of human language. Likelihood Estimation: NNLMs can estimate the likelihood or probability of a given text sequence. This is useful in various applications, such as machine translation or speech recognition, where determining the most likely sequence can enhance the accuracy of the system. Auto-regressive Modeling:The term \"auto-regressive\" in the context of language models refers to the process of using previous tokens (words or subwords) in a sequence to predict the next token.Steps in Auto-regressive Neural Network Language Modeling: Token Representation: Each word/token is typically represented as a high-dimensional vector. Initially, these vectors might be simple embeddings like word2vec or GloVe, but as the model is trained, these vectors (or embeddings) are fine-tuned to capture semantic and syntactic information. Sequence Input: Given a sequence of words (e.g., \"I have a\"), these words are converted into their corresponding vectors and fed into the neural network. Model Architecture: The architecture of the neural network can vary. Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformer-based models like GPT and BERT are common choices. These architectures are designed to capture the sequential nature of language, remembering context from previous tokens to aid in prediction. Prediction: The model's goal is to predict the next word in the sequence. For the example \"I have a\", the model might predict \"dream\" as the next word. The model outputs a probability distribution over the entire vocabulary, and the word with the highest probability is chosen as the prediction. Training: During training, the model is provided with a large corpus of text. It uses the actual next word in the sequence to adjust its weights (via backpropagation) and get better at its predictions. The aim is to minimize the difference between its predicted probabilities and the actual outcomes (known as the loss). Inference: During inference (or when the model is being used post-training), the predicted word can be fed back into the model as part of the input to predict the subsequent word, allowing for continued text generation. Neural Network Language Models, especially those based on the auto-regressive approach, have revolutionized various NLP tasks. Their ability to generate coherent text and predict the likelihood of sequences makes them integral to modern NLP systems, from chatbots to translation tools. As technology evolves, we can anticipate even more sophisticated and efficient models that push the boundaries of what machines can understand and produce in terms of human language.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157922,"modifiedTime":1737554783000,"sourceSize":3455,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/3. Neural Network Language Models.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html":{"title":"4. RNN LM","icon":"","description":"A Recurrent Neural Network (RNN) is a type of artificial neural network designed to recognize patterns in sequences of data, such as time series or text. When applied to language modeling, RNNs are trained to predict the next word in a sequence, based on the words that came before it.Basic Principles: Recurrent Architecture: Unlike traditional feedforward neural networks, where data flows in one direction, RNNs allow data to loop back or be 'recurrent.' This looping mechanism lets RNNs maintain information in 'memory' over time. Shared Weights: In an RNN, the weights are shared across time steps. This means that the same set of weights is used for each input, allowing the model to generalize patterns across different positions in a sequence. How RNN Language Model Works: Token Representation: Each word in a sequence is represented as a vector, often initialized with pre-trained embeddings like word2vec or GloVe. Sequential Processing: An RNN processes sequences word-by-word. For each word, it maintains a 'hidden state' that encapsulates the information seen in the sequence so far. This hidden state is updated as each new word is fed into the network. Prediction: At each step, based on the current word and the hidden state, the RNN produces a prediction for the next word. This prediction is in the form of a probability distribution over the entire vocabulary. Training: The RNN is trained using a large corpus of text. Given a sequence of words, it tries to predict the next word. The difference between the predicted probabilities and the actual next word (the target) is used to compute a loss. The model's weights are then adjusted using backpropagation through time (BPTT) to minimize this loss. Challenges: Vanishing and Exploding Gradients: During training, RNNs can suffer from the vanishing or exploding gradient problem, making them hard to train on long sequences.\nShort Memory: Basic RNNs can have difficulty maintaining long-term dependencies due to their inherent structure. Extensions and Variants: LSTM (Long Short-Term Memory): LSTMs are a type of RNN designed to remember long-term dependencies. They have a more sophisticated hidden state update mechanism involving gates (input, forget, and output gates), which helps them retain important information and forget irrelevant data. GRU (Gated Recurrent Unit): GRUs are a simplified version of LSTMs with fewer gates, often leading to faster training and requiring fewer parameters. Applications of RNN Language Models:\nText Generation: Given a seed phrase, the model can generate coherent sequences of text.\nSpeech Recognition: Predicting the next word or phoneme in a spoken sequence.\nMachine Translation: Used in sequence-to-sequence models to produce translations.\nRNNs, with their ability to process sequences and maintain a form of 'memory,' have been instrumental in advancing many NLP tasks. However, while vanilla RNNs laid the foundation for sequence modeling, modern applications often use their more advanced counterparts, like LSTMs and GRUs, to deal with the challenges RNNs present. Regardless, understanding the fundamental operation of RNNs is crucial for anyone delving into the field of deep learning and natural language processing.Sentence: \"The cat sat on the\" Tokenization: Sentence → Tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\"] Word Embedding: Convert tokens to vectors using an embedding layer. RNN Processing: Initialize a hidden state.\nProcess each token sequentially: For each token, input its embedding and the previous hidden state into the RNN.\nRNN updates its hidden state. After \"the\", the hidden state encapsulates: \"The cat sat on the\". Prediction: Use the final hidden state to predict the next word.\nOutput is a probability distribution over vocabulary.\nHighest probability word (e.g., \"mat\") becomes the prediction. Training (if applicable): Compare predicted word to actual next word.\nAdjust model weights based on error using backpropagation. Result: \"The cat sat on the mat\" (assuming \"mat\" had the highest probability).","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>RNN Language Model (LM) Demonstration:</strong>","level":3,"id":"**RNN_Language_Model_(LM)_Demonstration**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157926,"modifiedTime":1737554783000,"sourceSize":4473,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/4. RNN LM.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html":{"title":"5. Pre-trained LM","icon":"","description":"1. Self-supervised Learning:\nDefinition: A type of machine learning where the model is trained to predict part of the input from other parts of the input, using unlabeled data. The \"labels\" are automatically generated from the input data, hence the term \"self-supervised.\"\nExample: In the context of language, given the sentence \"The cat is on the ____\", the model might be trained to fill in the blank (predicting \"mat\" for instance).\n2. Training with Unlabelled/Self-labelled Data: Unlabelled Data: Data that hasn't been manually annotated with labels. Self-labelled Data: Data for which labels are derived or inferred from the data itself without human intervention. For instance, in the previous example, the word \"mat\" becomes a self-label derived from the rest of the sentence. Advantages: Enables leveraging vast amounts of available data without the need for expensive and time-consuming manual labeling. 3. Pre-trained Language Models: Definition: Models that have been trained on large corpora of text to understand and generate human language. Once trained, they can be adapted for specific tasks. Examples: Models like BERT, GPT-2, GPT-3, and T5 have been pre-trained on diverse and extensive datasets to capture wide-ranging language patterns. Benefits: Generalization: Due to training on diverse data, these models generalize well to different tasks with fine-tuning.\nEfficiency: Reduces the need for task-specific data, as the model has already learned a lot of language structure.\nPerformance: Often outperforms models trained only on task-specific data. 4. Transfer Learning through Fine-tuning:\nTransfer Learning: A machine learning technique where a pre-trained model is further trained (typically with fine-tuning) on a new, related task.\nFine-tuning: The process of slightly adjusting the parameters of an already trained model to adapt to a new task. This is typically done using a smaller learning rate to avoid large changes to the pre-learned parameters.\nProcess: Initialize: Start with a pre-trained language model.\nAdapt: Add task-specific layers if necessary (e.g., a classification layer for sentiment analysis).\nTrain: Fine-tune the model on the specific task's data. Advantages: Less Data Required: As the model has already learned generic language features, less task-specific data might be needed.\nFaster Convergence: The model often requires fewer epochs to achieve competitive performance.\nImproved Performance: Leveraging the knowledge from the pre-trained model can lead to better performance on the target task. Conclusion:\nSelf-supervised learning, especially in the form of pre-trained language models, has revolutionized natural language processing. By using these models and fine-tuning them, researchers and practitioners can achieve state-of-the-art performance across a myriad of tasks with reduced data and computational requirements.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157929,"modifiedTime":1737554783000,"sourceSize":3205,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5. Pre-trained LM.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html":{"title":"5.1 ELMO","icon":"","description":"1. Introduction:\nDefinition: ELMo (Embeddings from Language Models) represents contextualized word embeddings generated from a deep bidirectional language model.\nDevelopers: Developed by researchers at the Allen Institute for Artificial Intelligence in 2018.\n2. Key Features:\nContextualized Embeddings: Unlike traditional word embeddings (e.g., Word2Vec, GloVe) that assign a static embedding to each word, ELMo provides embeddings based on the word's context in a sentence. This means that the word \"bat\" will have different embeddings in \"baseball bat\" and \"night bat\".\nDeep and Bidirectional: ELMo utilizes a bidirectional LSTM (BiLSTM) to consider both the left and the right context in all layers, resulting in a rich representation of words.\n3. How ELMo Works: Pre-training on Language Model Task: A deep bidirectional language model is trained on a large corpus. This captures both semantic and syntactic information. Extraction of Embeddings: For any input sentence, embeddings are extracted from all layers of the BiLSTM. These embeddings are a function of the entire input sentence. Weighted Sum: The embeddings from different layers are combined into a single word representation using a weighted sum. These weights are learned for the specific downstream task. 4. Advantages:\nState-of-the-Art Performance: When introduced, ELMo achieved state-of-the-art results on several NLP benchmarks across tasks like question answering, sentiment analysis, and named entity recognition.\nTransfer Learning: ELMo embeddings, being pretrained on a large corpus, can be effectively transferred to various NLP tasks, reducing the need for extensive task-specific data.\nHandles Polysemy: ELMo can generate different embeddings for a word based on its context, addressing the challenge of polysemous words.\n5. Usage:\nTask-specific Models: ELMo embeddings can be easily incorporated into existing models for a variety of NLP tasks. These embeddings can serve as additional input features to enhance model performance.\nFine-tuning: Although ELMo is primarily used as a feature-based approach, the embeddings can also be fine-tuned for specific tasks, if necessary.\n6. Evolution in NLP Landscape:\nELMo represented a shift in the NLP community towards the use of deeply contextualized word embeddings. After its introduction, similar models like OpenAI's GPT and Google's BERT emerged, further pushing the boundaries of what's possible in NLP.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Embeddings from Language Models","level":3,"id":"Embeddings_from_Language_Models_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157943,"modifiedTime":1737554783000,"sourceSize":2689,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.1 ELMO.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html":{"title":"5.2 GPT Models","icon":"","description":"1. Introduction:\nDefinition: GPT (Generative Pre-trained Transformer) is a series of language models developed by OpenAI. These models leverage the Transformer architecture to achieve state-of-the-art results on numerous NLP tasks.\n2. GPT Models: Dataset Diversity and Size: GPT models are typically trained on vast and diverse text datasets to capture a broad spectrum of language patterns. Versions: GPT: The original model was trained on the BooksCorpus dataset containing 7,000 unpublished books.\nGPT-2: This version was trained on a dataset called WebText, which is derived from outbound links from Reddit posts. The model has 1.5 billion parameters.\nGPT-3: It's even more advanced, with 175 billion parameters, trained on the Common Crawl dataset, making it one of the largest models to date. Tokenization: GPT models often use byte-pair encoding (BPE) to handle a large vocabulary efficiently, ensuring that even rare words or entities can be represented as a combination of tokens. 3. Pre-training vs Fine-tuning: Pre-training: Objective: During this phase, GPT models are trained to predict the next word in a sequence. This unsupervised learning helps the model grasp grammar, facts about the world, reasoning abilities, and even some level of commonsense knowledge.\nLarge Dataset: This step uses vast amounts of general-purpose text data.\nResult: A generic model that is good at text generation but not specialized for any specific task. Fine-tuning: Objective: Once pre-trained, the model can be fine-tuned on a smaller, task-specific dataset. This phase tailors the general abilities of the GPT model to a particular task.\nTask-specific Dataset: For instance, if the desired task is sentiment analysis, the model is fine-tuned on labeled sentiment data.\nFine-tuning Strategies: Depending on the task, different parts of the model can be fine-tuned, or additional layers might be added. The learning rate is typically smaller during this phase to retain the previously learned features.\nResult: A specialized model optimized for the desired task, be it classification, translation, question-answering, or another NLP task. 4. Notable Mentions: Few-shot Learning with GPT-3: Unlike its predecessors, GPT-3 can perform specific tasks without explicit fine-tuning. Given a few examples (hence \"few-shot\"), it can generalize and perform the desired operation, showcasing its vast knowledge and adaptability. Zero-shot and Many-shot Learning: GPT-3 can also adapt to tasks without any examples (zero-shot) or with many examples (many-shot), providing flexibility in its application. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157947,"modifiedTime":1737554783000,"sourceSize":2747,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.2 GPT Models.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html":{"title":"5.3 Others","icon":"","description":"Few-shot Learning: Definition: Few-shot learning refers to training a machine learning model on a very limited set of data. The aim is to make the model generalize well from a minimal number of examples. Importance: In many real-world scenarios, obtaining large labeled datasets is impractical. Few-shot learning addresses this issue by leveraging prior knowledge and transferring it to new tasks. Methods: Techniques such as meta-learning, transfer learning, and utilizing pre-trained embeddings or models are popular approaches in few-shot learning. Scaling Laws for Large Language Models (LLMs): Definition: Scaling laws describe how the performance of large language models (LLMs) changes as one increases model size, data, and computational resources. Observations: Performance Improvements: As model size increases, there's a consistent improvement in model performance up to a point.\nDiminishing Returns: After a certain threshold, performance gains decrease, implying a need for more data or better architectures.\nTraining Time: Larger models require more computational resources and time but can achieve similar performance with fewer epochs. Implications: Understanding scaling laws is crucial for efficiently allocating resources when designing and training LLMs. Model Alignment: Definition: Model alignment refers to ensuring that machine learning models, especially powerful ones, align with human values and intentions. Importance: Misaligned models can produce unwanted or harmful outputs, which can be especially risky when models operate autonomously. Methods: Techniques like rule-based constraints, reinforcement learning from human feedback, and interpretability tools are used to better align models with desired outcomes. Open-source Large Language Models: Definition: These are LLMs released to the public domain, allowing researchers and developers to utilize, modify, and adapt them without restrictions. Examples: Models like GPT-2 (by OpenAI) have been released in an open-source manner. Benefits: Open-sourcing promotes transparency, research collaboration, and accelerates advancements in the field. Risks: It can also lead to misuse in generating misinformation, spam, or other malicious applications. Limitations &amp; Risks of LLMs: Data Biases: LLMs can inherit and amplify biases present in their training data, leading to biased or unfair outputs. Overfitting: On limited data or niche tasks, LLMs can overfit, capturing noise rather than the underlying pattern. Environmental Concerns: Training LLMs requires significant computational resources, leading to concerns about energy consumption and carbon footprint. Economic Impact: As LLMs become more capable, there are concerns about their impact on jobs, especially in areas like content creation. Safety &amp; Misuse: Powerful models can be used to generate fake news, spam, or even engage in cyber-attacks. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157950,"modifiedTime":1737554783000,"sourceSize":3175,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.3 Others.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html":{"title":"Review","icon":"","description":" If the data is: w = np.array([0.1,-0.2,0.4])\nx = np.array([0.2,0.5,-0.3])\nb = 0.6\nz = np.dot(w,x)+b\ny = pow((1+math.exp(-z)),-1)\nprint(\"predicted possibility: \", y)\nWhere:\nN is the total number of documents\nck​ is the set of documents in cluster k\ntj​ is the set of documents of the true class j\n∣ck ​∩ tj​∣ is the number of documents of class j in cluster k\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Regression Model","level":3,"id":"Regression_Model_0"},{"heading":"Pointwise mutual information","level":3,"id":"Pointwise_mutual_information_0"},{"heading":"Purity of the clustering","level":3,"id":"Purity_of_the_clustering_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157954,"modifiedTime":1737554783000,"sourceSize":1025,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/Review.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html":{"title":"1.1 Language Modelling","icon":"","description":"1. Introduction to Language Models:\nDefinition: A language model is a probabilistic framework that can predict the next word in a sentence given the previous words (n-grams), or can assign probabilities to sequences of words.\nUse Cases: They are used in various applications such as speech recognition, machine translation, text generation, and auto-complete features.\nExample:\nP(John Smith’s hotel room bugged) = P(John) P(Smith’s | John) P(hotel | John Smith’s) … P(bugged | John Smith’s hotel room)Maximum likelihood estimation (MLE):\nP(bugged | John Smith’s hotel room) = count(John Smith’s hotel room bugged)/count(John Smith’s hotel room)Markov Assumption:\nSimplification:\nP(bugged| John Smith's hotel room) ≈ P(bugged|room)\nor\nP(bugged| John Smith's hotel room) ≈ P(bugged|hotel room)","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Language Models:</strong>","level":3,"id":"**Language_Models**_0"},{"heading":"<strong>Chain rule of probability:</strong>","level":4,"id":"**Chain_rule_of_probability**_0"},{"heading":"<strong>Estimate the Probabilities:</strong>","level":4,"id":"**Estimate_the_Probabilities**_0"},{"heading":"<strong>Unigram Model-Zero-order Markov assumption:</strong>","level":4,"id":"**Unigram_Model-Zero-order_Markov_assumption**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157962,"modifiedTime":1737554783000,"sourceSize":2072,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.1 Language Modelling.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html":{"title":"1.2 Smoothing","icon":"","description":"Overfitting occurs when a statistical model learns the training data too well, including the noise and details that do not generalize to other data sets. This results in poor performance when the model is used to predict new, unseen data, which is the case with most real-world applications where the test corpus differs from the training corpus.When a language model has never encountered the phrase \"Chinese food\" in the training corpus, it will assign a probability of zero to this bigram (count(Chinese food) = 0). This becomes problematic when trying to calculate the probability of a sentence containing \"Chinese food\" since the product of any number and zero is zero, resulting in the entire sentence being assigned a zero probability, which isn't helpful. Purpose: Smoothing is a technique used to handle the problem of zero probabilities in language models for unseen words or sequences (n-grams) in the training corpus. Techniques: Additive (Laplace) Smoothing: It involves adding a small constant (typically 1) to all n-gram counts to adjust the probabilities, ensuring that no n-gram has a zero probability. Good-Turing Discounting: It estimates the probability of unseen n-grams based on the frequency of n-grams that appear once in the training data. Backoff and Interpolation: These methods dynamically adjust the use of lower-order n-gram statistics when higher-order n-grams have zero occurrences. Kneser-Ney Smoothing: An advanced method that takes into account the frequency of the n-gram's context rather than the n-gram itself, providing a more nuanced approach to probability distribution. Let's illustrate how backoff and interpolation methods would work with a simple example. We will consider a scenario where we're trying to determine the probability of the word \"dog\" following the bigram \"the brown\".Let's assume we have the following counts from our training corpus: (the trigram \"the brown dog\" was never observed) Using just the maximum likelihood estimate, we would get: This is problematic because it assigns a probability of zero to the trigram \"the brown dog,\" implying that it's impossible for \"dog\" to follow \"the brown,\" which is not a realistic representation of language.We can use the bigram \"brown dog\" or even the unigram \"dog\" to estimate this probability instead: So, we can back off to: This non-zero probability is more realistic than assuming it's impossible for \"dog\" to follow \"brown\".We can calculate a weighted combination of the unigram, bigram, and trigram probabilities. Assume that we determined our lambda values (which should sum to 1) from a validation set are as follows: (for unigram) (for bigram) (for trigram, but we know this is zero in our case)\nWe calculate the probabilities as follows: Now, using interpolation, we get: By using interpolation, we've assigned a small but non-zero probability to the trigram \"the brown dog,\" which reflects the reality that while \"dog\" may not have been observed following \"the brown\" in our training data, it is not impossible for it to occur.\nHeld-out data is a part of the dataset that the model never sees during training. It is used to tune the hyperparameters of the model, such as (\\lambda) in n-gram models, so that the model generalizes well to new, unseen data. Expectation Maximization (EM) is an iterative method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. Data-driven: values are larger for n-gram counts that are more frequently observed in the training data.\nN-gram specific: Each n-gram order (unigram, bigram, trigram, etc.) has a different value, allowing the model to weigh different n-gram orders differently based on their predictive power.\nSimplified parameter estimation: Only one parameter (presumably a global smoothing parameter) needs to be estimated, which simplifies the optimization process. Initialization: Begin with an initial guess for the values for each n-gram order.\nExpectation (E-step): Calculate the expected counts of each n-gram in the held-out data using the current values.\nMaximization (M-step): Adjust the (\\lambda) values to maximize the likelihood of the held-out data given the expected counts from the E-step.\nIteration: Repeat the E-step and M-step until the (\\lambda) values converge or the improvement falls below a certain threshold.\nFinalization: Once the values have been optimized, they are used in the language model to predict the likelihood of new sentences.\nProcess: Target of Discounting: Absolute discounting specifically targets sequences that occur infrequently (i.e., low-frequency n-grams). The intuition is that counts from infrequent events are often overestimated in their true probability of occurrence. Reserving Probability Mass: By discounting each observed count by a small constant , probability mass is freed up. This mass can then be redistributed to n-grams that haven't been seen in the training data, ensuring that the model can handle previously unseen sequences. Amount of Discount: Determining how much to discount, represented by, is critical. This value is often estimated by using a held-out dataset separate from the training corpus. The held-out corpus is not used for training the model but for tuning parameters like to ensure the model generalizes well to new data. Typically, is a small positive number such as 0.75, but the exact value that works best can depend on the specific dataset and model. Ensuring a True Probability Distribution: For a model to be useful, the probabilities for all possible next words given a context must sum to 1, forming a true probability distribution. With absolute discounting, after discounting the counts and redistributing probability mass to unseen n-grams, it's essential to ensure that the sum of probabilities over all possible next words still equals 1.\nTo make sure this is the case, a normalization factor is used. The redistributed mass is assigned to unseen n-grams in proportion to some base measure, often the unigram distribution. After redistribution, the probabilities should indeed sum to 1, thus maintaining a true probability distribution. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>2. Smoothing in Language Models:</strong>","level":3,"id":"**2._Smoothing_in_Language_Models**_0"},{"heading":"Overfitting","level":4,"id":"Overfitting_0"},{"heading":"Without Backoff or Interpolation:","level":4,"id":"Without_Backoff_or_Interpolation_0"},{"heading":"With Backoff:","level":4,"id":"With_Backoff_0"},{"heading":"With Interpolation:","level":3,"id":"With_Interpolation_0"},{"heading":"Held-out Data Maximization","level":4,"id":"Held-out_Data_Maximization_0"},{"heading":"Expectation Maximization (EM)","level":4,"id":"Expectation_Maximization_(EM)_0"},{"heading":"Collins et al.'s Approach","level":4,"id":"Collins_et_al.'s_Approach_0"},{"heading":"How it Works:","level":4,"id":"How_it_Works_0"},{"heading":"Absolute Discounting","level":4,"id":"Absolute_Discounting_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157966,"modifiedTime":1737554783000,"sourceSize":14097,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.2 Smoothing.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html":{"title":"1.3 Evaluation of Language Models","icon":"","description":" Intrinsic Evaluation: Perplexity: This is the primary metric used for evaluating language models. It measures how well a probability model predicts a sample. A lower perplexity indicates a better model that can more accurately predict the sample.\nLikelihood: Using the log-likelihood of a held-out dataset, which is unseen data, to evaluate how well the model predicts this data.\nCross-Entropy: It is closely related to perplexity and measures the average number of bits needed to encode the information provided by the model. Extrinsic Evaluation: Task-Based Metrics: Here, the language model is integrated into an end application (e.g., machine translation, speech recognition), and the performance improvement in that task is measured.\nHuman Judgment: Sometimes, human evaluators are used to assess the quality of the text generated by the model based on criteria like fluency, coherence, and relevance. Perplexity is a measurement used to quantify how well a probability model predicts a sample. In the context of language models, perplexity is a way to capture the uncertainty a model has in predicting (or \"understanding\") a sequence of text. Here's a detailed breakdown of the concept:\nPerplexity of a language model on a set of test data is defined as the inverse probability of the test set, normalized by the number of words.\nFor a test set , the perplexity is defined as:Where: is the probability of the test set according to the model. is the total number of words in the test set.\nIf the language model assigns equal probability to all words in the test set, the perplexity equals the size of the vocabulary. A lower perplexity score indicates that the model predicts the test data with higher likelihood. Lower Perplexity: A model with lower perplexity is considered better because it means that the model is less perplexed by the test data. It suggests the model has a higher predictive accuracy for the test set. Prediction Quality: Perplexity can be seen as a measurement of how surprised the model is when it encounters the actual outcome; less surprise (i.e., lower perplexity) implies better performance. Perplexity is most informative when the test and training data have similar statistical properties. If the test data differ significantly from the training data, perplexity may not be a good measure of the model's quality. Pre-processing and Vocabulary: The way text data is cleaned and prepared for the model can impact perplexity. Moreover, the set of words included in the model (the vocabulary) can affect the score. When comparing language models, it's important that they are evaluated using the same vocabulary. NLP Task Performance: A crucial point is that improving perplexity does not necessarily mean that an NLP task (like translation, summarization, etc.) will perform better. It simply means that the language model is better at predicting text data. Let's say we have two language models, Model A and Model B. We test both on the same set of sentences, and Model A has a perplexity of 80, while Model B has a perplexity of 100. We would generally prefer Model A for this particular test set since it has a lower perplexity, indicating that it predicts the words in the test sentences with higher likelihood than Model B. However, this does not mean that Model A will outperform Model B in tasks like speech recognition or machine translation, where other factors come into play.\nEvaluating Over Time: Language models may also be evaluated on their ability to retain performance over time, handling changes in language use (dynamic corpora). Out-of-Domain Generalization: A model that performs well on one dataset may not perform well on another, especially if there's a domain shift.\nThe Sensitivity of Perplexity: Perplexity is sensitive to the size of the test set and the smoothness of the model. It does not always correlate perfectly with human judgment.\nModel Biases: Evaluation metrics do not typically account for biases in the model's output, which may propagate or amplify societal biases.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>3. Evaluation of Language Models:</strong>","level":3,"id":"**3._Evaluation_of_Language_Models**_0"},{"heading":"<strong>4. Limitations and Challenges in Evaluation:</strong>","level":3,"id":"**4._Limitations_and_Challenges_in_Evaluation**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157970,"modifiedTime":1737554783000,"sourceSize":4546,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.3 Evaluation of Language Models.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html":{"title":"2. Syntactic Parsing","icon":"","description":"Syntactic parsing is the computational process of analyzing a string of symbols in natural language (sentences) according to the rules of formal grammar. The aim is to understand the structure of sentences and the relationships between words. This is crucial for numerous Natural Language Processing (NLP) tasks such as machine translation, speech recognition, and information extraction. Definition: A CFG is a type of formal grammar that is composed of a finite set of production rules. It is called \"context-free\" because the rules can be applied regardless of the context of the nonterminals. Components: Terminals: The basic symbols from which strings are formed (usually the words in a language).\nNonterminals: Symbols that denote sets of strings (syntactic categories like NP for noun phrase, VP for verb phrase).\nStart symbol: A special nonterminal which denotes the category of the entire sentence (often S).\nProduction rules: Rules that describe how terminals and nonterminals can be combined. Example Rule: S → NP VP\nNP → Det N | N “context-free-grammer.png” could not be found.\nChomsky Hierarchy: CFGs are part of the Chomsky hierarchy, which categorizes grammatical formalisms according to their expressive power. CFGs are more powerful than regular grammars but less powerful than context-sensitive grammars.\nAn extension of a regular context-free grammar. While a context-free grammar (CFG) provides rules for constructing valid strings in a language, a PCFG assigns probabilities to each of these rules. The main motivation for using PCFGs is to address the ambiguity in natural languages: a single sentence can often have multiple valid parses, and PCFGs provide a mechanism to rank these parses based on their likelihood.A PCFG consists of:\nA set of non-terminals (e.g., NP, VP, S, etc.).\nA set of terminals (words in the language).\nA set of production rules (e.g., S → NP VP).\nProbabilities associated with each production rule.\nThe probabilities of all rules that have the same left-hand side non-terminal should sum to 1.The main advantage of using a PCFG over a regular CFG is its ability to select the most probable parse tree for a given sentence, out of multiple possible trees. This is especially useful in natural language processing tasks like parsing where ambiguity is prevalent.Let's consider a simplified grammar for the English language:S → NP VP [0.9]\nS → VP [0.1]\nNP → \"she\" [0.5]\nNP → \"they\" [0.5]\nVP → \"run\" [0.7]\nVP → \"walk\" [0.3]\nIn the above grammar:\n\"S → NP VP\" has a probability of 0.9.\n\"S → VP\" has a probability of 0.1.\nIf we want to parse the sentence \"she run\", there can be multiple valid parse trees based on the CFG rules. However, using the PCFG, we can assign a probability to each tree and select the most likely one.PCFGs are commonly used in:\nSyntactic parsing of natural language sentences.\nMachine translation, to find the most probable translation given source and target language grammars.\nSpeech recognition, to assign probabilities to different interpretations of a spoken sentence.\nWhile PCFGs can be powerful tools, they also come with challenges:\nEstimating probabilities: Obtaining accurate probabilities for each rule can be challenging and requires a sufficiently large and representative training corpus.\nOverfitting: Like other probabilistic models, PCFGs can overfit to the training data, leading to poor generalization on unseen data.\nComputational complexity: Parsing with PCFGs can be computationally intensive, especially for long sentences.\nWhen learning a PCFG from a treebank, the goal is to estimate the probabilities for each rule in the grammar. The Maximum-Likelihood Estimation (MLE) provides a straightforward method to determine these probabilities.Given a rule ( \\alpha \\rightarrow \\beta ):\nCount(α → β) is the number of times the rule ( \\alpha \\rightarrow \\beta ) appears in the treebank.\nCount(α) is the total number of times the non-terminal ( \\alpha ) appears as the left-hand side of any rule in the treebank.\nThe MLE estimate of the rule's probability, denoted , is calculated as:\nThis provides an estimate of how often, when we see the non-terminal , it produces the specific right-hand side .Let's say we want to estimate the probability of the rule NP → Det Noun using the Penn Treebank: Suppose, after parsing the entire treebank, we find that Count(NP → Det Noun) = 10,000, meaning the rule NP → Det Noun appeared 10,000 times. We also find that Count(NP) = 20,000, meaning the non-terminal NP appeared as the left-hand side of any rule 20,000 times in total. Plugging into our formula: Thus, based on the treebank data, there's a 50% chance that a noun phrase (NP) will expand to a determiner followed by a noun. Purpose: To build a parse tree that represents the syntactic structure of a sentence according to a CFG. Parse Tree: A hierarchical tree where each node represents a grammar rule application.\nLeaves are the terminals (words of the sentence).\nInternal nodes are nonterminals (syntactic categories). Parsing Techniques: Top-Down Parsing: Starts at the start symbol and works down the tree, attempting to match the input sentence.\nBottom-Up Parsing: Starts with the input sentence and works up to the start symbol.\nEarley Parser: A dynamic programming approach that efficiently parses sentences in a CFG.\nChart Parsing: Uses a data structure called a \"chart\" to store intermediate parsing results and avoid redundant parsing steps. Evaluation Metrics: Precision and Recall: Measures of how many of the parse tree's constituents are correct according to a gold standard.\nF-Score: The harmonic mean of precision and recall. Definition: A dependency grammar is a class of grammars where the structure of a sentence is described in terms of a binary asymmetric relation called \"dependency\" between a head (a governor) and its dependents (modifiers). Structure: Head: The central word of a phrase which determines the syntactic type of that phrase.\nDependent: A word that modifies the head or adds some kind of information to it.\nThe head-dependent relations form a directed graph called a dependency tree, which is generally acyclic and connected. Principles: Each word in a sentence except the root has exactly one head.\nDependency links do not cross each other, making the structure a tree. In general, determining the head in a sentence or phrase depends on the syntactic rules of the language and the nature of the relationship between words. Here are some general guidelines for determining heads in English:\nIn verb phrases (VP): The main verb is usually the head. E.g., in \"She has been reading\", \"reading\" is the head.\nIn noun phrases (NP): The main noun is the head. E.g., in \"the big red ball\", \"ball\" is the head.\nIn prepositional phrases (PP): The preposition is usually the head. E.g., in \"on the table\", \"on\" is the head.\nIn adjective phrases (ADJP) and adverb phrases (ADVP): The main adjective or adverb is typically the head. E.g., in \"very quickly\", \"quickly\" is the head.\nIn coordinate structures: The conjunction is not the head; instead, the elements being coordinated are co-heads. E.g., in \"John and Mary\", neither \"John\" nor \"Mary\" is dependent on the other, and \"and\" is not the head. Purpose: To analyze the syntactic structure of a sentence by identifying dependency relations between \"head\" words and words which modify those heads. Dependency Tree: A directed graph with nodes for each word in the sentence.\nEdges indicate dependencies (the arrows point from heads to dependents). Parsing Techniques: Transition-Based Parsing: Constructs a dependency tree using a series of actions (shift, reduce, left-arc, right-arc) guided by a stack and a buffer.\nGraph-Based Parsing: Formulates parsing as a problem of finding the most probable dependency tree based on the entire sentence.\nHybrid Approaches: Combine aspects of both transition-based and graph-based methods. Evaluation Metrics: Unlabeled Attachment Score (UAS): Proportion of words in a sentence that have the correct head.\nLabeled Attachment Score (LAS): Proportion of words with both the correct head and the correct label for the dependency relation. Projectivity:\nIn the context of dependency trees, \"projectivity\" refers to the property where, if a word A governs another word B (i.e., A is the head of B), then there are no words outside of the span between A and B in the sentence which are related (dependent) to any word inside this span. In simpler terms, if you draw the tree on top of the sentence, you don't have to draw any lines crossing over other lines. Well-Formedness A dependency tree is well-formed iff\nSingle head: Each word has only one head Acyclic: The graph should be acyclic i.e. has no cycles Connected: There is a path between any pair of nodes Projective: if an edge from word A to word B implies that there exists a directed path in the graph from A to every word between A and B in the sentence\nSure! Let's dive a bit deeper into these two parsing algorithms:Transition-based parsing, also known as shift-reduce parsing, operates incrementally. At each step, the parser decides between shifting the next word from the buffer onto the stack or reducing words already on the stack, forming a dependency.\nStack: This is used for building the parse tree. Words or tokens are shifted onto the stack and later reduced to form dependencies.\nBuffer: Contains the tokens of the sentence that have yet to be parsed.\nParser: Determines the action (i.e., shift, reduce, or other transitions) to take based on the current configuration of the stack and buffer. Works best for capturing local dependencies because of its incremental nature.\nIs typically faster than graph-based methods as it processes the sentence linearly.\nExamples of this approach include the Nivre's arc-eager algorithm and the methods by Covington and Yamada &amp; Matsumoto.\nInitial State:\nStack: [ROOT]\nBuffer: [She, reads, books]\nStep 1: Shift \"She\" onto the stack.\nStack: [ROOT, She]\nBuffer: [reads, books]\nStep 2: Shift \"reads\" onto the stack.\nStack: [ROOT, She, reads]\nBuffer: [books]\nStep 3: Create a dependency between \"reads\" (head) and \"She\" (dependent).\nStack: [ROOT, reads]\nBuffer: [books]\nStep 4: Shift \"books\" onto the stack.\nStack: [ROOT, reads, books]\nBuffer: []\nStep 5: Create a dependency between \"reads\" (head) and \"books\" (dependent).\nStack: [ROOT, reads]\nBuffer: []\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Context-Free Grammars (CFG)","level":3,"id":"Context-Free_Grammars_(CFG)_0"},{"heading":"Probabilistic Context-Free Grammar (PCFG)","level":4,"id":"Probabilistic_Context-Free_Grammar_(PCFG)_0"},{"heading":"Maximum-Likelihood Estimation for PCFG:","level":2,"id":"Maximum-Likelihood_Estimation_for_PCFG_0"},{"heading":"An Example:","level":2,"id":"An_Example_0"},{"heading":"Constituency Parsing","level":3,"id":"Constituency_Parsing_0"},{"heading":"Dependency Grammars","level":3,"id":"Dependency_Grammars_0"},{"heading":"Dependency Parsing","level":3,"id":"Dependency_Parsing_0"},{"heading":"Transition-based Parsing","level":3,"id":"Transition-based_Parsing_0"},{"heading":"Overview:","level":4,"id":"Overview_0"},{"heading":"Components:","level":4,"id":"Components_0"},{"heading":"Notable Features:","level":4,"id":"Notable_Features_0"},{"heading":"Example","level":4,"id":"Example_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157975,"modifiedTime":1737554783000,"sourceSize":14669,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/2. Syntactic Parsing.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html":{"title":"3. Semantics & Coreference Resolution","icon":"","description":"\nSemantics deals with the study of meaning in language. It aims to understand how words, phrases, and sentences convey meaning, both in isolation and when combined in context.\nModelling semantics is the holy grail of NLP and a central question in Artificial Intelligence Building a robot that can follow natural language instructions to execute tasks\nAnswering questions, such as where is the nearest coffee shop? Translating a sentence from one language into another, while preserving the underlying meaning Fact-checking an article by searching the web for contradictory evidence Logic-checking an argument by identifying contradictions, ambiguity, and unsupported assertions Semantic theories explain how to linguistically represent meaning: Logical semantics Lexical semantics Focuses on representing the meaning of sentences in a formal system, often using symbolic logic.\nConstructs like propositions, truth values (true or false), and logical operators (AND, OR, NOT) are used.\nExamines how the truth values of larger expressions depend on the truth values of their parts. Concerns itself with predicates and their arguments. Predicates: Verbs or adjectives that indicate properties or actions.\nArguments: Entities (often nouns) that participate in the actions or have the properties described by the predicates. For instance, in \"John reads a book\", \"reads\" is the predicate, and \"John\" and \"a book\" are its arguments.\ncheck:\n<a data-href=\"1. Propositional Logic\" href=\"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Propositional Logic</a>\n<br><a data-href=\"2. First Order Logic\" href=\"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. First Order Logic</a> Explores the meaning of words and the relationships between them.\nStudies phenomena like synonymy (words with similar meanings, e.g., \"happy\" and \"joyful\"), antonymy (opposite meanings, e.g., \"happy\" and \"sad\"), and hyponymy (hierarchical relationships, e.g., \"sparrow\" is a hyponym of \"bird\").\nAlso includes the study of word senses and ambiguity.\nHomonymy: coincidentally share an orthographic form. e.g. bank\nThe bank took my deposit. (financial institution) The bank was grassy. (sloping mound) Polysemy: two senses are semantically related. e.g. solution Work out the solution in your head. Heat the solution to 75° Celsius. Homophone: same pronunciation, but different spellings. e.g., wood / would, to / two Homograph: same orthographic form, but different pronunciation (this is a problem in speech synthesis). e.g. bass\nI like to play the bass (a musical instrument – bass guitar)\nFresh bass is tasty (a fish)\nSynonyms are two word lemmas that are identical or nearly identical in meaning.\nExample: couch / sofa\nExample: car / automobile\nAntonyms are two word lemmas that have opposite meanings.\nExample: long / short\nExample: big / little\nExample: rise / fall\nExample: in / out\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3. Semantics &amp; Coreference Resolution","level":1,"id":"3._Semantics_&_Coreference_Resolution_0"},{"heading":"<strong>Logical Semantics:</strong>","level":3,"id":"**Logical_Semantics**_0"},{"heading":"<strong>Predicate-Argument Semantics:</strong>","level":3,"id":"**Predicate-Argument_Semantics**_0"},{"heading":"<strong>Lexical Semantics:</strong>","level":3,"id":"**Lexical_Semantics**_0"}],"links":["artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html#_0","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084157979,"modifiedTime":1737554783000,"sourceSize":11027,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/3. Semantics & Coreference Resolution.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html":{"title":"4. Evaluation In NLP","icon":"","description":"Intrinsic evaluation\nDirectly test a task correctness using a gold standard (also called ground truth), e.g.\nEvaluate a POS-Tagger Calculate the match between predicted and gold standard POS-tags\nExtrinsic evaluation\nTest whether the output is useful for downstream tasks, e.g.\nEvaluate a summarisation technique in an IR setting\nDoes using summaries instead of complete documents help a particul ar retrieval task?\nNLP methods are usually intrinsically evaluated against a gold standard\nAccuracy The number of correct predictions divide by the total number of instance\nNot suitable for class-imbalance datasets Precision, Recall, F-measure check <a data-href=\"Evaluation of Classifiers\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Evaluation of Classifiers</a> Evaluating multi-class classification Macro F-measure: Where are the F1-scores for each of the classes.\nMicro F-measure: Counting the total true positives, false negatives and false positives globally, then calculate the F-measure Threshold free metric: ROC-AUC“ROC-AUC.png” could not be found. AUC: Area Under The Curve ROC: Receiver Operating Characteristics\nFor binary classification, it allows to tradeoff between Precision and Recall FPR=FP/(FP+TN)\nTPR=TP/(TP+FN) AUROC of 0.5 (area under the red dashed line) corresponds to a coin flip, i.e. a random model\nAUROC less than 0.7 is usually sub- optimal performance AUROC of 0.70 to 0.80 is often good performance AUROC greater than 0.8 is often excellent performance AUROC of 1.0 (area under the purple line) corresponds to a perfect classifier output is a text sequence:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"General evaluation techniques","level":3,"id":"General_evaluation_techniques_0"},{"heading":"Classification metrics","level":4,"id":"Classification_metrics_0"},{"heading":"Term overlap metrics","level":3,"id":"Term_overlap_metrics_0"}],"links":[".html"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158007,"modifiedTime":1737554783000,"sourceSize":5505,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/4. Evaluation In NLP.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html":{"title":"5. Multilingual and Low Resource NLP","icon":"","description":"\nThere is a need for the development of Multilingual applications, for example, commercial applications including: forum moderation and product recommendation systems It's possible to build multilingual NLP resources with monolingual or parallel corpora.... → lots of data\nSolve NLP problems which require us to understand multiple languages\nStreamline our NLP tools to work seamlessly across many languages\nSolve NLP problems in languages without a lot of data\nExamples of Multilingual NLP problems:Machine translation\n• Translate a sentence from one language into another.\n• Typically modelled as a sequence-to-sequence problem.\n• Often we use parallel corpora to train the model.Intra-word code switching\nWhere the author switches languages part way through a word.\nFor example, in oetverkocht ‘sold out’, the particle oet ‘out’ is used that is associated with Limburgish whereas verkocht ‘sold’ is associated with Dutch. (Nguyen &amp; Cornips, 2016)\nCan be formulated as a sequence labelling problem at the character level, e.g. Tags with BIO encoding\nMultilingual resourcesUniversal Dependencies: POS-Tags, morphological features, syntactic dependencies, and treebanks across 100+ languages• Facilitates multilingual parser development\n• Cross-lingual learning\n• Parsing research from a language typology perspectiveMultilingual word embeddings\nA word embedding space that is shared across multiple languages. Similar words in different languages are close in the embedding space.\nMore recent approaches work by aligning mono-lingual embeddings, because it is easier to find large amounts of data.\nConstructing multilingual word embeddings:Supervised: using a small bilingual dictionary to learn a mapping from the source to the target space with (iterative) Procrustes alignmentInput: d\n• Word vectors from language A (denoted xi ∈ Rd)\n• Word vectors from language B (denoted yi ∈ R )\n• A small set of words in language A that are translations of words in language B Represented as word pairs, denoted: (xi, yi)i∈{1,...,n}\nOutput:\n• A mapping from vectors in language A to vectors in language B: yi ≈ WxiWe use the word pairs, denoted as for , as supervised examples to optimize a loss function. Our objective is to achieve an approximation:where we aim to minimize the average loss across all examples:In this context, is chosen under the constraint to be an orthogonal mapping, i.e.,","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Multilingual NLP","level":3,"id":"Multilingual_NLP_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158010,"modifiedTime":1737554783000,"sourceSize":5812,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/5. Multilingual and Low Resource NLP.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html":{"title":"0. Backpropagation","icon":"","description":"Backpropagation is a widely used algorithm for training neural networks. It is an efficient method to calculate the gradient of the loss function with respect to the weights of the network.The backpropagation algorithm is based on the chain rule of calculus, which allows the calculation of the derivative of a composite function. In the context of neural networks, the composite function is the network itself, which consists of multiple layers of neurons.The backpropagation algorithm starts by calculating the output of the neural network for a given input. This output is compared to the desired output, and the difference between the two is used to calculate the loss function. The goal of backpropagation is to minimize this loss function by adjusting the weights of the network.To do this, backpropagation computes the gradient of the loss function with respect to the weights of the network. This is done by applying the chain rule of calculus to calculate the partial derivatives of the loss function with respect to the weights in each layer of the network.The backpropagation algorithm is typically applied to a feedforward neural network, which consists of an input layer, one or more hidden layers, and an output layer. Each neuron in the network applies a weighted sum of its inputs, followed by a nonlinear activation function. The output of each neuron is then passed to the next layer of neurons.The backpropagation algorithm proceeds in two phases:\nForward Propagation: In this phase, the input data is fed forward through the network, and the output of each neuron is computed based on its weighted sum and activation function.\nBackward Propagation: In this phase, the error is propagated backwards through the network. The gradient of the loss function is calculated with respect to the output of each neuron, and then recursively with respect to the inputs and weights of each neuron in the network. The gradient is then used to update the weights of the network to minimize the loss function.\nThe backpropagation algorithm is often used in combination with gradient descent to optimize the weights of the network. The gradient descent algorithm uses the gradient calculated by backpropagation to update the weights in the direction that minimizes the loss function.The backpropagation algorithm can be summarized by the following formula:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158026,"modifiedTime":1737554783000,"sourceSize":6338,"sourcePath":"Artificial Intelligence/Deep Learning/0. Basic/0. Backpropagation.md","exportPath":"artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html":{"title":"1. Neural Network","icon":"","description":"Neural networks are a fundamental component of deep learning. They are composed of interconnected nodes, or neurons, that process and transmit information. Neural networks are used for a variety of tasks such as image and speech recognition, natural language processing, and more.A typical artificial neuron consists of:\nInput Weights: Each input is multiplied by a weight, which can be adjusted during training.\nSummation Function: The weighted inputs are summed up.\nActivation Function: The summation is passed through an activation function to introduce non-linearity. Input Layer: Receives raw input data.\nHidden Layers: Comprise neurons that process data. Deeper networks have more hidden layers.\nOutput Layer: Produces the final result, often representing class probabilities in classification tasks. Sigmoid: S-shaped curve, good for binary classification outputs.\nReLU (Rectified Linear Unit): Replaces negative values with zero, commonly used in hidden layers.\nSoftmax: Converts raw scores into class probabilities, commonly used in the output layer for multiclass classification. Mean Squared Error (MSE): Used for regression tasks.\nCross-Entropy Loss: Used for classification tasks, especially with softmax activation in the output layer. Forward Pass: Input data is passed through the network to get predictions.\nLoss Calculation: The difference between predictions and actual targets is calculated using a loss function.\nBackpropagation: Gradients of the loss with respect to weights are computed layer by layer, and weights are updated using optimization algorithms like gradient descent. Gradient Descent: Adjusts weights based on calculated gradients.\nStochastic Gradient Descent (SGD): Optimizes using a small batch of data at a time.\nAdam: Adaptive optimization algorithm combining features of SGD and RMSProp. Overfitting: Model performs well on training data but poorly on unseen data.\nRegularization: Techniques like L1 and L2 regularization prevent overfitting by adding penalty terms to the loss function based on weights. Learning Rate: Determines step size during weight updates.\nNumber of Hidden Layers: Affects model complexity and training time.\nNumber of Neurons in Hidden Layers: Impacts model capacity and complexity.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Structure of a Neuron","level":2,"id":"Structure_of_a_Neuron_0"},{"heading":"Layers in a Neural Network","level":2,"id":"Layers_in_a_Neural_Network_0"},{"heading":"Activation Functions","level":2,"id":"Activation_Functions_0"},{"heading":"Loss Functions","level":2,"id":"Loss_Functions_0"},{"heading":"Training a Neural Network","level":2,"id":"Training_a_Neural_Network_0"},{"heading":"Optimization Algorithms","level":2,"id":"Optimization_Algorithms_0"},{"heading":"Overfitting and Regularization","level":2,"id":"Overfitting_and_Regularization_0"},{"heading":"Hyperparameters","level":2,"id":"Hyperparameters_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158030,"modifiedTime":1737554783000,"sourceSize":2614,"sourcePath":"Artificial Intelligence/Deep Learning/0. Basic/1. Neural Network.md","exportPath":"artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/0.-basic/2.-resnet.html":{"title":"2. ResNet","icon":"","description":"Residual Networks, commonly known as ResNets, represent a groundbreaking advancement in deep neural network architectures. They were introduced by Kaiming He et al. in their influential 2015 paper titled \"Deep Residual Learning for Image Recognition.\" ResNets have proven to be extremely effective in tackling the challenges of training very deep neural networks, which was previously hindered by issues like vanishing gradients. These networks have achieved state-of-the-art performance across various computer vision tasks, including image classification, object detection, and image segmentation.As neural networks get deeper, training becomes more challenging due to the vanishing gradient problem. This occurs when gradients during backpropagation become extremely small, leading to slow convergence or even preventing learning altogether. Traditional networks struggled to effectively propagate gradients through many layers, limiting their depth and complexity.ResNets address the vanishing gradient problem through the introduction of residual blocks. A residual block is designed to learn and apply the residual mapping between input and output. It consists of two main components: Shortcut Connection (Identity Mapping): The original input is directly passed to the output of the block through a shortcut connection. This concept is based on the idea that, at worst, the block can learn to do nothing (i.e., set weights to zero) if it benefits the final network. Main Path: This part of the block contains one or more convolutional layers that transform the input to match the desired output. The convolutional layers learn the residual function that needs to be applied to the input. A residual unit, or residual block, is mathematically represented as:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction to Residual Networks (ResNets)","level":3,"id":"Introduction_to_Residual_Networks_(ResNets)_0"},{"heading":"The Problem: Vanishing Gradient","level":3,"id":"The_Problem_Vanishing_Gradient_0"},{"heading":"Introducing Residual Blocks","level":3,"id":"Introducing_Residual_Blocks_0"},{"heading":"The Residual Unit","level":3,"id":"The_Residual_Unit_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/0.-basic/2.-resnet.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158033,"modifiedTime":1737554783000,"sourceSize":5010,"sourcePath":"Artificial Intelligence/Deep Learning/0. Basic/2. ResNet.md","exportPath":"artificial-intelligence/deep-learning/0.-basic/2.-resnet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html":{"title":"Convolutional neural networks (CNNs)","icon":"","description":"It is a type of neural network that are especially well-suited for image recognition and other problems where the input has a spatial structure. They use specific architectures and connection patterns to learn features at different scales and locations in the input.\nConvolution operation:\nIn a convolutional layer, the convolution operation is used to apply a set of filters to the input image. This operation can be represented mathematically as follows:where is the output of the convolution operation at position , I is the input image, K is the filter/kernel, and * denotes the convolution operation.Similar to the process of sliding a small square horizontally and vertically, performing calculations to create a new square while also implementing zero padding to prevent the square from decreasing in size as shown in the image below:“convolutional_Process.jpg” could not be found.\nNon-linearity:\nRectified Linear Unit (ReLU) activation:ReLU is a popular activation function used in CNNs. It is defined as follows:where x is the input to the activation function.Softmax activation:Softmax is commonly used as the activation function for the output layer in classification problems. It maps the output of the last hidden layer to a probability distribution over the classes. Mathematically, softmax can be defined as follows:where is the probability of the j-th class given the input x, z_j is the j-th element of the output vector z of the last hidden layer, and K is the number of classes.\nPooling operation:\nThe pooling operation is used to downsample the feature maps obtained from the convolutional layer. The most common pooling operation is max pooling, which takes the maximum value in each pooling region. Mathematically, max pooling can be defined as follows:where M(i,j) is the output of the pooling operation at position (i,j), R_{i,j} is the pooling region centered at (i,j), and S is the input feature map.\nFully Connected Layers:\nAfter the feature maps are extracted through convolution, passed through activation functions, and downsampled through pooling, the output is flattened and passed through one or more fully connected layers to generate the final output. These layers are similar to the ones used in traditional neural networks and act as a classifier that uses the learned features to classify or identify the input image. The output of each fully connected layer is calculated as follows:where is the weight matrix, is the input vector, is the bias vector, and is the output vector.The output of the last fully connected layer is passed through a softmax activation function to produce a probability distribution over the classes. Mathematically, softmax can be defined as follows:\nBackpropagation:\nOnce the output is generated, the network's weights and biases are updated using backpropagation to minimize the difference between predicted and actual outputs. This involves calculating the gradient of the loss function with respect to the model's parameters, and updating these parameters in the opposite direction of the gradient. This process is repeated over several iterations until the model converges. The gradients of the loss function with respect to the output of the last layer can be calculated as follows:where is the error for the i-th output neuron, is the predicted output, and is the true output.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158041,"modifiedTime":1737554783000,"sourceSize":12027,"sourcePath":"Artificial Intelligence/Deep Learning/1. Convolutional Neural Networks/Convolutional neural networks (CNNs).md","exportPath":"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html":{"title":"1. Recurrent Neural Networks (RNNs)","icon":"","description":"RNNs are a type of neural network that are particularly well-suited for modeling sequences of data, such as time series data or natural language text. RNNs are designed to handle input sequences of variable length and are able to capture temporal dependencies between elements of a sequence.RNNs have been widely used in many fields, including natural language processing (NLP), image and video analysis, and time series analysis. In NLP, for example, RNNs have been used for tasks such as language modeling, sentiment analysis, and machine translation. In image and video analysis, RNNs have been used for tasks such as object recognition, activity recognition, and video captioning. Finally, in time series analysis, RNNs have been used for tasks such as forecasting, anomaly detection, and event detection.The key feature of an RNN is the recurrent connection between hidden units. The output of each hidden unit is a function of the input at the current time step and the output of the previous hidden unit. This allows the network to maintain a state or memory that is updated over time as new inputs are processed.RNNs can be visualized as a sequence of repeating modules, where each module takes an input and produces an output as well as an updated hidden state. The output of each module can be used to make a prediction or to update the hidden state of the next module in the sequence.“RNNs.png” could not be found.RNNs are trained using backpropagation through time (BPTT), which is an extension of standard backpropagation that takes into account the temporal dependencies in the sequence. During training, the weights of the network are adjusted to minimize a loss function that measures the discrepancy between the predicted output and the true output at each time step.RNNs have been used in a variety of applications, including language modeling, speech recognition, machine translation, and video analysis. One popular type of RNN is the Long Short-Term Memory (LSTM) network, which is designed to address the problem of vanishing gradients in standard RNNs.Despite their strengths, RNNs can be difficult to train and are prone to overfitting, especially when dealing with long sequences. In addition, RNNs can be slow to train and require large amounts of data to achieve good performance.The vanishing gradient problem is a common issue in recurrent neural networks (RNNs) that arises during the training process. It occurs when the gradients of the loss function with respect to the weights become very small, making it difficult to update the weights using gradient descent. This can result in the network learning very slowly or not at all.The vanishing gradient problem is caused by the chain rule of differentiation in backpropagation, which multiplies many small gradients together as it propagates backwards through the network. In RNNs, this problem is exacerbated by the fact that the same weights are reused across many time steps, causing the gradients to compound over time. This can lead to either vanishing or exploding gradients, depending on the values of the weights and the activation functions used in the network.One common solution to the vanishing gradient problem is to use an activation function that helps to preserve the magnitude of the gradients, such as the rectified linear unit (ReLU) or the leaky ReLU. Another approach is to use specialized RNN architectures that are designed to address the problem, such as the gated recurrent unit (GRU) or the long short-term memory (LSTM) network. These architectures include gating mechanisms that allow the network to selectively update its state over time and can help to prevent the gradients from vanishing or exploding.Despite these solutions, the vanishing gradient problem remains a challenging issue in RNNs, especially when dealing with long sequences of data. It is an active area of research in the field of deep learning, and many new techniques and architectures are being developed to address the problem and improve the performance of recurrent neural networks.import torch\nimport torch.nn as nn\nimport torch.optim as optim def train_model(model, dataloader_train, dataloader_val, num_epochs=10): \"\"\" Train a PyTorch model. Args: - model: The neural network model instance. - dataloader_train: DataLoader for the training dataset. - dataloader_val: DataLoader for the validation dataset. - num_epochs: Number of epochs to train. Returns: - A tuple of (trained model, history), where history is a dictionary containing training and validation loss for each epoch. \"\"\" device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) # Define the optimizer and loss function optimizer = optim.Adam(model.parameters(), lr=0.001) criterion = nn.CrossEntropyLoss() history = { 'train_loss': [], 'val_loss': [] } for epoch in range(num_epochs): model.train() # Set model to training mode # Training loop total_train_loss = 0.0 for inputs, targets in dataloader_train: inputs, targets = inputs.to(device), targets.to(device) # Zero the gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) # Compute loss loss = criterion(outputs, targets) # Backward pass and optimization loss.backward() optimizer.step() total_train_loss += loss.item() avg_train_loss = total_train_loss / len(dataloader_train) history['train_loss'].append(avg_train_loss) # Validation loop model.eval() # Set model to evaluation mode total_val_loss = 0.0 with torch.no_grad(): for inputs, targets in dataloader_val: inputs, targets = inputs.to(device), targets.to(device) outputs = model(inputs) loss = criterion(outputs, targets) total_val_loss += loss.item() avg_val_loss = total_val_loss / len(dataloader_val) history['val_loss'].append(avg_val_loss) print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\") return model, history\ndef calc_val_loss(model, Xval, Yval): \"\"\"Calculates the validation loss in average nats per character. Args: model (RNNLM): the RNNLM model. Xval (torch.tensor): The validation data input sequence of size B * S. B is the batch size, S is the sequence length. The sequences always start with the &lt;bos&gt; token id. The rest of sequence is just Yval shifted to the right one position. The sequence is zero padded. Yval (torch.tensor): The expected output sequence for the validation data of size B * S. Does not start with the &lt;bos&gt; token. Is zero padded. Returns: float: validation loss in average nats per character. \"\"\" # use cross entropy loss lossfn = nn.CrossEntropyLoss(ignore_index=0, reduction='sum') # put the model into eval mode because we don't need gradients model.eval() # calculate number of batches, we need to be precise this time batch_size = 32 num_batches = int(Xval.shape[0] / batch_size) if Xval.shape[0] % batch_size != 0: num_batches += 1 # sum up the total loss total_loss = 0 total_chars = 0 for n in range(num_batches): # calculate batch start end idxs s = n * batch_size e = (n+1)*batch_size if e &gt; Xval.shape[0]: e = Xval.shape[0] # compute output of model out,_ = model(Xval[s:e]) # compute loss and store loss = lossfn(out.permute(0, 2, 1), Yval[s:e]).detach().cpu().numpy() total_loss += loss char_count = torch.count_nonzero(Yval[s:e].flatten()) total_chars += char_count.detach().cpu().numpy() # compute average loss per character total_loss /= total_chars # set the model back to training mode in case we need gradients later model.train() return total_loss ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158050,"modifiedTime":1737554783000,"sourceSize":8660,"sourcePath":"Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/1. Recurrent Neural Networks (RNNs).md","exportPath":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html":{"title":"2. Gated Recurrent Unit (GRU)","icon":"","description":"Introduction:\nGated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) architecture introduced by Kyunghyun Cho et al. in 2014. They were developed to combat the vanishing gradient problem of traditional RNNs, making them more efficient at capturing long-term dependencies in sequences.“GRU.png” could not be found.Structure &amp; Mechanism: Gates:\nGRUs have two gates: Update Gate (z): Determines how much of the previous hidden state to keep and how much of the new candidate state to consider. It acts like a mixture of the forget and input gates of the LSTM.\nReset Gate (r): Decides how much of the past hidden state to forget, influencing the candidate hidden state. Hidden State:\nUnlike LSTMs, which have a separate cell state and hidden state, GRUs only have a hidden state. Computations:\nUsing matrix notation for weight matrices and bias vectors: Here: is the hidden state from the previous time step. is the current input. is the sigmoid function. represents element-wise multiplication. Advantages: Simpler Architecture: With only two gates, GRUs have fewer parameters than LSTMs, making them computationally more efficient and quicker to train in some cases. Memory Management: GRUs can capture long-term dependencies in data, similar to LSTMs, albeit using a different mechanism. Flexibility: The gating mechanism allows GRUs to ignore parts of the hidden state when deemed unnecessary, providing adaptive memory tracking. Limitations: No Cell State: GRUs lack a separate cell state that LSTMs have. While this makes them simpler, some argue that LSTMs have a slight edge in capturing very long-term dependencies because of the cell state's explicit design. Performance: In practice, the choice between GRUs and LSTMs often comes down to the specific dataset and problem. No one-size-fits-all answer dictates which is universally better. Applications: Sequence-to-Sequence Models: Common in tasks like machine translation. Time Series Analysis: Predicting stock prices, weather patterns, etc. Speech Recognition: Translating audio signals into text. Natural Language Processing: Sentiment analysis, named entity recognition, and more. Gated Recurrent Units (GRUs) represent a significant advancement in the RNN landscape, providing an efficient mechanism to capture sequential dependencies. While LSTMs remain a dominant force in many applications, GRUs provide an alternative that, in the right contexts, can offer competitive or even superior performance.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158054,"modifiedTime":1737554783000,"sourceSize":2964,"sourcePath":"Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/2. Gated Recurrent Unit (GRU).md","exportPath":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html":{"title":"3. Long Short-Term Memory","icon":"","description":"LSTMs are a type of recurrent neural network (RNN) <a data-href=\"1. Recurrent Neural Networks (RNNs)\" href=\"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Recurrent Neural Networks (RNNs)</a> that are designed to address the problem of vanishing gradients in standard RNNs. The key innovation of LSTMs is the addition of a \"memory cell\" that allows the network to selectively remember or forget information over time.The basic LSTM cell has three \"gates\" that control the flow of information: the input gate, the forget gate, and the output gate. These gates are controlled by sigmoid activation functions and determine how much information is passed through the cell at each time step.The input gate determines which information from the input should be stored in the cell, while the forget gate determines which information from the previous state should be forgotten. The output gate then determines which information from the current state should be passed on to the output.In addition to the three gates, LSTMs also have a \"cell state\" that allows the network to store and update information over time. The cell state is modified by the input and forget gates, which selectively add or remove information, and by the output gate, which determines which information is passed on to the output.“LSTM.png” could not be found.Given:\n: Input at time step : Hidden state from the previous time step\n: Memory cell state from the previous time step\n: Sigmoid activation function Hyperbolic tangent activation function\nWeights and biases associated with each gate\nThe LSTM computations are: Forget Gate: The forget gate determines how much of the past cell state information will be retained. Input Gate: The input gate determines how much of the new information will be stored in the cell state. Cell State Update: The cell state gets updated by forgetting certain information (using the forget gate's output) and then adding new information (using the input gate's output). Output Gate: The output gate determines the next hidden state, which is also the output for this time step. In these formulas: represents weight matrices (e.g., is the weight matrix for the forget gate). represents bias vectors (e.g., is the bias vector for the forget gate). denotes element-wise multiplication. indicates the concatenation of and .\nLike other neural networks, LSTMs are trained using backpropagation and gradient descent. During training, the weights of the network are adjusted to minimize a loss function that measures the discrepancy between the predicted output and the true output.LSTMs have been used in a variety of applications, including language modeling, speech recognition, machine translation, and video analysis. They are particularly well-suited for tasks that involve modeling long-term dependencies, such as predicting the next word in a sentence or generating captions for a video.Despite their strengths, LSTMs can be difficult to train and require large amounts of data to achieve good performance. They can also be prone to overfitting, especially when dealing with noisy or sparse data. In addition, LSTMs can be computationally expensive, especially when dealing with long sequences.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"}],"links":["artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158058,"modifiedTime":1737554783000,"sourceSize":3730,"sourcePath":"Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/3. Long Short-Term Memory.md","exportPath":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html":{"title":"Generative Adversarial Networks (GANs)","icon":"","description":"Generative Adversarial Networks (GANs) are a type of neural network that can learn to generate new data that is similar to a training dataset. GANs consist of two neural networks: a generator and a discriminator. The generator learns to create new data that looks like it came from the training dataset, while the discriminator learns to distinguish between real data from the training dataset and fake data generated by the generator.The generator network takes random noise as input and produces a new data sample that is meant to resemble the training data. The discriminator network takes in either a real sample from the training data or a fake sample from the generator and tries to determine whether it is real or fake.During training, the generator is given feedback from the discriminator on how well it is doing at creating realistic samples. The generator tries to improve its output to fool the discriminator into thinking that its output is real, while the discriminator tries to improve its ability to distinguish between real and fake data.GANs are trained using a minimax game between the generator and discriminator. The generator tries to minimize the probability that the discriminator can correctly identify its output as fake, while the discriminator tries to maximize this probability. This results in a game of cat-and-mouse, where the generator learns to create better and more realistic samples, while the discriminator learns to become better at identifying fake samples.GANs have been used in a variety of applications, including image generation, video generation, and text generation. One popular use of GANs is to create realistic images of faces, which can be used in a variety of contexts, such as video games or virtual reality.GANs can be difficult to train and can suffer from stability issues, such as mode collapse, where the generator learns to produce only a few types of output. In addition, GANs can be computationally expensive to train and require large amounts of data to achieve good performance. Finally, GANs can be prone to generating samples that contain biases or other undesirable features, which can be difficult to control. The discriminator network is trained to maximize the probability of assigning the correct label (real or fake) to each sample: where x is a real sample, z is a random noise vector, G(z) is the fake sample generated by the generator network, and D(x) and D(G(z)) are the outputs of the discriminator network for the real and fake samples, respectively. The generator network is trained to minimize the probability of the discriminator network correctly assigning the fake label to the generated samples: where G(z) is the fake sample generated by the generator network, and D(G(z)) is the output of the discriminator network for the fake sample. The overall objective of the GAN is to find a Nash equilibrium between the generator and discriminator networks, where the generator produces samples that are indistinguishable from real samples, and the discriminator cannot accurately distinguish between real and fake samples.where:\nG(z) is the output of the generator network given input z.\nD(x) is the output of the discriminator network given input x. is the true data distribution. is the noise distribution that the generator samples from. denotes the expected value.\nThe goal of the generator is to minimize this cost function by generating samples that fool the discriminator into thinking they are real, while the goal of the discriminator is to maximize this cost function by correctly distinguishing between real and generated samples.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"},{"heading":"Process","level":3,"id":"Process_0"},{"heading":"Cost function:","level":3,"id":"Cost_function_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158066,"modifiedTime":1737554783000,"sourceSize":4252,"sourcePath":"Artificial Intelligence/Deep Learning/3. Generative Adversarial Networks/Generative Adversarial Networks (GANs).md","exportPath":"artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html":{"title":"0. Transfer Learning","icon":"","description":"Transfer learning is a technique in machine learning where a pre-trained model is used as a starting point for training a new model on a different but related task. Transfer learning can significantly reduce the amount of data and computation required to train a new model, and can improve the performance of the new model by leveraging knowledge learned from the pre-trained model.Transfer learning can be achieved using a variety of architectures, including convolutional neural networks (CNNs) for image classification tasks and recurrent neural networks (RNNs) for natural language processing tasks. In transfer learning, the pre-trained model is often referred to as the \"base model\" and the new model as the \"head model\". The base model can be frozen or fine-tuned during training, depending on the specific task.Transfer learning involves two stages of training. In the first stage, the pre-trained model is trained on a large dataset of labeled examples. In the second stage, the head model is trained on a smaller dataset of labeled examples for the new task, with the weights of the base model either frozen or fine-tuned. Fine-tuning involves adjusting the weights of the base model to better fit the new task, while freezing the weights involves keeping the weights fixed and only training the head model.Transfer learning has been successfully applied in a variety of domains, including computer vision, natural language processing, and speech recognition. Transfer learning can be used to improve the performance of a model on a specific task, such as image classification or sentiment analysis, by leveraging the knowledge learned from a pre-trained model.The success of transfer learning depends on the similarity of the pre-trained and new tasks. If the tasks are too dissimilar, the pre-trained model may not provide much benefit. In addition, transfer learning can also suffer from the problem of catastrophic forgetting, where the pre-trained knowledge may be lost when fine-tuning the base model for a new task.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Application","level":3,"id":"Application_0"},{"heading":"Limitation","level":3,"id":"Limitation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158106,"modifiedTime":1737554783000,"sourceSize":2330,"sourcePath":"Artificial Intelligence/Deep Learning/4. Transfer Learning/0. Transfer Learning.md","exportPath":"artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html":{"title":"1. Low-Rank Adaptation","icon":"","description":"Low-Rank Adaptation (LoRA) is a technique for fine-tuning large-scale pre-trained models efficiently by adapting their weights in a low-rank subspace. This method significantly reduces the computational cost and memory requirements during the fine-tuning process while maintaining or even improving performance.At the heart of LoRA is the concept of matrix decomposition, specifically the decomposition of weight matrices into low-rank matrices. Weight Matrix (W): In a neural network, each layer has a weight matrix , where and are the dimensions of the input and output features, respectively. Low-Rank Approximation: The weight matrix (W) can be approximated as the product of two lower-dimensional matrices:\nwhere and , with . The parameter is the rank of the approximation and determines the dimensionality of the adaptation space. Adapted Weights: During fine-tuning, the adapted weight matrix is computed as:\nwhere . The rank is a crucial hyperparameter in LoRA. It determines the trade-off between the number of parameters and the expressive power of the adaptation. A lower rank reduces the number of parameters but may limit the model's ability to capture complex adaptations. Pre-trained Model: Begin with a large pre-trained model, such as BERT or GPT. LoRA Layers: Insert LoRA layers in parallel with the original layers. These layers consist of the low-rank matrices and . During the forward pass, the adapted weights are computed as follows:\nwhere is the original weight matrix from the pre-trained model. Gradient Computation: Compute gradients with respect to the low-rank matrices and . Parameter Update: Update only the parameters of and using gradient descent or a similar optimization algorithm. Fine-tuning Learning Rate: A smaller learning rate is typically used for the low-rank matrices compared to full fine-tuning of the model.\nInitialization: The low-rank matrices and are usually initialized with small random values. Weight Decay: Applying weight decay regularization helps prevent overfitting by penalizing large weights in the low-rank matrices. Efficiency: LoRA significantly reduces the number of parameters to be fine-tuned, leading to lower computational and memory requirements. Scalability: Allows fine-tuning of very large models on modest hardware, making it accessible to a wider range of researchers and practitioners. Performance: Often achieves comparable or even better performance than traditional fine-tuning, particularly in domain adaptation and transfer learning tasks. Consider a weight matrix (W) of dimensions (1000 \\times 1000): Full Fine-Tuning: Total parameters: (1000 \\times 1000 = 1,000,000) LoRA with Rank (r = 10): Parameters for (A): (1000 \\times 10 = 10,000)\nParameters for (B): (10 \\times 1000 = 10,000)\nTotal parameters: (10,000 + 10,000 = 20,000) This example shows a reduction in the number of parameters from 1,000,000 to 20,000, a 50-fold decrease.Assume we have a weight matrix of size , and we want to use LoRA with rank . Original Weight Matrix (W): Low-Rank Matrices (A) and (B): Let and : Adapted Weight Matrix : Compute the product of (A) and (B): Adapted Weight Matrix : Add to the original weight matrix : Domain Adaptation: Adapting large language models to specific domains such as medical or legal texts.\nTask Adaptation: Fine-tuning models for specific tasks like sentiment analysis, named entity recognition, or machine translation. Image Classification: Adapting pre-trained models to new image classification tasks with limited data.\nObject Detection: Enhancing object detection models to perform better on specific datasets or environments. Rank Selection: Choosing the appropriate rank (r) can be challenging and may require empirical tuning. Overfitting: Despite regularization, there is a risk of overfitting, especially when the rank is too high or the dataset is small. Integration: Integrating LoRA into existing model architectures may require significant modifications and expertise. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Mathematical Foundation","level":3,"id":"Mathematical_Foundation_0"},{"heading":"Matrix Decomposition","level":4,"id":"Matrix_Decomposition_0"},{"heading":"Rank Selection","level":4,"id":"Rank_Selection_0"},{"heading":"Implementation","level":3,"id":"Implementation_0"},{"heading":"Insertion of LoRA Layers","level":4,"id":"Insertion_of_LoRA_Layers_0"},{"heading":"Forward Pass","level":4,"id":"Forward_Pass_0"},{"heading":"Backward Pass","level":4,"id":"Backward_Pass_0"},{"heading":"Optimization","level":3,"id":"Optimization_0"},{"heading":"Learning Rate","level":4,"id":"Learning_Rate_0"},{"heading":"Regularization","level":4,"id":"Regularization_0"},{"heading":"Advantages","level":3,"id":"Advantages_0"},{"heading":"Example Calculation","level":3,"id":"Example_Calculation_0"},{"heading":"Detailed Example","level":3,"id":"Detailed_Example_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Natural Language Processing (NLP)","level":4,"id":"Natural_Language_Processing_(NLP)_0"},{"heading":"Computer Vision","level":4,"id":"Computer_Vision_0"},{"heading":"Challenges and Limitations","level":3,"id":"Challenges_and_Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158110,"modifiedTime":1737554783000,"sourceSize":6938,"sourcePath":"Artificial Intelligence/Deep Learning/4. Transfer Learning/1. Low-Rank Adaptation.md","exportPath":"artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html":{"title":"0. Self-Attention","icon":"","description":"Self-attention is a crucial mechanism in deep learning, primarily used in neural network architectures like Transformers. It enables models to weigh the importance of different parts of the input data, allowing them to focus on relevant information while ignoring irrelevant parts. Self-attention is at the core of various natural language processing (NLP) and computer vision models, as it facilitates capturing complex dependencies within sequences or images.At its core, self-attention is a mechanism for computing a weighted sum of values based on their relevance to a query. It is used to analyze relationships between elements in a sequence, such as words in a sentence or pixels in an image. The key components of self-attention are queries, keys, values, and attention scores:\nQueries (Q): These are representations of the elements in the sequence (e.g., words or pixels) that you want to analyze. Queries are used to determine what parts of the input are relevant. Keys (K): Keys are also representations of the same elements and are used to determine the relevance of each element to the queries. Values (V): Values are the information associated with each element in the sequence. The values are combined according to the attention scores to produce the output. Attention Scores: The attention scores quantify how much one element (query) should focus on other elements (keys). These scores are computed by measuring the similarity between queries and keys.\nThe attention mechanism typically involves three main steps:\nScoring: Compute the attention scores by comparing each query with all keys. Common scoring methods include dot-product, scaled dot-product, and cosine similarity. The higher the score, the more relevant the key is to the query.\nSoftmax: Apply the softmax function to the attention scores to convert them into a probability distribution. This ensures that the weights sum to 1, making them interpretable as probabilities.\nWeighted Sum: Use the softmax-weighted attention scores to compute a weighted sum of the values. This weighted sum is the output of the attention mechanism and represents the aggregated information from the input sequence.\nThere are several variants and enhancements of self-attention:\nMulti-Head Attention: In multi-head attention, the self-attention mechanism is applied multiple times in parallel, each with different sets of learned queries, keys, and values. This allows the model to attend to different aspects of the input data simultaneously. Positional Encoding: Self-attention doesn't inherently capture the order of elements in a sequence. To address this, positional encodings are added to the input embeddings to provide information about the positions of elements. Scaled Dot-Product Attention: In this variant, the dot-products between queries and keys are scaled by a factor to prevent the gradients from becoming too large during training. Scaling helps stabilize the optimization process. Relative Positional Encoding: This extension to self-attention considers the relative positions of elements in the sequence, allowing the model to capture more fine-grained dependencies.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Understanding Self-Attention","level":2,"id":"Understanding_Self-Attention_0"},{"heading":"The Attention Mechanism","level":2,"id":"The_Attention_Mechanism_0"},{"heading":"Variants of Self-Attention","level":2,"id":"Variants_of_Self-Attention_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158118,"modifiedTime":1737554783000,"sourceSize":3403,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/0. Self-Attention.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html":{"title":"1. Encoder","icon":"","description":"The Encoder in a Transformer model is designed to process the input sequence and output a sequence of continuous representations. It's composed of a stack of identical layers, each with two sub-layers: a multi-head self-attention mechanism, and a position-wise fully connected feed-forward network.“attention_research_1.webp” could not be found. Components of the Encoder:\nA. Multi-head Self-Attention Mechanism: The purpose of this component is to allow each word (or more generally, each token) in the input sequence to interact with every other token. This is accomplished by creating multiple 'heads', each of which performs an attention operation on the input sequence. The results from each head are then concatenated and linearly transformed to result in the final output of the self-attention mechanism.\nB. Position-wise Fully Connected Feed-Forward Network: This is essentially a two-layer neural network that's applied to each position separately and identically. It's used to transform the output of the self-attention mechanism. It contains two linear transformations, with a ReLU activation function in between.\nC. Add &amp; Norm (Residual Connection and Layer Normalization): After each of the above two layers, the transformer adds a residual connection and then applies layer normalization. This is crucial for the successful training of deep networks, as it helps to stabilize the network and reduce training times. Working Together of the Components:\nIn the multi-head self-attention mechanism, each token in the input sequence is mapped to a query, a key, and a value vector by applying a linear transformation to the input vector. The attention scores (how much focus to put on other parts of the input sentence) are then calculated by taking the dot product of the query vector with the key vector of all tokens in the sentence, followed by a softmax operation.\nThe output of the attention mechanism for each token is then obtained by taking a weighted sum of all value vectors, where the weights are the attention scores. This is done separately for each attention head, and the results are concatenated and linearly transformed to get the final output of the self-attention layer.\nThe output of the self-attention layer is then fed into the position-wise feed-forward network. This is a simple neural network that's applied independently to each position. It doesn't have any recurrent or convolutional connections and is composed of two linear layers with a ReLU activation in between.\nFinally, the Add &amp; Norm (residual connections and layer normalization) help to stabilize the outputs and aid in training the network. Full Process of the Encoder Running:\nThe process begins with the tokenized input sequence being converted into vectors, usually through an embedding layer. Each token in the input sequence is then processed by the multi-head self-attention mechanism, which computes an output vector for each token that's a weighted sum of all tokens' value vectors in the sequence.\nThe weights in this sum (the attention scores) are computed using the query and key vectors of the tokens, and they determine how much each token in the sequence should contribute to the output of each other token.\nThe output of the self-attention mechanism for each token is then passed through a position-wise feed-forward network. This network applies a simple transformation to each position independently and identically.\nAfter both the self-attention and position-wise feed-forward operations, there's a residual connection and layer normalization to aid in training the network and stabilize the output.\nThe final output of the encoder is a sequence of vectors that can be used as input to the decoder. These vectors are a representation of the input sequence that takes into account both the individual meanings of the tokens and their context within the sequence.\nNote that each layer in the encoder processes the input independently, resulting in multiple layers of encoding. The outputs from one layer are used as inputs to the next, until the final layer's outputs are produced. This is why the encoder is said to be composed of a 'stack' of layers. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158123,"modifiedTime":1737554783000,"sourceSize":4363,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/1. Encoder.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html":{"title":"2. Decorder","icon":"","description":"The Decoder in the Transformer model is responsible for generating the output sequence based on the continuous representations provided by the Encoder. Similar to the Encoder, the Decoder is also composed of a stack of identical layers, but with three sub-layers: a masked multi-head self-attention mechanism, a multi-head attention mechanism over the encoder's output, and a position-wise fully connected feed-forward network. Components of the Decoder:\nA. Masked Multi-head Self-Attention Mechanism: The first sub-layer in each decoder layer is a self-attention mechanism that works almost identically to the one in the encoder. However, it has an additional feature: masking. The purpose of this masking is to prevent each token in the output sequence from attending to subsequent tokens in the output sequence, ensuring that the prediction for a particular step does not depend on any future steps.\nB. Multi-head Attention over Encoder's Output: The second sub-layer is a multi-head attention mechanism where the queries come from the previous decoder layer, and the keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence, incorporating information from the input sequence into the output sequence.\nC. Position-wise Fully Connected Feed-Forward Network: Similar to the encoder, the decoder also contains a position-wise fully connected feed-forward network, which is applied to each position separately and identically.\nD. Add &amp; Norm (Residual Connection and Layer Normalization): The Transformer also adds a residual connection and applies layer normalization after each of the three sub-layers in the decoder, just as in the encoder. Working Together of the Components:\nThe masked multi-head self-attention mechanism works similarly to the self-attention mechanism in the encoder, but with the added masking to prevent future information flow. This mechanism allows each token in the output sequence to interact with all other tokens in the output sequence up to and including that position.\nIn the multi-head attention over the encoder's output, the attention scores are calculated using the query vectors from the output of the previous decoder layer and the key vectors from the output of the encoder. The output is then computed as a weighted sum of the value vectors from the encoder's output, where the weights are the attention scores.\nThe position-wise feed-forward network works exactly the same as in the encoder. It applies a simple transformation to the output of the multi-head attention layer.\nThe Add &amp; Norm steps also function similarly to those in the encoder, aiding in training and stabilizing the output. Full Process of the Decoder Running:\nThe decoder processes the input sequence one token at a time, producing an output token at each step. For each token, the decoder first applies the masked multi-head self-attention mechanism, allowing the token to interact with all previous tokens in the output sequence.\nThe output of the self-attention mechanism is then passed to the multi-head attention layer that attends over the encoder's output. This allows the decoder to incorporate information from the input sequence into its output.\nThe output of this attention layer is then passed through a position-wise feed-forward network, which applies a simple transformation to the output.\nFinally, after each of the three sub-layers (self-attention, attention over the encoder's output, and position-wise feed-forward), the Transformer adds a residual connection and applies layer normalization.\nThe final output of the decoder is a sequence of tokens that are predicted based on the input sequence and the previously generated tokens in the output sequence.\nEach layer in the decoder processes the input independently, with the output from one layer serving as input to the next. This means that the decoder is composed of a 'stack' of layers, with the final output being produced by the last layer. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158127,"modifiedTime":1737554783000,"sourceSize":4189,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/2. Decorder.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html":{"title":"3. BERT","icon":"","description":"BERT, short for Bidirectional Encoder Representations from Transformers, is a groundbreaking natural language processing (NLP) model introduced by Google AI researchers in a 2018 paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" BERT has had a profound impact on the field of NLP, setting new state-of-the-art results on a wide range of language understanding tasks.The key motivation behind BERT was to overcome the limitations of previous NLP models, particularly those that used unidirectional (left-to-right or right-to-left) context in understanding language. BERT introduced bidirectional context by training on a large corpus of text, enabling it to understand the meaning of words in the context of both their preceding and following words.BERT follows a two-step process: pre-training and fine-tuning. Pre-training: In the pre-training phase, BERT is trained on a massive corpus of text data, such as the entire English Wikipedia. During this phase, BERT learns to predict missing words (masked language model) and to understand the relationships between words in a sentence (next sentence prediction). This pre-training results in a language model with a deep understanding of language. Fine-tuning: After pre-training, BERT can be fine-tuned on specific NLP tasks, such as text classification, named entity recognition, question-answering, and more. Fine-tuning involves training the model on a smaller dataset related to the specific task, adapting the pre-trained knowledge to perform well on that task. BERT employs the transformer architecture, which is a neural network architecture known for its effectiveness in sequence-to-sequence tasks. The transformer architecture consists of: Multi-Head Self-Attention Mechanism: This mechanism allows the model to weigh the importance of different words in the input sentence when making predictions. It captures contextual information effectively. Transformer Encoder Layers: BERT consists of multiple transformer encoder layers stacked on top of each other. Each layer refines the representation of the input text. Embedding Layers: BERT uses word embeddings, which map words to high-dimensional vectors. It also uses segment embeddings to distinguish between different sentences in the input and position embeddings to encode word positions. Masked Language Modeling: BERT predicts missing words in a sentence by masking some of them. This bidirectional context prediction helps BERT capture deeper semantic understanding. Next Sentence Prediction: BERT is trained to predict whether two sentences are consecutive in the original text. This enables it to understand discourse and relationships between sentences. Bidirectional Context: Unlike previous models that were limited to left-to-right or right-to-left context, BERT considers both directions, allowing it to capture context effectively. Transfer Learning: BERT's pre-training and fine-tuning approach enables it to transfer knowledge from pre-training tasks to various downstream NLP tasks, reducing the need for extensive task-specific data. BERT has had a transformative impact on NLP and has been widely adopted for various applications: Text Classification: BERT-based models achieve state-of-the-art results in sentiment analysis, topic classification, and spam detection. Named Entity Recognition: BERT helps identify entities (e.g., names of people, places) in text with high accuracy. Question Answering: BERT-based models perform exceptionally well in tasks like question answering, where the model must provide answers based on a given passage. Machine Translation: BERT's contextual understanding improves machine translation systems. Since the release of BERT, several variants and improvements have emerged, including: RoBERTa: An optimized variant of BERT that uses larger batch sizes and more training data. ALBERT: A \"lite\" version of BERT that reduces model size while maintaining performance. DistilBERT: A smaller, distilled version of BERT that retains much of its performance but is more computationally efficient. While BERT has achieved remarkable results, challenges remain in NLP research. Some areas of future development and research include: Efficiency: Making large models like BERT more efficient for deployment on resource-constrained devices. Multimodal Understanding: Extending BERT-like models to understand both text and other modalities like images and audio. Cross-Lingual Understanding: Improving the ability of models like BERT to understand multiple languages. Common Sense Reasoning: Enhancing models' ability to perform common-sense reasoning and handle ambiguous language. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Motivation for BERT","level":3,"id":"Motivation_for_BERT_0"},{"heading":"Pre-training and Fine-tuning","level":3,"id":"Pre-training_and_Fine-tuning_0"},{"heading":"Architecture of BERT","level":3,"id":"Architecture_of_BERT_0"},{"heading":"Key Innovations in BERT","level":3,"id":"Key_Innovations_in_BERT_0"},{"heading":"Applications and Impact","level":3,"id":"Applications_and_Impact_0"},{"heading":"Variants of BERT","level":3,"id":"Variants_of_BERT_0"},{"heading":"Challenges and Future Directions","level":3,"id":"Challenges_and_Future_Directions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158130,"modifiedTime":1737554783000,"sourceSize":5259,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/3. BERT.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html":{"title":"3.1 DeBERTa","icon":"","description":"DeBERTa, developed by Microsoft, is a transformer-based language model designed to improve upon BERT's architecture and performance in natural language processing (NLP) tasks. Below are the key concepts and features of DeBERTa, along with a detailed explanation of its components and advantages.One of the core innovations in DeBERTa is the disentangled attention mechanism. Unlike BERT, which uses a single vector to represent both content and position, DeBERTa separates these into two vectors:\nContent Vector: Represents the semantic content of a token.\nPosition Vector: Represents the position information of a token within the sequence.\nBy disentangling these two types of information, DeBERTa can more effectively model the relationship between tokens and their positions, leading to better context understanding.DeBERTa introduces an enhanced mask decoder, improving the model's ability to predict masked tokens during pre-training. This decoder uses both content and position information to make more accurate predictions, enhancing the model's overall language representation capabilities.\nMasking Strategy: DeBERTa uses a dynamic masking strategy where different tokens are masked during each epoch, ensuring the model sees varied contexts and improving its generalization.\nDeBERTa incorporates a relative position bias in its attention mechanism, which allows the model to learn the relative positions of tokens dynamically. This bias is learned during the training process and helps the model better understand the order of words, crucial for tasks like syntactic parsing and coreference resolution.\nAttention Score Calculation: The attention scores in DeBERTa are computed using both the content and relative position biases, providing a richer and more nuanced representation of token relationships.\nDeBERTa benefits from extensive pre-training on large-scale corpora using a combination of tasks:\nMasked Language Modeling (MLM): Similar to BERT, DeBERTa predicts masked tokens in a sentence.\nNext Sentence Prediction (NSP): Helps the model understand sentence-level relationships and coherence.\nThis large-scale pre-training enables DeBERTa to learn robust language representations, making it effective across a wide range of NLP tasks.DeBERTa has demonstrated superior performance on several benchmark NLP tasks compared to BERT and other transformer models. Key benchmarks where DeBERTa excels include:\nGLUE (General Language Understanding Evaluation): A suite of tasks measuring language understanding and reasoning.\nSQuAD (Stanford Question Answering Dataset): A reading comprehension dataset where the model answers questions based on a given passage.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Decoding-enhanced BERT with Disentangled Attention","level":3,"id":"Decoding-enhanced_BERT_with_Disentangled_Attention_0"},{"heading":"Disentangled Attention Mechanism","level":3,"id":"Disentangled_Attention_Mechanism_0"},{"heading":"Enhanced Mask Decoder","level":3,"id":"Enhanced_Mask_Decoder_0"},{"heading":"Relative Position Bias","level":3,"id":"Relative_Position_Bias_0"},{"heading":"Large-scale Pre-training","level":3,"id":"Large-scale_Pre-training_0"},{"heading":"Performance and Benchmarks","level":3,"id":"Performance_and_Benchmarks_0"},{"heading":"Architectural Differences from BERT","level":3,"id":"Architectural_Differences_from_BERT_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158135,"modifiedTime":1737554783000,"sourceSize":3463,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/3.1 DeBERTa.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html":{"title":"4. XLNet","icon":"","description":"1. Introduction:\nDefinition: XLNet stands for \"Extra Long Transformer.\" It is a generalized autoregressive pre-training model for natural language understanding tasks. Developers: Developed by researchers at Google/Brain, Carnegie Mellon University, and Stanford University in 2019.\n2. Background &amp; Motivation:\nAddressing BERT's Limitations: While BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by using a masked language model, it has limitations, especially regarding its pre-training objective. BERT's masking process doesn't account for the prediction order of words. XLNet aims to tackle this issue.\n3. Key Features &amp; Innovations: Permutation-based Training: Unlike BERT, which masks and predicts words in parallel, XLNet predicts words auto-regressively in all possible orders. It leverages all permutations of the input sequence, thereby learning bidirectional context. Two-stream Self-attention: This mechanism allows XLNet to handle target (to be predicted) and observed words differently during training, maintaining separate representations for each. Segment Recurrence: XLNet uses segment-level recurrence and relative positional encoding. This allows it to remember the content from a previous segment, making it beneficial for tasks with longer contexts. 4. Training Data and Model Architecture:\nDatasets: XLNet is trained on a combination of the Toronto Book Corpus and English Wikipedia, similar to BERT. Architecture: It leverages the Transformer-XL architecture, which introduces recurrence mechanisms to the standard Transformer, making it particularly effective for modeling longer sequences.\n5. Comparison with BERT: Modeling Capabilities: While BERT's bidirectionality comes from masking tokens and predicting them, XLNet's bidirectionality arises from its permutation-based training. This allows XLNet to potentially capture more intricate patterns in data. Performance: Upon release, XLNet outperformed BERT on several NLP benchmarks, showcasing the advantages of its training methodology. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158139,"modifiedTime":1737554783000,"sourceSize":2930,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/4. XLNet.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html":{"title":"5. GPT","icon":"","description":"GPT (Generative Pre-trained Transformer) is a type of language model developed by OpenAI that has revolutionized the field of natural language processing (NLP) and artificial intelligence. GPT models are designed to generate human-like text by predicting the next word in a sequence given the words that precede it. These models have been applied in a wide range of tasks, including text completion, translation, question answering, and content generation. Transformer Architecture: The core of GPT's effectiveness lies in the Transformer architecture, which relies on self-attention mechanisms to process sequences of input data. Unlike previous models that processed data sequentially (like RNNs and LSTMs), Transformers can handle sequences in parallel, significantly improving efficiency and performance on large datasets. Self-Attention Mechanism: This allows the model to weigh the importance of different words within the input data. For instance, in a sentence, the model can learn to pay more attention to nouns when predicting adjectives. This mechanism is key to understanding context and relationships within the text. Pre-training and Fine-tuning: GPT models undergo two main phases of training. In the pre-training phase, the model is trained on a large corpus of text data without specific task instructions, learning the general patterns of language. During the fine-tuning phase, the pre-trained model is further trained on a smaller dataset tailored to a specific task, allowing it to adapt its generalized knowledge to perform well on that task. Generative Capabilities: GPT models are generative, meaning they can generate new text sequences that are coherent and contextually relevant to the input provided. This is what makes GPT particularly powerful for creative writing, chatbots, and any task requiring content generation. GPT-1: The first version, introduced by OpenAI in 2018, featured 12 layers and 117 million parameters. It demonstrated the potential of Transformer-based models for NLP tasks. GPT-2: Released in 2019, GPT-2 expanded on GPT-1 with 48 layers and 1.5 billion parameters. It showed significant improvements in generating coherent and contextually relevant text, sparking discussions about the ethical implications of such powerful generative models. GPT-3: With its release in 2020, GPT-3 became the largest version at the time, featuring 175 billion parameters. It demonstrated remarkable abilities in generating human-like text, performing a wide range of NLP tasks without task-specific training, and even exhibiting rudimentary understanding of logic, arithmetic, and style. GPT-4 and beyond: While specific details and advancements continue to evolve, subsequent versions of GPT aim to improve upon the capabilities of their predecessors, addressing challenges such as bias, ethical concerns, and the need for even more efficient processing and better understanding of context and nuances in language. GPT models have a wide array of applications, from automating customer service inquiries and enhancing virtual assistants to creating content and aiding in language translation services. Their ability to understand and generate human-like text opens up possibilities for AI in fields like journalism, creative writing, and education.However, the development and deployment of GPT models also raise important ethical and societal questions, including issues of misinformation, privacy, and the potential for automating jobs. The generative power of GPT models necessitates careful consideration of their use cases and the establishment of guidelines to mitigate potential harms.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Key Concepts and Components","level":3,"id":"Key_Concepts_and_Components_0"},{"heading":"Versions of GPT","level":3,"id":"Versions_of_GPT_0"},{"heading":"Applications and Implications","level":3,"id":"Applications_and_Implications_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158173,"modifiedTime":1737554783000,"sourceSize":3782,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/5. GPT.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html":{"title":"0 Multimodal Learning","icon":"","description":"Multimodal learning refers to an approach in machine learning and artificial intelligence that integrates and processes information from multiple types of inputs or modes, such as text, images, audio, and video. The central premise is that combining different forms of data can provide a more comprehensive understanding of an object or phenomenon than relying on a single type of input.The significance of multimodal learning lies in its ability to mimic human-like processing by interpreting complex data from various sources. This approach can lead to more accurate, robust, and nuanced AI models. For instance, in understanding human communication, considering both verbal (text or audio) and non-verbal cues (gestures, facial expressions) offers a fuller picture than either alone.Potential Applications:\nHealthcare: Diagnosing diseases by analyzing medical images (X-rays, MRIs) alongside clinical notes.\nAutonomous Vehicles: Combining visual data (cameras), spatial information (LIDAR), and audio signals for safer navigation.\nEducation: Enhancing learning experiences through interactive content that combines text, video, and interactive simulations.\nCustomer Service: Improving automated support by analyzing customer queries across text, voice, and video to understand and resolve issues more effectively.\n1. Alignment: Determining how to align or synchronize different types of data that may not naturally correspond to each other in time or space is a fundamental challenge. For example, aligning the transcript of a speech with the speaker's facial expressions requires sophisticated models that can understand and map the relationships between audio and visual cues.2. Fusion: Fusion involves integrating the information from various modalities into a unified representation that can be effectively used for learning. This process is challenging because it requires models to not only handle data of different types and structures but also to weigh the importance of each modality in the context of the specific task. Techniques for fusion vary widely, from early fusion (combining features at the input level) to late fusion (combining outputs or decisions from separate models for each modality).3. Representation Learning: Learning representations that can effectively capture the essence of multimodal data is critical. This involves finding a common feature space where information from all modalities can be represented and compared. Deep learning models, such as neural networks, are often employed for this purpose, but the complexity and diversity of multimodal data pose unique challenges in designing and training these models.Overcoming Challenges:\nCross-Modal Learning: Developing algorithms that can learn the relationships between different modalities, even in the absence of explicit alignment, by leveraging correlations and patterns across the data.\nAttention Mechanisms: Using attention mechanisms to dynamically focus on relevant parts of the data from different modalities, which can help in better alignment and fusion.\nTransfer Learning: Applying knowledge learned from one modality (or task) to improve learning in another, which can be particularly useful when data in some modalities is scarce or noisy.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Challenges in Multimodal Learning","level":3,"id":"Challenges_in_Multimodal_Learning_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158181,"modifiedTime":1737554783000,"sourceSize":3710,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/0 Multimodal Learning.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html":{"title":"1. Alignment","icon":"","description":"Temporal alignment is a process of synchronizing different types of data that change over time, ensuring that each piece of information is matched with its correct temporal context. This is especially critical in applications where the timing of data streams is crucial for accurate interpretation, such as in audio-visual speech recognition, where the goal is to match the audio speech with the lip movements in the video. Techniques for achieving temporal alignment include:\nDynamic Time Warping (DTW): An algorithm used to find an optimal match between two given sequences with certain restrictions and along with a specified distance metric. DTW has been effectively used in speech recognition to align sequences of differing lengths.\nHidden Markov Models (HMMs): Employed to model temporal relationships in time-series data, HMMs can be used to align sequences by modeling the transitions between hidden states that represent different features or segments of the data.\nSpatial alignment focuses on matching data across different spatial dimensions. This involves ensuring that information from one modality (e.g., objects in an image) is correctly matched with relevant information in another modality (e.g., textual descriptions). Techniques include:\nFeature Matching and Registration: Techniques such as SIFT (Scale-Invariant Feature Transform) and SURF (Speeded Up Robust Features) are used to detect and match features across images or between images and other spatial data types.\nGeometric Transformation Models: These models, including affine and perspective transformations, help in aligning images or spatial features by adjusting for rotation, scale, translation, and skew.\nCross-modal correspondence involves identifying relationships between elements from different modalities, such as text and images, without explicit pairing instructions. Achieving cross-modal correspondence allows for richer interpretation and interaction between diverse data types. Key approaches include:\nCanonical Correlation Analysis (CCA): A statistical method used to uncover the correlations between two sets of variables (modalities), helping to identify the shared information between them.\nDeep Canonical Correlation Analysis (DCCA): An extension of CCA that uses deep learning to model complex nonlinear relationships between modalities, offering improved performance over traditional CCA in discovering hidden correspondences.\nRecent advances have introduced sophisticated models that enhance the alignment of multimodal data:\nSequence-to-Sequence Models with Attention Mechanisms: These models, initially developed for tasks like machine translation, have been adapted for multimodal contexts. The attention mechanism allows the model to dynamically focus on relevant parts of the input sequence, facilitating better alignment between modalities.\nTransformers: Offering a significant leap forward, transformers use self-attention mechanisms to process entire input sequences simultaneously, as opposed to the sequential processing of traditional models. This allows for more effective modeling of the relationships within and across modalities.\nMultimodal BERT (MMBERT): An adaptation of the BERT (Bidirectional Encoder Representations from Transformers) model for multimodal data. MMBERT can handle inputs from different modalities, learning to align and integrate the information to perform tasks that rely on understanding complex relationships across modalities.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Temporal and Spatial Alignment","level":3,"id":"Temporal_and_Spatial_Alignment_0"},{"heading":"<strong>Temporal Alignment</strong>","level":4,"id":"**Temporal_Alignment**_0"},{"heading":"<strong>Spatial Alignment</strong>","level":4,"id":"**Spatial_Alignment**_0"},{"heading":"Cross-Modal Correspondence","level":3,"id":"Cross-Modal_Correspondence_0"},{"heading":"Techniques and Models for Alignment","level":3,"id":"Techniques_and_Models_for_Alignment_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158184,"modifiedTime":1737554783000,"sourceSize":3872,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/1. Alignment.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html":{"title":"2. Fusion","icon":"","description":"Early fusion, also known as feature-level fusion, involves integrating features from different modalities at the beginning of the processing pipeline. This integration allows the learning model to simultaneously consider information from all modalities during the learning process. Key aspects include:\nFeature Concatenation: This technique involves directly combining features from different modalities into a single feature vector. This concatenated vector is then processed by the learning model, allowing the model to learn from the combined features. The challenge here lies in effectively normalizing and scaling features from different modalities to ensure that no single modality dominates the learning process.\nFeature Transformation and Combination: Beyond simple concatenation, some approaches involve transforming features from each modality into a common space before combining them. Techniques such as Principal Component Analysis (PCA) or Autoencoders can be used to reduce dimensionality and align features across modalities.\nLate fusion, or decision-level fusion, occurs at the end of the processing pipeline. Here, separate models are trained on each modality, and their predictions are combined to make a final decision. Strategies include:\nVoting Systems: Simple yet effective, voting systems aggregate the predictions from each modality-based model and select the majority or weighted majority vote as the final output.\nEnsemble Learning: This method involves combining the outputs of multiple models to improve prediction accuracy. Techniques such as bagging, boosting, or stacking can be employed, where the ensemble model learns how to best integrate the diverse predictions.\nIntermediate fusion strikes a balance between early and late fusion by integrating modalities at various stages of the learning process.\nCross-Modal Attention Mechanisms: Leveraging architectures like Transformers, cross-modal attention allows the model to dynamically focus on and integrate relevant information from different modalities during intermediate stages of processing. This selective attention mechanism facilitates a more nuanced understanding and integration of multimodal data.\nMultimodal Bottleneck Architecture (MBA): MBA constrains the flow of information from different modalities through a bottleneck layer, compelling the model to distill and combine only the most pertinent features from each modality. This approach encourages efficient information processing and helps in learning more abstract and generalized representations.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Early vs. Late Fusion Specifics","level":3,"id":"Early_vs._Late_Fusion_Specifics_0"},{"heading":"<strong>Early Fusion</strong>","level":4,"id":"**Early_Fusion**_0"},{"heading":"<strong>Late Fusion</strong>","level":4,"id":"**Late_Fusion**_0"},{"heading":"Intermediate Fusion Techniques","level":3,"id":"Intermediate_Fusion_Techniques_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158188,"modifiedTime":1737554783000,"sourceSize":3952,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/2. Fusion.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html":{"title":"3. Representation Learning","icon":"","description":"Variational Autoencoders stand out in the realm of representation learning for their ability to generate dense, meaningful representations of data. VAEs introduce a probabilistic twist to the traditional autoencoder architecture by encoding input data into a distribution over the latent space rather than a fixed point. This approach not only aids in learning generalizable features but also in handling the uncertainty inherent in multimodal data. Cross-modal autoencoders extend the concept of autoencoders to learn joint representations across different modalities. By encoding data from one modality and reconstructing data in another, these models foster a shared feature space that encapsulates the essence of both modalities. This cross-modal reconstruction capability is pivotal for tasks requiring an understanding of the relationships between different types of data, such as text-to-image synthesis or vice versa.Conditional GANs have revolutionized the generation of data across modalities by conditioning the generation process on input from another modality. This conditioning allows the model to learn shared representations that accurately capture the interplay between modalities. For example, a Conditional GAN could generate realistic images based on textual descriptions, effectively bridging the gap between visual and linguistic domains.Transformer-based architectures like ViLBERT (Vision-and-Language BERT) have been specifically designed to process and learn from multimodal data. By handling visual and textual inputs simultaneously, these models learn representations that effectively combine information from both modalities. The self-attention mechanism inherent in transformers plays a crucial role here, enabling the model to focus on the most relevant aspects of the data from each modality.Learning a shared embedding space for multiple modalities is a direct approach to ensuring that representations capture the essence of the data across modalities. This shared space facilitates tasks like cross-modal retrieval, where the goal is to find corresponding data in one modality based on a query from another. Techniques for learning these embeddings often involve contrastive loss functions, which enforce similarity between representations of related data points across modalities while pushing unrelated points apart.Triplet loss functions are employed to refine the quality of learned representations further. By using triplets of data points, consisting of an anchor, a positive example (similar to the anchor), and a negative example (dissimilar from the anchor), triplet loss encourages the model to learn representations where matched pairs (anchor and positive) are closer in the embedding space than unmatched pairs (anchor and negative). This approach is instrumental in enhancing the specificity and discriminability of the learned representations, crucial for accurately linking data across modalities.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Common Feature Space Algorithms","level":3,"id":"Common_Feature_Space_Algorithms_0"},{"heading":"<strong>Variational Autoencoders (VAEs)</strong>","level":4,"id":"**Variational_Autoencoders_(VAEs)**_0"},{"heading":"<strong>Cross-Modal Autoencoders</strong>","level":4,"id":"**Cross-Modal_Autoencoders**_0"},{"heading":"Deep Learning Approaches for Representation Learning","level":3,"id":"Deep_Learning_Approaches_for_Representation_Learning_0"},{"heading":"<strong>Generative Adversarial Networks (GANs)</strong>","level":4,"id":"**Generative_Adversarial_Networks_(GANs)**_0"},{"heading":"<strong>Transformer Models in Multimodal Learning</strong>","level":4,"id":"**Transformer_Models_in_Multimodal_Learning**_0"},{"heading":"Addressing Multimodal Representation Learning Challenges","level":3,"id":"Addressing_Multimodal_Representation_Learning_Challenges_0"},{"heading":"<strong>Joint Embedding Learning</strong>","level":4,"id":"**Joint_Embedding_Learning**_0"},{"heading":"<strong>Triplet Loss Functions</strong>","level":4,"id":"**Triplet_Loss_Functions**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158192,"modifiedTime":1737554783000,"sourceSize":3489,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/3. Representation Learning.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html":{"title":"4. Key Multimodal Models","icon":"","description":"ViLBERT revolutionizes the handling of multimodal inputs by extending the BERT architecture with a two-stream approach. It processes visual and textual information separately before fusing them through co-attentional transformer layers. This architecture enables ViLBERT to learn rich, task-agnostic joint representations that encapsulate the intricate interplay between image content and textual descriptions. The use of co-attentional layers is particularly noteworthy, as it allows the model to dynamically focus on and integrate relevant aspects of both modalities, fostering a deeper understanding of the multimodal input.ViLBERT's utility spans across various domains, showcasing its adaptability and effectiveness. It excels in visual question answering (VQA), where it leverages its joint representations to accurately interpret and respond to queries about images. In image captioning, ViLBERT generates descriptive and contextually appropriate captions, while in object recognition, it identifies and classifies objects within images with high precision, demonstrating its comprehensive grasp of visual and textual cues.The dual-stream architecture affords ViLBERT a nuanced understanding of each modality, significantly enhancing its performance on tasks requiring detailed modal analysis. However, this architectural complexity increases computational demands and necessitates substantial multimodal data for training, potentially limiting its accessibility and scalability.VisualBERT simplifies the multimodal learning process with a single-stream architecture that seamlessly integrates visual and textual inputs. By processing concatenated visual features and textual tokens through a unified series of transformer layers, VisualBERT directly learns joint multimodal representations. This direct integration approach facilitates an efficient and effective fusion of modalities, enabling the model to capture the synergies between visual and textual data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"ViLBERT (Vision-and-Language BERT)","level":3,"id":"ViLBERT_(Vision-and-Language_BERT)_0"},{"heading":"<strong>Architecture &amp; Functionality</strong>","level":4,"id":"**Architecture_&_Functionality**_0"},{"heading":"<strong>Applications</strong>","level":4,"id":"**Applications**_0"},{"heading":"<strong>Strengths and Weaknesses</strong>","level":4,"id":"**Strengths_and_Weaknesses**_0"},{"heading":"VisualBERT","level":3,"id":"VisualBERT_0"},{"heading":"<strong>Architecture &amp; Functionality</strong>","level":4,"id":"**Architecture_&_Functionality**_1"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158195,"modifiedTime":1737554783000,"sourceSize":5223,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/4. Key Multimodal Models.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html":{"title":"0. Overview","icon":"","description":"Localization is the process of determining the position and orientation of a robot within a known map. It is a critical component of Simultaneous Localization and Mapping (SLAM), as accurate localization is necessary for reliable mapping and navigation. Without precise localization, a robot cannot correctly interpret its environment or plan its movements effectively. Sensor Noise: Inaccuracies in sensor readings can lead to errors in position estimation.\nExample: Inertial measurement units (IMUs) and GPS signals can be noisy or affected by external factors. Dynamic Environments: Changes in the environment, such as moving objects or people, can complicate localization.\nExample: A crowded area where people frequently move around. Computational Complexity: Real-time localization requires efficient algorithms to process sensor data and update the robot's position rapidly.\nExample: High-frequency sensor data from LIDAR or cameras needing real-time processing. Initialization: Determining the initial position of the robot in an unknown environment can be challenging.\nExample: A robot starting operation in a new location without prior knowledge. Dead Reckoning: Method: Estimating the current position based on previous positions and motion data.\nAdvantage: Simple and does not require external references.\nDisadvantage: Prone to drift over time due to accumulated errors. Landmark-Based Localization: Method: Using known landmarks in the environment to correct the robot's position.\nAdvantage: Provides accurate corrections when landmarks are reliably detected.\nDisadvantage: Requires the presence of distinguishable landmarks. Map Matching: Method: Aligning sensor data with a pre-existing map to determine the robot's position.\nAdvantage: Effective when a detailed and accurate map is available.\nDisadvantage: Less effective in environments that change frequently. Probabilistic Methods: Method: Using statistical techniques to estimate the robot's position based on uncertain sensor data.\nExamples: Kalman Filters: Optimal for linear systems with Gaussian noise.\nParticle Filters: Suitable for non-linear systems and non-Gaussian noise. Advantage: Robust to sensor noise and uncertainties.\nDisadvantage: Computationally intensive, especially with high-dimensional state spaces. Given the robot's state at time t, and sensor measurements , the goal is to estimate the posterior distribution:where:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Challenges","level":3,"id":"Challenges_0"},{"heading":"Common Approaches","level":3,"id":"Common_Approaches_0"},{"heading":"Mathematical Formulation","level":3,"id":"Mathematical_Formulation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158212,"modifiedTime":1737554783000,"sourceSize":4213,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/0. Overview.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html":{"title":"1. Mathematical Expressions","icon":"","description":"In SLAM, the state vector encapsulates the robot's pose and the map features: Robot Pose: typically includes position and orientation.\nMap Features: includes the coordinates of landmarks or features in the environment.\nThe motion model predicts the next state of the robot based on its current state and control inputs:\nwhere: is the previous state. is the control input (e.g., velocity, steering angle). is the nonlinear motion model function. is the process noise, often modeled as zero-mean Gaussian noise: .\nExample:\nFor a simple 2D robot, the motion model can be:\nwhere is the linear velocity, is the angular velocity.The observation model relates the state of the robot and the map to the sensor measurements:\nwhere: is the observation (sensor measurement). is the nonlinear observation model function. is the observation noise, often modeled as zero-mean Gaussian noise: .\nExample:\nFor a range-bearing sensor, the observation model might be:\nwhere are the coordinates of a landmark, is the range, and is the bearing.SLAM often uses Bayesian filtering to estimate the state distribution over time.The prediction step uses the motion model to estimate the state and covariance:\nwhere: is the predicted state. is the predicted covariance. is the Jacobian of with respect to .\nExample:\nFor a linear motion model:\nThe update step incorporates the observation model to correct the predicted state:\nwhere: is the innovation. is the innovation covariance. is the Kalman gain. is the Jacobian of with respect to .\nExample:\nFor a linear observation model:\nGraph-based SLAM represents the problem as a graph optimization problem, minimizing the error in the constraints.\nNodes: Represent robot poses and landmarks.\nEdges: Represent constraints (measurements) between nodes.\nThe optimization problem is to find the state that minimizes the overall error:\nwhere: is the measurement between nodes and . is the expected measurement. is the information matrix of the measurement.\nExample:\nFor a 2D pose graph, the error term for an edge between nodes and could be:\nWe'll use the following assumptions:\nThe robot's state vector consists of position (x, y) and orientation (θ).\nLinear motion model: Linear observation model: The initial state and covariance are given.\nThe control input and observation are given.\nProcess noise and observation noise are given.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"State Representation","level":3,"id":"State_Representation_0"},{"heading":"Motion Model","level":3,"id":"Motion_Model_0"},{"heading":"Observation Model","level":3,"id":"Observation_Model_0"},{"heading":"Bayesian Filtering","level":3,"id":"Bayesian_Filtering_0"},{"heading":"Prediction Step","level":4,"id":"Prediction_Step_0"},{"heading":"Update Step","level":4,"id":"Update_Step_0"},{"heading":"Graph-Based SLAM","level":3,"id":"Graph-Based_SLAM_0"},{"heading":"Graph Representation","level":4,"id":"Graph_Representation_0"},{"heading":"Optimization Problem","level":4,"id":"Optimization_Problem_0"},{"heading":"Detailed Example: SLAM Update Step with Matrix Calculation","level":3,"id":"Detailed_Example_SLAM_Update_Step_with_Matrix_Calculation_0"},{"heading":"Initial State and Covariance","level":4,"id":"Initial_State_and_Covariance_0"},{"heading":"Control Input","level":4,"id":"Control_Input_0"},{"heading":"Process Noise Covariance","level":4,"id":"Process_Noise_Covariance_0"},{"heading":"Observation","level":4,"id":"Observation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158216,"modifiedTime":1737554783000,"sourceSize":10710,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/1. Mathematical Expressions.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html":{"title":"2. Kalman Filter","icon":"","description":"The Kalman Filter is an optimal recursive algorithm used for estimating the state of a dynamic system from a series of noisy measurements. It is widely used in various fields such as robotics, control systems, and signal processing.The Kalman Filter operates on linear dynamic systems modeled in state-space form. The state-space representation consists of two equations: the process (or system) model and the measurement (or observation) model.\nProcess Model:\nwhere: is the state vector at time step . is the state transition matrix. is the control input matrix. is the control input vector. is the process noise, modeled as a zero-mean Gaussian noise with covariance : . Measurement Model:\nwhere: is the measurement vector at time step . is the observation matrix. is the measurement noise, modeled as a zero-mean Gaussian noise with covariance .\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Mathematical Foundations","level":3,"id":"Mathematical_Foundations_0"},{"heading":"State Space Representation","level":4,"id":"State_Space_Representation_0"},{"heading":"Kalman Filter Algorithm","level":3,"id":"Kalman_Filter_Algorithm_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158220,"modifiedTime":1737554783000,"sourceSize":4219,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/2. Kalman Filter.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html":{"title":"3. Extended Kalman Filter","icon":"","description":"The Extended Kalman Filter (EKF) is an extension of the Kalman Filter, which is designed for linear systems. The EKF is tailored to handle nonlinear systems by linearizing them around the current estimate. It is widely used in SLAM for both localization and mapping due to its ability to estimate the state of a system in the presence of noise.State Representation:\nThe state vector typically includes the robot's position and the positions of landmarks .\nPrediction Step: Uses the robot's motion model to predict the next state. The state prediction is given by: where: is the predicted state. is the nonlinear motion model. represents the process noise. The covariance prediction is given by: where: is the predicted covariance. is the Jacobian of the motion model with respect to the state. is the process noise covariance. Update Step: Incorporates sensor measurements to correct the predicted state. The Kalman gain is computed as: where: is the Jacobian of the observation model with respect to the state. is the measurement noise covariance. The state update is given by: where: is the measurement. is the nonlinear observation model. The covariance update is given by: ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Basic Principles","level":3,"id":"Basic_Principles_0"},{"heading":"Mathematical Formulation","level":3,"id":"Mathematical_Formulation_0"},{"heading":"Advantages and Limitations","level":3,"id":"Advantages_and_Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158224,"modifiedTime":1737554783000,"sourceSize":3629,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/3. Extended Kalman Filter.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html":{"title":"4. Particle Filter","icon":"","description":"The Particle Filter (PF), also known as Sequential Monte Carlo (SMC), is a non-parametric Bayesian filtering algorithm used for estimating the state of a dynamic system. Unlike the Kalman Filter, which is limited to linear and Gaussian systems, the Particle Filter can handle non-linear and non-Gaussian systems by representing the state distribution with a set of random samples, or \"particles.\"In the Particle Filter, the state distribution is represented by a set of particles, each with an associated weight:\nwhere: is the state of the i-th particle at time step k. is the weight of the i-th particle at time step k. is the number of particles.\nThe Particle Filter algorithm consists of three main steps: prediction, update, and resampling.In the prediction step, each particle is propagated forward according to the process model, incorporating process noise.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Mathematical Foundations","level":3,"id":"Mathematical_Foundations_0"},{"heading":"State Representation","level":4,"id":"State_Representation_0"},{"heading":"Particle Filter Algorithm","level":3,"id":"Particle_Filter_Algorithm_0"},{"heading":"1. Prediction Step","level":4,"id":"1._Prediction_Step_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158228,"modifiedTime":1737554783000,"sourceSize":4218,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/4. Particle Filter.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html":{"title":"0. Introduction to SLAM","icon":"","description":"SLAM (Simultaneous Localization and Mapping) is a computational problem in robotics and computer vision where a robot or autonomous vehicle constructs or updates a map of an unknown environment while simultaneously keeping track of its own location within that environment. SLAM is essential for autonomous navigation, enabling robots to move through and understand their surroundings without relying on external infrastructure. Localization: Determining the robot's position and orientation within a known map. This involves estimating the state of the robot based on sensor data. Mapping: Building a map of the environment as the robot explores it. The map can be represented in various forms, such as occupancy grids, feature maps, or point clouds. Sensor Fusion: Combining data from multiple sensors (e.g., LIDAR, cameras, IMU) to improve the accuracy of localization and mapping. Each sensor provides different types of information, which, when integrated, can enhance the overall understanding of the environment. Data Association: Identifying and matching features or landmarks observed at different times to correctly update the map and the robot's location. This helps in dealing with issues like sensor noise and dynamic changes in the environment. Loop Closure: Detecting when the robot has returned to a previously visited location, which helps in correcting drift and improving the accuracy of the map. Several algorithms and approaches have been developed to solve the SLAM problem, each with its own advantages and limitations. Some common SLAM algorithms include: Extended Kalman Filter (EKF) SLAM: Uses an extended Kalman filter to estimate the robot's pose and the positions of landmarks. It is suitable for environments with Gaussian noise but can be computationally expensive for large-scale maps. Particle Filter (PF) SLAM: Also known as Monte Carlo Localization (MCL), this approach uses a set of particles to represent the probability distribution of the robot's pose. It is robust to non-Gaussian noise but requires a large number of particles for accurate results. Graph-Based SLAM: Constructs a graph where nodes represent robot poses and edges represent spatial constraints between poses. Optimization techniques are used to find the best map and pose configuration. This approach is efficient for large-scale environments and can handle loop closure effectively. Visual SLAM (vSLAM): Utilizes visual data from cameras to perform SLAM. It can be either feature-based (detecting and tracking features like corners and edges) or direct (using pixel intensities directly). Popular vSLAM algorithms include ORB-SLAM and LSD-SLAM. Autonomous Vehicles: Enabling self-driving cars to navigate urban environments safely and efficiently.\nRobotics: Allowing robots to perform tasks such as warehouse automation, search and rescue, and home cleaning.\nAugmented Reality (AR): Enhancing AR experiences by providing accurate tracking and mapping of the real world.\nDrones: Enabling drones to perform tasks such as aerial mapping, inspection, and delivery.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Key Concepts","level":3,"id":"Key_Concepts_0"},{"heading":"Algorithms","level":3,"id":"Algorithms_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"SLAM Implementation Workflow","level":3,"id":"SLAM_Implementation_Workflow_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158203,"modifiedTime":1737554783000,"sourceSize":5490,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/0. Introduction to SLAM.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/deep-learning/0.2.-resnet.html":{"title":"0.2. ResNet","icon":"","description":"Residual Networks, commonly known as ResNets, represent a groundbreaking advancement in deep neural network architectures. They were introduced by Kaiming He et al. in their influential 2015 paper titled \"Deep Residual Learning for Image Recognition.\" ResNets have proven to be extremely effective in tackling the challenges of training very deep neural networks, which was previously hindered by issues like vanishing gradients. These networks have achieved state-of-the-art performance across various computer vision tasks, including image classification, object detection, and image segmentation.As neural networks get deeper, training becomes more challenging due to the vanishing gradient problem. This occurs when gradients during backpropagation become extremely small, leading to slow convergence or even preventing learning altogether. Traditional networks struggled to effectively propagate gradients through many layers, limiting their depth and complexity.ResNets address the vanishing gradient problem through the introduction of residual blocks. A residual block is designed to learn and apply the residual mapping between input and output. It consists of two main components: Shortcut Connection (Identity Mapping): The original input is directly passed to the output of the block through a shortcut connection. This concept is based on the idea that, at worst, the block can learn to do nothing (i.e., set weights to zero) if it benefits the final network. Main Path: This part of the block contains one or more convolutional layers that transform the input to match the desired output. The convolutional layers learn the residual function that needs to be applied to the input. A residual unit, or residual block, is mathematically represented as:Where:\nx is the input to the block.\nF is the residual function learned by the convolutional layers.\n{Wi} are the weights of the convolutional layers.\ny is the output of the block, which is the sum of the residual function and the input.\nThe introduction of skip connections (shortcut connections) brings several advantages: Gradient Flow: Skip connections enable gradients to flow directly through the identity path during backpropagation, mitigating the vanishing gradient problem. Training Deep Networks: ResNets allow the training of much deeper networks without encountering gradient-related issues. Ease of Learning Identity: If the residual function is close to identity, the block can learn to make the weights close to zero, effectively learning to do nothing. ResNets come in various architectures, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. The number indicates the depth of the network, with ResNet-50 and above incorporating bottleneck structures for efficiency. ResNet architectures utilize either pre-activation (with batch normalization before activation) or post-activation structures.ResNets have demonstrated remarkable performance across a wide range of computer vision tasks. They achieved top ranks in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and have become a foundational architecture in modern deep learning. Their contributions to tasks like image classification, object detection, semantic segmentation, and image generation have been significant.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction to Residual Networks (ResNets)","level":3,"id":"Introduction_to_Residual_Networks_(ResNets)_0"},{"heading":"The Problem: Vanishing Gradient","level":3,"id":"The_Problem_Vanishing_Gradient_0"},{"heading":"Introducing Residual Blocks","level":3,"id":"Introducing_Residual_Blocks_0"},{"heading":"The Residual Unit","level":3,"id":"The_Residual_Unit_0"},{"heading":"Benefits of Skip Connections","level":3,"id":"Benefits_of_Skip_Connections_0"},{"heading":"ResNet Architectures","level":3,"id":"ResNet_Architectures_0"},{"heading":"Applications and Performance","level":3,"id":"Applications_and_Performance_0"},{"heading":"Training and Optimization","level":3,"id":"Training_and_Optimization_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/deep-learning/0.2.-resnet.html","pathToRoot":"../..","attachments":[],"createdTime":1741084158018,"modifiedTime":1737554783000,"sourceSize":5010,"sourcePath":"Artificial Intelligence/Deep Learning/0.2. ResNet.md","exportPath":"artificial-intelligence/deep-learning/0.2.-resnet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html":{"title":"0. Introduction","icon":"","description":"A Knowledge Graph (KG) is an advanced data structure that represents semantic networks of real-world entities — such as objects, events, situations, or concepts — and illustrates the relationships that connect these entities. In knowledge graphs, nodes represent the entities, and edges depict the relationships that interconnect these entities. This graph-based representation facilitates a structured exploration and systematic organization of knowledge, supporting a variety of applications in data integration, AI, and complex systems analysis.Entities are the fundamental components of knowledge graphs, representing real-world objects or concepts. Each entity is a distinct node in the graph.Relationships are the edges that connect entities within the graph, depicting how entities are related to one another. These relationships are directional and can vary in type and complexity.Properties provide additional details about entities and relationships. For entities, properties might include attributes or descriptive features. For relationships, properties might encapsulate the strength or nature of the connection.Knowledge graphs contribute to more sophisticated search functionalities by leveraging the semantic context of data, rather than merely matching keywords.By analyzing the intricate web of relationships between entities, knowledge graphs can deliver more accurate and contextually appropriate recommendations.Knowledge graphs excel in integrating heterogeneous data from multiple sources into a unified framework, thereby simplifying data management and accessibility.The relational nature of knowledge graphs allows for robust inference mechanisms, making it possible to deduce new relationships and insights from existing data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Core Components","level":3,"id":"Core_Components_0"},{"heading":"Entities","level":4,"id":"Entities_0"},{"heading":"Relationships","level":4,"id":"Relationships_0"},{"heading":"Properties","level":4,"id":"Properties_0"},{"heading":"Advantages","level":3,"id":"Advantages_0"},{"heading":"Enhanced Semantic Search","level":4,"id":"Enhanced_Semantic_Search_0"},{"heading":"Improved Recommendation Systems","level":4,"id":"Improved_Recommendation_Systems_0"},{"heading":"Efficient Data Integration","level":4,"id":"Efficient_Data_Integration_0"},{"heading":"Inference Capabilities","level":4,"id":"Inference_Capabilities_0"},{"heading":"Prominent Use Cases","level":3,"id":"Prominent_Use_Cases_0"},{"heading":"Google Knowledge Graph","level":4,"id":"Google_Knowledge_Graph_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158241,"modifiedTime":1737554783000,"sourceSize":2901,"sourcePath":"Artificial Intelligence/Knowledge Graph/0. Foundations/0. Introduction.md","exportPath":"artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html":{"title":"1. Graph Theory Essentials","icon":"","description":"Graph theory is a branch of mathematics concerned with the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph is made up of vertices (also called nodes or points) connected by edges (also called links or lines). This fundamental concept is applied across various fields, from computer science to biology and social science, providing a powerful framework for solving problems related to networked structures.\nVertices: The fundamental units or points in a graph.\nEdges: The connections between vertices. Edges can be directed (showing a one-way relationship) or undirected (showing a bidirectional relationship). Path: A sequence of edges that connects a sequence of vertices, with no vertex repeated.\nCycle: A path in which the start and end vertices are the same, forming a loop. Undirected Graphs: Graphs where edges have no direction.\nDirected Graphs: Graphs where each edge has a direction associated with it.\nWeighted Graphs: Graphs where edges carry weights, representing costs, distances, or other metrics.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Basic Concepts","level":3,"id":"Basic_Concepts_0"},{"heading":"Vertices and Edges","level":4,"id":"Vertices_and_Edges_0"},{"heading":"Paths and Cycles","level":4,"id":"Paths_and_Cycles_0"},{"heading":"Graph Types","level":4,"id":"Graph_Types_0"},{"heading":"Properties of Graphs","level":3,"id":"Properties_of_Graphs_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158245,"modifiedTime":1737554783000,"sourceSize":3161,"sourcePath":"Artificial Intelligence/Knowledge Graph/0. Foundations/1. Graph Theory Essentials.md","exportPath":"artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html":{"title":"2. Semantic Web Technologies","icon":"","description":"The Semantic Web is an extension of the current web where information is given well-defined meaning, enabling computers and people to work in cooperation. It aims to create a universal medium for information exchange by putting documents with computable semantics (meaning) on the web. The Semantic Web is structured to allow data to be shared and reused across application, enterprise, and community boundaries.\nRDF: A standard model for data interchange on the web. RDF utilizes a simple data model that expresses information as triples, each consisting of a subject, predicate, and object. This triple model is flexible, allowing data to be easily merged in spite of differing schemas, and supports schema evolution without necessitating modifications to data consumers. RDF can be serialized in various formats, such as RDF/XML, N-Triples, Turtle, and others, making it versatile for different use cases. The framework also supports the creation of vocabularies which enable users to define their own structured data terms. Triple-based Model: Information is expressed in triples, making data structured yet flexible.\nSchema Independence: Allows merging of data even when underlying schemas are different.\nSerialization Formats: Supports multiple formats for diverse application needs.\nVocabulary Support: Users can create specific vocabularies, enhancing the expressiveness of data. OWL: A comprehensive ontology language designed for creating and interpreting web ontologies. OWL is built on top of RDF and extends it by providing additional vocabulary along with a formal semantics. It allows users to write explicit, complex and hierarchically structured concepts to describe the relationships between them. OWL supports three sublanguages - OWL Lite, OWL DL, and OWL Full - catering to different levels of expressiveness and computational complexity, making it suitable for applications requiring detailed, machine-interpretable content to enable robust automation and reasoning.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Key Technologies","level":3,"id":"Key_Technologies_0"},{"heading":"RDF (Resource Description Framework)","level":4,"id":"RDF_(Resource_Description_Framework)_0"},{"heading":"Key Features of RDF:","level":5,"id":"Key_Features_of_RDF_0"},{"heading":"OWL (Web Ontology Language)","level":4,"id":"OWL_(Web_Ontology_Language)_0"},{"heading":"Key Features of OWL:","level":5,"id":"Key_Features_of_OWL_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158249,"modifiedTime":1737554783000,"sourceSize":4283,"sourcePath":"Artificial Intelligence/Knowledge Graph/0. Foundations/2. Semantic Web Technologies.md","exportPath":"artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html":{"title":"0. Building and Modeling","icon":"","description":"Building a knowledge graph involves the extraction, integration, and organization of information from various data sources to form a network of entities and their interrelations that are stored and managed as a graph. The goal is to create a structured semantic framework that enhances data usability for various applications such as search engines, recommendation systems, and AI-driven analytics.\nData Sources: The construction of a knowledge graph begins with the identification of relevant data sources which can include structured data (e.g., databases), semi-structured data (e.g., XML, JSON files), and unstructured data (e.g., text documents, web pages). Automated Extraction: Techniques such as natural language processing (NLP) are used to extract entities and their relationships from unstructured and semi-structured data sources.\nManual Annotation: Involves human experts who annotate the data, ensuring high accuracy of the extracted information, especially in complex domains. Entities as Nodes: Entities are represented as nodes in the graph. Each node represents an object, concept, or instance.\nRelationships as Edges: Relationships between entities are depicted as edges. These can be labeled and directed, showing how entities are connected. Defining Ontologies: Ontologies define the types of entities and relationships within the graph. They provide a schema that dictates how nodes and edges are structured.\nUsing RDF/OWL: These frameworks are often used to define and store ontologies. RDF describes relationships between entities as triples, while OWL provides additional vocabulary and semantics. Data Integration: Involves merging data from multiple sources, resolving inconsistencies, and ensuring data quality. Techniques such as entity resolution and schema matching are crucial at this stage. Graph Databases: Knowledge graphs are typically stored in graph databases like Neo4j, Amazon Neptune, or OrientDB, which are optimized for storing and querying complex networks of data. Adding Data: After the initial graph structure and schema are defined, data is added to the graph. This can involve transforming data into the graph format and linking new data with existing nodes and edges.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Data Sources and Extraction","level":3,"id":"Data_Sources_and_Extraction_0"},{"heading":"Identifying Data Sources","level":4,"id":"Identifying_Data_Sources_0"},{"heading":"Data Extraction Techniques","level":4,"id":"Data_Extraction_Techniques_0"},{"heading":"Modeling Techniques","level":3,"id":"Modeling_Techniques_0"},{"heading":"Graph Structure","level":4,"id":"Graph_Structure_0"},{"heading":"Schema Definition","level":4,"id":"Schema_Definition_0"},{"heading":"Building the Knowledge Graph","level":3,"id":"Building_the_Knowledge_Graph_0"},{"heading":"Integration","level":4,"id":"Integration_0"},{"heading":"Storage","level":4,"id":"Storage_0"},{"heading":"Populating the Graph","level":4,"id":"Populating_the_Graph_0"},{"heading":"Tools and Technologies","level":3,"id":"Tools_and_Technologies_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158258,"modifiedTime":1737554783000,"sourceSize":3284,"sourcePath":"Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/0. Building and Modeling.md","exportPath":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html":{"title":"1. Querying and Integration","icon":"","description":"Querying a knowledge graph involves retrieving and manipulating data stored in the graph format to extract meaningful information, answer specific questions, or derive insights. Effective querying mechanisms are critical for the utilization of knowledge graphs in applications such as semantic search, data analysis, and artificial intelligence.\nSPARQL (SPARQL Protocol and RDF Query Language): The primary language used to query RDF-based knowledge graphs. It allows for the querying of diverse data forms and supports complex queries like recursion, aggregation, and sorting. SPARQL can also be used to perform updates, ask questions, and construct or describe resources. Cypher: A declarative graph query language that allows for expressive and efficient querying of the graph database. It is specifically designed for Neo4j and focuses on patterns in graphs naturally and efficiently. Gremlin: A functional, data-flow language that enables users to traverse graphs. It is part of the Apache TinkerPop graph computing framework and is adaptable to different graph database systems, making it suitable for complex queries across large datasets.\nData integration in knowledge graphs involves combining data from different sources to provide a unified view. This process is crucial for ensuring the data's consistency, completeness, and usability across applications.\nEntity Resolution (ER): The process of identifying, linking, or merging records that refer to the same entity across different data sources. Techniques such as matching algorithms, clustering, and supervised learning models are commonly used. Schema Mapping: Aligning various data models or schemas from different sources to a unified schema that is suitable for the knowledge graph. This includes translating different terminologies and structures into a coherent format. Data Fusion: The process of merging information from disparate sources to create a consistent, accurate, and comprehensive representation of data. This often involves resolving conflicts and redundancies. Apache Jena: An open-source framework for building knowledge graphs that supports RDF, OWL, and SPARQL. It includes a programmable environment for managing semantic data and can be integrated with Java applications.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Key Query Languages","level":3,"id":"Key_Query_Languages_0"},{"heading":"SPARQL","level":4,"id":"SPARQL_0"},{"heading":"Cypher","level":4,"id":"Cypher_0"},{"heading":"Gremlin","level":4,"id":"Gremlin_0"},{"heading":"Data Integration Techniques","level":3,"id":"Data_Integration_Techniques_0"},{"heading":"Entity Resolution","level":4,"id":"Entity_Resolution_0"},{"heading":"Schema Mapping","level":4,"id":"Schema_Mapping_0"},{"heading":"Data Fusion","level":4,"id":"Data_Fusion_0"},{"heading":"Tools and Frameworks","level":3,"id":"Tools_and_Frameworks_0"},{"heading":"Apache Jena","level":4,"id":"Apache_Jena_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158262,"modifiedTime":1737554783000,"sourceSize":3130,"sourcePath":"Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/1. Querying and Integration.md","exportPath":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html":{"title":"2. Visualizing","icon":"","description":"Visualizing knowledge graphs involves representing graphical data in a visual format that is easy to understand and interact with. Effective visualization helps in revealing hidden patterns, understanding complex structures, and communicating insights clearly. This is crucial for data scientists, researchers, and decision-makers who rely on the graphical representation of data for analysis and presentation.\nNode-Link Diagrams: The most common form of visualization for knowledge graphs, where nodes represent entities and links represent relationships. These diagrams can be enhanced with various layouts such as force-directed, hierarchical, or circular, depending on the nature of the graph and the specific insights required. Matrix Views: An alternative to node-link diagrams, especially useful for dense graphs. Entities are represented as rows and columns in a matrix, and relationships are indicated by marks at the intersection of rows and columns. This view is particularly effective for spotting patterns and anomalies in the relationships between entities. Interactive Visualization: Tools that allow users to interact with the graph visualization dynamically. Features may include zooming, panning, filtering, and detailed on-demand information about nodes and edges. This interactivity enhances user engagement and facilitates deeper exploration of data. Gephi: An open-source network analysis and visualization software package, designed for various types of networks and complex systems, dynamic and hierarchical graphs. Gephi is known for its speed and efficiency, handling large graphs smoothly with its rendering engine. Graphviz: A collection of software tools for creating and manipulating graph visualizations. It uses a simple text language to describe graphs and produces output in several graphical formats, such as SVG, PDF, and PNG. Graphviz is especially useful for automated graph drawing. Neo4j Bloom: A visualization tool designed specifically for Neo4j databases, allowing for beautiful and insightful graph visualizations. Bloom enables users to explore and interact with their data through a simple and intuitive interface, making it accessible even to those without technical expertise. Sigma.js: A JavaScript library dedicated to graph drawing. It enables the display of interactive graphs on web pages, offering a range of customizations and is highly adaptable for web-based applications.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Key Techniques in Visualization","level":3,"id":"Key_Techniques_in_Visualization_0"},{"heading":"Node-Link Diagrams","level":4,"id":"Node-Link_Diagrams_0"},{"heading":"Matrix Views","level":4,"id":"Matrix_Views_0"},{"heading":"Interactive Visualization","level":4,"id":"Interactive_Visualization_0"},{"heading":"Popular Tools for Visualizing Knowledge Graphs","level":3,"id":"Popular_Tools_for_Visualizing_Knowledge_Graphs_0"},{"heading":"Gephi","level":4,"id":"Gephi_0"},{"heading":"Graphviz","level":4,"id":"Graphviz_0"},{"heading":"Neo4j Bloom","level":4,"id":"Neo4j_Bloom_0"},{"heading":"Sigma.js","level":4,"id":"Sigma.js_0"},{"heading":"Considerations for Effective Visualization","level":3,"id":"Considerations_for_Effective_Visualization_0"},{"heading":"Scalability","level":4,"id":"Scalability_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158266,"modifiedTime":1737554783000,"sourceSize":3490,"sourcePath":"Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/2. Visualizing.md","exportPath":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html":{"title":"0. AI Enhancement","icon":"","description":"Knowledge graphs play a crucial role in enhancing artificial intelligence (AI) systems by providing structured semantic frameworks that enable machines to understand and reason about the relationships between different pieces of information. They serve as a backbone for AI applications, facilitating more sophisticated decision-making processes, improved data interpretation, and personalized user experiences.\nFeature Enrichment: Knowledge graphs contribute to machine learning by enriching feature sets. They provide additional context and connections between data points, which can be leveraged to improve the accuracy and robustness of predictive models. Semantic Reasoning: AI systems can utilize the structured data in knowledge graphs to perform reasoning tasks. This includes drawing inferences based on the relationships and hierarchies present in the graph, which is vital for applications like question answering and decision support systems. Contextual Understanding: In NLP, knowledge graphs help in understanding the context of words and phrases within large text datasets. By mapping terms to entities in the graph, AI systems gain a deeper understanding of language nuances, significantly improving language-based tasks such as text classification, sentiment analysis, and machine translation.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Enhancing Machine Learning Models","level":3,"id":"Enhancing_Machine_Learning_Models_0"},{"heading":"Feature Enrichment","level":4,"id":"Feature_Enrichment_0"},{"heading":"Semantic Reasoning","level":4,"id":"Semantic_Reasoning_0"},{"heading":"Improving Natural Language Processing","level":3,"id":"Improving_Natural_Language_Processing_0"},{"heading":"Contextual Understanding","level":4,"id":"Contextual_Understanding_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158274,"modifiedTime":1737554783000,"sourceSize":3346,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/0. AI Enhancement.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html":{"title":"1. Knowledge Graph Embeddings","icon":"","description":"Knowledge graph embeddings are a powerful technique for representing entities and relationships in a knowledge graph as continuous vector spaces. This transformation facilitates the application of machine learning algorithms to graph data, enabling tasks such as link prediction, entity resolution, and node classification. Embeddings help capture the semantic relationships between entities in a lower-dimensional space, preserving the structure of the graph while making the data computationally manageable.\nTransE: A popular model that interprets relationships as translations in the embedding space. If a relationship ( (h, r, t) ) holds, then the embedding of the tail entity ( t ) should be close to the embedding of the head entity ( h ) plus some vector ( r ).\nTransH: Builds on TransE by allowing entities to have role-specific embeddings, depending on the relationship. DistMult: Simplifies the relationship modeling by using a bilinear model with a diagonal weight matrix. It excels in symmetric relation predictions.\nComplEx: Extends DistMult to model asymmetric and complex relations using complex-valued embeddings. ConvE: Utilizes convolutional neural networks to model the interactions between entities and relationships. It reshapes embeddings and uses convolution to capture feature interactions.\nR-GCN: Relational Graph Convolutional Networks extend GCNs to the relational setting by incorporating information from different types of relationships in the graph. Link Prediction: Embeddings can predict the likelihood of a relationship between two nodes, useful for discovering potential relationships or completing missing ones in the graph. Entity Resolution: Embeddings help in identifying entities that refer to the same item across different datasets or within a large graph, improving data quality and integration. Node Classification: By learning comprehensive representations of nodes, embeddings facilitate the classification of nodes into categories or types based on their features and connections in the graph. Dimensionality: Choosing the right dimensionality for embeddings is critical as it balances between sufficient expressiveness and computational efficiency.\nSparsity: Many real-world graphs are sparse, which can challenge learning effective embeddings. Techniques to handle sparsity include incorporating additional information sources or using more complex models.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Techniques for Generating Embeddings","level":3,"id":"Techniques_for_Generating_Embeddings_0"},{"heading":"Translational Distance Models","level":4,"id":"Translational_Distance_Models_0"},{"heading":"Semantic Matching Models","level":4,"id":"Semantic_Matching_Models_0"},{"heading":"Neural Network Models","level":4,"id":"Neural_Network_Models_0"},{"heading":"Applications of Knowledge Graph Embeddings","level":3,"id":"Applications_of_Knowledge_Graph_Embeddings_0"},{"heading":"Link Prediction","level":4,"id":"Link_Prediction_0"},{"heading":"Entity Resolution","level":4,"id":"Entity_Resolution_0"},{"heading":"Node Classification","level":4,"id":"Node_Classification_0"},{"heading":"Challenges and Considerations","level":3,"id":"Challenges_and_Considerations_0"},{"heading":"Dimensionality and Sparsity","level":4,"id":"Dimensionality_and_Sparsity_0"},{"heading":"Training and Scalability","level":4,"id":"Training_and_Scalability_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158278,"modifiedTime":1737554783000,"sourceSize":3754,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/1. Knowledge Graph Embeddings.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html":{"title":"2. Dynamic and Temporal Knowledge Graphs","icon":"","description":"Dynamic and temporal knowledge graphs are specialized forms of knowledge graphs designed to capture changes over time in the relationships and attributes of entities. Unlike static graphs, these graphs accommodate temporal dimensions, allowing for the representation of how relationships form, evolve, or dissolve across different time periods. This temporal aspect is critical for applications requiring historical context, trend analysis, and predictive modeling.\nTime-stamped Edges: Each relationship (edge) is associated with a timestamp or a time interval, indicating when the relationship is valid.\nTime-specific Entities: Entities that have properties changing over time, captured through successive snapshots or versioning. Temporal Queries: Queries that not only consider the structure and properties of the graph but also the temporal aspect, such as querying the state of the graph at a specific time or changes within a time range. Versioning: Managing different versions of the graph corresponding to different time points or intervals. This approach helps in maintaining a historical record of the graph's evolution.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Key Concepts and Definitions","level":3,"id":"Key_Concepts_and_Definitions_0"},{"heading":"Temporal Data Modeling","level":4,"id":"Temporal_Data_Modeling_0"},{"heading":"Temporal Queries","level":4,"id":"Temporal_Queries_0"},{"heading":"Techniques for Managing Dynamic Data","level":3,"id":"Techniques_for_Managing_Dynamic_Data_0"},{"heading":"Versioning","level":4,"id":"Versioning_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158283,"modifiedTime":1737554783000,"sourceSize":3392,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/2. Dynamic and Temporal Knowledge Graphs.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html":{"title":"3. Scalability and Real-Time Updates","icon":"","description":"Scalability and real-time updates are critical challenges in the management and utilization of knowledge graphs, especially as they grow in size and complexity. Scalability involves the ability of the system to handle increasing amounts of data and queries without degrading performance. Real-time updates refer to the capability of the system to incorporate changes into the knowledge graph efficiently as new data arrives, ensuring that the graph remains current and accurate.\nVertical Scaling: Increasing the capacity of a single server by adding more resources (e.g., CPU, RAM). This is often simpler but can hit physical and cost-effective limits.\nHorizontal Scaling: Adding more servers to distribute the load. This approach is generally more scalable and is supported by technologies like distributed databases and cloud computing. Graph Partitioning: Dividing the graph into smaller, manageable sub-graphs that can be processed independently. Effective partitioning minimizes the number of inter-partition relationships to reduce the overhead of cross-partition queries.\nData Sharding: Distributing data across multiple machines to balance the load and reduce the risk of data hotspots. Indexing: Creating indexes to speed up query performance. Indexes can be designed for specific types of queries that are frequent within the application.\nCaching: Storing frequently accessed data in a faster-access storage layer. This reduces access times and alleviates the load on the primary storage.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Scalability in Knowledge Graphs","level":3,"id":"Scalability_in_Knowledge_Graphs_0"},{"heading":"Vertical and Horizontal Scaling","level":4,"id":"Vertical_and_Horizontal_Scaling_0"},{"heading":"Partitioning Strategies","level":4,"id":"Partitioning_Strategies_0"},{"heading":"Indexing and Caching","level":4,"id":"Indexing_and_Caching_0"},{"heading":"Real-Time Updates in Knowledge Graphs","level":3,"id":"Real-Time_Updates_in_Knowledge_Graphs_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158287,"modifiedTime":1737554783000,"sourceSize":3748,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/3. Scalability and Real-Time Updates.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html":{"title":"0. Feature Selection","icon":"","description":"Feature selection stands as a pivotal component of feature engineering, aiming at identifying the most beneficial subset of features for a given model. Its primary objective is to discard irrelevant or redundant features, which, in turn, leads to a reduction in the feature count, enhancement of model accuracy, and a decrease in runtime. Furthermore, by focusing on genuinely relevant features, the complexity of the model is simplified, enriching the understanding of the data generation process. A common maxim in the field asserts, \"Data and features set the upper limit for machine learning, and models and algorithms merely strive to reach this limit,\" underscoring the critical role of feature selection. Despite its significance, feature selection often lacks a dedicated chapter in machine learning textbooks. Nevertheless, the success of machine learning endeavors is significantly influenced by the adept use of feature engineering. Feature selection is especially pertinent as it addresses the challenge of overfitting—a scenario where a model is too finely tuned to the training data, leading to poor performance on unseen data due to high variance and lackluster generalization capabilities. Overfitting is typically a consequence of an overly complex model for the given training data. Strategies to combat overfitting include:\nAccumulating more data\nImplementing regularization to curb complexity\nOpting for simpler models with fewer parameters\nDiminishing data dimensionality via feature selection or feature extraction\nGiven the difficulty in collecting more data, emphasis often shifts to the latter three strategies, particularly regularization and dimensionality reduction.\nSubset Generation: Initiates the search for potential feature subsets for the evaluation function.\nEvaluation Function: Gauges the utility of the feature subsets.\nStopping Criterion: Tied to the evaluation function, typically a predefined threshold. The search concludes when the evaluation function attains a specific benchmark.\nValidation Process: Confirms the effectiveness of the chosen feature subset using a validation dataset.\nThe challenge amplifies with an increase in the number of features, as the search space expands, necessitating expertise for optimal feature identification. Filter Methods: These methods score each feature based on metrics like diversity or relevance, filtering out features that fail to meet a certain threshold or the desired feature count. Utilizing correlation coefficients to eliminate features uncorrelated with the target variable exemplifies this approach. Wrapper Methods: Features are selected or excluded based on their impact on the target function, often a prediction performance score. Recursive feature elimination (RFE) represents this method well, starting with all features, fitting a model, and sequentially removing the least significant feature until reaching the predetermined feature count. Embedded Methods: This approach involves using machine learning models to both train and derive feature importance scores, selecting features based on these scores. This method is akin to filter methods, albeit with scores derived from model training. An example includes employing a regularization method like Lasso, which can reduce the coefficients of less important features to zero, thereby selecting more relevant features. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"General Process of Feature Selection","level":2,"id":"General_Process_of_Feature_Selection_0"},{"heading":"Three Main Methods of Feature Selection","level":2,"id":"Three_Main_Methods_of_Feature_Selection_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158300,"modifiedTime":1737554783000,"sourceSize":3836,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/0. Feature Selection.md","exportPath":"artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html":{"title":"1. Linear Regression","icon":"","description":"In&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Statistics\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Statistics\" href=\"https://en.wikipedia.org/wiki/Statistics\" target=\"_self\">statistics</a>,&nbsp;linear regression&nbsp;is a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Linearity\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Linearity\" href=\"https://en.wikipedia.org/wiki/Linearity\" target=\"_self\">linear</a>&nbsp;approach for modelling the relationship between a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Scalar_(mathematics)\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Scalar (mathematics)\" href=\"https://en.wikipedia.org/wiki/Scalar_(mathematics)\" target=\"_self\">scalar</a>&nbsp;response and one or more explanatory variables (also known as&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Dependent_and_independent_variables\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Dependent and independent variables\" href=\"https://en.wikipedia.org/wiki/Dependent_and_independent_variables\" target=\"_self\">dependent and independent variables</a>). The case of one explanatory variable is called&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Simple_linear_regression\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Simple linear regression\" href=\"https://en.wikipedia.org/wiki/Simple_linear_regression\" target=\"_self\">simple linear regression</a>; for more than one, the process is called&nbsp;multiple linear regression.\nLinear Regression models the relationship between one dependent variable and multiple explanatory variables.\nThe input to the model includes independent, explanatory, and predictor variables, while the output is a dependent, target, or response variable.\nRegression provides a way to predict a continuous variable given other variables, which are typically also continuous.\nOrdinal independent or dependent variables can be mapped to numeric values, and nominal variables, including binary, can also be mapped to numeric values, but this requires careful assessment of their effect on results.\nTypes of relationships considered in regression include deterministic and statistical relationships.\nA deterministic relationship is one where the relationship between explanatory and dependent variables is perfect.\nA statistical relationship is one where the relationship between explanatory and dependent variables is not perfect.\na statistical method used to study relationships between two continuous variables, where one variable is regarded as the predictor and the other as the response variable. Simple linear regression models this relation between predictor and response variables using the following equation:where (y-intercept or bias) and (slope, or weight) are coefficient parameters, and is the predicted response variable based on the predictor variable .To estimate the proper values of and , we define the sum of square error as:where is the predicted response variable, and the sum of square error computes the sum of the squared differences across all training data points.The performance of the linear regression model can be evaluated using the following methods:\nMean Absolute Error (MAE): the average of the individual errors, each one computed as the absolute value of the difference between the actual value and the predicted value. Root Mean Square Error (RMSE): the square root of the average of squared errors for each value of the dependent variable, where error is the simple difference of the actual value and predicted value. R-squared: the proportion of the dependent variable variance that is explained by the linear model. It is equivalently the square of the Pearson's correlation between the predicted values and the actual values.\nIn addition, plotting the predictions vs observations on a Cartesian plane and visually assessing if the points appear to line up can also be useful in evaluating the performance of the model. All of these approaches should be applied to test data rather than training data for a proper assessment, and multifold cross-validation may be applied using the quantitative performance measures above in place of accuracy.In many applications, it is more general to have more than one predictor variable.Multiple linear regression is a direct extension of the linear regression model. Let's assume that we have n predictor variables for each response variable, i.e., .In the multiple linear regression model, the relation between a response variable and predictor variables are modelled as:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Details:","level":3,"id":"Details_0"},{"heading":"Simple Linear Regression","level":3,"id":"Simple_Linear_Regression_0"},{"heading":"Multiple Linear Regression","level":4,"id":"Multiple_Linear_Regression_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158304,"modifiedTime":1737554783000,"sourceSize":6456,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/1. Linear Regression.md","exportPath":"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html":{"title":"2. Logistic Regression","icon":"","description":"Logistic regression is a popular statistical model used for binary classification problems. It is commonly used when the dependent variable is categorical, taking on two possible outcomes (e.g., yes/no, true/false, 0/1). Logistic regression allows us to predict the probability of an observation belonging to a specific class based on one or more independent variables. Logistic Function (Sigmoid): Logistic regression uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the outcome. The logistic function maps any real-valued number to a value between 0 and 1, representing the probability. Logit Transformation: The logit transformation is the inverse of the logistic function. It transforms the probability of the outcome to a linear function of the independent variables. The logit transformation allows us to model the relationship between the independent variables and the log-odds (logarithm of the odds) of the outcome. Maximum Likelihood Estimation (MLE): Logistic regression estimates the model parameters using the maximum likelihood estimation method. The goal is to find the parameter values that maximize the likelihood of observing the given data. The MLE method calculates the probabilities of observing the actual outcomes and adjusts the parameter values to increase the likelihood of the observed data. <a data-href=\"1. Maximum Likelihood Estimation (MLE)\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Maximum Likelihood Estimation (MLE)</a> Odds Ratio: In logistic regression, the odds ratio is used to interpret the effect of independent variables on the probability of the outcome. It represents the change in odds for a one-unit increase in the independent variable. An odds ratio greater than 1 indicates a positive effect, while an odds ratio less than 1 indicates a negative effect. Logistic Function (Sigmoid): Logit Transformation:\n3. Logistic Regression Hypothesis:\nCost Function (Negative Log-Likelihood):\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Key Concepts","level":3,"id":"Key_Concepts_0"},{"heading":"Formula","level":3,"id":"Formula_0"}],"links":[".html"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158339,"modifiedTime":1737554783000,"sourceSize":5800,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/2. Logistic Regression.md","exportPath":"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html":{"title":"3. K-nearest Neighbors","icon":"","description":"K-nearest Neighbors (KNN) is a popular supervised learning algorithm used for both classification and regression tasks. It is a non-parametric method that makes predictions based on the similarity of data points in the feature space. The basic idea behind KNN is to find the K nearest neighbors of a given data point and use their labels (in the case of classification) or values (in the case of regression) to make predictions.In the k-nearest neighbors (k-NN) algorithm, the training examples are vectors in a multidimensional feature space, each with a class label. During the training phase, the algorithm stores the feature vectors and class labels of the training samples. In the classification phase, an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.The value of k is a user-defined constant, and a commonly used distance metric for continuous variables is Euclidean distance: For discrete variables, such as text classification, the overlap metric (or Hamming distance) can be used. In other contexts, such as gene expression microarray data, correlation coefficients like Pearson and Spearman can be employed as a metric for k-NN. The classification accuracy of k-NN can be improved by learning the distance metric with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.However, a drawback of the basic \"majority voting\" classification is that it can be skewed by class imbalance. This means that examples of a more frequent class tend to dominate the prediction of the new example because they tend to be common among the k nearest neighbors due to their large number. To overcome this problem, one solution is to weight the classification by taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another solution is abstraction in data representation, where each node in a self-organizing map (SOM) is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. k-NN can then be applied to the SOM.The k-NN algorithm is known as a \"lazy learner\" because it doesn't build a model or make generalizations during training. Instead, it memorizes the training data and uses it during classification. This makes it computationally inexpensive during the training phase, but potentially slower during the classification phase as it has to compare the test point to all training points.Use various evaluation metrics depending on the problem type (classification or regression) and the specific requirements of your task. Here are some common evaluation techniques for KNN:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Evaluate the performance","level":3,"id":"Evaluate_the_performance_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158343,"modifiedTime":1737554783000,"sourceSize":9029,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/3. K-nearest Neighbors.md","exportPath":"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html":{"title":"4. Naive Bayesian Classifier","icon":"","description":"The Naive Bayes Classifier is a probabilistic machine learning algorithm that is commonly used for classification tasks. It is based on Bayes' theorem and assumes that the features are conditionally independent given the class. Despite its simplicity, Naive Bayes can be very effective in various domains, especially with high-dimensional data.The Naive Bayesian Classifier assumes that the features used in the classification task are independent of each other, which is a strong assumption and is not always true in practice. However, despite this limitation, the Naive Bayesian Classifier is still widely used due to its simplicity and good performance on many tasks.The Naive Bayesian Classifier works by calculating the probability of a class given a set of features. It does this by calculating the prior probability of each class, which is the probability of each class occurring in the dataset, and the likelihood of each feature given each class, which is the probability of each feature occurring in each class.Using Bayes theorem, the Naive Bayesian Classifier calculates the posterior probability of each class given the features, and selects the class with the highest probability as the predicted class.The Naive Bayesian Classifier is trained using a labeled dataset, where each instance is labeled with a class. The algorithm calculates the prior probability and likelihood of each feature given each class, based on the frequency of occurrence of each feature in each class.The Naive Bayesian Classifier is commonly used in text classification tasks, such as spam filtering and sentiment analysis. It can also be used in other classification tasks where the features are assumed to be independent, such as medical diagnosis and fraud detection.The Naive Bayesian Classifier's assumption of feature independence is a strong limitation and can lead to inaccurate predictions if this assumption is not met. In addition, the algorithm requires a large amount of training data to accurately estimate the probabilities, and may suffer from overfitting if the dataset is too small.The Naive Bayes classifier makes a naive assumption that the input features are conditionally independent given the class, which allows us to simplify the likelihood term as:where:\nxi is the ith input feature.\nTherefore, the Naive Bayes classifier can be written as:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"},{"heading":"Formula","level":3,"id":"Formula_0"},{"heading":"Steps:","level":3,"id":"Steps_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158347,"modifiedTime":1737554783000,"sourceSize":7408,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/4. Naive Bayesian Classifier.md","exportPath":"artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html":{"title":"5. Support Vector Machine","icon":"","description":"SVMs are one of the most successful classification methods for both linear and nonlinear data. It uses a nonlinear mapping to transform the original training data into a higher dimension. With the new dimension, it searches for the linear optimal separating hyperplane (i.e., “decision boundary”) and with an appropriate nonlinear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane. SVM finds this hyperplane using support vectors (“essential” training tuples) and margins (defined by the support vectors).History and ApplicationsVapnik and colleagues introduced the Support Vector Machine in 1992. The groundwork for SVM was laid by Vapnik &amp; Chervonenkis’ statistical learning theory in the 1960s. SVM's features include slow training but high accuracy owing to their ability to model complex nonlinear decision boundaries (margin maximisation). SVMs can be used for classification and numeric prediction (regression). Some applications of SVM include handwritten digit recognition, object recognition, speaker identification, and benchmarking time-series prediction tests.AlgorithmsTraining of SVM can be distinguished by:\nLinearly separable case\nNon-linearly separable case\nLinearly separable case refers to the scenario where two classes of data can be completely separated by a straight line or hyperplane in the feature space. That is, there exists a hyperplane that can perfectly separate the two classes of data with a margin, which is the distance between the hyperplane and the closest data points from each class.“lssvm.png” could not be found.In the context of Support Vector Machines (SVMs), the linearly separable case is a scenario where the SVM can effectively learn the decision boundary separating the two classes by maximizing the margin between them.SVMs work by identifying the optimal hyperplane that separates the two classes in the training data with the largest margin possible. In the linearly separable case, the SVM algorithm can find a unique hyperplane that can perfectly separate the two classes of data.Two-dimensional training tuple case:\nIn two-dimensional space ( plane), a hyperplane corresponds to a line, and every hyperplane can be written as:\nFor a more general representation, if we replace and by and , then the above hyperplane can be rewritten as:\nwhere , , .We can represent any hyperplane (line) in two-dimensional space with , , and .In the linearly separable case, every training tuple satisfies the following condition:\nH1 (positive class):\nIf H2 (negative class):\nIf Support vector:\nTherefore, every training tuple that satisfies is a support vector.N-dimensional training tuple case:\nLet be a training tuple with class label . Then a separating hyperplane can be written as:\nwhere is a weight vector and is a scalar (bias).\nThe hyperplane defining the sides of the margin:\nH1: for , and\nH2: for These two equations can be combined into one equation:\nThis equation can be solved as a constrained (convex) quadratic optimization problem that maximizes the margins to estimate the weights from the training set, and is the SVM version of training the model.Classify test tuple using trained model:\nDuring the testing phase, the trained model classifies a new tuple using the rules:Using hyperplane:\nH1 (positive class):\nIf Then will be classified as a positive class.\nH2 (negative class):\nIf Then will be classified as a negative class.Alternatively, we can use the support vectors , each with class label , to classify test tuples. For test tuple ,\nwhere is the number of support vectors, and and are automatically determined by the optimization/training algorithm.If the sign of is positive then is classified as , otherwise .Note that we need to keep only the support vectors for testing. This fact will be used in the non-linearly separable case.\nComplexity: The complexity of a trained SVM classifier is characterized by the number of support vectors rather than the dimensionality of the data. Support vectors are the essential or critical training examples that lie closest to the decision boundary (i.e., the maximum margin hyperplane). If all other training examples are removed and the training is repeated, the same separating hyperplane would be found from the support vectors alone.\nGeneralization: The number of support vectors found can be used to compute an upper bound on the expected error rate of the SVM classifier, which is independent of the data dimensionality. Thus, an SVM with a small number of support vectors can have good generalization, even when the dimensionality of the data is high.\nNon-linear mapping: SVMs are effective on high-dimensional data because they use a non-linear mapping to transform the original training data into a higher dimension. With an appropriate non-linear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane.\nMargin maximization: SVMs are designed to maximize the margin between the classes. This helps to prevent overfitting and allows the SVM to generalize well to new data.\nIn some cases, the data may not be linearly separable, meaning it's not possible to draw a hyperplane that perfectly separates the two classes. In this case, we need to allow for some misclassification errors in order to find a separating hyperplane. This is done by introducing a slack variable ξ_i for each training example, which allows it to be on the wrong side of the hyperplane with a penalty.“dataset_nonsep.png” could not be found.The objective of the SVM is to minimize the sum of the slack variables, subject to the constraint that all training examples are classified correctly (or as correctly as possible) by the hyperplane. This leads to a modified optimization problem, where the goal is to minimize the following equation:\nHere, C is a hyperparameter that determines the trade-off between minimizing the sum of the slack variables and maximizing the margin. A larger value of C results in a more complex model that tries to minimize misclassification errors as much as possible. The solution to this optimization problem leads to a hyperplane that may still misclassify some training examples, but does so with the smallest possible sum of slack variables.In practice, we can use a kernel function to map the input data into a higher-dimensional space, where it may become separable. The kernel function allows us to perform the same optimization on the transformed data, without explicitly computing the transformation. In this case, the hyperplane we find is in the higher-dimensional space, but can be projected back to the original space using the kernel function.Kernel TrickThe idea is to obtain linear separation by&nbsp;mapping the data to a higher dimensional space. Let's see the example first:“data_2d_to_3d.png” could not be found.\nLeft: A dataset in , not linearly separable. Right: The same dataset transformed by the transformation: “data_2d_to_3d_hyperplane.png” could not be found.Hyperplane (green plane) that linearly separates two classes in the higher dimensional space.In the above example, we can train a linear SVM classifier that successfully finds a good decision boundary in .However, we are given the dataset in . The challenge is to find a transformation : , such that the transformed dataset is linearly separable in . For example, consider the transformation function that maps a 2-dimensional space to a 3-dimensional space: , which, after applied to every point in the original tuples yields the linearly separable dataset in our example.The decision rule used in the linearly separable case, for test vector X was Now it is converted, in the higher-dimensional space to But all these dot-products over potentially very high-dimensional vectors are expensive.\nWe define a kernel function on the data in the original lower-dimensional space:\nLet , be vectors. Then Now, every can be replaced by K(X_i, X_j) in the training algorithm and decision rule. We do not need to make calculations over the higher-dimensional transformed vectors. We do not even need a formulation of , as a kernel function is enough for determining the SVM. But we do need the kernel function to have certain properties, so that not just any function will do.\nHere are some widely used kernel functions. They do result in different classifiers, although they are commonly of similar accuracy and the only way to find the best one is to try it.Polynomial kernel of degree h: Gaussian radial basis function kernel: Sigmoid Kernel: Linear Kernel: Decision hyperplanes&nbsp;found by non-linear SVM are similar to those found by neural network classifiers. e.g. SVM with Gaussian radial basis function is the same as a radial basis function neural network. e.g. A sigmoid kernel SVM is the same as a two-layer neural network (i.e with no hidden layers). Neural Network: Nondeterministic algorithm which finds local minima Generalises well but doesn't have strong mathematical foundation Can easily be learned in incremental fashion To learn complex functions, use a&nbsp; multi-layer neural network, but adding more layers adds more training time Gets more complex with higher dimensions and can easily overfit. SVM Deterministic algorithm that finds the global minimum Nice generalisation property. Tends not to overfit. Hard to learn - learned in batch mode using quadratic programming techniques Using kernels can learn very complex functions Well suited to high-dimensional data import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm # Create the dataset\nX = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 2], [6, 1]])\ny = np.array([1, 1, 1, -1, -1, -1]) # Create and fit the SVM model\nmodel = svm.SVC(kernel='linear')\nmodel.fit(X, y) # Plot the decision boundary\nw = model.coef_[0]\nb = model.intercept_[0]\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('SVM Classifier')\nplt.show()\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Linearly Separable Case","level":3,"id":"Linearly_Separable_Case_0"},{"heading":"Hyperplanes and support vectors:","level":3,"id":"Hyperplanes_and_support_vectors_0"},{"heading":"Support vector machines (SVMs) are effective on high-dimensional data for several reasons:","level":4,"id":"Support_vector_machines_(SVMs)_are_effective_on_high-dimensional_data_for_several_reasons_0"},{"heading":"Linearly Inseparable Case","level":3,"id":"Linearly_Inseparable_Case_0"},{"heading":"Kernels","level":4,"id":"Kernels_0"},{"heading":"Comparison between neural network and SVM","level":3,"id":"Comparison_between_neural_network_and_SVM_0"},{"heading":"Extra","level":4,"id":"Extra_0"},{"heading":"Code:","level":4,"id":"Code_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158351,"modifiedTime":1737554783000,"sourceSize":13260,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/5. Support Vector Machine.md","exportPath":"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html":{"title":"6. Decision Tree","icon":"","description":"Decision Trees are non-parametric supervised machine learning algorithms used for both classification and regression tasks. A decision tree is built through a process called recursive partitioning, where the data is repeatedly split into subsets based on the values of one of its features, such that the resulting subsets are as homogeneous as possible in terms of the target variable.A decision tree consists of a root node, which represents the entire dataset, and a series of decision nodes and leaf nodes that represent the subsets of the data. At each decision node, the algorithm chooses a feature to split the data and assigns a threshold value to that feature. The data is then partitioned into subsets based on the value of the feature relative to the threshold. This process is repeated recursively until each subset is homogeneous, or a predefined stopping criterion is met.The goal of training a decision tree is to create a tree that can accurately predict the target variable. The training process involves recursively splitting the dataset into smaller subsets, selecting the best feature to split on at each node, and continuing this process until each leaf node is pure or until a stopping criterion is met.Decision trees are widely used in various fields such as finance, marketing, healthcare, and engineering for classification and prediction tasks. Some examples of applications of decision trees include customer segmentation, fraud detection, disease diagnosis, and fault detection in industrial processes.One of the main limitations of decision trees is that they are prone to overfitting, especially when the tree is deep or when there is noise in the data. Overfitting occurs when the model is too complex and captures noise in the data rather than the underlying patterns. Additionally, decision trees are sensitive to the order of the features and may produce different trees depending on the order in which the features are considered. Finally, decision trees can be biased towards features with a large number of categories or values, which can lead to an unbalanced tree.The general algorithm for decision tree induction is as follows:\nSelect the best attribute to split the dataset based on some measure of impurity, such as entropy or Gini index.\nSplit the dataset into subsets based on the selected attribute.\nRecursively apply steps 1 and 2 to each subset until all instances in a subset belong to the same class or have the same attribute values.\n“structure-of-a-decision-tree.png” could not be found.Decision trees can be used for both classification and regression problems. In classification problems, the class label is a categorical variable, and in regression problems, the target variable is a continuous variable.Decision trees have several advantages, such as being easy to interpret and visualize, handling both continuous and categorical variables, and handling missing values. However, they also have some limitations, such as being prone to overfitting and being sensitive to the training data.The following are some commonly used formulas in decision tree algorithms: Entropy: a measure of impurity in a set of examples. Information gain: the expected reduction in entropy by splitting a set of examples based on an attribute. Gini index: a measure of impurity in a set of examples, similar to entropy.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158355,"modifiedTime":1737554783000,"sourceSize":3882,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/6. Decision Tree.md","exportPath":"artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/7.-random-forest.html":{"title":"7. Random Forest","icon":"","description":"Random Forest is a popular machine learning algorithm used for both classification and regression tasks. It is an ensemble method that combines multiple decision trees to create a more accurate and stable model.The algorithm works by constructing a large number of decision trees during training and using them to make predictions. Each decision tree is constructed using a different subset of the training data, and a random subset of the features. This introduces randomness and diversity in the model, making it less prone to overfitting and more robust to noisy data.To make a prediction for a new sample, the algorithm passes it through all the decision trees in the forest and takes the average (for regression) or the mode (for classification) of the outputs as the final prediction.\nData Preparation: Prepare the dataset by cleaning, transforming, and encoding the features and labels.\nBootstrap Sampling: Randomly select a subset of the data (with replacement) to build a decision tree.\nFeature Selection: Randomly select a subset of features to use for each decision tree.\nDecision Tree Building: Build a decision tree using the selected data and features.\nRepeat: Repeat steps 2-4 to build multiple decision trees.\nPrediction: For each test sample, predict the outcome by taking the majority vote of the predictions from all the decision trees.\nEvaluate: Evaluate the performance of the Random Forest model using appropriate metrics such as accuracy, precision, recall, and F1 score.\nOptimize: Optimize the model by adjusting hyperparameters such as the number of trees, the maximum depth of each tree, and the minimum number of samples required to split a node.\nTest: Test the optimized model on new data to ensure that it generalizes well and does not overfit.\n“Random_forest_diagram_complete.png” could not be found.Some of the advantages of Random Forest are:\nIt is relatively easy to use and can handle both categorical and numerical data.\nIt can handle high dimensional data and large datasets.\nIt is robust to outliers and missing values.\nIt can provide feature importance scores, indicating which features are most important for making predictions.\nHowever, some of the limitations of Random Forest are:\nIt may not perform well on imbalanced datasets.\nIt may not capture complex relationships between features.\nIt can be computationally expensive to train on large datasets.\nIt may not perform well on datasets with a large number of irrelevant features.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Steps:","level":3,"id":"Steps_0"},{"heading":"Advantages and Limitations","level":3,"id":"Advantages_and_Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/7.-random-forest.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158360,"modifiedTime":1737554783000,"sourceSize":2922,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/7. Random Forest.md","exportPath":"artificial-intelligence/machine-learning/algorithm/7.-random-forest.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/8.-adaboost.html":{"title":"8. AdaBoost","icon":"","description":"AdaBoost (Adaptive Boosting) is a machine learning algorithm that combines multiple weak classifiers to create a strong classifier. It is particularly effective in binary classification tasks.The main idea behind AdaBoost is to iteratively train a sequence of classifiers, where each subsequent classifier focuses more on the misclassified samples from previous iterations.Steps in the AdaBoost algorithm:\nInitialize weights for each training sample. Initially, all weights are set equally to 1/N, where N is the number of samples.\nFor each iteration: a. Train a weak classifier on the training data using the current weights. b. Evaluate the classifier's performance on the training data. c. Calculate the weighted error rate of the classifier, which measures how well it classifies the samples. d. Compute the weight of the classifier based on its error rate. e. Update the weights of the training samples, giving higher weights to the misclassified samples. f. Normalize the weights to ensure they sum up to 1. Repeat the above steps for a fixed number of iterations or until a certain stopping criterion is met.\nCombine the individual weak classifiers into a strong classifier by assigning weights to each classifier based on their performance.\nTo make predictions on new data, the AdaBoost classifier combines the predictions of all the weak classifiers using their assigned weights.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/8.-adaboost.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158363,"modifiedTime":1737554783000,"sourceSize":1595,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/8. AdaBoost.md","exportPath":"artificial-intelligence/machine-learning/algorithm/8.-adaboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/8.-k-means.html":{"title":"8. K means","icon":"","description":"K-Means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into distinct, non-overlapping groups or clusters. It is widely applied in various domains such as image compression, customer segmentation, and anomaly detection. The essence of K-Means lies in minimizing the variance within each cluster while maximizing the variance between different clusters.\nCentroid: The center point of a cluster, represented by the mean of all data points within that cluster.\nCluster: A group of data points that are more similar to each other than to those in other clusters. Initialization: Choose the number of clusters (K) and initialize K centroids randomly.\nAssignment Step (Expectation Step): Assign each data point to the nearest centroid, forming clusters based on proximity.\nUpdate Step (Maximization Step): Recalculate centroids for each cluster based on the current assignments.\nConvergence Check: Repeat the assignment and update steps until centroids stabilize or a specified number of iterations is reached.\nResult: Formation of K clusters, with each data point belonging to a cluster. K: The number of clusters, a critical parameter that needs specification before running the algorithm. Initialization Sensitivity: The final results can be affected by the initial placement of centroids.\nOutliers: The algorithm is sensitive to outliers, which can influence the position of centroids.\nDeterministic: It produces consistent results for a given dataset and initializations, which might be a limitation for complex datasets. Customer Segmentation, Image Compression, Anomaly Detection, Document Classification, Genetic Clustering: Illustrate the algorithm's versatility and effectiveness in various fields. K-Means++: Enhances the initialization process to reduce the impact of initial centroid selection on clustering outcomes. It prefers points that are farther away from existing centroids for the selection of the next centroid. Mini-Batch K-Means: Utilizes batches of data to update centroids, significantly improving computational efficiency on large datasets. Elkan K-Means: Reduces unnecessary distance calculations through the triangle inequality, enhancing algorithm efficiency. ISODATA Algorithm: An iterative self-organizing data analysis technique that allows for dynamic adjustment of the number of clusters during the iteration process. Preprocessing: Cleanse the data by removing or correcting outliers before applying K-Means. Kernel K-Means: Uses kernel techniques to map data into a higher-dimensional space, thereby reducing the impact of outliers. Robust Clustering Methods: Such as density-based clustering algorithms (DBSCAN) or distribution-based clustering algorithms (Gaussian Mixture Models), which are less sensitive to outliers. Silhouette Coefficient: Assesses cluster cohesion versus separation, aiming for higher values.\nDavies-Bouldin Index: Prefers lower values indicating better clustering.\nCalinski-Harabasz Index: Higher values suggest more distinct clustering.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Key Concepts","level":4,"id":"Key_Concepts_0"},{"heading":"Algorithm Steps","level":4,"id":"Algorithm_Steps_0"},{"heading":"Parameters","level":4,"id":"Parameters_0"},{"heading":"Challenges","level":4,"id":"Challenges_0"},{"heading":"Applications","level":4,"id":"Applications_0"},{"heading":"Improvements to the K-Means Algorithm","level":3,"id":"Improvements_to_the_K-Means_Algorithm_0"},{"heading":"Handling Outliers","level":3,"id":"Handling_Outliers_0"},{"heading":"Metrics for Evaluating the Algorithm","level":3,"id":"Metrics_for_Evaluating_the_Algorithm_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/8.-k-means.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158367,"modifiedTime":1737554783000,"sourceSize":3791,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/8. K means.md","exportPath":"artificial-intelligence/machine-learning/algorithm/8.-k-means.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html":{"title":"9. Ensemble Learning","icon":"","description":"Ensemble Learning is a powerful machine learning paradigm that involves combining the predictions from multiple models to improve the overall performance, robustness, and accuracy of predictions. This technique is particularly effective in reducing variance, bias, or improving predictions in the face of noisy datasets. Ensemble methods are widely used across various applications, from predictive modeling and classification tasks to feature selection and anomaly detection. Here, we delve into the intricacies of ensemble learning, covering its fundamental principles, key methods, practical considerations, and implementation insights. Diversity Among Models: Ensemble learning relies on the principle that a group of weak learners can come together to form a strong learner. The diversity among the base models is crucial because it enables the ensemble to capture various aspects of the data, reducing the overall error. This diversity can be introduced by using different algorithms, training models on different subsets of the data, or by varying the initial conditions of the same algorithm. Error Reduction Techniques: Ensemble methods aim to reduce two main types of errors: bias and variance, along with reducing overfitting. Bias is the error due to overly simplistic assumptions in the learning algorithm. Variance is the error due to too much complexity in the learning process. Ensemble methods balance these errors to improve the model's prediction accuracy. Weighted Voting/Averaging: In ensemble learning, the predictions from individual models can be combined through simple majority voting (for classification) or averaging (for regression). More sophisticated approaches assign weights to the predictions from each model, based on their performance, to make a weighted vote or average. Bagging (Bootstrap Aggregating): Bagging involves training multiple instances of the same algorithm on different subsets of the training data, selected with replacement. This method aims to reduce variance and is effective for algorithms with high variance. Random Forest is a popular bagging ensemble of decision trees. Boosting: Boosting methods train models sequentially, where each new model focuses on the errors of the previous ones. The goal is to reduce both bias and variance. Examples include AdaBoost, Gradient Boosting, and XGBoost, which adjust the weights of incorrectly predicted instances so that subsequent models pay more attention to them. Stacking (Stacked Generalization): Stacking involves training multiple different models and then training a meta-model to combine their predictions. The base level models are trained on the full dataset, and their predictions are used as inputs to the meta-model to make the final prediction. Model Selection: The choice of base models in an ensemble is critical. It is often beneficial to combine models of different types or architectures to introduce diversity in the predictions. Computational Cost: Training multiple models can be computationally intensive. Efficient resource management and parallel computing techniques can help mitigate these costs. Overfitting: While ensemble methods generally help reduce overfitting by combining multiple models, it's possible to overfit the training data if the ensemble is too complex or if the models are too correlated. Hyperparameter Tuning: Tuning the hyperparameters of the ensemble method itself, along with those of the base models, can significantly affect performance. Techniques like grid search and random search are commonly used for this purpose. Scikit-learn: Provides straightforward implementations for various ensemble methods, including RandomForestClassifier, GradientBoostingClassifier, and VotingClassifier. It's suitable for both beginners and experienced practitioners to experiment with ensemble learning techniques. XGBoost: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. LightGBM: A gradient boosting framework that uses tree-based learning algorithms and is designed for distributed and efficient training, particularly with large datasets. CatBoost: An algorithm for gradient boosting on decision trees, designed to handle categorical variables well and provide robust, scalable training. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Fundamental Principles","level":3,"id":"Fundamental_Principles_0"},{"heading":"Key Ensemble Methods","level":3,"id":"Key_Ensemble_Methods_0"},{"heading":"Practical Considerations","level":3,"id":"Practical_Considerations_0"},{"heading":"Implementation with Python Libraries","level":3,"id":"Implementation_with_Python_Libraries_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158371,"modifiedTime":1737554783000,"sourceSize":5095,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/9. Ensemble Learning.md","exportPath":"artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/9.-k-means.html":{"title":"9. K means","icon":"","description":"K-Means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into distinct, non-overlapping groups or clusters. Each data point in the dataset belongs to one of the clusters, and the goal is to minimize the variance within each cluster while maximizing the variance between different clusters. K-Means is widely used in various applications such as image compression, customer segmentation, and anomaly detection.Key Concepts: Centroid: K-Means revolves around the concept of centroids. A centroid represents the center point of a cluster, and it is the mean of all data points within that cluster. Cluster: A cluster is a group of data points that are similar to each other. The goal is to group data points into clusters in such a way that data points within the same cluster are more similar to each other than to those in other clusters. Algorithm Steps: Initialization: Choose the number of clusters (K) that you want to create.\nInitialize K centroids randomly. These centroids serve as the initial cluster centers. Assignment Step (Expectation Step): For each data point in the dataset, calculate the distance (typically Euclidean distance) between that data point and each centroid.\nAssign the data point to the cluster represented by the nearest centroid. This step creates clusters based on proximity. Update Step (Maximization Step): Recalculate the centroids of each cluster by taking the mean of all data points assigned to that cluster.\nThe centroids represent the new cluster centers. Convergence Check: Repeat the Assignment and Update steps until one of the convergence criteria is met: Centroids no longer change significantly.\nA specified number of iterations have been completed. Result: Once the algorithm converges, you have K clusters, and each data point belongs to one of these clusters. Parameters:\nK: The number of clusters is a crucial parameter. It must be specified before running the algorithm. An inappropriate choice of K can lead to suboptimal results.\nChallenges: Initialization Sensitivity: The initial placement of centroids can affect the final results. K-Means can converge to a local minimum, so multiple runs with different initializations may be necessary. Outliers: K-Means is sensitive to outliers because it minimizes the variance within clusters. Outliers can disproportionately influence the position of centroids. Deterministic: K-Means is a deterministic algorithm, meaning it will always produce the same results for a given dataset and initializations. This can be a limitation when dealing with complex data. Applications:K-Means clustering has various practical applications, including:\nCustomer Segmentation: Segmenting customers based on their purchasing behavior.\nImage Compression: Reducing the number of colors in an image while preserving its quality.\nAnomaly Detection: Identifying unusual patterns or outliers in data.\nDocument Classification: Grouping similar documents together.\nGenetic Clustering: Identifying similar gene expression patterns.\nIn summary, K-Means clustering is a powerful and widely used technique for unsupervised data analysis and clustering. It's relatively simple to implement, interpretable, and versatile for various applications. However, its performance can depend on the choice of K and the quality of the initial centroids.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/9.-k-means.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158375,"modifiedTime":1737554783000,"sourceSize":3647,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/9. K means.md","exportPath":"artificial-intelligence/machine-learning/algorithm/9.-k-means.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/10.-gbdt.html":{"title":"10. GBDT","icon":"","description":"Gradient Boosting Decision Trees (GBDT) is an ensemble machine learning algorithm renowned for its effectiveness in both regression and classification tasks. It builds upon the concept of boosting, where multiple weak learners (typically decision trees) are combined to form a strong learner. The core idea behind GBDT is to iteratively add trees to the model, where each new tree corrects the errors made by the ensemble of previously added trees. This iterative correction is guided by the gradient of the loss function, hence the name \"Gradient Boosting.\"GBDT models are particularly valued for their ability to handle various types of data, including numerical and categorical features, and their robustness to overfitting when appropriately regularized and tuned. The algorithm's flexibility allows it to be applied to a wide range of data science problems, making it a staple in many machine learning practitioners' toolkits.Despite its many advantages, the traditional GBDT algorithm can be computationally intensive, especially with large datasets, due to the sequential nature of boosting. It also requires careful tuning of parameters such as the number of trees, tree depth, and learning rate to achieve optimal performance.XGBoost incorporates a more sophisticated approach to regularization than traditional GBDT. Alongside the loss function used to measure the difference between the predicted and actual values, XGBoost adds regularization terms directly into the objective function it optimizes. Objective Function in XGBoost:\nWhere is the traditional loss function (such as squared loss for regression or logistic loss for classification), and represents the regularization term. The regularization term includes both the L1 norm (lasso regression) and the L2 norm (ridge regression) of the model parameters. This helps in controlling the model's complexity and prevents overfitting. Regularization in Traditional GBDT: Traditional GBDT algorithms focus on minimizing the loss function without explicitly including regularization terms in the objective function. While some form of regularization might be achieved through hyperparameters (such as tree depth or learning rate), it's not as directly controlled as in XGBoost. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Comparison: Boosting, GBDT, and XGBoost","level":3,"id":"Comparison_Boosting,_GBDT,_and_XGBoost_0"},{"heading":"Loss Function Regularization","level":3,"id":"Loss_Function_Regularization_0"},{"heading":"Optimization Technique","level":3,"id":"Optimization_Technique_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/10.-gbdt.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158307,"modifiedTime":1737554783000,"sourceSize":6338,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/10. GBDT.md","exportPath":"artificial-intelligence/machine-learning/algorithm/10.-gbdt.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/11.-adaboost.html":{"title":"11. AdaBoost","icon":"","description":"AdaBoost (Adaptive Boosting) is a machine learning algorithm that combines multiple weak classifiers to create a strong classifier. It is particularly effective in binary classification tasks.The main idea behind AdaBoost is to iteratively train a sequence of classifiers, where each subsequent classifier focuses more on the misclassified samples from previous iterations.Steps in the AdaBoost algorithm:\nInitialize weights for each training sample. Initially, all weights are set equally to 1/N, where N is the number of samples.\nFor each iteration: a. Train a weak classifier on the training data using the current weights. b. Evaluate the classifier's performance on the training data. c. Calculate the weighted error rate of the classifier, which measures how well it classifies the samples. d. Compute the weight of the classifier based on its error rate. e. Update the weights of the training samples, giving higher weights to the misclassified samples. f. Normalize the weights to ensure they sum up to 1. Repeat the above steps for a fixed number of iterations or until a certain stopping criterion is met.\nCombine the individual weak classifiers into a strong classifier by assigning weights to each classifier based on their performance.\nTo make predictions on new data, the AdaBoost classifier combines the predictions of all the weak classifiers using their assigned weights.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/11.-adaboost.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158311,"modifiedTime":1737554783000,"sourceSize":1595,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/11. AdaBoost.md","exportPath":"artificial-intelligence/machine-learning/algorithm/11.-adaboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/12.-xgboost.html":{"title":"12. XGBoost","icon":"","description":"XGBoost, standing for eXtreme Gradient Boosting, is a highly efficient and scalable implementation of gradient boosted trees designed for speed and performance. It's a library that provides a powerful, flexible, and portable machine learning algorithm capable of solving a wide range of problems, particularly in structured or tabular data. XGBoost has gained popularity for its performance in various machine learning competitions and its versatility in handling regression, classification, ranking, and user-defined prediction problems. Here's a detailed exploration of XGBoost, covering its key features, core concepts, advantages, and practical considerations. Gradient Boosting Framework: XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. Gradient boosting is an ensemble technique that constructs new models that predict the residuals or errors of prior models and then combines them together to make the final prediction. Regularization: Unlike traditional gradient boosting methods, XGBoost incorporates L1 (lasso regression) and L2 (ridge regression) regularization terms in the loss function to prevent overfitting. This regularization aspect is a key differentiator that improves the model's generalization capabilities. Tree Pruning: XGBoost uses a depth-first approach and prunes trees backwards. Unlike the traditional method of growing trees greedily, XGBoost grows to the maximum depth specified and then prunes back the splits that have little gain, which results in more optimal and efficient trees. Handling of Missing Values: XGBoost has an in-built routine to handle missing values. When the model encounters a missing value at a split, it assigns a direction (to go left or right) to the missing values depending on which choice leads to a higher gain. System Optimization: XGBoost is designed for computational efficiency and performance. It implements several system optimizations including cache awareness by allocating internal buffers in each thread to store gradient statistics, and block structure for parallel learning. Parallel and Distributed Computing: XGBoost can utilize multi-threading on a single machine and distributed computing across multiple machines, speeding up the training process significantly. This makes it highly scalable to large datasets. Cross-validation: XGBoost includes a built-in cross-validation method at each iteration, making it easier to get accurate performance estimates as the model builds. Performance: XGBoost often delivers superior performance on a wide range of problems, partly due to its efficient handling of sparse data and its ability to capture complex patterns in the data. Scalability: Its optimizations for hardware and efficient distributed computing make XGBoost scalable to large datasets. Flexibility: XGBoost supports various objective functions and evaluation criteria, making it adaptable to a wide range of regression, classification, and ranking problems. Portability: XGBoost runs on many platforms, including Linux, Windows, and macOS. It supports major programming languages like Python, R, Java, Scala, and Julia, making it accessible to a wide audience. Parameter Tuning: XGBoost comes with a number of hyperparameters that control the model's complexity, the degree of fit to the training data, and the computational efficiency of the learning process. Key parameters include max_depth for the depth of the trees, eta (learning rate), subsample and colsample_bytree for the fraction of samples and features to use for each tree, and regularization parameters lambda and alpha. Overfitting: Despite its regularization capabilities, careful cross-validation and parameter tuning are essential to avoid overfitting, especially when working with small datasets. Computational Resources: While XGBoost is designed for efficiency, training large models on very large datasets can be computationally intensive and may require significant memory and processing power, especially in a distributed setting. XGBoost's objective function is a combination of a loss function and a regularization term. The objective function to be minimized is given by:where ( \\Theta ) represents the parameters of the model, ( L(\\Theta) ) is the loss function that measures how well the model's predictions match the actual data, and ( \\Omega(\\Theta) ) is the regularization term used to control the model's complexity and prevent overfitting.The loss function ( L(\\Theta) ) depends on the specific problem being solved (e.g., regression, classification). For a regression problem with squared loss, the loss function can be:where ( ) is the actual value and ( ) is the predicted value for the ( i )-th instance.For binary classification problems, a common choice is the logistic loss:The regularization term ( \\Omega(\\Theta) ) is used to penalize complex models. In XGBoost, it includes both the L1 (lasso regression) and L2 (ridge regression) norms of the weights, as well as the complexity of the trees (number of leaves, depth of the tree). It can be expressed as:where ( T ) is the number of leaves in the tree, is the score on the j-th leaf, ( \\gamma ) is the complexity control on the tree's structure, and ( is the L2 regularization term on the leaf weights.XGBoost improves upon the idea of gradient boosting by systematically using gradient information. At each step, it builds a tree that best fits the negative gradient of the loss function regarding the current prediction. The model is updated as:where ( ) is the prediction at step ( ), ( ) is the tree added at step ( t ), and ( ) is the learning rate.To find the best split at each node of the tree, XGBoost uses an approximation to the gain in the objective function. The gain from splitting a node into two child nodes is given by:where ( ) and ( ) are the sums of gradients in the left and right child, respectively, ( ) and ( ) are the sums of second-order gradients in the left and right child, respectively, and ( ) is the regularization on the tree structure.Before any trees are added, XGBoost makes an initial prediction for all instances. For simplicity, let's denote this initial prediction as ( ). Often, this is just the average of the target values for regression problems. Build Tree: A tree is built to predict the residuals from ). Regularization Term for First Tree:\nwhere is the number of leaves in , and are the weights of these leaves. Update Prediction: Build Tree: A tree is built to predict the residuals from . Regularization Term for Second Tree: Update Prediction: Build Tree: A tree is built to predict the residuals from . Regularization Term for Third Tree: Update Prediction: At each step, the objective function that XGBoost aims to minimize for the tree being added is:where: is the loss function evaluated for the (i)-th instance, comparing the actual target value (y_i) to the prediction , which is updated to include the contribution from the (t)-th tree. is the updated prediction for the (i)-th instance after adding the (t)-th tree, where is the learning rate and (f) is the output of the (t)-th tree for the (i)-th instance. is the regularization term for the (j)-th tree, and the sum \\ accumulates the regularization terms for all trees from 1 to (t).\nThe regularization for the model up to the third tree is the sum of the regularization terms for all individual trees:This illustrates how the regularization term is computed and applied for each tree individually but contributes to controlling the overall complexity of the model as trees are added sequentially.The final model after three trees would yield predictions for each instance, which are the sum of the initial predictions and the contributions from all three trees, each adjusted by the learning rate and shaped by the regularization terms that penalized over-complexity at each step.For a given iteration (t), the objective function considering a single tree can be expressed as:XGBoost uses a second-order approximation to optimize the objective function. For each instance (i), let's denote: as the gradient of the loss function with respect to the prediction for instance , as the Hessian (the second derivative of the loss function with respect to the prediction) for instance .\nThese are calculated with respect to the predictions from the previous iteration :The simplified objective function for a single tree, using the second-order approximation, is:For each leaf of the tree, we group instances that fall into the leaf, and the objective becomes a function of the weight assigned to each leaf . Without loss of generality, assuming all instances fall into some leaf , we can rewrite the objective focusing on the weights:where (I_j) is the set of indices of instances that fall into leaf (j).To find the optimal weight for each leaf, we take the derivative of the objective function with respect to and set it to zero:Solving for (w_j), we get the optimal weight for leaf (j):This formula gives us the optimal weight for each leaf that minimizes the objective function, taking into account both the loss function through the gradients and Hessians, and the regularization term.The greedy algorithm for splitting nodes in XGBoost operates by exhaustively searching through all possible splits for each feature. Here's how it works:\nEnumerate Splits: For each feature, the algorithm enumerates all possible splitting points based on the feature values in the training data. This enumeration can involve sorting the values or using unique values for categorical features.\nCalculate Gain: For each potential split, the algorithm calculates the gain in terms of the objective function, which includes both the loss reduction and the regularization penalty. The gain formula used to evaluate splits is derived from the objective function and considers the gradient and Hessian statistics of the loss function for the instances that fall on each side of the split.\nChoose Best Split: The algorithm selects the split that yields the highest gain. If the gain exceeds a predefined threshold (and meets other criteria like minimum child weight), the split is made, creating two new leaf nodes. Otherwise, the node is considered a terminal leaf.\nThe greedy algorithm is thorough and can find very effective splits, but it can be computationally expensive, especially with large datasets and a high number of features, because it requires evaluating every possible split for every feature.To handle large datasets more efficiently, XGBoost also offers an approximation algorithm. This method approximates the optimal split points by bucketing feature values into quantiles, which significantly reduces the number of potential splits to consider. The approximation algorithm involves:\nQuantile Sketch: The algorithm begins by generating a summary of the feature distribution using a quantile sketch. This process groups the continuous feature values into discrete bins (quantiles), reducing the granularity of the split points to a manageable number.\nCandidate Splits: It then considers these quantile points as candidate splits, rather than all possible values of the feature. This approach significantly reduces the computational load.\nEvaluate Splits: Similar to the greedy algorithm, for each candidate split, the algorithm calculates the gain. However, because it operates on quantiles rather than individual values, this step is much faster.\nSelect Split: The best split among the candidates is chosen based on the calculated gains, following similar criteria for gain improvement and regularization adjustments. Data Size: The greedy algorithm is suitable for smaller datasets where computational resources allow for an exhaustive search. The approximation algorithm, on the other hand, is designed for large datasets, offering a scalable solution without extensively compromising on the quality of splits. Accuracy vs. Speed: The greedy algorithm can potentially find more accurate splits by considering every possible value, leading to a possibly more accurate but slower model training process. The approximation algorithm trades a bit of split accuracy for significant gains in speed and scalability. Use Case: The choice between these algorithms can also depend on the specific requirements of the application, including the acceptable training time, the computational resources available, and the criticality of the model's accuracy.\n# Import necessary libraries\nimport xgboost as xgb\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score # Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target # Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Instantiate an XGBoost classifier\n# Use the XGBClassifier for a classification task\nclf = xgb.XGBClassifier( objective='multi:softprob', # Objective for multiclass classification num_class=3, # Number of classes in the dataset learning_rate=0.1, # Learning rate n_estimators=100, # Number of trees to build max_depth=3, # Depth of the trees seed=42 # Random seed for reproducibility\n) # Train the classifier on the training data\nclf.fit(X_train, y_train) # Make predictions on the test set\ny_pred = clf.predict(X_test) # Calculate the accuracy of the predictions\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Core Concepts and Features","level":3,"id":"Core_Concepts_and_Features_0"},{"heading":"Advantages of XGBoost","level":3,"id":"Advantages_of_XGBoost_0"},{"heading":"Practical Considerations","level":3,"id":"Practical_Considerations_0"},{"heading":"Objective Function","level":3,"id":"Objective_Function_0"},{"heading":"Loss Function","level":3,"id":"Loss_Function_0"},{"heading":"Regularization Term","level":3,"id":"Regularization_Term_0"},{"heading":"Gradient Boosting","level":3,"id":"Gradient_Boosting_0"},{"heading":"Split Finding","level":3,"id":"Split_Finding_0"},{"heading":"Step by Step","level":3,"id":"Step_by_Step_0"},{"heading":"Step 0: Initial Prediction","level":4,"id":"Step_0_Initial_Prediction_0"},{"heading":"Step 1: First Tree","level":4,"id":"Step_1_First_Tree_0"},{"heading":"Step 2: Second Tree","level":4,"id":"Step_2_Second_Tree_0"},{"heading":"Step 3: Third Tree","level":4,"id":"Step_3_Third_Tree_0"},{"heading":"Overall Objective Function","level":4,"id":"Overall_Objective_Function_0"},{"heading":"Cumulative Regularization","level":4,"id":"Cumulative_Regularization_0"},{"heading":"Final Model","level":4,"id":"Final_Model_0"},{"heading":"Optimization","level":3,"id":"Optimization_0"},{"heading":"Gradient and Hessian","level":3,"id":"Gradient_and_Hessian_0"},{"heading":"Simplified Objective Function with Second-Order Approximation","level":3,"id":"Simplified_Objective_Function_with_Second-Order_Approximation_0"},{"heading":"Finding the Optimal Weight (w_j) for Each Leaf","level":3,"id":"Finding_the_Optimal_Weight_\\(w_j\\)_for_Each_Leaf_0"},{"heading":"Spliting","level":3,"id":"Spliting_0"},{"heading":"Greedy Algorithm","level":4,"id":"Greedy_Algorithm_0"},{"heading":"Approximation Algorithm","level":4,"id":"Approximation_Algorithm_0"},{"heading":"Choosing Between Greedy and Approximation","level":4,"id":"Choosing_Between_Greedy_and_Approximation_0"},{"heading":"Sample","level":3,"id":"Sample_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/12.-xgboost.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158316,"modifiedTime":1737554783000,"sourceSize":17210,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/12. XGBoost.md","exportPath":"artificial-intelligence/machine-learning/algorithm/12.-xgboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html":{"title":"13. LightGBM","icon":"","description":"LightGBM, short for Light Gradient Boosting Machine, is a popular open-source, distributed, and efficient gradient boosting framework designed to be fast, scalable, and effective. It is developed by Microsoft and offers several advantages over other gradient boosting frameworks, such as XGBoost and CatBoost, particularly in terms of speed and efficiency in handling large datasets. Here's a very detailed note covering its key features, architecture, and usage: Faster Training Speed and Higher Efficiency: LightGBM uses a novel technique called Gradient-based One-Side Sampling (GOSS) to filter out data instances with small gradients, focusing on those with larger gradients for faster training without compromising accuracy. Lower Memory Usage: It employs a histogram-based algorithm that buckets continuous feature values into discrete bins, significantly reducing memory consumption compared to methods that use pre-sorted data. Higher Accuracy: With features like GOSS and Exclusive Feature Bundling (EFB), which bundles mutually exclusive features, LightGBM improves accuracy by focusing on important data and reducing feature dimensions. Support for Categorical Features: LightGBM can handle categorical features directly without the need for manual encoding, unlike many other models that require preprocessing steps like one-hot encoding. Compatibility with Large Datasets: Its design is optimized for efficiency, which makes it suitable for large datasets that might not fit into memory for other gradient boosting frameworks. Support for Parallel and GPU Learning: LightGBM supports parallel and GPU learning, which can significantly speed up computation times for large datasets. Decision Trees as Base Learners: LightGBM uses decision trees as its base learners. Specifically, it employs leaf-wise (best-first) tree growth, unlike other gradient boosting frameworks that grow trees level-wise. This approach often leads to better reductions in loss, leading to more accurate models. Gradient-based One-Side Sampling (GOSS): This feature selection strategy focuses on instances with large gradients. LightGBM keeps these instances and samples the ones with small gradients, which reduces the number of data instances to consider without significant accuracy loss. Exclusive Feature Bundling (EFB): This technique reduces the dimensionality of the dataset by bundling mutually exclusive features, thus speeding up the training process. Parameter Tuning: Start with default parameters, then use grid search or randomized search for hyperparameter tuning to find the best parameters for your specific dataset. Handling Overfitting: To prevent overfitting, consider using parameters like max_depth to limit the depth of trees, min_data_in_leaf to specify the minimum number of data points in a leaf, and bagging_fraction to use a subset of data. Use Early Stopping: LightGBM supports early stopping, which automatically stops training if the validation score does not improve for a specified number of rounds, helping to save time and prevent overfitting. Understanding the mathematics and architecture behind LightGBM is crucial for leveraging its full potential, especially for a computer science major. Let's dive deeper into the mathematical foundations and the architectural nuances of LightGBM.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Key Features","level":3,"id":"Key_Features_0"},{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Best Practices","level":3,"id":"Best_Practices_0"},{"heading":"Mathematical Foundations","level":3,"id":"Mathematical_Foundations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158319,"modifiedTime":1737554783000,"sourceSize":7012,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/13. LightGBM.md","exportPath":"artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html":{"title":"0. Spliting","icon":"","description":"Data Splitting:\nGeneralization: Data splitting is a fundamental concept in machine learning that helps in assessing how well a model will generalize to unseen data. It involves dividing a dataset into multiple subsets, each serving a specific purpose in the model development process.\nTraining, Validation, and Test Sets:\nTraining Set: The training set is the largest portion of the dataset, typically around 70-80% of the data. It's used to train the machine learning model. The model learns patterns, features, and relationships in the data from this set. Validation Set: A portion of the data, usually around 10-15%, is set aside as the validation set. It's used during training to tune hyperparameters and evaluate the model's performance. The validation set helps prevent overfitting (a model performing well on training data but poorly on unseen data) by providing an independent evaluation. Test Set: The test set is a separate portion, typically 10-20% of the data, used for the final evaluation of the trained model. It serves as a proxy for real-world, unseen data. The test set assesses how well the model will perform in practice. It should never be used during model development or tuning. Data Splitting for Cross-Validation:\nCross-Validation (CV): Cross-validation is a resampling technique used to assess how well a model will perform on an independent dataset. It involves dividing the dataset into several subsets or \"folds.\" The model is trained and evaluated multiple times, with each fold serving as both the validation set and the test set at different iterations. K-Fold Cross-Validation: In K-Fold CV, the dataset is divided into K equal-sized folds. The model is trained K times, each time using K-1 folds for training and the remaining fold for validation. This process provides K different estimates of model performance, which can be averaged to obtain a more reliable evaluation. Stratified Cross-Validation: In cases where the dataset is imbalanced (one class significantly outnumbering others), stratified cross-validation ensures that each fold maintains the same class distribution as the original dataset. This helps prevent bias in model evaluation. Key Takeaways:\nData splitting involves dividing the dataset into training, validation, and test sets.\nThe training set is used for model training.\nThe validation set is used for hyperparameter tuning and model evaluation during training.\nThe test set is reserved for the final model evaluation and should not be used during model development.\nCross-validation is a technique for robustly estimating model performance by repeatedly splitting the data into training and validation subsets.\nProper data splitting and cross-validation are critical steps to ensure that machine learning models generalize well to new, unseen data and provide accurate and reliable results.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158391,"modifiedTime":1737554783000,"sourceSize":3134,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/0. Spliting.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html":{"title":"1. Feature scaling","icon":"","description":"also known as normalization or standardization, is a preprocessing step in machine learning that rescales the values of a feature to a common range. This is important because many machine learning algorithms, including linear regression and k-nearest neighbors, are sensitive to the scale of the input features and can lead to suboptimal performance or even failure to converge if the features have different ranges.There are two common methods of feature scaling: Min-Max scaling: Also known as normalization, this method scales the values of a feature to a specified range, usually between 0 and 1. Min-Max scaling is performed by subtracting the minimum value of the feature from each value and dividing the result by the range of the feature (the difference between the minimum and maximum values).\nx_scaled = (x - x_min) / (x_max - x_min) import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler # Define a sample data array\nX = np.array([[1], [2], [3], [4]]) # Create a MinMaxScaler object\nscaler = MinMaxScaler() # Fit and transform the data using the scaler object\nX_scaled = scaler.fit_transform(X) # Print the scaled data\nprint(X_scaled) Standardization: This method scales the values of a feature to have a mean of zero and a standard deviation of one. Standardization is performed by subtracting the mean of the feature from each value and dividing the result by the standard deviation of the feature.\nx_scaled = (x - x_mean) / x_std import numpy as np\nfrom sklearn.preprocessing import StandardScaler # Define a sample data array\nX = np.array([[1], [2], [3], [4]]) # Create a StandardScaler object\nscaler = StandardScaler() # Fit and transform the data using the scaler object\nX_scaled = scaler.fit_transform(X) # Print the scaled data\nprint(X_scaled)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158394,"modifiedTime":1737554783000,"sourceSize":1839,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/1. Feature scaling.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html":{"title":"2. Fix overfitting","icon":"","description":"Common techniques:\nRegularization: Regularization adds a penalty term to the loss function to discourage the model from fitting the noise in the data. Common regularization techniques include L1 and L2 regularization, also known as Lasso and Ridge regression, respectively.\nEarly stopping: Early stopping involves monitoring the performance of the model on a validation set and stopping the training process when the performance on the validation set starts to decrease. This helps to prevent the model from learning the noise in the data.\nCross-validation: Cross-validation provides an estimate of the generalization performance of the model and can be used to select the best model based on its ability to generalize to unseen data.\nFeature selection: Feature selection involves removing irrelevant or redundant features from the data, which can help to reduce overfitting.\nEnsemble methods: Ensemble methods, such as random forests and gradient boosting, can be used to reduce overfitting by combining the predictions of multiple models.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158398,"modifiedTime":1737554783000,"sourceSize":1089,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/2. Fix overfitting.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html":{"title":"3. Seasonal Dummy","icon":"","description":"A seasonal dummy is a binary (0 or 1) indicator variable used in time-series analysis to capture the seasonal effect in the data. For example, if the data has a yearly seasonal pattern, then one could create a binary variable to indicate the season, such as \"Summer\", \"Fall\", \"Winter\", and \"Spring\". The presence of seasonal dummies in the model helps to capture the seasonality of the data, which can help improve the accuracy of the predictions.import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split # Load the time series data\ndata = pd.read_csv(\"time_series_data.csv\") # Create a date index\ndata['date'] = pd.to_datetime(data['date'])\ndata = data.set_index('date') # Create seasonal dummy variables\nseasons = ['spring', 'summer', 'fall', 'winter']\ndummies = pd.get_dummies(pd.cut(data.index.month, bins=[0, 3, 6, 9, 12], labels=seasons, right=False)) # Add the dummies to the data\ndata = pd.concat([data, dummies], axis=1) # Split the data into features and target\nX = data.drop('target', axis=1)\ny = data['target'] # Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit a linear regression model to the data\nreg = LinearRegression().fit(X_train, y_train) # Evaluate the model on the test data\ny_pred = reg.predict(X_test)\nprint(\"Test R-Squared: \", reg.score(X_test, y_test))\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158401,"modifiedTime":1737554783000,"sourceSize":1576,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/3. Seasonal Dummy.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html":{"title":"4. Regular Expression (Regex)","icon":"","description":"1. Basics:\nRegular expressions are patterns used to match character combinations in strings.\nIn Python, the re module provides functions to work with regex.\n2. Simple Patterns:\na: Matches the character \"a\".\napple: Matches the string \"apple\".\n3. Meta-characters (Special Characters): These characters have special meanings when used in a regex pattern:\n.: Matches any character except a newline.\n^: Matches the start of a string.\n$: Matches the end of a string.\n*: Matches 0 or more repetitions of the preceding character or group.\n+: Matches 1 or more repetitions of the preceding character or group.\n?: Matches 0 or 1 repetition of the preceding character or group.\n|: Acts like an \"OR\" operator. E.g., a|b matches \"a\" or \"b\".\n(): Groups patterns together.\n4. Character Classes:\n[abc]: Matches any one of the characters \"a\", \"b\", or \"c\".\n[^abc]: Matches any character except \"a\", \"b\", or \"c\".\n[a-z]: Matches any lowercase letter.\n[A-Z]: Matches any uppercase letter.\n[0-9]: Matches any digit.\n5. Predefined Character Classes:\n\\d: Matches any digit, equivalent to [0-9].\n\\D: Matches any non-digit.\n\\w: Matches any alphanumeric character or underscore, equivalent to [a-zA-Z0-9_].\n\\W: Matches any non-alphanumeric character (opposite of \\w).\n\\s: Matches any whitespace character (spaces, tabs, line breaks).\n\\S: Matches any non-whitespace character.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158405,"modifiedTime":1737554783000,"sourceSize":2368,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/4. Regular Expression (Regex).md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html":{"title":"5. Principal Component Analysis","icon":"","description":"Principal Component Analysis (PCA) is a sophisticated statistical technique that has widespread applications in fields ranging from machine learning and data science to finance and neuroscience. Its primary goal is to simplify the complexity in high-dimensional data while retaining as much of the original data's variability as possible. Here's a detailed note on PCA, including its methodology, advantages, disadvantages, and varied examples from different fields.PCA transforms the original variables into a new set of variables, which are linear combinations of the original set. These new variables, called principal components (PCs), are orthogonal (meaning they are uncorrelated), and they capture the maximum variance in the data in a sequential manner. The first principal component captures the most variance, the second captures the second most, and so on.The process of performing PCA generally involves several key steps: Standardization: This is the preliminary step where data is standardized to have a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the variances of the initial variables. Covariance Matrix Calculation: This step involves calculating the covariance matrix to examine the covariance (and correlation) between pairs of variables in the data. Eigenvalue and Eigenvector Calculation: From the covariance matrix, calculate the eigenvalues and eigenvectors. Eigenvectors point in the direction of the largest variance, and eigenvalues correspond to the magnitude of this variance. Ranking and Selection of Principal Components: Eigenvalues are ranked in descending order, and their corresponding eigenvectors are selected to form the principal components. The number of principal components chosen depends on the amount of total variance one aims to capture. Projection: Finally, the original data is projected onto the space spanned by the selected principal components, resulting in a new dataset of reduced dimensionality. Finance: In portfolio management, PCA can be used to identify the underlying factors that drive the returns of a large set of investments. By reducing dimensionality, PCA helps in understanding the structure of market risks and diversifying the portfolio effectively. Genomics: PCA is employed to analyze and visualize genetic data from thousands of genomes. It can highlight the genetic diversity and population structure, aiding in evolutionary biology studies. Image Processing: PCA can reduce the dimensionality of image data by capturing the essence of images with fewer components. This technique is useful in facial recognition systems, where it's known as Eigenfaces. Marketing: PCA can help in customer segmentation by reducing the number of demographic and psychographic variables to a few principal components, thus identifying distinct customer groups more easily. Neuroscience: In brain imaging studies, PCA is used to reduce the dimensionality of fMRI data, helping researchers identify patterns of brain activity associated with different mental processes. Efficiency: PCA can drastically reduce the dimensions of the data without losing much information.\nVisualization: Reducing data to 2 or 3 principal components can make it easier to graph high-dimensional data on a two-dimensional plot.\nNoise Reduction: By ignoring components with low variance, PCA can help in filtering noise from the data. Interpretability: Principal components are combinations of the original variables and might not be easily interpretable.\nVariance-Centric: PCA focuses on variance, which might not always be the best measure of the data's structure.\nLinear Assumptions: PCA assumes linear relationships among variables. It might not perform well with complex, nonlinear data structures.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Methodology","level":3,"id":"Methodology_0"},{"heading":"Examples of PCA Application","level":3,"id":"Examples_of_PCA_Application_0"},{"heading":"Advantages of PCA","level":3,"id":"Advantages_of_PCA_0"},{"heading":"Disadvantages of PCA","level":3,"id":"Disadvantages_of_PCA_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158409,"modifiedTime":1737554783000,"sourceSize":4274,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/5. Principal Component Analysis.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html":{"title":"1. Cross Validation","icon":"","description":"A technique used in machine learning to assess the performance of a model. The goal of cross-validation is to estimate the performance of a model on unseen data, which is useful for determining whether the model has overfitted to the training data.Cross-validation works by dividing the dataset into several subsets, called \"folds\". The model is trained on k-1 folds of the data and tested on the remaining fold. This process is repeated k times, each time using a different fold as the test set and averaging the results to obtain a performance metric. The most common performance metric used in cross-validation is accuracy for classification problems and mean squared error for regression problems.There are several types of cross-validation techniques, including k-fold cross-validation, stratified k-fold cross-validation, and leave-one-out cross-validation. The choice of cross-validation technique depends on the size of the dataset and the specific requirements of the problem.import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression # Define a sample data array\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1, 2, 3, 4, 5]) # Create a Linear Regression model\nreg = LinearRegression() # Define the number of folds\nk = 3 # Create a KFold object\nkf = KFold(n_splits=k) # Perform cross-validation on the model\nscores = []\nfor train_index, test_index in kf.split(X): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] reg.fit(X_train, y_train) score = reg.score(X_test, y_test) scores.append(score) # Print the mean score of the cross-validation\nprint(np.mean(scores))\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158457,"modifiedTime":1737554783000,"sourceSize":1786,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Special Evaluation Method/1. Cross Validation.md","exportPath":"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html":{"title":"2. Regularization","icon":"","description":"Regularization is a technique used in machine learning to prevent overfitting and improve the generalization performance of models. Overfitting occurs when a model is too complex and captures the noise in the training data, leading to poor performance on unseen data.Regularization adds a penalty term to the loss function, which discourages the model from fitting the noise in the data. The regularization term is controlled by a hyperparameter, which determines the strength of the regularization.There are two common types of regularization in machine learning: L1 regularization and L2 regularization.L1 regularization, also known as Lasso regularization, adds a penalty term to the loss function that is proportional to the absolute value of the coefficients of the model. This encourages the model to have sparse coefficients, with many coefficients being zero. where MSE(w) is the mean squared error of the model, α is the regularization hyperparameter that controls the strength of the regularization, and ∑(|w|) is the L1 norm of the weight vector w.L2 regularization, also known as Ridge regularization, adds a penalty term to the loss function that is proportional to the square of the coefficients of the model. This encourages the model to have small coefficients, which helps to prevent overfitting.where MSE(w) is the mean squared error of the model, α is the regularization hyperparameter that controls the strength of the regularization, and ∑(w^2) is the L2 norm of the weight vector w.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158460,"modifiedTime":1737554783000,"sourceSize":3630,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Special Evaluation Method/2. Regularization.md","exportPath":"artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/data/preprocessing.html":{"title":"Preprocessing","icon":"","description":" Data Cleaning: Missing Value Handling: Address missing values by deleting rows/columns with missing data, filling in missing values (e.g., using mean, median, mode, or other algorithms), or predicting missing values.\nNoise Data Handling: Identify and correct errors or biased values in the dataset.\nOutlier Handling: Deal with outliers by deleting, correcting, or using statistical methods (such as IQR, Z-score). Data Integration: Merging Data Sources: Combine data from multiple sources into a consistent dataset.\nResolving Data Conflicts: Harmonize differences across data sources, such as different units of measurement or data formats. Data Transformation: Normalization/Standardization: Adjust data scales to a specific range (e.g., 0 to 1) or standardize data to have a 0 mean and unit variance for more efficient algorithm performance.\nFeature Construction: Create new features from existing data to enhance the predictive power of the model.\nDiscretization: Convert continuous features into discrete values, which benefits certain types of models. Dimensionality Reduction: Feature Selection: Select the most relevant features to reduce dimensions and remove irrelevant or redundant features.\nFeature Extraction: Use methods like Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), etc., to transform high-dimensional data into lower-dimensional data while retaining as much important information as possible. Data Encoding: Categorical Data Encoding: Convert textual or categorical data into numerical form, for example using One-Hot Encoding or Label Encoding. Data Sampling: Balancing the Dataset: Balance the distribution of classes in an imbalanced dataset by oversampling minority classes or undersampling majority classes.\nSplitting the Dataset: Divide the data into training, validation, and test sets to evaluate model performance. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/data/preprocessing.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158383,"modifiedTime":1737554783000,"sourceSize":2035,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Preprocessing.md","exportPath":"artificial-intelligence/machine-learning/data/preprocessing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/math/1.-linear-algebra.html":{"title":"1. Linear Algebra","icon":"","description":"\nVectors\nMatrices\nInverse and Transpose\nRow-echelon form (REF)\nMoore-Penrose pseudo-inverse\nGaussian elimination\nExampleGiven:A = np.array([[2, 3], [0, 1]])\nx = np.array([[1], [3]])\nBasic matrix calculations:Remember the matrix multiplication uses @ in python:#Matrix Multiplication Example\nb = A @ x\nprint('\\nMatrix Multiplication')\nprint(b)\nOther basics:#Matrix Addition Example\nb = A + x\nprint('\\nMatrix Addition')\nprint(b) #Elementwise Multiplication Example\nb = A * x\nprint('\\nElementwise Matrix Multiplication')\nprint(b) #Extract a single element of a matrix:\nprint('\\nSingle Element Extraction')\nb = A[0, 0]\nprint(b) #Extract an entire column of a matrix:\nprint('\\nColumn Extraction')\nb = A[:, 0]\nprint(b) #Extract an entire row of a matrix:\nprint('\\nRow Extraction')\nb = A[0, :]\nprint(b) #Transpose of a matrix:\nprint('\\nTranspose')\nA_Transpose = A.T\nprint(A_Transpose) Linear equations:The above system of linear equations can also be written down in a compact matrix form as follows:where,Solution for X by using numpy:A = np.array([[2, 0, 3], [1, -1, 2], [1, -3, 4]])\nB = np.array([[2], [1], [2]]) def solve_with_numpy(A,B): x = np.linalg.solve(A, B) return x\nres = solve_with_numpy(A,B)\nInverse of matrix: A = np.array([[-3, -2], [1, 1]])\nfrom numpy.linalg import inv\nA_inv = inv(A)\nUse inverse solve Linear equations:A = np.array([[-3, -2], [1, 1]])\nB = np.array([[4], [6]])\nX = A_inv @ B Moore-Penrose pseudo inverse: (ATA)-1ATA = np.array([[1, 3], [2, 7], [5, 1]])\nB = np.array([[13], [30], [9]]) A_pseudo_inverse = inv(A.T @ A)@(A.T) X = A_pseudo_inverse @ B # another methond\nprint(np.linalg.pinv(A)@ B)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/math/1.-linear-algebra.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158468,"modifiedTime":1737554783000,"sourceSize":2388,"sourcePath":"Artificial Intelligence/Machine Learning/Math/1. Linear Algebra.md","exportPath":"artificial-intelligence/machine-learning/math/1.-linear-algebra.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/math/2.-gradient-descent.html":{"title":"2. Gradient descent","icon":"","description":"In mathematics,&nbsp;gradient descent&nbsp;(also often called&nbsp;steepest descent) is a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Category:First_order_methods\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Category:First order methods\" href=\"https://en.wikipedia.org/wiki/Category:First_order_methods\" target=\"_self\">first-order</a>&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Iterative_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Iterative algorithm\" href=\"https://en.wikipedia.org/wiki/Iterative_algorithm\" target=\"_self\">iterative</a>&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Mathematical_optimization\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Mathematical optimization\" href=\"https://en.wikipedia.org/wiki/Mathematical_optimization\" target=\"_self\">optimization</a>&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Algorithm\" href=\"https://en.wikipedia.org/wiki/Algorithm\" target=\"_self\">algorithm</a>&nbsp;for finding a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Local_minimum\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Local minimum\" href=\"https://en.wikipedia.org/wiki/Local_minimum\" target=\"_self\">local minimum</a>&nbsp;of a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Differentiable_function\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Differentiable function\" href=\"https://en.wikipedia.org/wiki/Differentiable_function\" target=\"_self\">differentiable function</a>. The idea is to take repeated steps in the opposite direction of the&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Gradient\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Gradient\" href=\"https://en.wikipedia.org/wiki/Gradient\" target=\"_self\">gradient</a>&nbsp;(or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a&nbsp;<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Local_maximum\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" title=\"Local maximum\" href=\"https://en.wikipedia.org/wiki/Local_maximum\" target=\"_self\">local maximum</a>&nbsp;of that function; the procedure is then known as gradient ascent.Gradient descent is an algorithm used to find the minimum value of a function. We will use the gradient descent algorithm to find the minimum value of the cost function 𝐽(𝜃0, 𝜃1).\nThe idea behind gradient descent is: at the beginning we randomly select a combination of parameters (𝜃0, 𝜃1, . . We continue to do this until we reach a local minimum, however, because we haven't tried all the parameter combinations, so we still don't know whether the local minimum we get is the global minimum. Choosing different initial parameter combinations may find different local minimum.“Gradient descent.png” could not be found.Batch gradient descent:“gradiant decs.png” could not be found.𝑎 is the learning rate, which determines how big a step we take down in the direction that can make the cost function decrease the most. In batch gradient descent, we subtract all parameters at the same time by multiplying the learning rate by Derivative of the cost function.If 𝑎 is too small, that is, my learning rate is too small, the result is that I can only move a little bit, trying to get close to the lowest point, so it takes many steps to reach the lowest point, so if 𝑎 is too small , can be slow because it will move a little bit, and it will take many steps to reach the global minimum.If 𝑎 is too large, then the gradient descent method may cross the lowest point, and may not even be able to converge, and the next iteration will move a big step, crossing once, crossing again, crossing the lowest point again and again, until you find that it is actually far from the lowest point The points are getting farther and farther away, so if 𝑎 is too large, it will cause no convergence, or even divergence.“gradient formula.png” could not be found.Convex better than none-convex when doing the gradient descent for finding the global min:“convex.png” could not be found.Code:# create data\nX = 2 * np.random.rand(100,1)\ny = 4 +3 * X+np.random.randn(100,1)\n# cost function\ndef cal_cost(theta,X,y): ''' Calculates the cost for given X and Y. The following shows and example of a single dimensional X theta = Vector of thetas X = Row of X's np.zeros((2,j)) y = Actual y's np.zeros((2,1)) where: j is the no of features ''' m = len(y) predictions = X.dot(theta) cost = (1/2*m) * np.sum(np.square(predictions-y)) return cost\ndef gradient_descent(X,y,theta,learning_rate=0.01,iterations=100): ''' X = Matrix of X with added bias units y = Vector of Y theta=Vector of thetas np.random.randn(j,1) learning_rate iterations = no of iterations Returns the final theta vector and array of cost history over no of iterations ''' m = len(y) cost_history = np.zeros(iterations) theta_history = np.zeros((iterations,2)) for it in range(iterations): prediction = np.dot(X,theta) theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y))) theta_history[it,:] =theta.T cost_history[it] = cal_cost(theta,X,y) return theta, cost_history, theta_history\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/math/2.-gradient-descent.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158473,"modifiedTime":1737554783000,"sourceSize":4303,"sourcePath":"Artificial Intelligence/Machine Learning/Math/2. Gradient descent.md","exportPath":"artificial-intelligence/machine-learning/math/2.-gradient-descent.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html":{"title":"3. Full Rank Matrix","icon":"","description":"A full-rank matrix is a matrix that has a rank equal to its maximum possible rank. The rank of a matrix is the dimension of the linear space generated by its columns or rows. The maximum possible rank of a matrix is the minimum of its number of rows and columns. A matrix is said to be of full rank if it has the maximum possible rank for its size.For example, if a matrix has m rows and n columns, its maximum rank is min(m, n). If the matrix has rank min(m, n), it is said to be of full rank. Full-rank matrices are important in linear algebra and machine learning because they have an inverse and can be used to solve systems of linear equations.A matrix that is not full-rank is called a rank-deficient matrix. Rank-deficient matrices are not invertible and cannot be used to solve systems of linear equations.Example:import numpy as np # Full-rank matrix\nA = np.array([[1, 2], [3, 4]])\nprint(\"Rank of A:\", np.linalg.matrix_rank(A)) # Output: Rank of A: 2 # Rank-deficient matrix\nB = np.array([[1, 2], [2, 4]])\nprint(\"Rank of B:\", np.linalg.matrix_rank(B)) # Output: Rank of B: 1 In the case of an overdetermined system, where the data size n is larger than the number of variables x, it is possible for the matrix to be rank-deficient. However, it depends on the specific data and how the matrix is constructed. If the data is linearly dependent, the matrix will be rank-deficient, and a unique solution may not exist. On the other hand, if the data is linearly independent, the matrix will have full rank and a unique solution can be found.In general, it's good practice to check the rank of the matrix before solving the system of equations to make sure that a unique solution exists. This can be done using matrix decomposition techniques or linear algebra libraries, such as numpy in Python.One common approach to handle an overdetermined system is to use a least squares method, such as linear regression or polynomial regression. These methods aim to minimize the sum of the squared differences between the actual data points and the predicted values. By finding the coefficients that minimize this sum, the model can find the best approximation that fits the data.Another approach is to use regularization, such as ridge regression or Lasso, to prevent overfitting and improve the generalization ability of the model. These methods introduce a penalty term to the objective function that discourages extreme coefficients and helps prevent overfitting.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158476,"modifiedTime":1737554783000,"sourceSize":2516,"sourcePath":"Artificial Intelligence/Machine Learning/Math/3. Full Rank Matrix.md","exportPath":"artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/1.-variance.html":{"title":"1. Variance","icon":"","description":"The expected value tells us a single number that we can use to represent our random variable, which is the average value. It's important to also measure risk, or how close to the expected value we're actually likely to get. For this we use use&nbsp;variance, which can quantify the difference between, say, playing the game above with a fair coin, and simply being handed $0.50. The variance of a random variable is defined asIt is also sometimes written as , and (the positive square root of the variance) is called the&nbsp;standard deviation. This is often more useful because it is in the same units as the original data, so it is directly comprable.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/1.-variance.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158488,"modifiedTime":1737554783000,"sourceSize":722,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/1. Variance.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/1.-variance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/2.-covariance.html":{"title":"2. Covariance","icon":"","description":"Concept:“255px-Covariance_trends.svg.png” could not be found.Formula:“cov-form1.svg” could not be found.Arithmetic:“cov-form2.svg” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/2.-covariance.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158534,"modifiedTime":1737554783000,"sourceSize":116,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/2. Covariance.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/2.-covariance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html":{"title":"3.1 Cost function - Least Square","icon":"","description":"“cost function.jpg” could not be found.“cost-func-graph.png” could not be found.Multiple Linear regression shares same idea.\nSame as <a data-href=\"6. Normal Equation\" href=\"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">6. Normal Equation</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158538,"modifiedTime":1737554783000,"sourceSize":137,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/3.1 Cost function - Least Square.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html":{"title":"3.2 Cost Function - Logistic Regression","icon":"","description":"“logistic cost func.png” could not be found.The cost function for logistic regression is called the log-loss or cross-entropy loss. It is defined as follows:where is the hypothesis function, which represents the predicted probability that given the input features for the training example. The hypothesis function is defined as:where is the sigmoid function:The goal is to find the values of that minimize the cost function . This is typically done using optimization algorithms such as gradient descent. The gradient of the cost function with respect to each is given by:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158542,"modifiedTime":1737554783000,"sourceSize":971,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/3.2 Cost Function - Logistic Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/4.-correlation.html":{"title":"4. Correlation","icon":"","description":"“correlation.png” could not be found.More detials: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.probabilitycourse.com/chapter5/5_3_1_covariance_correlation.php\" target=\"_self\">https://www.probabilitycourse.com/chapter5/5_3_1_covariance_correlation.php</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/4.-correlation.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158546,"modifiedTime":1737554783000,"sourceSize":112,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/4. Correlation.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/4.-correlation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html":{"title":"5. Sum of Squares","icon":"","description":"“sum of squares form.png” could not be found.“sse.png” could not be found.“ssr.png” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158568,"modifiedTime":1737554783000,"sourceSize":65,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/5. Sum of Squares.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html":{"title":"6. Normal Equation","icon":"","description":"Same as Least Square <a data-href=\"3. Cost function - Least Square\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3. Cost function - Least Square</a>The normal equation is a closed-form solution to the linear regression problem, which is used to find the optimal coefficients of the linear model that minimize the sum of squared errors.“normal Equation.png” could not be found.θ = (X^T * X)^-1 * X^T * y\nwhere:\nθ is the coefficient vector of the linear regression model, with dimensions (n + 1) x 1, where n is the number of independent variables.\nX is the design matrix, with dimensions m x (n + 1), where m is the number of observations and n is the number of independent variables. The design matrix includes an extra column of ones to represent the intercept term.\ny is the dependent variable vector, with dimensions m x 1, where m is the number of observations.\n^T represents the transpose operation, and ^-1 represents the inverse operation.\nThe normal equation finds the values of the coefficients θ that minimize the sum of squared errors by solving for θ directly, without the need for iterative optimization algorithms like gradient descent.import numpy as np # Define the design matrix\nX = np.array([[1, 2], [1, 3], [1, 4]]) # Define the dependent variable vector\ny = np.array([[3], [4], [5]]) # Calculate the transpose of the design matrix\nXT = np.transpose(X) # Calculate the product of the transpose of the design matrix and the design matrix\nXTX = np.dot(XT, X) # Calculate the inverse of the product of the transpose of the design matrix and the design matrix\nXTX_inv = np.linalg.inv(XTX) # Calculate the product of the inverse of the product of the transpose of the design matrix and the design matrix and the transpose of the design matrix\nXTX_inv_XT = np.dot(XTX_inv, XT) # Calculate the product of the inverse of the product of the transpose of the design matrix and the design matrix and the transpose of the design matrix and the dependent variable vector\ntheta = np.dot(XTX_inv_XT, y) # Print the result\nprint(theta)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[".html"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158583,"modifiedTime":1737554783000,"sourceSize":2026,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/6. Normal Equation.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html":{"title":"7. Cosine Distance","icon":"","description":" Sparse Vectors: Use the Coordinate list (COO) format, storing only non-zero elements with their row and column indices.\nExample: (0, 0) 2 and (0, 3) 5 corresponds to the vector [ 2, 0, 0, 5 ].\nCommon in text processing (e.g., TF-IDF vectors) and recommendation systems.\nOther formats include CSR, CSC, and more. Dense Vectors: Most elements are non-zero, typically stored in arrays or lists.\nExample: [ 1.2, 3.4, -0.5, 4.1, 5.6 ].\nUsed in physics, graphics, and machine learning. Cosine Distance: Derived from cosine similarity: (d = 1 - s).\nRanges between 0 and 2; 0 indicates perfect alignment, and 2 indicates diametric opposition.\nUsed in machine learning, text processing, and recommendation systems. import numpy as np\nfrom scipy.sparse import csr_matrix, issparse\nfrom sklearn.metrics.pairwise import cosine_distances def cosine_distance(v1, v2): if issparse(v1) and not issparse(v2): v2 = csr_matrix(v2) elif not issparse(v1) and issparse(v2): v1 = csr_matrix(v1) distance = cosine_distances(v1, v2) return distance\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Cosine Distance - Sparse &amp; Dense Vectors","level":3,"id":"Cosine_Distance_-_Sparse_&_Dense_Vectors_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158587,"modifiedTime":1737554783000,"sourceSize":1189,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/7. Cosine Distance.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html":{"title":"1. Linear Regression stats","icon":"","description":"whereand","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Regression-Single beta","level":3,"id":"Regression-Single_beta_0"},{"heading":"Population Regression Line","level":4,"id":"Population_Regression_Line_0"},{"heading":"Least Squares Line","level":4,"id":"Least_Squares_Line_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158595,"modifiedTime":1737554783000,"sourceSize":1026,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/1. Linear Regression stats.md","exportPath":"artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html":{"title":"2. Polynomial Regression","icon":"","description":"Polynomial Regression is a regression model that extends the linear regression model by adding polynomial features to the independent variables. It is used when the relationship between the independent and dependent variables is non-linear.The polynomial regression equation can be represented as:where:\ny is the dependent variable\nx is the independent variable\nβ are the coefficients of the polynomial\nε is the error term\nx^k is the kth power of x\nThe goal of polynomial regression is to find the optimal coefficients β that minimize the sum of squared errors between the predicted values and the actual values of y. This can be achieved using various optimization algorithms, such as gradient descent or least squares.import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures # generate some data\nnp.random.seed(0)\nX = np.random.rand(100, 1)\ny = X ** 2 + np.random.rand(100, 1) # add polynomial features\npoly_features = PolynomialFeatures(degree=2)\nX_poly = poly_features.fit_transform(X) # fit a linear regression model\nregressor = LinearRegression()\nregressor.fit(X_poly, y) # make predictions on a grid\nX_grid = np.linspace(0, 1, 100).reshape(-1, 1)\nX_grid_poly = poly_features.fit_transform(X_grid)\ny_pred = regressor.predict(X_grid_poly) # plot the results\nplt.scatter(X, y, color=\"red\")\nplt.plot(X_grid, y_pred, color=\"blue\")\nplt.show()\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158600,"modifiedTime":1737554783000,"sourceSize":1600,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/2. Polynomial Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html":{"title":"3. Ridge Regression","icon":"","description":"“ridge.png” could not be found.Ridge Regression is a regularized linear regression model that is used to address the issue of multicollinearity in linear regression. It works by adding a L2 regularization term to the loss function of linear regression, which helps to prevent overfitting by penalizing large coefficients.The L2 regularization term is defined as the sum of the squares of the coefficients, multiplied by a regularization strength parameter (λ). The loss function for Ridge Regression is given by:where:\ny is the actual target value\ny_pred is the predicted value\nw is the vector of coefficients\nλ is the regularization strength\nThe goal of Ridge Regression is to find the coefficients that minimize this loss function.Ridge Regression can be implemented in Python using the Ridge class from the sklearn.linear_model module in scikit-learn library.import numpy as np A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) rank = np.linalg.matrix_rank(A)\nprint(\"Rank of matrix A:\", rank)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158604,"modifiedTime":1737554783000,"sourceSize":1490,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/3. Ridge Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html":{"title":"4. Elastic Net","icon":"","description":"Elastic Net is a linear regression algorithm that combines the properties of both Ridge Regression and Lasso Regression. It addresses overfitting problems caused by multicollinearity in the data by adding a penalty term to the least square loss function. The loss function for Elastic Net is defined as:where: is the actual target value is the predicted value is the vector of coefficients is the strength of the regularization term is the mixing parameter that determines the proportion of L1 and L2 penalty terms in the loss function\nWhen , Elastic Net reduces to Lasso Regression, and when , it reduces to Ridge Regression. The optimal value of can be determined using cross-validation or using a grid search.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158608,"modifiedTime":1737554783000,"sourceSize":927,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/4. Elastic Net.md","exportPath":"artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html":{"title":"5.  Logistic Regression","icon":"","description":"Redirect: <a data-href=\"2. Logistic Regression\" href=\"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. Logistic Regression</a>Precision is defined as the number of true positive results divided by the number of true positive results plus the number of false positive results.Recall is defined as the number of true positive results divided by the number of true positive results plus the number of false negative results.F1 Score is a metric that calculates the harmonic mean between precision and recall. It is often used as a single metric to summarize the overall performance of a binary classifier system.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"5. Logistic Regression","level":1,"id":"5._Logistic_Regression_0"},{"heading":"Precision","level":2,"id":"Precision_0"},{"heading":"Recall","level":2,"id":"Recall_0"},{"heading":"F1 Score","level":2,"id":"F1_Score_0"}],"links":["artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158611,"modifiedTime":1737554783000,"sourceSize":958,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/5.  Logistic Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html":{"title":"6. Neural Network Learning","icon":"","description":"Neural networks are a type of machine learning model that are based on the structure and function of the human brain. They consist of a set of connected nodes, where each node is an input, output, or hidden layer, and each connection between nodes has a weight associated with it. During training, the weights are adjusted to allow the network to correctly output the desired output for a given input. The classic training algorithm for neural networks is called backpropagation.Strengths of neural networks as a classifier include their high tolerance to noisy data, ability to classify untrained patterns, and suitability for continuous-valued inputs and outputs. They have been successful on a wide range of real-world data, such as hand-written letters, images, and voice. Neural network algorithms are inherently parallel and can take advantage of modern hardware to speed up training and inference. Techniques have also been developed to extract rules from trained neural networks.Weaknesses of neural networks include their long training time and the need to determine a number of parameters empirically, such as network topology or structure. They also tend to overfit when there are a large number of nodes, which implies a large number of weights to be learned. Neural networks are generally difficult to interpret, as the learned weights and hidden units have no clear symbolic meaning, and they are often referred to as \"black box\" models.“Multi.png” could not be found.The inputs to the network correspond to the attributes measured for each training tuple. Inputs are fed simultaneously into the units making up the input layer. They are then weighted and fed simultaneously to a hidden layer. The number of hidden layers is arbitrary (1 hidden layer in the example above). The number of hidden units is arbitrary (3 hidden units in the example above). The weighted outputs of the last hidden layer are input to units making up the output layer, which emits the network's prediction.The network is feed-forward: none of the weights cycles back to an input unit or to an output unit of a previous layer. From a statistical point of view, networks perform nonlinear regression: given enough hidden units and enough training samples, they can closely approximate any function (output). Decide the network topology: Specify the number of units in the input layer: usually one input unit per attribute in the data (but nominal attributes can have one input per value).\nThe number of hidden layers (at least one, and commonly only one).\nThe number of units in each hidden layer.\nThe number of units in the output layer. Output, one unit per response variable. In a typical binary classification problem, only one output unit is used and a threshold is applied to the output value to select the class label. The value can be interpreted as a probability of belonging to the positive class, and in this way, a neural network can be used as a probabilistic model. For classification of more than two classes, one output unit per class is used and the highest-valued class is selected for the label. Choose an activation function for each hidden and output unit . Usually randomly, determine initial values for the weights. Once a network has been trained, if its accuracy is unacceptable, try a different network topology or a different set of initial weights or maybe different activation functions. Use trial-and-error or there are methods that systematically search for a high-performing topology.\nInitialise the weights by training The weights in the network are set to small numbers (e.g., ranging from −1.0 to 1.0, or −0.5 to 0.5) by training.\nEach unit has a bias associated with it, as explained later. The biases are similarly set to small numbers by training.\nEach testing tuple is first normalised to [0.0 ~ 1.0]. Consider a normalised tuple X, processed by the following steps. Propagate the inputs forward First, the testing tuple is normalised to [0.0 ~ 1.0] and the normalised tuple X is fed to the network’s input layer. The inputs pass through the input units, unchanged.\nHidden layer Input of hidden unit: all outputs of the previous layer, e.g. if the hidden unit is in the first hidden layer, then the inputs are .\nOutput of hidden unit j: weighted linear combination of its input followed by an activation function where is the weight of the connection from input i in the previous layer to unit j is the bias variable of unit j is a non-linear activation function which will be described later. If there is more than one hidden layer, the above procedure will be repeated until the final hidden layer will result outputs. Output layer The outputs of the final hidden layer will be used as inputs of the output layer.\nThe number of output units are determined by a task\nIf our goal is to predict a single numerical variable, then one output unit is enough\nFinal output : or is the final predicted value given . Computation graph of hidden unit j\nGraphical illustration of the computational flow of hidden unit j: The inputs to unit j are outputs from the previous layer. These are multiplied by their corresponding weights to form a weighted sum, which is added to the bias associated with unit j. A nonlinear activation function f is applied to the net input. Again the output of this unit will be used as an input of the successive layer.\nNon-linear activation function\nAn activation function imposes a certain non-linearity in a neural network. There are several possible choices of activation functions, but sigmoid (or logistic) function is the most widely used activation function.\nSigmoid function Sigmoid functions have domain of all real numbers, with return value monotonically increasing most often from 0 to 1. Also referred to as a squashing function, because it maps the input domain onto the range of 0 to 1 Other activation functions Hyperbolic tangent function (tanh)\nSoftmax function (use for output nodes for classification problems to push the output value towards binary 0 or 1)\nReLU\netc. “hidden_unit.png” could not be found.\nEach training tuple is first normalized to [0.0 ~ 1.0].\nThe error is propagated backward by updating the weights and biases to reflect the error of the network's prediction for the dependent variable for a training tuple.\nFor unit k in the output layer, its error is computed by: where is the actual output of unit k, and is the known target value of the given training tuple.\nTo compute the error of a hidden layer unit j, the weighted sum of the errors of the units connected to unit k in the next layer is considered: where is the weight of the connection from unit j to a unit k in the next higher layer.\nThe weights and biases are updated to reflect the propagated errors. Each weight in the network is updated by the following equations, where is the change in weight : Each bias in the network is updated by the following equations, where is the change in bias : The parameter is the learning rate, typically a value between 0.0 and 1.0. If the learning rate is too small, then learning will occur at a very slow pace. If the learning rate is too large, then oscillation between inadequate solutions may occur. A rule of thumb is to set the learning rate to 1/t, where t is the number of iterations through the training set so far.\nThe weight and bias increments can be accumulated in variables, so that the weights and biases are updated after all the tuples in the training set have been presented. Alternatively, the backpropagation algorithm given here updates the weights and biases immediately after the presentation of each tuple.\nEither way, we keep updating until we have a satisfactory error, presenting each tuple of the training set many times over. In theory, the mathematical derivation of backpropagation employs epoch updating, yet in practice, case updating is more common because it tends to yield more accurate results. When your weights and biases are changing very little , ie. all the are small; or Accuracy on the training set is good enough; or\nA prespecified number of epochs have passed, or\nYour error on a validation dataset tested after each epoch is changing very little.\nFor numerical prediction tasks, neural networks can be evaluated using methods such as mean squared error, mean absolute error, and correlation coefficient.For classification tasks, neural networks can be evaluated using methods such as confusion matrix, accuracy, precision, recall, F1 score, and ROC curve.Neural networks can also be used as probabilistic classifiers, and methods such as ROC analysis can be used to evaluate their performance.Interpretability of Neural Networks:One of the major drawbacks of neural networks is their lack of interpretability. The acquired knowledge in the form of a network of units connected by weighted links is difficult for humans to interpret. However, there are some methods that can be used to extract easier-to-interpret rules from the network, such as:\nRule extraction by network pruning: This method involves simplifying the network structure by removing weighted links that have the least effect on the trained network. The set of input and activation values are then studied to derive rules describing the relationship between the input and hidden unit layers.\nSensitivity analysis for interpretability: This method involves assessing the impact that a given input variable has on a network output. The input to the variable is varied while the remaining input variables are fixed at some value. The knowledge gained from this analysis can be represented in rules, such as “IF X decreases 5% THEN Y increases 8%.”\n“neuralnetworks.png” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Strengths &amp; Weakness","level":3,"id":"Strengths_&_Weakness_0"},{"heading":"Multi-Layer Feed-Forward Neural Network","level":3,"id":"Multi-Layer_Feed-Forward_Neural_Network_0"},{"heading":"Defining a Network Topology","level":3,"id":"Defining_a_Network_Topology_0"},{"heading":"Prediction with Neural Network","level":3,"id":"Prediction_with_Neural_Network_0"},{"heading":"Backpropagation","level":3,"id":"Backpropagation_0"},{"heading":"Terminate training","level":3,"id":"Terminate_training_0"},{"heading":"Performance Evaluation","level":3,"id":"Performance_Evaluation_0"},{"heading":"Other Neural Net Architectures","level":3,"id":"Other_Neural_Net_Architectures_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158615,"modifiedTime":1737554783000,"sourceSize":11582,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/6. Neural Network Learning.md","exportPath":"artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html":{"title":"7. Lazy Leaner","icon":"","description":"Lazy learning, also known as instance-based learning, is a type of machine learning where the model is not explicitly trained but rather stores the training data and waits until a new input is given to it. The prediction is made by finding the most similar training examples to the new input and using their known output labels to make a prediction.The k-Nearest Neighbor (k-NN) algorithm is a popular example of a lazy learner. In the k-NN algorithm, the output label of a new input is predicted based on the majority label of its k-nearest neighbors in the training data. Check: <a data-href=\"3. K-nearest Neighbors\" href=\"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3. K-nearest Neighbors</a>Formally, given a training set of N tuples , where is an input tuple and is the corresponding output label, and a new input tuple x, the k-NN algorithm finds the k closest training tuples to x based on a distance metric . The output label of the new input x is then predicted as the majority label of the k closest tuples.The distance metric can be defined in different ways, such as Euclidean distance, Manhattan distance, and cosine similarity. The value of k is a hyperparameter that needs to be tuned to achieve the best performance on the validation set.Compared to other types of machine learning algorithms such as decision trees and neural networks, lazy learners have several advantages and disadvantages:Advantages:\nSimple to implement and understand\nCan handle complex decision boundaries\nCan adapt to changes in the data without requiring retraining\nDisadvantages:\nRequire a lot of memory to store the training data\nSlow to make predictions, especially for large datasets\nSensitive to the choice of distance metric and value of k\nDespite its drawbacks, the k-NN algorithm is still widely used in many applications such as recommendation systems, image recognition, and anomaly detection.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158647,"modifiedTime":1737554783000,"sourceSize":1958,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/7. Lazy Leaner.md","exportPath":"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html":{"title":"8. Eager Learning","icon":"","description":"Eager learning, also known as eager training, is a machine learning approach where the model is trained on all available data in advance, before making any predictions. In eager learning, the model is built during the training phase, and the learned parameters are used directly for making predictions on new, unseen data.One of the primary advantages of eager learning is that it allows for fast prediction times, as the model is pre-built and can make predictions immediately. Additionally, eager learning can be used to build complex models, as it allows for the incorporation of a large number of features and a wide range of learning algorithms.However, eager learning also has some limitations. One drawback is that it can be computationally expensive to train a model on large datasets, especially when using complex learning algorithms. Additionally, the model can suffer from overfitting, as it is trained on all available data and may not generalize well to new data.Examples of machine learning algorithms that use eager learning include decision trees, random forests, and gradient boosting.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158650,"modifiedTime":1737554783000,"sourceSize":1156,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/8. Eager Learning.md","exportPath":"artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/basics/1.-components.html":{"title":"1. Components","icon":"","description":"NLP stands for Natural Language Processing, and it is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. NLP enables machines to understand, interpret, and generate natural language, allowing them to communicate with humans more effectively.NLP can be broken down into two main components:\nNatural Language Understanding (NLU): NLU is the part of NLP that deals with the comprehension of human language by computers. It involves tasks such as speech recognition, text understanding, sentiment analysis, named entity recognition, part-of-speech tagging, and parsing. NLU helps computers understand the meaning and context behind the words and phrases in a piece of text or speech.\nNatural Language Generation (NLG): NLG is the part of NLP that focuses on the generation of human-like language by machines. It involves tasks such as machine translation, text summarization, chatbot responses, and content generation. NLG allows computers to produce coherent and contextually appropriate sentences and paragraphs based on the given input or data. Syntax: Syntax refers to the structure and arrangement of words or tokens in a sentence to form grammatically correct phrases or sentences. Understanding syntax is essential for tasks like part-of-speech tagging, parsing, and grammar checking. NLP models need to capture the relationships between words to comprehend the meaning of a sentence accurately.\nSemantics: Semantics deals with the meaning of words, phrases, and sentences in a language. It involves understanding the context and the intended meaning behind the words. Tasks like word sense disambiguation, sentiment analysis, and information retrieval rely heavily on semantic understanding. NLP models aim to represent the semantic content of text to make accurate predictions and interpretations.\nDiscourse: Discourse refers to the organization and cohesion of sentences and ideas in a larger piece of text, such as paragraphs or documents. It involves understanding how sentences relate to each other and how they contribute to the overall message or topic. Coreference resolution, document summarization, and text generation are some NLP tasks that deal with discourse.\nPragmatics: Pragmatics goes beyond the literal meaning of words and takes into account the context, intentions, and implications of the speaker or writer. It deals with the pragmatic aspects of language use, including implicatures, presuppositions, and speech acts. NLP models must be sensitive to pragmatics to understand the true intent of a message and to generate contextually appropriate responses.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Four key aspects","level":3,"id":"Four_key_aspects_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/basics/1.-components.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158711,"modifiedTime":1737554783000,"sourceSize":2869,"sourcePath":"Artificial Intelligence/Natural Language Processing/Basics/1. Components.md","exportPath":"artificial-intelligence/natural-language-processing/basics/1.-components.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html":{"title":"1. Pipeline","icon":"","description":"The NLP pipeline, also known as the NLP workflow, refers to the sequence of processing steps and techniques used to convert raw natural language text into a format that can be analyzed and understood by a computer. The NLP pipeline typically involves a series of preprocessing, analysis, and post-processing steps to extract meaningful information from text data. While the specific steps in the pipeline can vary depending on the task and the complexity of the NLP system, a basic NLP pipeline may include the following steps:\nText Preprocessing: The first step is to preprocess the raw text to make it suitable for analysis. Common preprocessing steps include converting text to lowercase, removing punctuation, tokenizing (splitting) the text into words or subword units, and removing stop words (commonly occurring words with little semantic value, e.g., \"the,\" \"is,\" \"and\").\nPart-of-Speech Tagging: Part-of-speech (POS) tagging is the process of assigning a grammatical category (noun, verb, adjective, etc.) to each word in the text. This step is crucial for understanding the syntactic structure of the text and is useful for various NLP tasks like parsing and information extraction.\nNamed Entity Recognition (NER): NER involves identifying and classifying named entities in the text, such as names of people, organizations, locations, dates, and more. This step is important for extracting relevant information from the text and is used in information retrieval and question answering systems.\nParsing and Dependency Parsing: Parsing involves analyzing the grammatical structure of a sentence to determine how words relate to each other. Dependency parsing specifically focuses on identifying the syntactic relationships (dependencies) between words in a sentence, typically represented as a tree or graph structure.\nSentiment Analysis: Sentiment analysis is the process of determining the sentiment or emotion expressed in a piece of text. It can be used to classify text as positive, negative, or neutral, and it finds applications in social media monitoring, customer feedback analysis, and more.\nMachine Translation (optional): In some NLP pipelines, machine translation can be included as a step to translate text from one language to another using various translation models and techniques.\nInformation Extraction: Information extraction involves identifying and extracting structured information from unstructured text. This can include extracting relations between entities, events, or other structured data points.\nText Generation (optional): In certain cases, text generation models like language models or chatbot systems may be included in the pipeline to produce natural language responses based on input data or context.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158715,"modifiedTime":1737554783000,"sourceSize":2835,"sourcePath":"Artificial Intelligence/Natural Language Processing/Basics/1. Pipeline.md","exportPath":"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/basics/2.-tasks.html":{"title":"2. Tasks","icon":"","description":"1. Text Processing:\nTokenization: Breaking text into smaller units (words, subword units) for analysis.\nSentence Segmentation: Splitting text into sentences for processing.\nStemming and Lemmatization: Reducing words to their base or root forms.\nStop Word Removal: Eliminating common words with little semantic value.\nSpell Checking: Correcting spelling errors in the text.\nPunctuation Removal: Stripping punctuation marks from the text.\nCase Conversion: Converting text to lowercase or uppercase.\nNoise Removal: Removing irrelevant or noisy elements from text data.\nNormalization: Converting text to a standardized format (e.g., converting dates to a common format).\nTokenization for Agglutinative Languages: Handling word segmentation in languages with complex morphology.\n2. Language Formalization &amp; Understanding:\nPart-of-Speech Tagging (POS Tagging): Assigning grammatical categories to words.\nNamed Entity Recognition (NER): Identifying entities like names, locations, and organizations.\nSyntax and Dependency Parsing: Analyzing the grammatical structure of sentences.\nCoreference Resolution: Resolving references to the same entity in a text.\nSemantic Role Labeling: Identifying the roles of words in relation to predicates.\nWord Sense Disambiguation: Determining the correct meaning of ambiguous words.\nSentiment Analysis: Identifying the sentiment or emotion expressed in text.\nTopic Modeling: Identifying themes or topics in a collection of documents.\nQuestion Answering: Automatically answering questions based on text data.\nMachine Translation: Translating text from one language to another.\nInformation Extraction: Extracting structured information from unstructured text.\nText Summarization: Generating concise summaries of longer text.\n3. NLP Applications:\nText Classification: Categorizing text into predefined classes or categories.\nSentiment Analysis for Customer Feedback: Analyzing sentiments in customer reviews.\nChatbots and Virtual Assistants: Generating natural language responses to user queries.\nMachine Translation Services: Enabling translation between different languages.\nSpeech Recognition: Converting spoken language into written text.\nText-to-Speech (TTS): Converting written text into spoken language.\nNamed Entity Recognition for Information Retrieval: Extracting relevant entities from text for search.\nText Generation for Language Models: Generating coherent and contextually relevant text.\nText Summarization for News and Articles: Generating summaries for news stories and articles.\nSpeech Emotion Recognition: Identifying emotions expressed in spoken language.\nLanguage Model Pre-training: Training language models to learn patterns in language data.\nSpam Detection: Identifying and filtering spam emails or messages.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/basics/2.-tasks.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158718,"modifiedTime":1737554783000,"sourceSize":2841,"sourcePath":"Artificial Intelligence/Natural Language Processing/Basics/2. Tasks.md","exportPath":"artificial-intelligence/natural-language-processing/basics/2.-tasks.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html":{"title":"0. Word Segmentation","icon":"","description":"Process of dividing a continuous sequence of characters (usually in a sentence) into individual words or tokens. Word segmentation is a fundamental step in many NLP tasks because it allows the computer to understand the structure and meaning of the text.In some languages, like English, words are typically separated by spaces, making word segmentation relatively straightforward. However, in languages like Chinese, Thai, and Japanese, words are often written without explicit spaces between them, posing a significant challenge for natural language processing.Here's a brief overview of word segmentation in different types of languages:\nSpace-Separated Languages (e.g., English): In languages where words are separated by spaces, word segmentation is as simple as splitting the text on spaces. For example, the sentence \"I love NLP\" would be segmented into the words \"I,\" \"love,\" and \"NLP\".\nAgglutinative Languages (e.g., Turkish): Some languages have agglutinative morphology, where words are formed by adding suffixes and prefixes to root words. In these languages, word segmentation may involve identifying morphemes and breaking down words into their constituent parts.\nNon-Space-Separated Languages (e.g., Chinese): In languages like Chinese, words are not separated by spaces, and word segmentation becomes a more challenging task. Word segmentation algorithms in these languages often use statistical methods, machine learning models, or rule-based approaches to identify word boundaries based on context and frequency statistics.\nHybrid Languages (e.g., Thai): Some languages use a mix of space-separated and non-space-separated writing conventions. In these cases, word segmentation algorithms need to be adaptive and context-aware to handle both types of word boundaries effectively.\nFor English and many European languages, word segmentation is relatively straightforward, as words are typically separated by spaces. However, in other languages where words are not explicitly delimited, word segmentation is a crucial and often complex step in NLP pipelines, as it directly impacts the accuracy of downstream NLP tasks such as part-of-speech tagging, named entity recognition, and machine translation. Researchers and developers continuously work on improving word segmentation algorithms to support a wide range of languages and improve NLP performance across diverse linguistic contexts.\nWhitespace Tokenization: For space-separated languages like English, simple whitespace tokenization is often sufficient for word segmentation. This involves splitting the text on spaces to obtain individual words.\nNLTK (Natural Language Toolkit): NLTK is a popular Python library for NLP, and it provides various tokenization functions, including word tokenization. It can handle different languages and tokenization styles.\nspaCy: spaCy is another powerful Python library for NLP that includes word tokenization capabilities. It is known for its speed and efficiency, making it suitable for processing large volumes of text.\nStanford NLP Toolkit: The Stanford NLP Toolkit offers word segmentation modules for multiple languages. It provides pre-trained models for various tasks, including tokenization, for different languages.\nMecab: Mecab is a popular open-source tokenizer for Japanese text. It is designed to handle the complex word segmentation in the Japanese language and is widely used in NLP projects involving Japanese text.\njieba: jieba is a popular Chinese text segmentation library for Python. It uses a dictionary-based approach and is widely used for word segmentation in Chinese text.\nSentencePiece: SentencePiece is a language-agnostic subword tokenizer and detokenizer designed for Neural Machine Translation and other NLP tasks. It is effective for word segmentation in languages with complex writing systems.\nCharacter-Based Approaches: In languages where words are not explicitly separated, character-based approaches can be used for word segmentation. These methods leverage statistical models or neural networks to predict word boundaries based on character sequences and context.\nConditional Random Fields (CRF): CRF models can be used for word segmentation, especially when dealing with languages where word boundaries are not apparent. CRF models learn to predict word boundaries based on contextual features and linguistic information.\nNeural Network Models: Deep learning models, such as recurrent neural networks (RNNs) and transformers, have also been applied to word segmentation tasks. These models can learn to segment words based on character sequences and language patterns.\nimport nltk def segment_sentence(sentence): # Download the required resources for tokenization (only need to do this once) nltk.download('punkt') # Tokenize the sentence into words words = nltk.word_tokenize(sentence) return words # Input sentence\nsentence = \"Practice makes perfect.\" # Call the function to segment the sentence into words\nsegmented_words = segment_sentence(sentence) # Print the segmented words\nprint(segmented_words)\nimport jieba def segment_sentence(sentence): # Tokenize the sentence into words using jieba words = jieba.cut(sentence) return list(words) # Input sentence\nsentence = \"爱是一种复杂的情感。\" # Call the function to segment the sentence into words\nsegmented_words = segment_sentence(sentence) # Print the segmented words\nprint(segmented_words)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Tools","level":3,"id":"Tools_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158726,"modifiedTime":1737554783000,"sourceSize":5931,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/0. Word Segmentation.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html":{"title":"1. Segmentation Method","icon":"","description":"Segmentation methods in NLP are techniques used to break down continuous streams of text into meaningful units, such as words, phrases, or subword units. Proper segmentation is crucial for many NLP tasks, as it provides the foundation for understanding and analyzing natural language data. Here are some key notes about segmentation methods in NLP:\nWord Segmentation: Word segmentation is the most common form of segmentation, especially in space-separated languages like English. It involves dividing text into individual words based on spaces or other punctuation marks. For languages without explicit word delimiters, specialized algorithms or statistical models are used for word segmentation.\nSubword Segmentation: In some NLP applications, subword segmentation is employed instead of word segmentation. Subword units can include character n-grams, morphemes, or other subword representations. Subword segmentation can be beneficial for languages with complex morphologies or for handling out-of-vocabulary words in machine translation and speech recognition tasks.\nDictionary-Based Segmentation: Some segmentation methods rely on pre-built dictionaries or word lists to identify and split words in the text. Dictionary-based segmentation is suitable for languages with fixed vocabularies or well-defined word boundaries.\nRule-Based Segmentation: Rule-based methods utilize handcrafted linguistic rules to identify word boundaries or segment text. These rules may be language-specific or designed for specific types of texts or domains.\nStatistical Segmentation: Statistical models, such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs), can be used for segmentation tasks. These models learn from annotated data and make predictions based on statistical patterns in the text.\nNeural Network-Based Segmentation: Deep learning techniques, such as recurrent neural networks (RNNs) and transformers, have been increasingly applied to segmentation tasks. Neural networks can capture complex linguistic patterns and context, making them effective for word and subword segmentation.\nUnsupervised Segmentation: Unsupervised segmentation methods do not rely on annotated data or linguistic rules. Instead, they use statistical properties of the text to identify segments, making them useful for low-resource languages or domains with limited training data.\nHybrid Approaches: In practice, hybrid segmentation approaches that combine multiple methods may be used to achieve better segmentation results. For instance, a system may use a dictionary-based approach for known words and a neural network for handling unknown or out-of-vocabulary terms.\nLanguage-Specific Considerations: Different languages and writing systems may require tailored segmentation methods due to variations in word formation, word boundaries, and linguistic characteristics.\nTask-Dependent Segmentation: The choice of segmentation method may also be influenced by the specific NLP task at hand. Certain tasks, such as machine translation, may benefit from subword segmentation to handle unseen words, while others, like part-of-speech tagging, may require precise word boundaries.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158730,"modifiedTime":1737554783000,"sourceSize":3287,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/1. Segmentation Method.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html":{"title":"2. Max Matching","icon":"","description":"Introduction: Max Matching is a word segmentation algorithm used primarily for languages where words are written continuously without explicit spaces or word delimiters. It is a widely used approach for Chinese word segmentation, where words are written as a sequence of characters without spaces between them. The Max Matching algorithm aims to segment the input text into meaningful words using a lexicon or dictionary.Forward Max Matching:\nInput Text: Given a continuous stream of text without spaces, such as \"我喜欢学习自然语言处理。\" (I like studying Natural Language Processing.)\nLexicon or Dictionary: The algorithm uses a lexicon containing known words and their corresponding meanings. For example, it may contain words like \"我\" (I), \"喜欢\" (like), \"学习\" (study), \"自然\" (natural), \"语言处理\" (language processing), etc.\nMatching Process: Forward Max Matching starts from the beginning of the input text. It attempts to match the longest possible word from the lexicon in the input text. For example, it checks if \"我喜欢学习自然语言处理。\" starts with a word in the lexicon. If a match is found, the algorithm considers this segment as a word and moves to the next segment in the input text. If a match is not found, it reduces the search window and checks for the next longest word. The process continues until the entire text is segmented into words.\nOutput: The output of Forward Max Matching is the segmented text with words separated. For the given example, the output would be [\"我\", \"喜欢\", \"学习\", \"自然\", \"语言处理\", \"。\"].\nBackward Max Matching:\nInput Text: Same continuous stream of text without spaces, such as \"我喜欢学习自然语言处理。\" (I like studying Natural Language Processing.)\nLexicon or Dictionary: Same lexicon containing known words and their meanings.\nMatching Process: Backward Max Matching starts from the end of the input text. It attempts to match the longest possible word from the lexicon at the end of the input text. For example, it checks if \"我喜欢学习自然语言处理。\" ends with a word in the lexicon. If a match is found, the algorithm considers this segment as a word and moves to the previous segment in the input text. If a match is not found, it reduces the search window and checks for the next longest word. The process continues until the entire text is segmented into words.\nOutput: The output of Backward Max Matching is the segmented text with words separated. For the given example, the output would be [\"我\", \"喜欢\", \"学习\", \"自然语言\", \"处理\", \"。\"].\nComparison and Considerations:\nForward Max Matching and Backward Max Matching may produce different segmentation results, especially for ambiguous cases.\nCombining the results of Forward and Backward Max Matching (Bidirectional Max Matching) can improve the overall segmentation accuracy.\nMax Matching is a relatively simple and efficient method but may not handle unseen words well or consider the global context in some cases.\nMore advanced methods, like Conditional Random Fields (CRF) or neural network-based models, can achieve higher accuracy for Chinese word segmentation by considering context and statistical patterns.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158734,"modifiedTime":1737554783000,"sourceSize":3436,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/2. Max Matching.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html":{"title":"3. Incorporate Semantic","icon":"","description":"Introduction: Segmentation methods with semantic incorporation go beyond traditional word segmentation algorithms by considering not only the surface forms of words but also their underlying semantic meanings and context. These methods leverage linguistic knowledge, semantic relationships, and contextual information to achieve more accurate and meaningful word segmentation, especially in languages with complex morphology and word boundaries.\nWord Embeddings: Word embeddings, such as Word2Vec, GloVe, or FastText, are dense vector representations of words that capture semantic similarities and relationships. By using pre-trained word embeddings or learning embeddings specific to the task or domain, word segmentation algorithms can leverage semantic information to make better segmentation decisions. For example, the word \"king\" minus the word \"man\" might give a result similar to the word \"queen\" minus \"woman\". This suggests the word embeddings have learned the relationship \"male\" to \"female\" or \"king\" to \"queen\".\nContextual Word Embeddings: Models like ELMo (Embeddings from Language Models) and BERT (Bidirectional Encoder Representations from Transformers) provide contextualized word embeddings, which encode the meaning of words based on the context in which they appear. Incorporating contextual embeddings into segmentation models allows them to understand word boundaries in different linguistic contexts.\nSemantic Role Labeling (SRL): SRL identifies the roles of words in relation to predicates (e.g., subject, object, etc.). By considering the semantic roles of words, segmentation methods can better distinguish word boundaries in sentences with complex grammatical structures.\nDependency Parsing: Dependency parsing analyzes the grammatical structure of a sentence, representing words as nodes and their syntactic relationships as edges in a dependency tree. Dependency parsing can help in identifying word boundaries based on their syntactic connections.\nWord Sense Disambiguation (WSD): WSD is the task of determining the correct meaning of ambiguous words based on the context in which they appear. Incorporating WSD into segmentation models can help in distinguishing different senses of polysemous words.\nMorphological Analysis: For languages with rich morphologies, incorporating morphological analysis can help identify word boundaries and morphemes within complex words.\nNamed Entity Recognition (NER): NER identifies named entities like names, locations, and organizations in text. Incorporating NER can help segment texts containing named entities more accurately. Semantic incorporation allows segmentation models to capture the meaning and context of words, leading to more accurate word boundaries and segmentation results.\nContextual embeddings enable segmentation models to handle language ambiguity and disambiguate words based on context.\nSemantic incorporation is particularly useful for languages with complex word formation and word boundaries.\nThe main challenge lies in the complexity of incorporating semantic information, which may require more extensive data, computational resources, and sophisticated models.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Semantic Incorporation Techniques:","level":3,"id":"Semantic_Incorporation_Techniques_0"},{"heading":"Advantages and Challenges:","level":3,"id":"Advantages_and_Challenges_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158737,"modifiedTime":1737554783000,"sourceSize":3459,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/3. Incorporate Semantic.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html":{"title":"3.1 Viterbi Algorithm","icon":"","description":"The Viterbi algorithm is a dynamic programming algorithm used in various fields, including natural language processing (NLP) and speech recognition, to find the most likely sequence of hidden states (e.g., part-of-speech tags, states in a Hidden Markov Model) that generates a given sequence of observations (e.g., words, acoustic features).The algorithm is based on the principle of dynamic programming and makes use of the concept of optimal substructure, which states that an optimal solution to a larger problem can be constructed by combining optimal solutions to its smaller subproblems.Notation:\nN: Number of possible hidden states.\nT: Length of the observation sequence.\nq_i: Hidden state i.\nO_t: Observation at time t.\nAlgorithm Steps: Initialization: Create a matrix V with dimensions (N x T), where V[i][t] represents the maximum probability of any path ending in hidden state q_i and generating observations O_1, O_2, ..., O_t.\nInitialize the first column of V (V[:, 0]) with the initial probabilities of each hidden state multiplied by the emission probabilities for the first observation O_1. Recursion: For each time step t from 1 to T-1: For each hidden state q_i at time t, calculate V[i][t] as the maximum of the probabilities of transitioning from any previous state q_j at time t-1 to q_i and multiplying it by the emission probability of the observation O_t for state q_i. Termination: The final probability of the most likely path is the maximum value in the last column of matrix V (i.e., max(V[:, T-1])).\nTrace back the most likely path by following the highest probability transitions from the last column of V to the first column. Example:Let's illustrate the Viterbi algorithm with a simple example of part-of-speech tagging. We have a sequence of observations (words) and want to find the most likely sequence of part-of-speech tags for these words.\nHidden states (part-of-speech tags): Noun (N), Verb (V), Adjective (Adj).\nObservation sequence (words): \"I\", \"like\", \"green\", \"apples\".\nAssuming we have precomputed transition probabilities and emission probabilities, the Viterbi algorithm will calculate the most likely sequence of part-of-speech tags for the given observation sequence.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158741,"modifiedTime":1737554783000,"sourceSize":2389,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/3.1 Viterbi Algorithm.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html":{"title":"0. Basics","icon":"","description":"Machine translation is a subfield of Natural Language Processing (NLP) that focuses on developing automated systems or algorithms capable of translating text or speech from one language to another without human intervention. The goal of machine translation is to enable seamless communication between people who speak different languages, breaking down language barriers and fostering global understanding and collaboration.There are different approaches to machine translation, and some of the commonly used methods include:\nRule-based Machine Translation (RBMT): In RBMT, linguistic experts create a set of rules and dictionaries that govern how a sentence is translated from one language to another. The translation process follows these predefined rules, which can be complex and time-consuming to build. While RBMT can provide accurate translations in specific domains, it may struggle with more complex sentences and nuances in natural language.\nStatistical Machine Translation (SMT): SMT relies on statistical models and algorithms that learn from vast amounts of bilingual text corpora. These models identify patterns and associations between words and phrases in different languages to make translations. SMT was a significant improvement over RBMT and became popular in the early 2000s.\nNeural Machine Translation (NMT): NMT is a more recent and currently dominant approach to machine translation. It uses deep learning techniques, particularly neural networks, to process entire sentences or sequences of words simultaneously. NMT models can capture complex patterns and contextual information, leading to more fluent and accurate translations. They outperform traditional SMT models in most scenarios.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158749,"modifiedTime":1737554783000,"sourceSize":1747,"sourcePath":"Artificial Intelligence/Natural Language Processing/Machine Translation/0. Basics.md","exportPath":"artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html":{"title":"1. Statistical Machine Translation (SMT)","icon":"","description":"Statistical Machine Translation (SMT) is a popular approach to machine translation that gained prominence in the early 2000s. It relies on statistical models and algorithms to translate text from one language to another. Here are some key notes about Statistical Machine Translation:\nCorpora-Based Approach: SMT utilizes large bilingual text corpora, which are collections of parallel sentences in the source and target languages. These corpora serve as training data to build statistical models for translation.\nPhrase-Based Translation: One of the fundamental units of translation in SMT is the phrase. Phrases are short sequences of words that frequently appear together in the bilingual corpora. Instead of translating word by word, SMT translates these phrases, which allows for better context preservation.\nTranslation Model: The translation model in SMT calculates the likelihood of translating a source language phrase into a target language phrase. This likelihood is learned from the bilingual corpora, and it helps determine the best translation for a given input.\nAlignment Model: The alignment model identifies the correspondence between words or phrases in the source and target languages. It determines how different phrases align with each other in the parallel sentences of the bilingual corpora.\nDecoding: During translation, SMT explores various possible translations and chooses the most likely translation based on the translation and alignment models. The process of selecting the best translation is known as decoding.\nN-gram Language Models: To improve fluency and coherence of translations, SMT often incorporates n-gram language models. These models calculate the likelihood of word sequences in the target language based on monolingual text data.\nLimitations: SMT may struggle with handling long-distance dependencies and preserving word order, especially in languages with different word orders. It can also encounter difficulties with translating idiomatic expressions and handling rare or unseen phrases.\nAdvantages: SMT was a significant advancement over rule-based machine translation, as it does not rely on handcrafted linguistic rules. It can leverage vast amounts of bilingual data and scale well with more data.\nChallenges in Training: Training SMT models requires substantial computational resources and large parallel corpora. Additionally, the quality of translations heavily depends on the quality and size of the training data.\nTransition to Neural Machine Translation (NMT): While SMT was a major breakthrough, Neural Machine Translation (NMT) has largely replaced it as the dominant approach due to its ability to capture complex patterns and contextual information more effectively.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158754,"modifiedTime":1737554783000,"sourceSize":2844,"sourcePath":"Artificial Intelligence/Natural Language Processing/Machine Translation/1. Statistical Machine Translation (SMT).md","exportPath":"artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html":{"title":"Conditional Random Fields (CRF)","icon":"","description":"Conditional Random Fields (CRF) are a type of graphical model that is used to model sequential data, such as speech or text. CRFs are designed to model the conditional probability distribution of a sequence of labels, given a sequence of observed data. The basic architecture of a CRF consists of a set of observed features, which are used to make predictions about the labels, and a set of hidden variables, which represent the labels themselves.CRFs are typically trained using maximum likelihood estimation (MLE), which involves finding the model parameters that maximize the likelihood of the training data. This can be done using iterative optimization algorithms such as gradient descent or L-BFGS. During training, the model learns to assign higher probabilities to sequences of labels that are more likely to occur in the training data.CRFs have been used in a variety of applications, including speech recognition, named entity recognition, and part-of-speech tagging. One advantage of CRFs over other sequence labeling models, such as Hidden Markov Models (HMMs), is that they can incorporate a larger number of features, including higher-order dependencies between labels.One limitation of CRFs is that they can be computationally expensive to train and evaluate, especially when dealing with large datasets or models with many parameters. In addition, CRFs are often prone to overfitting, which can lead to poor generalization performance on new data.Despite these limitations, CRFs remain a popular choice for many sequence labeling tasks, and they have been successfully applied in a wide range of domains.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158762,"modifiedTime":1737554783000,"sourceSize":1828,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Conditional Random Fields (CRF).md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html":{"title":"Decoding Algorithm","icon":"","description":"The decoding algorithm is a critical component of machine translation systems, including both Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). It determines how a machine translation system generates the most likely translation for a given input sentence in the source language. The decoding process involves exploring different possible translations and selecting the one with the highest probability or score according to the translation model and other relevant components. Here's a general overview of the decoding algorithm:\nInput Sentence Representation: The decoding process starts with representing the input sentence in the source language. In SMT, this may involve breaking the sentence into phrases or subword units, while in NMT, it's typically encoded as a sequence of embeddings or vectors.\nTranslation Model Scoring: The translation model calculates the likelihood of translating each phrase (in SMT) or word (in NMT) from the source language to the target language. This score is based on the statistical models learned during training from the bilingual corpora.\nLanguage Model Scoring: In SMT and NMT, language models are used to ensure fluency and coherence in the translated output. The language model assigns a score to the target language sentence based on the likelihood of the word sequence in the target language.\nBeam Search (In SMT): In SMT, a beam search algorithm is commonly used for decoding. Beam search is a heuristic search algorithm that explores the translation possibilities in a breadth-first manner. At each step, it keeps track of the \"top-k\" hypotheses based on translation model scores and pruning less promising candidates. This helps reduce the computational complexity while still considering multiple translation candidates.\nGreedy Decoding (In NMT): In Neural Machine Translation, greedy decoding is a simple and fast decoding strategy. It involves selecting the word with the highest probability at each time step, iteratively building the translation from left to right. Greedy decoding is fast but may not always produce the most fluent or accurate translations.\nBeam Search (In NMT): Similar to SMT, beam search can also be applied to NMT for decoding. It explores multiple translation hypotheses simultaneously and keeps the top-k candidates based on a combined score of translation and language model probabilities. Beam search in NMT allows for considering the context and generating more fluent and coherent translations.\nLength Normalization: Since longer sentences tend to have lower probabilities, length normalization is often applied to penalize longer translations. This prevents the decoding algorithm from favoring shorter translations simply due to higher probability.\nStopping Criteria: The decoding algorithm continues until a certain stopping criterion is met. For example, it may stop when a maximum sentence length is reached or when the algorithm has generated a fixed number of candidate translations.\nFinal Selection: After exploring all possible translations, the decoding algorithm selects the translation with the highest combined score from the translation model and the language model as the final output.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158811,"modifiedTime":1737554783000,"sourceSize":3357,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Decoding Algorithm.md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html":{"title":"Hidden Markov Models (HMM)","icon":"","description":"Hidden Markov Models (HMMs) are statistical models used to analyze time-series data in which the underlying system being studied is assumed to be a Markov process with hidden states. They are commonly used in speech recognition, natural language processing, bioinformatics, and many other fields.An HMM is a doubly stochastic process that models both the observed output and the hidden state of a system. The model consists of a set of states, each of which emits an observation, and a set of transitions between the states. The transitions between states are probabilistic and are determined by a transition matrix, which specifies the probability of moving from one state to another. The observations emitted by each state are also probabilistic and are determined by an emission matrix, which specifies the probability of observing a particular output given the current state.Training an HMM typically involves estimating the parameters of the transition and emission matrices from a set of training data. One common approach is the Baum-Welch algorithm, which uses the expectation-maximization (EM) algorithm to iteratively estimate the parameters of the model.HMMs have many applications, including speech recognition, where they are used to model the phoneme sequence of spoken words, and bioinformatics, where they are used to analyze DNA and protein sequences. HMMs can also be used in natural language processing to model the sequence of words in a sentence and to perform part-of-speech tagging.One limitation of HMMs is that they assume that the system being modeled is a Markov process, which may not always be the case. In addition, the performance of HMMs can be sensitive to the choice of model parameters and the quality of the training data. Finally, HMMs are computationally intensive, which can limit their applicability in real-time systems.\nDefine the states: The first step is to define the states of the system. These can represent anything from physical states (e.g., position, velocity) to abstract states (e.g., emotions, intentions).\nDefine the observations: Next, we define the observations that we can make about the system. These are often noisy or incomplete measurements of the underlying states.\nDefine the initial state probabilities: We specify the probability distribution over the initial state of the system. where is the probability of the system starting in state .\nDefine the state transition probabilities: We define the probability of transitioning from one state to another, given the current state. where is the probability of transitioning from state to state .\nDefine the observation probabilities: We define the probability of observing a given observation, given the current state.1. where is the probability of observing output symbol given that the system is in state .\nTraining the model: We use the training data to estimate the parameters of the model, i.e., the state transition probabilities and the observation probabilities.\nInference: Once we have trained the model, we can use it to perform inference, i.e., given a sequence of observations, we can infer the most likely sequence of states that generated those observations.\nApplications: HMMs have a wide range of applications, including speech recognition, machine translation, and bioinformatics.\nimport numpy as np\nfrom hmmlearn import hmm # Define the HMM model\nmodel = hmm.MultinomialHMM(n_components=2) # Set the initial probabilities for the states\nmodel.startprob_ = np.array([0.5, 0.5]) # Set the transition probabilities between states\nmodel.transmat_ = np.array([[0.7, 0.3], [0.3, 0.7]]) # Set the emission probabilities for each state\nmodel.emissionprob_ = np.array([[0.9, 0.1], [0.2, 0.8]]) # Define the observation sequence\nobs = np.array([0, 1, 0, 0, 1, 1, 0, 1, 1, 1]) # Train the HMM model on the observation sequence\nmodel.fit(obs.reshape(-1, 1)) # Predict the most likely sequence of states for the observation sequence\nstates = model.predict(obs.reshape(-1, 1)) print(states)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Limitations","level":3,"id":"Limitations_0"},{"heading":"Process","level":3,"id":"Process_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158815,"modifiedTime":1737554783000,"sourceSize":4524,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Hidden Markov Models (HMM).md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html":{"title":"Word2Vec","icon":"","description":"Word2Vec is a popular technique in natural language processing (NLP) that is used to convert words or phrases into vectors of numerical values. These vectors capture the semantic meaning of words in a continuous vector space, allowing mathematical operations to be performed on them. Word2Vec models are trained on large text corpora to learn word embeddings that represent words in a way that similar words are closer to each other in the vector space.Motivation: Word2Vec addresses the limitations of traditional text representation methods like one-hot encoding, which cannot capture semantic relationships between words. Word2Vec aims to create dense, continuous-valued vectors that encode semantic similarities between words.Two Variants of Word2Vec:Both are neural networks, ffter training, we extract the weights from the hidden layer corresponding to each word. These weights form the dense vectors or embeddings for each word. Skip-gram: The skip-gram model aims to predict the context words (surrounding words) given a target word.\nThe target word is used as input, and the model tries to predict the context words.\nGood for capturing meaning in less frequent words. Continuous Bag of Words (CBOW): The CBOW model aims to predict the target word given a context of surrounding words.\nThe context words are used as input, and the model tries to predict the target word.\nGood for capturing meaning of more frequent words. Training Process: Data Preparation: Large text corpora are used for training.\nText is preprocessed by tokenization and removing stop words, punctuation, etc. Creating Training Pairs: For the skip-gram model, pairs of (target word, context word) are generated from the text.\nFor the CBOW model, pairs of (context words, target word) are generated. Neural Network Architecture: A neural network is designed with an input layer, one or more hidden layers, and an output layer.\nThe hidden layer(s) represent the word embeddings. Training: The neural network is trained using backpropagation and gradient descent.\nThe weights of the network are updated to minimize the difference between predicted and actual context words. Learning Word Embeddings: After training, the weights of the hidden layer(s) are used as the word embeddings. Tokenize the text data into words.\nCreate a vocabulary of unique words. Represent each word in the vocabulary as a one-hot encoded vector.\nIf vocabulary size is , each word's one-hot vector has length , with '1' at the word's index and '0' elsewhere. The neural network consists of: Input layer: Equal to vocabulary size ().\nHidden layer: The embedding layer with size , where is the desired size of word embeddings.\nOutput layer: Equal to vocabulary size (). The loss function is the negative log likelihood.\nFor a single training pair (target word, context word): Loss: Calculate predicted probabilities using the softmax function: ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Word2Vec Training using Skip-Gram Model","level":3,"id":"Word2Vec_Training_using_Skip-Gram_Model_0"},{"heading":"Data Preparation","level":4,"id":"Data_Preparation_0"},{"heading":"One-Hot Encoding","level":4,"id":"One-Hot_Encoding_0"},{"heading":"Neural Network Architecture","level":4,"id":"Neural_Network_Architecture_0"},{"heading":"Objective Function (Loss Function)","level":4,"id":"Objective_Function_(Loss_Function)_0"},{"heading":"Softmax Function","level":4,"id":"Softmax_Function_0"},{"heading":"Training","level":4,"id":"Training_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158818,"modifiedTime":1737554783000,"sourceSize":4520,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Word2Vec.md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html":{"title":"Parsing","icon":"","description":"Parsing is a fundamental task in natural language processing (NLP) that involves the analysis of the grammatical structure of a sentence. It plays a crucial role in understanding the syntax of a sentence, which is essential for various NLP applications, including machine translation, information retrieval, sentiment analysis, and more. Here are some key points to understand about parsing in NLP:1. Parsing Definitions:\nSyntactic Parsing: This involves analyzing the grammatical structure of a sentence, determining the relationships between words, and identifying the sentence's constituents, such as noun phrases, verb phrases, and clauses.\nSemantic Parsing: While syntactic parsing focuses on sentence structure, semantic parsing deals with extracting the meaning or semantics of a sentence, often by mapping it to a formal representation like logical forms or knowledge graphs.\n2. Syntactic Parsing:\nConstituency Parsing: This type of parsing breaks down a sentence into its grammatical constituents, such as noun phrases and verb phrases, and represents the sentence's structure as a tree (usually a parse tree or a syntactic tree).\nDependency Parsing: In dependency parsing, the focus is on identifying the grammatical relationships between words in a sentence. This results in a dependency tree, where each word is linked to a headword through various types of dependencies.\n3. Parsing Algorithms:\nTop-Down Parsing: This approach starts from the sentence's root and recursively applies grammar rules to generate constituents until the entire sentence is parsed.\nBottom-Up Parsing: It begins with individual words and combines them into larger constituents following grammar rules until the root of the sentence is reached.\nChart Parsing: Chart parsing is a dynamic programming approach that uses a chart (data structure) to store intermediate parsing results, which helps avoid redundant computations.\nTransition-Based Parsing: In this approach, a parser uses a sequence of parsing actions (transitions) to build a parse tree. It's commonly used in dependency parsing.\n4. Parsing Models:\nRule-Based Parsing: Traditional rule-based parsers use handcrafted grammar rules and lexicons to parse sentences. These parsers can be accurate but require extensive manual effort.\nStatistical Parsing: Statistical parsers use machine learning techniques to learn parsing patterns from annotated data. Popular models include PCFG (Probabilistic Context-Free Grammar) parsers.\nNeural Parsing: Recent advancements in deep learning have led to the development of neural parsers that use neural networks, such as LSTM or Transformer models, to predict syntactic structures.\n5. Applications of Parsing:\nQuestion Answering: Parsing helps identify the syntactic structure of questions and answers, making it easier to match them.\nMachine Translation: Understanding the structure of sentences in different languages aids in translating them accurately.\nInformation Extraction: Parsing helps extract structured information, such as entities and relationships, from unstructured text.\nGrammar Checking: Parsing can be used to identify grammatical errors and suggest corrections in text.\n6. Challenges in Parsing:\nAmbiguity: Natural language is often ambiguous, and parsing must deal with different valid interpretations of a sentence.\nOut-of-Domain Text: Parsing models may struggle with parsing sentences in domains or topics not well-represented in the training data.\nParsing Errors: Parsing errors can propagate and affect the performance of downstream NLP tasks.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158826,"modifiedTime":1737554783000,"sourceSize":3834,"sourcePath":"Artificial Intelligence/Natural Language Processing/Parsing & Semantic/Parsing.md","exportPath":"artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html":{"title":"Semantic","icon":"","description":"Semantic understanding is a fundamental aspect of NLP, aiming to comprehend the meaning of language beyond its surface structure. It deals with understanding the meaning of words, phrases, sentences, and even entire documents. Semantic understanding in NLP can be categorized into two main dimensions: logical semantics and lexical semantics.1. Logical Semantics:Logical semantics focuses on the formal representation of meaning in natural language. It aims to capture the truth conditions of sentences and the relationships between different linguistic expressions. Here are some key concepts within logical semantics:a. Propositional Logic:\nIn propositional logic, statements are represented as propositions that can be either true or false.\nLogical operators like AND, OR, NOT, and IMPLIES are used to connect propositions.\nFor example, the sentence \"It is raining, and the sun is shining\" can be represented as (P \\wedge Q), where (P) represents \"It is raining\" and (Q) represents \"The sun is shining.\"\nb. First-Order Logic (Predicate Logic):\nFirst-order logic extends propositional logic by introducing variables, predicates, and quantifiers.\nPredicates represent relationships between objects, and quantifiers specify the scope of variables.\nFor example, the sentence \"All cats chase mice\" can be represented as ().\nc. Semantic Roles and Frames:\nSemantic roles identify the specific functions that words or phrases play within a sentence's structure.\nFrames are knowledge structures that capture the typical attributes and actions associated with certain concepts.\nFor example, in the sentence \"The cat chased the mouse,\" \"cat\" would be associated with the frame for \"animal\" and the role of \"agent,\" while \"mouse\" would be associated with the role of \"patient.\"\n2. Lexical Semantics:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158830,"modifiedTime":1737554783000,"sourceSize":3386,"sourcePath":"Artificial Intelligence/Natural Language Processing/Parsing & Semantic/Semantic.md","exportPath":"artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html":{"title":"0. Prepare Data","icon":"","description":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder def prepare_dataset(filename): \"\"\" Prepare the training/validation/test dataset. Args: filename (str): The name of the file from which data will be loaded. Returns: Xr_train (list[str]): Documents in the training set, each represented as a string. y_train (np.ndarray): A vector of class labels for documents in the training set, each element of the vector is either 0 or 1. Xr_val (list[str]): Documents in the validation set, each represented as a string. y_val (np.ndarray): A vector of class labels for documents in the validation set, each element of the vector is either 0 or 1. Xr_test (list[str]): Documents in the test set, each represented as a string. y_test (np.ndarray): A vector of class labels for documents in the test set, each element of the vector is either 0 or 1. \"\"\" print('Preparing train/val/test dataset ...') # Load raw data df = pd.read_csv(filename) # Shuffle the rows df = df.sample(frac=1, random_state=123).reset_index(drop=True) # Get the train, val, test splits train_frac, val_frac, test_frac = 0.7, 0.1, 0.2 Xr = df[\"text\"].tolist() train_end = int(train_frac * len(Xr)) val_end = int((train_frac + val_frac) * len(Xr)) Xr_train = Xr[:train_end] Xr_val = Xr[train_end:val_end] Xr_test = Xr[val_end:] # Encode sentiment labels ('pos' and 'neg') yr = df[\"label\"].tolist() le = LabelEncoder() y = le.fit_transform(yr) y_train = np.array(y[:train_end]) y_val = np.array(y[train_end:val_end]) y_test = np.array(y[val_end:]) return Xr_train, y_train, Xr_val, y_val, Xr_test, y_test\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158872,"modifiedTime":1737554783000,"sourceSize":1794,"sourcePath":"Artificial Intelligence/Natural Language Processing/Techniques/0. Prepare Data.md","exportPath":"artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html":{"title":"1. Feature Extraction & Model","icon":"","description":"tokeniser = nltk.tokenize.TreebankWordTokenizer()\nstopwords = frozenset(nltk.corpus.stopwords.words(\"english\"))\ntrans_table = str.maketrans(dict.fromkeys(string.punctuation)) def tokenise_text(str_): # remove non-ASCII characters str_ = str_.encode(encoding='ascii', errors='ignore').decode() return [t for t in tokeniser.tokenize(str_.lower().translate(trans_table)) if t not in stopwords]\nHere using TF vectors, which can be changed for fitting different demands.def fit_model(Xtr, Ytr, C): \"\"\"Tokenizes the sentences, calculates TF vectors, and trains a logistic regression model. Args: - Xtr: A list of training documents provided as text - Ytr: A list of training class labels - C: The regularization parameter \"\"\" # write model fitting code using CountVectorizer and LogisticRegression # CountVectorizer is used to convert the text into sparse TF vectors # See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html # LogisticRegression will train the classifier using these vectors # See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html # return the model and CountVectorizer # Note: we need to return the CountVectorizer because # it stores a mapping from words -&gt; ids which we need for testing count_vectorizer = CountVectorizer() X_train_tf = count_vectorizer.fit_transform(Xtr) model = LogisticRegression(C=C, max_iter=1000) model.fit(X_train_tf, Ytr) return model, count_vectorizer\ndef test_model(Xtst, Ytst, model, count_vectoriser): # test the logistic regression classifier and calculate the accuracy y_pred = model.predict(X_test) score = accuracy_score(y_test, y_pred) return score\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Feature Extraction &amp; Model","level":1,"id":"1._Feature_Extraction_&_Model_0"},{"heading":"Tokenise","level":3,"id":"Tokenise_0"},{"heading":"Vectors &amp; Fit Model","level":3,"id":"Vectors_&_Fit_Model_0"},{"heading":"Evaluate","level":3,"id":"Evaluate_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158875,"modifiedTime":1737554783000,"sourceSize":1931,"sourcePath":"Artificial Intelligence/Natural Language Processing/Techniques/1. Feature Extraction & Model.md","exportPath":"artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/reinforcement-learning/0.-q-learning.html":{"title":"0. Q-Learning","icon":"","description":"Q-Learning is a model-free reinforcement learning algorithm that aims to find the optimal policy for a Markov decision process (MDP). In this algorithm, an agent learns to take optimal actions by learning the values of state-action pairs, known as Q-values. The Q-values represent the expected discounted reward that an agent will receive if it takes a particular action in a particular state. Q-Learning is a widely-used algorithm due to its simplicity, effectiveness, and ease of implementation.Q-learning is a model-free reinforcement learning algorithm that uses a lookup table to store the expected value (Q-value) of each state-action pair. The Q-value represents the expected total reward for taking a particular action in a given state and following the optimal policy thereafter. The optimal policy is determined by selecting the action with the highest Q-value in each state.Q-learning works by iteratively updating the Q-values using the Bellman equation:where:\nQ(s, a) is the Q-value for taking action a in state s.\nr is the reward for taking action a in state s.\ns' is the next state.\na' is the action that maximizes the Q-value in the next state.\nγ is the discount factor, which determines the importance of future rewards.\nThe Q-values are initialized to arbitrary values and are updated using the Bellman equation after each time step.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Training","level":3,"id":"Training_0"},{"heading":"Applications","level":3,"id":"Applications_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/reinforcement-learning/0.-q-learning.html","pathToRoot":"../..","attachments":[],"createdTime":1741084158884,"modifiedTime":1737554783000,"sourceSize":4669,"sourcePath":"Artificial Intelligence/Reinforcement Learning/0. Q-Learning.md","exportPath":"artificial-intelligence/reinforcement-learning/0.-q-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html":{"title":"0. Introduction","icon":"","description":"\nReasoning and inference involve deriving new information or conclusions from existing knowledge\nIn Symbolic AI, this is done by representing knowledge in a symbolic form and applying logical rules\nThese processes are crucial for making decisions, solving problems, and learning from experience in AI systems Deductive reasoning: deriving logical conclusions from given premises using logical rules\nInductive reasoning: generalizing from specific observations to form more general conclusions\nAbductive reasoning: generating hypotheses to explain observations or data Forward chaining: using available knowledge to derive new conclusions\nBackward chaining: starting with a goal and working backward to determine the necessary conditions for that goal to be achieved Reasoning and inference are fundamental concepts in Symbolic AI\nUnderstanding the different types of reasoning and methods of inference is crucial for developing effective Symbolic AI systems.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"What is Reasoning and Inference?","level":3,"id":"What_is_Reasoning_and_Inference?_0"},{"heading":"Types of Reasoning in Symbolic AI","level":3,"id":"Types_of_Reasoning_in_Symbolic_AI_0"},{"heading":"Inference Methods in Symbolic AI","level":3,"id":"Inference_Methods_in_Symbolic_AI_0"},{"heading":"Conclusion","level":3,"id":"Conclusion_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158946,"modifiedTime":1737554783000,"sourceSize":1132,"sourcePath":"Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/0. Introduction.md","exportPath":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html":{"title":"1. Propositional Logic","icon":"","description":"Propositional logic, also known as sentential logic or propositional calculus, is a branch of logic that deals with propositions, or declarative statements that are either true or false. In propositional logic, propositions are represented by symbols or letters, and logical connectives are used to form compound propositions.Propositional logic uses symbols to represent propositions. These symbols can be any letter or combination of letters, but are typically represented by capital letters (e.g. P, Q, R).Propositional logic also uses logical connectives to form compound propositions from simpler propositions. The most common logical connectives in propositional logic are:\nConjunction ( ∧ ): represents \"and\"\nDisjunction ( ∨ ): represents \"or\"\nNegation ( ¬ ): represents \"not\"\nImplication ( → ): represents \"if...then\"\nEquivalence ( ↔ ): represents \"if and only if\"\nIn propositional logic, truth tables are used to determine the truth value of a compound proposition based on the truth values of its component propositions. Truth tables show all possible combinations of truth values for the component propositions and the resulting truth value of the compound proposition.Propositional logic also deals with logical equivalence and validity. Two propositions are logically equivalent if they always have the same truth value, regardless of the truth values of their component propositions. A compound proposition is valid if it is true for all possible combinations of truth values of its component propositions.\nP: \"It is raining outside.\"\nQ: \"I am staying indoors.\"\nP ∧ Q: \"It is raining outside and I am staying indoors.\"\nP ∨ Q: \"Either it is raining outside or I am staying indoors.\"\n¬P: \"It is not raining outside.\"\nP → Q: \"If it is raining outside, then I am staying indoors.\"\nP ↔ Q: \"It is raining outside if and only if I am staying indoors.\" P: \"The class is cancelled.\"\nQ: \"I can sleep in.\"\nP ∧ Q: \"The class is cancelled and I can sleep in.\"\nP ∨ Q: \"Either the class is cancelled or I can sleep in.\"\n¬Q: \"I cannot sleep in.\"\nP → Q: \"If the class is cancelled, then I can sleep in.\"\nP ↔ Q: \"The class is cancelled if and only if I can sleep in.\" P: \"I am hungry.\"\nQ: \"I will eat a sandwich.\"\nP ∧ Q: \"I am hungry and I will eat a sandwich.\"\nP ∨ Q: \"Either I am hungry or I will eat a sandwich.\"\n¬P: \"I am not hungry.\"\nP → Q: \"If I am hungry, then I will eat a sandwich.\"\nP ↔ Q: \"I am hungry if and only if I will eat a sandwich.\"\nP: \"It is raining outside.\" Q: \"I am staying indoors.\" R: \"I am reading a book.\"\n(P ∧ Q) ∨ ¬R: \"Either it is raining outside and I am staying indoors, or I am not reading a book.\"\nThis example combines three propositions using the conjunction ( ∧ ), disjunction ( ∨ ), and negation ( ¬ ) logical operators.The entire formula reads as \"Either it is raining outside and I am staying indoors, or I am not reading a book.\"Here is the truth table for this formula:As we can see from the truth table, the formula is true in four out of eight possible truth value combinations for P, Q, and R.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Symbols and Connectives","level":2,"id":"Symbols_and_Connectives_0"},{"heading":"Truth Tables","level":3,"id":"Truth_Tables_0"},{"heading":"Logical Equivalence and Validity","level":3,"id":"Logical_Equivalence_and_Validity_0"},{"heading":"Examples of Propositional Logic","level":3,"id":"Examples_of_Propositional_Logic_0"},{"heading":"Example 1","level":4,"id":"Example_1_0"},{"heading":"Example 2","level":4,"id":"Example_2_0"},{"heading":"Example 3","level":4,"id":"Example_3_0"},{"heading":"Example 4","level":4,"id":"Example_4_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158950,"modifiedTime":1737554783000,"sourceSize":4000,"sourcePath":"Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/1. Propositional Logic.md","exportPath":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html":{"title":"2. First Order Logic","icon":"","description":"\nConstant: A symbol that represents a fixed object or value, such as 3, \"apple\", or \"John\".\nVariable: A symbol that represents any object or value, such as x, y, or z. Predicate: A symbol that represents a property or relation between objects or values.\nExamples: Even(x): x is an even number.\nRed(x): x is red.\nLoves(x,y): x loves y. Quantifier: A symbol that indicates how many objects or values satisfy a predicate.\nUniversal quantifier (∀): All objects or values satisfy the predicate.\nExistential quantifier (∃): At least one object or value satisfies the predicate.\nExample: ∀x Red(x): All objects are red.\n∃x Loves(John,x): There is someone that John loves. Connective: A symbol that combines two or more logical expressions.\nConjunction (∧): Both expressions are true.\nDisjunction (∨): At least one expression is true.\nNegation (¬): The expression is not true.\nImplication (→): If the first expression is true, then the second expression is also true.\nExample: Red(x) ∧ Round(x): x is both red and round.\nRed(x) ∨ Blue(x): x is either red or blue.\n¬Red(x): x is not red.\nLoves(John,x) → Loves(x,John): If John loves x, then x loves John. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Constants and variables","level":3,"id":"Constants_and_variables_0"},{"heading":"Predicates","level":3,"id":"Predicates_0"},{"heading":"Quantifiers","level":3,"id":"Quantifiers_0"},{"heading":"Connectives","level":3,"id":"Connectives_0"},{"heading":"Transform first-order formulae into first-order clauses","level":3,"id":"Transform_first-order_formulae_into_first-order_clauses_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html","pathToRoot":"../../..","attachments":[],"createdTime":1741084158954,"modifiedTime":1737554783000,"sourceSize":3723,"sourcePath":"Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/2. First Order Logic.md","exportPath":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html":{"title":"0. Agents","icon":"","description":"To design a rational agent, we must specify the task environment\nPerformance measure: the criterion for success in the agent's task\nEnvironment: the external context in which the agent operates\nActuators: the output devices through which the agent interacts with the environment\nSensors: the input devices through which the agent perceives the environment\n• Fully vs partially observable – do the agent sensors give access to all relevant information about the environment state?\n• Deterministic vs stochastic – is the next state completely determined by the current state and executed action?\n• Known vs unknown – does the agent know the environment’s laws of physics?\n• Episodic vs sequential – is the next decision independent of the previous ones? • Static vs dynamic – can the environment change whilst the agent is deliberating? – Semi-dynamic:only the performance score changes.\n• Discrete vs continuous – can time, states, actions, percepts be represented in a discrete way?\n• Single vs multi-agent – is a single agent making decisions, or do multiple agents need to compete or cooperate to maximise interdependent performance measures? 5 Properties of Task Environments Simple reflex agents\nThese agents select actions based solely on the current percept, without considering past percepts or future consequences. They use a set of condition-action rules, known as a rule-based system, to choose the appropriate action. For example, a simple reflex agent might turn on the air conditioning when the temperature exceeds a certain threshold. Reflex agents with state\nThese agents maintain an internal state, which allows them to take into account past percepts when selecting actions. The internal state is updated based on the current percept and the chosen action. For example, a reflex agent with state might adjust the thermostat based on both the current temperature and whether the air conditioning has been turned on or off in the recent past. Goal-based agents\nThese agents select actions based on a desired goal state. They use a problem-solving approach to determine the sequence of actions that will lead to the desired goal state. Goal-based agents can handle complex situations and plan ahead. For example, a goal-based agent might plan a route to a destination by considering the current location, the desired destination, and the available transportation options. Utility-based agents\nThese agents select actions based on a utility function, which assigns a numerical value to each possible state of the environment. The agent's goal is to maximize the expected utility of its actions. Utility-based agents take into account not only the immediate consequences of their actions but also their long-term effects. For example, a utility-based agent might choose to take a more expensive flight that arrives at a more convenient time, based on the utility it assigns to different combinations of price and convenience. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"PEAS:","level":3,"id":"PEAS_0"},{"heading":"Properties of Task Environments","level":3,"id":"Properties_of_Task_Environments_0"},{"heading":"Agent types","level":3,"id":"Agent_types_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158966,"modifiedTime":1737554783000,"sourceSize":5226,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/0. Agents.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html":{"title":"1. Search","icon":"","description":"the process of defining a problem, identifying the resources available to solve it, and determining the goals that need to be achieved. In artificial intelligence, problem formulation is a crucial step in designing an intelligent agent that can reason and act effectively in a given environment.The problem formulation process involves several steps:\nDefining the problem: The first step is to define the problem that needs to be solved. This involves identifying the task, the environment, and the goals of the agent.\nIdentifying the resources: The next step is to identify the resources available to the agent. This includes the sensors that can be used to observe the environment and the actuators that can be used to change it.\nIdentifying the state space: The state space is the set of all possible states that the agent can be in. This includes the initial state of the agent and the possible states that it can transition to based on its actions.\nDefining the actions: The actions are the set of all possible actions that the agent can take. These actions are typically defined in terms of the available actuators.\nDefining the goal: The final step is to define the goal of the agent. This involves specifying the desired state or outcome that the agent should achieve.\nOnce the problem has been formulated, the agent can begin to reason and act to achieve its goals. The process of problem formulation is an iterative one, and the agent may need to refine its understanding of the problem and its resources as it interacts with the environment.Basic idea: offline, simulated exploration of state space by generating successors of already-explored nodes (a.k.a. expanding nodes)\nImplementation: states vs nodes ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Problem formulation:","level":3,"id":"Problem_formulation_0"},{"heading":"Tree search algorithm","level":3,"id":"Tree_search_algorithm_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158970,"modifiedTime":1737554783000,"sourceSize":2586,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/1. Search.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html":{"title":"2. Uninformed search strategies","icon":"","description":"Uninformed search refers to search algorithms that do not use any problem-specific knowledge to guide the search process. These algorithms only rely on the information provided by the problem definition and do not take into account any additional knowledge that might be available.\nA strategy is defined by picking the order of node expansion. This is the order used for the priority queue implementing the frontier. Uninformed strategies use only the information available in the definition of the problem: Breadth-first search Uniform-cost search Depth-first search Depth-limited search Iterative deepening search Complete: the algorithm is guaranteed to find a solution (if one exists) for any valid input. In other words, the algorithm will always terminate with a solution (if there is one), and it will not run indefinitely or get stuck in an infinite loop. Completeness is an important property for algorithms, especially for problems where finding a solution is critical (e.g. safety-critical systems). Optimal: The solution is the best possible solution among all possible solutions. In other words, it is the solution with the lowest cost or shortest path. However, it's important to note that an optimal solution is not always the shortest. For example, in a problem where the cost of a path is measured not only by its length but also by other factors such as time or energy consumption, the optimal solution might not be the shortest path. ItBreadth First Search (BFS) is a graph traversal algorithm that starts at the root node and explores all the neighboring nodes at the current depth level before moving on to the next level. BFS visits nodes in breadth-first order, meaning that all the nodes at a particular depth level are visited before moving on to the next depth level.BFS is used to find the shortest path between two nodes or to visit all the nodes in a graph, given that the graph is connected. The algorithm is implemented using a queue data structure to keep track of the nodes to be visited. (FIFO)Here is the step-by-step process of BFS: Initialize an empty queue and enqueue the root node.\nMark the root node as visited.\nDequeue the next node from the queue and examine its adjacent nodes.\nFor each adjacent node that has not been visited, mark it as visited and enqueue it.\nRepeat steps 3-4 until the queue is empty.\nBFS can be implemented using both iterative and recursive approaches, but the iterative approach is more commonly used because it is more efficient in terms of memory usage.BFS has a time complexity of O(V+E), where V is the number of vertices and E is the number of edges in the graph. The space complexity is also O(V+E) because it requires a queue and a visited array to keep track of the visited nodes.n terms of time complexity, Breadth First Search has a time complexity of 𝑂(𝑏^(𝑑+1)), where 𝑏 is the maximum branching factor of the search tree, and 𝑑 is the depth of the goal node from the start node. Complete: Yes (if 𝑏 and 𝑑 are finite)In terms of space complexity, BFS stores all generated nodes in a queue until they are expanded, which can consume a lot of memory. Therefore, the space complexity of BFS is 𝑂(𝑏^(𝑑+1)).BFS is optimal if the path cost is a non-decreasing function of the depth of the node because BFS always expands the shallowest unexpanded node first, so it is guaranteed to find the optimal solution.In the case of BFS, the time complexity and space complexity are often the same. This is because BFS explores the tree level-by-level, and at each level, it adds all the nodes to the queue before visiting any of their children.In summary, the time complexity of BFS is exponential in the worst case, and its space complexity can be quite high as it needs to keep track of all the generated nodes. However, BFS guarantees to find the optimal solution if it exists.Uniform-cost search is a search algorithm used for finding the shortest path in a weighted graph. It is a unformed search algorithm that uses a priority queue to explore the search space. In uniform-cost search, the priority of each node is determined by the cost of the path from the start node to that node. The algorithm starts at the initial state and explores the search space by repeatedly selecting the node with the lowest cumulative cost.The algorithm maintains a set of visited nodes and a priority queue of unvisited nodes. The priority queue is implemented as a min-heap. Initially, the start node is added to the priority queue with a priority of zero. At each iteration, the algorithm selects the node with the lowest priority from the priority queue and expands it. The algorithm stops when the goal node is expanded or the priority queue is empty.Uniform-cost search guarantees to find the optimal solution if the cost function satisfies the following conditions:\nnon-negative: the cost of each action is non-negative\nfinite graph or state\nTime and Space Complexity:\nComplete: Yes, if step cost ≥ 𝜖 (for 𝜖 &gt; 0)\nTime: number of nodes with 𝑔≤𝐶 , 𝑂(𝑏 ^(1+(C/𝜖))) where 𝐶 is the cost of the optimal solution\nSpace: number of nodes with 𝑔≤𝐶 , 𝑂(𝑏 ^(1+(C/𝜖))), since it needs to store all the nodes in the memory until the goal node is reached.\na search algorithm that explores the deepest paths first before backtracking to less deep nodes. In other words, it starts at the root node and explores as far as possible along each branch before backtracking.\nInitialize a stack and add the starting node to it.\nWhile the stack is not empty:\na. Pop the top node from the stack.\nb. If the node has not been visited, mark it as visited and expand its children.\nc. Add the unvisited children to the stack.\nDFS has the following properties:\nTime complexity: O(b^m), where b is the branching factor and m is the maximum depth of the search tree.\nSpace complexity: O(bm), because it needs to store the nodes on the current path from the root to the current node (deepest node + ancestors + their siblings).\nCompleteness: DFS is not complete if the search space contains infinite paths, because it will get stuck exploring an infinite path and never return to explore other parts of the search space.\nOptimality: DFS is not optimal because it may find a non-optimal solution if it reaches a dead end before finding the optimal solution. However, if the search space is a tree and the goal node is at a shallow depth, DFS can be optimal. Combines advantages of breadth-first and depth-first search\nIs complete\nReturns shallowest solution\nUses linear amount of memory\nDepth-limited search:\nDepth-first search with depth limit l, i.e., nodes at depth l have no successors\nComplete: Yes (if b and d are finite)\nTime: (d+1)_b^0 + d_b^1 + (d-1)*b^2 + ... + b^d = O(b^d)\nSpace: O(bd)\nOptimal: Yes, if step cost = 1 (can be modified to explore uniform-cost tree)\nWhere:\nb is the max branching factor\nd is the depth of the shallowest solution\nm is the max depth of the state space Only if b and d are finite. Only if all step costs are identical.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"BFS: <a data-href=\"Breadth First Search\" href=\"Breadth First Search\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Breadth First Search</a>","level":3,"id":"BFS_[[Breadth_First_Search]]_0"},{"heading":"Uniform-cost search (<a data-href=\"Dijkstra\" href=\"Dijkstra\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Dijkstra</a>)","level":3,"id":"Uniform-cost_search_([[Dijkstra]])_0"},{"heading":"Depth-first search (<a data-href=\"Depth First Search\" href=\"Depth First Search\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Depth First Search</a>)","level":3,"id":"Depth-first_search_([[Depth_First_Search]])_0"},{"heading":"Iterative deepening search (<a data-href=\"Iterative Deepening\" href=\"Iterative Deepening\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Iterative Deepening</a>)","level":3,"id":"Iterative_deepening_search_([[Iterative_Deepening]])_0"},{"heading":"Comparision","level":3,"id":"Comparision_0"}],"links":["algorithms/algo/search/breadth-first-search.html#_0","graph/dijkstra.html#_0","algorithms/algo/search/depth-first-search.html#_0","algorithms/algo/search/iterative-deepening.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158974,"modifiedTime":1737554783000,"sourceSize":9836,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/2. Uninformed search strategies.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html":{"title":"3. Informed search algorithms","icon":"","description":"Evaluation function:\nEvaluation function f (n) = g(n) + h(n) estimate of “desirability”, usually problem-specific. g(n) = cost so far to reach n h(n) = estimated cost from n to the closest goal (heuristic) f (n) = estimated total cost of path through n to goal\n-&gt; The lower f (n), the more desirable n is\n• Evaluation function f (n) = h(n) (entirely heuristic) = estimate of cost from n to the closest goal\n• E.g., hSLD(n) = straight-line distance from n to goal\n• Greedy search expands the node that appears to be closest to goalProperties:\nComplete: No. It can get stuck in loops Complete in finite space with repeated-state checking Time: O(b^m), but a good heuristic can give dramatic improvement\nSpace: O(b^m)\nOptimal: No\n• Idea: avoid expanding paths that are already expensive\n• Evaluation function f (n) = g(n) + h(n)\n- g(n) = cost so far to reach n\n- h(n) = estimated cost from n to the closest goal\n- f(n) = estimated total cost of path through n to goal","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Greedy Search","level":3,"id":"Greedy_Search_0"},{"heading":"A star Search (<a data-href=\"A star\" href=\"A star\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">A star</a>)","level":3,"id":"A_star_Search_([[A_star]])_0"}],"links":["algorithms/algo/search/a-star.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158977,"modifiedTime":1737554783000,"sourceSize":4324,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/3. Informed search algorithms.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html":{"title":"4. Adversarial Search Problems","icon":"","description":" Adversarial search problems involve two or more agents competing against each other. The agents take turns making moves, and each agent tries to maximize their own utility or minimize their opponent's utility. Adversarial search problems are often used in games like chess, checkers, and Go, but can also be applied in other domains like cybersecurity. The minimax algorithm is a popular algorithm used for solving adversarial search problems. It assumes that both players play optimally and tries to minimize the maximum possible loss. The minimax algorithm can be improved by using alpha-beta pruning, which is a technique for reducing the number of nodes explored in the search tree. Adversarial search problems with incomplete information are more difficult to solve, as the agents do not have complete information about the state of the game. Monte Carlo Tree Search (MCTS) is a popular algorithm used for solving adversarial search problems with incomplete information. It works by repeatedly simulating games and using the results to guide the search towards more promising moves. MCTS has been successfully applied in games like poker and Go, where the state of the game is not fully observable. S is the set of all possible game states\nA is the set of all possible actions that can be taken in each state\nP: S × A → S is a transition function that takes a state and an action and returns the resulting state\nR: S × A × S → ℤ is a reward function that takes a state, an action, and a resulting state and returns a numeric reward\nO is the set of all possible outcomes of the game (e.g., win, lose, draw)\nT is a function that maps each outcome in O to the corresponding terminal states in S\nIn addition, a game in AI is often assumed to be:\ndeterministic: the next state is fully determined by the current state and action\nturn-taking: each player takes turns making a move\ntwo-player: there are only two players\nzero-sum: the total reward for one player is equal and opposite to the total reward for the other player\nof perfect information: all players have complete knowledge of the game state at all times.\nAdversarial search problems involve two or more agents or players that are in conflict with each other, where each agent aims to maximize its own utility or minimize its own loss. These types of problems are commonly encountered in game playing, such as chess, poker, and Go.To develop a strategy for adversarial search problems, you can follow these general steps:\nDefine the game: You need to define the game by specifying the initial state, the set of legal moves, and the rules for transitioning from one state to another.\nDefine the evaluation function: An evaluation function is used to evaluate how good a given state is for a particular player. It assigns a numerical score to each state based on how advantageous it is for the player.\nChoose a search algorithm: There are various search algorithms that can be used to find the best move in a game, such as minimax, alpha-beta pruning, and Monte Carlo tree search. The choice of algorithm will depend on the size and complexity of the game, as well as the available computational resources.\nImplement the search algorithm: Once you have chosen a search algorithm, you need to implement it in code. This involves defining the search tree, which represents all possible moves and their resulting states.\nApply the search algorithm: You can apply the search algorithm to the search tree to find the best move for a given player. The search algorithm will explore the search tree and determine the best move based on the evaluation function.\nRepeat steps 4 and 5: As the game progresses, you will need to update the search tree and reapply the search algorithm to find the best move at each turn.\nOverall, developing a strategy for adversarial search problems involves carefully defining the game, choosing an appropriate search algorithm, and implementing it effectively to find the best move for a given player.Tic-Tac-Toe as an example of an adversarial search problem. X | O | X ----------- O | X | O ----------- X | O | X Define the game: The game of Tic-Tac-Toe is played on a 3x3 grid. The game starts with an empty grid, and players take turns placing their symbol (either X or O) on an empty cell. The game ends when one player has three symbols in a row (horizontally, vertically, or diagonally) or when the grid is full and there is no winner.\nDefine the evaluation function: An evaluation function for Tic-Tac-Toe could assign a positive score to a state where the player has three symbols in a row, a negative score to a state where the opponent has three symbols in a row, and a score of zero to a state where there is no winner yet.\nChoose a search algorithm: In the case of Tic-Tac-Toe, the search space is relatively small, so a simple minimax algorithm could be used to search the game tree.\nImplement the search algorithm: The search tree for Tic-Tac-Toe is relatively small, as there are only 9 possible moves at each turn. The search tree can be represented as a recursive tree structure, where each node represents a game state and each edge represents a legal move.\nApply the search algorithm: At each turn, the minimax algorithm would explore the search tree to determine the best move for the current player. The algorithm would choose the move that maximizes the evaluation function for the current player, assuming that the opponent will choose the move that minimizes the evaluation function for the current player.\nRepeat steps 4 and 5: As the game progresses, the search tree will be updated to reflect the current game state, and the minimax algorithm will be reapplied to find the best move at each turn.\nOverall, the strategy for playing Tic-Tac-Toe involves using a search algorithm, such as minimax, to explore the game tree and determine the best move at each turn based on the evaluation function. By doing so, the player can maximize their chances of winning the game.\nStates: The states of the game are the possible configurations of X's and O's on the 3x3 board. Each state can be represented as a 3x3 matrix, with empty cells represented by a special symbol (e.g., \"E\").\nMoves: A move in Tic-Tac-Toe consists of placing an X or an O in an empty cell of the board.\nResult: The result of a move is a new state of the game, where the board has been updated with the new X or O in the corresponding cell.\nTerminal test: The game ends in a terminal state when one player has three symbols in a row (horizontally, vertically, or diagonally) or when the board is full and there is no winner. A terminal state is a state where the game is over and no further moves can be made.\nUtility function: The utility function assigns a score to a terminal state based on whether it represents a win for X, a win for O, or a tie. One common utility function for Tic-Tac-Toe is to assign a score of 1 to a win for X, -1 to a win for O, and 0 to a tie. We start at the root node, which represents the current state of the game. If the game is over (i.e., a terminal state), we return the utility value of that state.\nIf it's the turn of the MAX player (i.e., X), we evaluate the utility value of each child node (i.e., each possible move), by recursively applying the Minimax algorithm to each child node. We select the child node with the highest utility value, since this represents the best move for X.\nIf it's the turn of the MIN player (i.e., O), we evaluate the utility value of each child node by recursively applying the Minimax algorithm to each child node. We select the child node with the lowest utility value, since this represents the best move for O.\nOnce we have evaluated the utility values of all child nodes, we return the selected value (i.e., the maximum or minimum utility value, depending on whose turn it is).\n<a data-href=\"Alpha-Beta\" href=\"algorithms/algo/graph/alpha-beta.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Alpha-Beta</a> A / | \\ B C D / \\ / \\ E F G H\nAssume that we are the maximising player and our opponent is the minimising player. Our goal is to find the best move in this game tree.Without Alpha-Beta Pruning, we would explore every path in the game tree to find the optimal move. However, with Alpha-Beta Pruning, we can reduce the number of nodes that need to be explored.At the start of the algorithm, we set alpha to negative infinity and beta to positive infinity. We start exploring the game tree from the root node A.\nWe first explore node B. We set alpha to the maximum of alpha and the value of B. Since B has not been explored yet, we set alpha to the value of B. We continue exploring the children of B. We explore node E. We set alpha to the maximum of alpha and the value of E. Since E has not been explored yet, we set alpha to the value of E. We backtrack to node B and explore node F. We set alpha to the maximum of alpha and the value of F. Since F has not been explored yet, we set alpha to the value of F.\nWe backtrack to node B and update its value to the maximum of its children, which is F. We continue exploring the children of A.\nWe explore node C. We set beta to the minimum of beta and the value of C. Since C has not been explored yet, we set beta to the value of C. We continue exploring the children of C.\nWe explore node G. We set beta to the minimum of beta and the value of G. Since G has not been explored yet, we set beta to the value of G.\nWe backtrack to node C and explore node D. We set beta to the minimum of beta and the value of D. Since D has not been explored yet, we set beta to the value of D.\nWe backtrack to node C and update its value to the minimum of its children, which is G. We update the value of node A to the maximum of its children, which is F.\nSince alpha is greater than or equal to beta, we know that we can prune the entire subtree rooted at node D, since it cannot possibly lead to a better outcome than what we have already explored.\nWe update the value of node A to the maximum of its children, which is F.\nWe have explored all the nodes in the game tree and have determined that the best move is to choose the path that leads to node F.\nBy using Alpha-Beta Pruning, we were able to prune the subtree rooted at node D, which greatly reduced the number of nodes that needed to be explored.a generalization of normal-form games, which involve multiple players who make decisions simultaneously. In a stochastic game, the environment is also involved in decision-making and introduces uncertainty into the outcome of actions. Stochastic games are typically defined by a set of states, a set of actions available to each player, a set of transition probabilities, and a reward function that depends on the state and the actions of all players.ExpectiMinimax is an extension of the minimax algorithm for games where chance is involved. It is used to find the best move for an agent in games that involve random outcomes, such as card games or board games with dice.The algorithm assumes that the agent is playing against an opponent who is also trying to maximize their own score. The goal of the agent is to choose a move that maximizes their expected score, taking into account the possible random outcomes of the game.The ExpectiMinimax algorithm works by constructing a game tree that represents all possible outcomes of the game. At each node of the tree, the agent chooses a move that maximizes their expected score, assuming that the opponent will also choose the move that maximizes their own expected score. The expected score is calculated by taking the weighted average of the scores for all possible outcomes of the move, where the weights are the probabilities of each outcome.The algorithm then recursively applies this process to each child node of the current node, alternating between maximizing the expected score for the agent and minimizing it for the opponent. This process continues until the algorithm reaches a terminal node, which represents the end of the game.The ExpectiMinimax algorithm is a generalization of the minimax algorithm, which assumes that the game is deterministic and that the opponent is always trying to minimize the agent's score. By taking into account the random outcomes of the game, the ExpectiMinimax algorithm provides a more accurate evaluation of the best move for the agent.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<a data-href=\"MiniMax\" href=\"MiniMax\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">MiniMax</a>","level":3,"id":"[[MiniMax]]_0"},{"heading":"A game in AI can be represented as a tuple (S, A, P, R, O, T), where:","level":3,"id":"A_game_in_AI_can_be_represented_as_a_tuple_(S,_A,_P,_R,_O,_T),_where_0"},{"heading":"Search","level":3,"id":"Search_0"},{"heading":"Example","level":4,"id":"Example_0"},{"heading":"Analyse:","level":4,"id":"Analyse_0"},{"heading":"Minimax algorithm works for Tic-Tac-Toe:","level":3,"id":"Minimax_algorithm_works_for_Tic-Tac-Toe_0"},{"heading":"Stochastic game","level":3,"id":"Stochastic_game_0"},{"heading":"ExpectiMinimax","level":3,"id":"ExpectiMinimax_0"}],"links":["algorithms/algo/graph/minimax.html#_0","algorithms/algo/graph/alpha-beta.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084158981,"modifiedTime":1737554783000,"sourceSize":14346,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/4. Adversarial Search Problems.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html":{"title":"0. Exercises 1","icon":"","description":"\nDepth-first graph search is guaranteed to return a shortest solution.\nFalse: Depth-first provides no optimality guarantee at all. (It is only complete in finite search spaces).\nBreadth-first graph search is guaranteed to return a shortest solution.\nTrue. This is because frontier nodes are ranked by increasing order of depth. (The strategy expands all nodes whose depth is less than the depth of the shallowest goal node).\nUniform-cost graph search is guaranteed to return an optimal solution.\nTrue: This is because frontier nodes are ranked by increasing order of g(n). (The strategy expands all nodes such that g(n) ≤ C ∗ , and some nodes such that g(n) = C ∗ including one single goal node.)\nGreedy graph search is guaranteed to return an optimal solution.\nFalse: This is because it completely ignores the cost to reach frontier nodes from the initial state, and only looks into the future estimated cost to the goal.\nBreadth-first graph search is a special case of uniform-cost search.\nTrue: This is because breadth-first search behaves like uniform-cost search when all step costs are identical.\nA* graph search with an admissible heuristic is guaranteed to return an optimal solution.\nTrue: Note that this is only true of the version with node re-expansion. (Consistency is a stronger property than admissibility which does not require node re-expansion.)","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Properties of search strategies:","level":2,"id":"Properties_of_search_strategies_0"},{"heading":"1. ","level":3,"id":"1._0"},{"heading":"2. ","level":3,"id":"2._0"},{"heading":"3. ","level":3,"id":"3._0"},{"heading":"4. ","level":3,"id":"4._0"},{"heading":"5. ","level":3,"id":"5._0"},{"heading":"6. ","level":3,"id":"6._0"},{"heading":"7. ","level":3,"id":"7._0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159008,"modifiedTime":1737554783000,"sourceSize":6521,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/0. Exercises 1.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html":{"title":"1. Exercises 2","icon":"","description":"Admissible:\nIn the context of heuristic functions for search problems, a heuristic function h is said to be admissible if it never overestimates the actual cost to reach the goal node from a given state. In other words, the heuristic value h(n) for any node n is always less than or equal to the true cost of the optimal path from n to the goal node. A* search algorithm, which is widely used for solving pathfinding problems, guarantees an optimal solution if the heuristic function used is admissible.where h(n) is the estimated cost of the cheapest path from node n to the goal, and h*(n) is the actual cost of the cheapest path from node n to the goal.A heuristic function h(n) is said to be consistent if, for every node n and every successor n' of n generated by any action a, the estimated cost of reaching the goal from n is no greater than the step cost of getting to n' plus the estimated cost of reaching the goal from n'. This can be written as:where c(n, a, n') is the cost of applying action a from node n to reach n'.\nConsider the search problem shown on the left. It has only three states, and three directed edges. A is the start node and G is the goal node. To the right, four different heuristic functions are defined, numbered I through IV.\ngraph = {A:{G:6,B:2}, B{C:3}}\nAdmissible:A heuristic function h(n) is admissible if it satisfies the condition that at every node n, the function does not overestimate the cost h*(n) of the shortest path from n to the closest goal node, that is, h(n) ≤ h*(n). It should be noted that if n is a goal node, then h(n) = 0. Heuristic II is the only inadmissible heuristic as it overestimates the cost from A. Specifically, h(A) = 6, while the actual cost of the shortest path from A to G is 5. The other heuristics are admissible and this can be easily verified from the given table. For instance, heuristic I satisfies the admissibility condition since h(A) = 4 ≤ 5, h(B) = 1 ≤ 3, and h(G) = 0 ≤ 0.\nConsistentA consistent heuristic satisfies the condition that the heuristic doesn’t decrease by more than the cost between any node n and its successor n0: i.e. h(n) − h(n0) ≤ c(n, n0). In this problem, h(G) is always 0, so making sure that the direct paths to the goal (A → G and B → G) are consistent is the same as making sure that the heuristic value at the start of the path is admissible. Now, for the path from A to B, the analysis is as follows: - Heuristic I is not consistent: h(A) − h(B) = 4 − 1 = 3 &gt; c(A, B) = 2.\n- Heuristic II is not consistent: recall that consistency implies admissibility, and II is not admissible.\n- Heuristic III is consistent: h(A) − h(B) = 4 − 3 = 1 ≤ 2\n- Heuristic IV is not consistent: h(A) − h(B) = 5 − 2 = 3 &gt; 2. In summary, a heuristic is consistent if it doesn't overestimate the actual cost between a node and its successor, and if it satisfies the condition h(n) − h(n0) ≤ c(n, n0) for all nodes n and their successors n0. Among the four heuristics given in this problem, only heuristic III is both admissible and consistent.\nThere are green and red objects on a grid. An agent must collect exactly one object of each color to reach the goal. The actions are moving south, north, east or west, and are only applicable when they don’t result in colliding with an obstacle (black) or exiting the grid. The agent collects an object when it first reaches the cell at which this object is. The state of the problem is represented as follows. Each state is a triple (a, G, R) where a is the location of the agent on the grid, G is the set locations of yet uncollected green objects, and R is the set of locations of yet uncollected red objects. Given two locations l1 and l2 on the grid, dist(l1, l2) returns the Manhattan distance between l1 and l2.\nThe sum of the Manhattan distances to the remaining objects?\nNo. We only need to collect 2 objects (one of each kind). The sum of the Manhattan distances to all objects would greatly over-estimate the optimal cost of reaching just two of them. In fact, even if we had to collect all objects, it would be an over-estimate, as it assumes that we need to come back to the current location after collecting each object.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"0. Concepts","level":3,"id":"0._Concepts_0"},{"heading":"1. Heuristic function properties","level":3,"id":"1._Heuristic_function_properties_0"},{"heading":"2. Combining heuristics and heuristics for multiple goals","level":3,"id":"2._Combining_heuristics_and_heuristics_for_multiple_goals_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159012,"modifiedTime":1737554783000,"sourceSize":5888,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/1. Exercises 2.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html":{"title":"2. Exercises 3","icon":"","description":"\nTrue/False\n(a) There exists a task environment (PEAS) in which every agent is rational.\nTRUE: e.g., let the performance measure be zero for every history.\nA rational agent is one that takes action in order to maximize its expected performance measure, given its current perceptual inputs and internal state. In other words, a rational agent is one that chooses the best action in each situation based on its knowledge and understanding of the environment.\nNow, consider an environment where the performance measure is always zero for every history. In this case, every action taken by an agent will result in the same performance measure, which is zero. Therefore, any action that the agent takes will be optimal in terms of maximizing the performance measure. Since the agent is always taking the optimal action, it can be said that the agent is rational in this environment.(b) Suppose agent A selects its action uniformly at random from the set of possible actions. There exists a deterministic, fully observable task environment in which A is rational.\nTRUE: see (a). Many other environments have this property too. (c) No logical agent can behave rationally in partially observable environment.\nFALSE: a logical agent can track the environment by inferring a description of the set of possible states, as in the wumpus world, and basing reflex rules on this description. (d) ∀ x, y: x = y is satisfiable.\nTRUE: satisfied by a model with exactly one object.\nA formula is said to be satisfiable if there exists at least one interpretation or model under which the formula is true.(e) If θ unifies the atomic sentences α and β, then α |= Subst(θ, β).\nTRUE: If θ unifies α and β, then Subst(θ, β) ≡ Subst(θ, α), whcih is entailed by α for any θ. (f) In any finite state space, random-restart hillclimbing is an optimal algorithm.\nTRUE (or maybe FALSE): it will eventually find the optimal solution, if necessary by generating it at random. But it has no way of knowing it has found the optimal solution, so strictly speaking it does not “return” it unless the time bound is finite—but in that case, it may not find it. The description in the book says that it returns the first goal it finds, which is obviously suboptimal; but one would not write the algorithm this way if optimal or near-optimal solutions were desired. We decided to give everyone full credit for this question.\nSearch Suppose there are two friends living in different cities on a map, such as the Romania map shown in Figure 3.2 of AIMA2e. On every turn, we can move each friend simultaneously to a neighboring city on the map. The amount of time needed to move from city i to neighbor j is equal to the road distance d(i, j) between the cities, but on each turn the friend that arrives first must wait until the other one arrives (and calls the first on his/her cell phone) before the next turn can begin. We want the two friends to meet as quickly as possible. Let us formulate this as a search problem.\n(a) What is the state space? (You will find it helpful to define some formal notation here.)\nStates are all possible city pairs (i, j). The map is not the state space. (b) What is the successor function?\nThe successors of (i, j) are all pairs (x, y) such that Adjacent(x,i) and Adjacent(y, j). (c) What is the goal?\nBe at (i,i) for some i. (d) What is the step cost function?\nThe cost to go from (i, j) to (x, y) is max(d(i, x), d(j, y)). (e) (6) Let SLD(i, j) be the straight-line distance between any two cities i and j. Which, if any, of the following heuristic functions are admissible? (If none, write NONE.) (i) SLD(i, j) (ii) 2 · SLD(i, j) (iii) SLD(i, j)/2\nIn the best case, the friends head straight for each other in steps of equal size, reducing their separation by twice the time cost on each step. Hence (iii) is admissible.\nThe reason why heuristic function (i) SLD(i, j) may not be admissible is that it does not take into account the fact that the two friends are moving simultaneously towards each other, and that the distance between them decreases as they move towards each other. This means that the actual distance that they will need to travel to meet is less than the straight-line distance between their starting points.(f) There are completely connected maps for which no solution exists.\nTRUE: e.g., a map with two nodes connected by one link.\nGive a complete problem formulation for each of the following. Choose a formulation that is precise enough to be implemented.\n(b) A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot ceiling. He would like to get the bananas. The room contains two stackable, movable, climbable 3-foot-high crates.\nGoal test: Monkey has bananas. Successor function: Hop on crate; Hop off crate; Push crate from one spot to another; Walk from one spot to another; grab bananas (if standing on crate). Cost function: Number of actions.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. ","level":3,"id":"1._0"},{"heading":"2. ","level":3,"id":"2._0"},{"heading":"3. ","level":3,"id":"3._0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159015,"modifiedTime":1737554783000,"sourceSize":17273,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/2. Exercises 3.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html":{"title":"3. Exercises 4","icon":"","description":"\nSearch strategies\nUsing example from <a data-href=\"0. Exercises 1\" href=\"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">0. Exercises 1</a>State is expanded when it is removed from the frontier, alphabetically removed from queueConsider the search space below, where S is the initial state and G1 and G2 both satisfy the goal test. Arcs are labelled with the cost of traversing them and the estimated cost to a goal is reported inside nodes.“search.png” could not be found.Expanded nodes (layers show in bracket, \"/\" meaning expanding this node):\nS(0)/, A(1)/, C(1)/, B(2), E(2), D(2), G2(2) =&gt; found The BFS algorithm finds the shortest path in terms of the number of edges or levels in the graph, not necessarily the shortest path in terms of distance or cost. Therefore, it can be used to find the shortest path in a uniform cost graph where all edges have the same cost, but not in a graph where the edge costs vary.Would simply follow the alphabetical order and ignore the cost for this one when everything same for next two nodes.\nExpanded nodes: S(0), A(1), B(2), D(3), G1(4)Expanded nodes:\n1st: S(0), 2nd: S(0), A(1), C(1), 3rd: S(0), A(1), B(2),E(2), C(1), D(2), G2(2) Queue: S(0), A(2), C(2), B(3),D(3),\nG2(6), G1(7), E(8)\nExpanded: S(0), A(2), C(2), B(3), D(3), G2(6) Queue: S(5), A(2), B(1), G1(0), D(1), C(4), E(6)\nExpanded: S(5), A(2), B(1), G1(0) Queue: S(5), A(4), B(4), D(5), C(6), D(4), G2(6), G1(8), E(16), Expanded nodes: S(0+5=5), A(2+2=4), B(3+1=4), D(4+1=5), C(2+4=6), D(3+1=4), G2(6+0=6) Various autonomous agents are already in use all around us. Consider a rover on Mars, which is given an instruction to reach a place it can see, and plans a safe route to that location before traversing that path. Based on this description, what type of agent is this Rover?\nBased on the description given, the rover on Mars can be classified as a goal-based agent.\nA goal-based agent selects its actions based on a desired goal or objective. In the case of the Mars rover, the goal is to reach a specific location that it can see. The agent considers the current state of the environment, but also takes into account the desired outcome or state that it wants to achieve. The agent uses problem-solving algorithms to search through the space of possible actions and plans a sequence of actions that will lead to the desired goal.\nIn the case of the Mars rover, the agent plans a safe route to the location it can see before traversing that path. This involves considering factors such as terrain, obstacles, and the condition of the rover itself. The agent also needs to take into account the fact that it is operating in a remote and harsh environment, where communication delays and limited resources need to be taken into consideration.\nA* search with the heuristic&nbsp;h(n)=0&nbsp;for all nodes&nbsp;n will always find an optimal solution.\nWhen the heuristic function h(n) is zero for all nodes, it is an admissible heuristic (and also a consistent heuristic), since it never overestimates the actual cost to reach the goal. A search uses the heuristic function to guide its search towards the goal node and prioritize nodes that are likely to lead to the optimal solution. Since the heuristic function h(n) always returns a value of 0, A search will simply explore nodes in the order of their actual cost from the start node, similar to uniform-cost search.Since the heuristic function h(n) is admissible, A* search will still guarantee to find the optimal solution if one exists, although it may take longer to reach the goal node compared to using a non-zero heuristic function that provides more guidance. Therefore, the statement is true.Tips:\nUniform-cost search (UCS) is a special case of A search where the heuristic function is always zero (i.e., h(n) = 0 for all nodes n). Therefore, if the heuristic function h(n) is always zero, both A search and UCS will behave identically and expand nodes in the order of their actual cost from the start node, guaranteeing to find an optimal solution if one exists.\nIn fact, UCS can be seen as a special case of A search where the weight of the heuristic function is zero (i.e., w = 0 in the weighted A formula f(n) = g(n) + w*h(n)), which reduces the formula to the actual cost from the start node (i.e., f(n) = g(n) for UCS).\nThe big advantage of iterative deepening over depth-first search is its linear space requirements.\nBoth algorithms have linear space requirements.\nIn DFS, the space complexity is O(bd), where b is the branching factor and d is the maximum depth of the search tree. This is because DFS stores all the visited nodes on the path from the root to the current node in the stack.\nIn iterative deepening search, the space complexity is O(bd) as well, but the factor b is smaller. This is because iterative deepening search performs depth-limited search repeatedly, but only needs to store the current path and not the entire tree. The space complexity is dominated by the maximum depth limit, which is d, and not the branching factor.\nWhen all step costs of the problem are equal, Uniform Cost search is equivalent to Breadth-First search.\nYes, that is correct. When all step costs of a problem are equal, the Uniform Cost search is equivalent to Breadth-First search, as they both explore the nodes in increasing order of path cost. In this case, the only difference between the","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"BFS","level":3,"id":"BFS_0"},{"heading":"DFS","level":3,"id":"DFS_0"},{"heading":"Iterative deepening","level":3,"id":"Iterative_deepening_0"},{"heading":"Uniform cost","level":3,"id":"Uniform_cost_0"},{"heading":"Greedy","level":3,"id":"Greedy_0"},{"heading":"A Star","level":3,"id":"A_Star_0"}],"links":["artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159019,"modifiedTime":1737554783000,"sourceSize":9391,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/3. Exercises 4.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html":{"title":"0. Intro","icon":"","description":"Knowledge representation and reasoning (KRR) in Symbolic AI refers to the process of representing and manipulating knowledge in a way that is suitable for automated reasoning. The goal of KRR is to create a formal representation of the world that can be used to reason about complex problems and make decisions.Symbolic AI approaches KRR by representing knowledge using symbols, such as logical propositions or rules, and manipulating those symbols using logical inference rules to derive new knowledge. This allows for a more structured and systematic way of reasoning than other AI approaches, such as machine learning, which rely on statistical patterns in data.Some common methods used in KRR include:\nPropositional logic: a formal language for representing and manipulating logical propositions. Check:<a data-href=\"1. Propositional Logic\" href=\"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1. Propositional Logic</a> <br>First-order logic: an extension of propositional logic that includes variables, quantifiers, and functions, allowing for more complex representations of knowledge. Check: <a data-href=\"2. First Order Logic\" href=\"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. First Order Logic</a> Description logics: a family of logic-based knowledge representation languages that are particularly useful for representing knowledge about objects and their properties.\nRule-based systems: a way of representing knowledge in the form of production rules, which specify how to infer new knowledge from existing knowledge.\nKRR has many applications in areas such as natural language processing, expert systems, and intelligent agents. By representing and reasoning about knowledge in a structured and systematic way, KRR provides a powerful tool for automated decision-making and problem-solving in complex domains.\nA knowledge-based agent uses an internal representation of the world to reason and make decisions.\nThe internal representation is called a knowledge base (KB), and it contains information about the agent's environment, including the agent's goals, beliefs, and assumptions.\nThe agent uses inference rules to draw conclusions from the information in the KB. Inference is the process of deriving new information from existing information.\nThe agent's KB can be updated as new information becomes available or as the agent's beliefs change.\nThe KB can be represented using different formalisms, such as propositional logic, first-order logic, or semantic networks.\nThe agent's decision-making process involves selecting actions that are expected to achieve the agent's goals based on the current state of the KB and the agent's beliefs.\nKnowledge-based agents can be used in a wide range of applications, including expert systems, natural language processing, robotics, and intelligent tutoring systems.\nfunction KB-Agent(percept): persistent: KB, a knowledge base t, a counter, initially 0, indicating time KB ← Tell(KB, Make-Percept-Sentence(percept)) action ← Ask(KB, Make-Action-Query()) KB ← Tell(KB, Make-Action-Sentence(action)) t = t + 1 return action\ndef simple_kb_agent(): # initialize the knowledge base with some initial facts knowledge_base = [\"A is true\", \"B is false\"] while True: # observe the environment percept = observe_environment() # update the knowledge base based on the percept knowledge_base = update_kb(knowledge_base, percept) # choose an action based on the current state of the knowledge base action = choose_action(knowledge_base) # perform the action in the environment perform_action(action)\n|---|---|---|---|\n| G | | W | |\n|---|---|---|---|\n| P | W | | P |\n|---|---|---|---|\n| | | P | |\n|---|---|---|---|\n| | W | | G |\n|---|---|---|---| Performance measure: A quantitative measure of the agent's success in achieving its goal, which is to find the gold and exit the environment while avoiding the Wumpus and pits. The agent receives +1000 for achieving the goal, -1000 for being eaten by the Wumpus or falling into a pit, and -1 for each action taken.\nEnvironment: The Wumpus World is a 2D grid where the agent can move to adjacent squares, pick up gold, shoot arrows to kill the Wumpus, and perceive its surroundings. The environment contains the agent, the gold, the Wumpus, pits, and walls.\nActuators: The agent can move forward, turn left or right, pick up the gold, shoot an arrow in the current direction, and grab the gold if it is adjacent. The agent also receives feedback when it bumps into a wall or when the arrow hits something.\nSensors: The agent can perceive whether there is a wall, a pit, the gold, or the Wumpus in the adjacent squares. The agent also senses a scream when the Wumpus is killed by an arrow.\nfunction wumpusExplore(): // initialize the knowledge base KB = initializeKB() // initialize the agent's position and direction agentPos = [1, 1] agentDir = \"right\" // move forward until a wall is reached while true: // check for stench, breeze, and glitter percept = getPercept(agentPos) // update the knowledge base with the percept KB = updateKB(KB, percept) // if there is a glitter, grab the gold and return if percept.hasGlitter: grabGold() return // if there is no danger, move forward if not percept.hasBreeze and not percept.hasStench: moveForward(agentPos, agentDir) // if there is a breeze or stench, shoot the arrow or change direction else: if percept.hasBreeze: shootArrow(agentDir, agentPos, KB) if percept.hasStench: agentDir = changeDirection(agentDir) // if the agent falls into a pit or is eaten by the wumpus, return if agentPos == [0, 0]: return if KB.hasWumpus or KB.hasPit: return\ndef wumpus_explore():\ndef wumpus_explore(): # initialize the knowledge base KB = initialize_kb() # initialize the agent's position and direction agent_pos = [1, 1] agent_dir = \"right\" # move forward until a wall is reached while True: # check for stench, breeze, and glitter percept = get_percept(agent_pos) # update the knowledge base with the percept KB = update_KB(KB, percept) # if there is a glitter, grab the gold and return if percept.has_glitter: grab_gold() return # if there is no danger, move forward if not percept.has_breeze and not percept.has_stench: move_forward(agent_pos, agent_dir) # if there is a breeze or stench, shoot the arrow or change direction else: if percept.has_breeze: shoot_arrow(agent_dir, agent_pos, KB) if percept.has_stench: agent_dir = change_direction(agent_dir) # if the agent falls into a pit or is eaten by the wumpus, return if agent_pos == [0, 0]: return if KB.has_wumpus or KB.has_pit: return KRR is a subfield of AI that focuses on how knowledge can be represented, manipulated, and used to reason about the world. The goal of KRR is to develop formalisms that enable knowledge to be expressed in a way that can be understood and processed by computers. KRR plays an important role in various AI applications, such as expert systems, natural language processing, and intelligent agents. SAT stands for propositional satisfiability, which is a classic problem in computer science that involves determining whether a given Boolean formula can be made true by assigning values to its variables. SAT is a central problem in many areas of computer science, including logic, complexity theory, and artificial intelligence. SAT solvers are widely used in many applications, such as software verification, automated planning, and electronic design automation. However:\nThe brute force method of trying all possible combinations quickly becomes infeasible for large SAT problems. This is why more efficient algorithms like DPLL and CDCL are used instead.Consider a simple knowledge base about animals:\nA dog is a mammal.\nA cat is a mammal.\nA bird is not a mammal.\nA mammal has fur.\nWe can represent this knowledge base using first-order logic and predicates. For example, we can define the following predicates:\nmammal(x): x is a mammal\ndog(x): x is a dog\ncat(x): x is a cat\nbird(x): x is a bird\nfur(x): x has fur\nUsing these predicates, we can represent the knowledge base as follows:\n∀x (dog(x) → mammal(x))\n∀x (cat(x) → mammal(x))\n∀x (bird(x) → ¬mammal(x))\n∀x (mammal(x) → fur(x))\nWe can use a KRR system to reason about this knowledge base. For example, we can use inference rules to derive new knowledge. One inference rule is modus ponens, which states that if we know \"p → q\" and \"p\", then we can infer \"q\". Using modus ponens, we can infer that a dog has fur:\n\"dog(x) → mammal(x)\" and \"dog(fido)\" imply \"mammal(fido)\"\n\"mammal(fido) → fur(fido)\" implies \"fur(fido)\"\nWe can also use a SAT solver to check if a given statement is entailed by the knowledge base. For example, we can ask whether a bird has fur. To do this, we can negate the statement \"bird(x) → fur(x)\" and check if it is satisfiable. If it is unsatisfiable, then the statement is entailed by the knowledge base. If it is satisfiable, then the statement is not entailed by the knowledge base. In this case, negating the statement gives us \"bird(x) ∧ ¬fur(x)\", which is satisfiable. Therefore, we can conclude that a bird does not have fur.A form of propositional logic where a formula is expressed as a conjunction (AND) of one or more clauses, where each clause is a disjunction (OR) of one or more literals. A literal is either a propositional variable or its negation.For example, the formula (p OR q) AND (NOT p OR r) AND (NOT q OR NOT r) is in CNF form, where each clause is a disjunction of literals and the whole formula is a conjunction of these clauses.CNF is an important form of propositional logic because any propositional formula can be converted to an equivalent formula in CNF. This makes it useful for automated theorem proving, model checking, and other applications of propositional logic.Equivalent formulas:\n¬(p ∧ q) ≡ ¬p ∨ ¬q\n¬(p ∨ q) ≡ ¬p ∧ ¬q\n(p ∧ q) ∨ r ≡ (p ∨ r) ∧ (q ∨ r)\n(p ∨ q) ∧ r ≡ (p ∧ r) ∨ (q ∧ r)\n¬¬p ≡ p\np ∨ (q ∧ r) ≡ (p ∨ q) ∧ (p ∨ r)\np ∧ (q ∨ r) ≡ (p ∧ q) ∨ (p ∧ r)\np → q ≡ ¬p ∨ q\nSummary:A clause is a disjunction of literals (e.g. a or b or c), and a formula in CNF is a conjunction of clauses (e.g. (a or b) and (not a or c) and (not c or d)). The conversion of a propositional logic statement to CNF involves several steps:\nEliminate implications by replacing any occurrence of A =&gt; B with (not A) or B.\nUse De Morgan's laws and double negation elimination to push negations inward and eliminate double negations.\nDistribute disjunctions over conjunctions using the distributive law.\nConvert any remaining compound propositions to clauses.\nOnce a statement is in CNF, it can be used in various logical algorithms, such as DPLL and resolution, to determine its satisfiability.a logical inference rule used in automated theorem proving and propositional logic. It involves taking two clauses, where each clause is a disjunction of literals, and resolving them by finding a pair of complementary literals (i.e., one positive and one negative) that can be eliminated from the clauses. The result of the resolution is a new clause that is the disjunction of the remaining literals from both original clauses. If the new clause contains a contradiction, such as a literal and its negation, then the original clauses are said to be unsatisfiable. If no such contradiction is found, then the process can be repeated with other pairs of clauses until no more resolutions are possible. If an empty clause is ever produced, the resolution process is terminated and the original set of clauses is determined to be unsatisfiable.The general form of resolution can be described as follows:Given two clauses C1 and C2, if they share a complementary literal (i.e., a literal and its negation), then a new clause C can be derived by resolving C1 and C2, which consists of the union of all literals from C1 and C2 except for the complementary literals. For example:In this example, the complementary literals are P(x) and ¬P(x), and the resolvent is Q(x) ∨ R(x).Resolution can be used to prove that a set of clauses is unsatisfiable by repeatedly applying the resolution rule until either an empty clause is derived, indicating that the set is unsatisfiable, or no new clauses can be derived.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Knowledge-based agents","level":3,"id":"Knowledge-based_agents_0"},{"heading":"Wumpus World","level":3,"id":"Wumpus_World_0"},{"heading":"Knowledge Representation and Reasoning (KRR)","level":3,"id":"Knowledge_Representation_and_Reasoning_(KRR)_0"},{"heading":"SAT","level":4,"id":"SAT_0"},{"heading":"Example of using KRR and SAT in AI:","level":3,"id":"Example_of_using_KRR_and_SAT_in_AI_0"},{"heading":"Conjunction Normal Form (CNF)","level":3,"id":"Conjunction_Normal_Form_(CNF)_0"},{"heading":"Resolution","level":3,"id":"Resolution_0"}],"links":["artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html#_0","artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159031,"modifiedTime":1737554783000,"sourceSize":13888,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/0. Intro.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html":{"title":"1. DPLL","icon":"","description":"DPLL stands for Davis-Putnam-Logemann-Loveland, which is a well-known algorithm used for solving the Boolean satisfiability problem (SAT). The SAT problem is the problem of determining whether a given Boolean formula can be satisfied by assigning true or false values to its variables.The DPLL algorithm is a backtracking search algorithm that works by iteratively simplifying the input formula until a satisfying assignment is found or it is determined that no such assignment exists. At each step, the algorithm selects a variable and assigns it a value (true or false) and then simplifies the formula by removing clauses that are satisfied by that assignment, as well as any literals that are contradictory to that assignment. This process is repeated until the formula is reduced to either a single clause (which is trivially satisfiable) or an unsatisfiable formula, in which case the algorithm backtracks to the most recent decision point and tries a different assignment.One of the strengths of the DPLL algorithm is its ability to take advantage of the structure of the input formula and to use heuristics to guide the search. For example, it can use unit propagation, which identifies clauses with only one unassigned literal and assigns that literal a value, and pure literal elimination, which identifies literals that appear only with a single polarity in the formula and assigns them the corresponding value. These techniques can help to reduce the size of the search space and make the algorithm more efficient.The DPLL algorithm is widely used in automated reasoning and is a key component of many SAT solvers, which are used in a variety of applications such as circuit design, verification, planning, and scheduling.A technique used in the DPLL algorithm for solving the satisfiability problem. It is a rule of inference that says that if a clause contains only one literal that has not been assigned a truth value, then that literal must be assigned a truth value that makes the clause true.The idea behind unit propagation is to simplify the search space by identifying these unit clauses and assigning the necessary truth value to their literals. This process can create new unit clauses that are identified in subsequent iterations, leading to a chain reaction of assignments until either a satisfying truth assignment is found or a contradiction is reached.The steps of unit propagation are:\nAssign truth values to pure literals (literals that always appear with the same sign in the formula / whose complement does not appear anywhere).\nIdentify unit clauses (clauses with only one unassigned literal) and assign truth values to the unassigned literal(s) in them.\nPropagate these truth assignments by removing all clauses containing the assigned literals, and removing the negation of the assigned literals from all remaining clauses.\nRepeat steps 1-3 until no more unit clauses or pure literals can be found.\nA technique used to improve the efficiency of the DPLL algorithm for solving propositional satisfiability (SAT) problems. In DPLL, when a conflict is detected in the search process, the algorithm backtracks to a previous decision level and makes a new choice. Clause learning is a way to avoid making the same choice again by learning new clauses from the conflict.When a conflict occurs, the DPLL algorithm learns a new clause that captures the reason for the conflict, called the \"conflict clause.\" This clause is added to the knowledge base and prevents the algorithm from making the same choices that led to the conflict. By adding more and more conflict clauses to the knowledge base, the algorithm becomes more efficient and can solve SAT problems faster.The basic idea of clause learning is to use the conflict clause to prune the search space. When a conflict is detected, the algorithm backtracks to the last decision level and adds the conflict clause to the knowledge base. The algorithm then tries to find a new assignment that satisfies the updated knowledge base. If the updated knowledge base is still unsatisfiable, the algorithm generates a new conflict clause and adds it to the knowledge base. This process continues until a satisfying assignment is found or the algorithm determines that the problem is unsatisfiable.Use a more sophisticated branching heuristic to choose which variable to assign a value to. A good heuristic can help the algorithm to quickly find a satisfying assignment. One popular heuristic is the \"most frequent literal\" rule, which chooses the literal that appears most frequently in the clauses of the formula. Another common heuristic is the \"maximum occurences\" rule, which chooses the literal that appears in the most number of clauses, regardless of whether it appears positively or negatively. Other heuristics include the \"Jeroslow-Wang rule,\" which assigns scores to literals based on how often they appear in long clauses, and the \"VSIDS\" (Variable State Independent Decaying Sum) rule, which assigns scores to literals based on how often they were involved in conflicts during the search. In addition to branching heuristics, other techniques such as clause deletion and backjumping can also be used to improve the efficiency of the algorithm. The general form of the DPLL algorithm is as follows:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Unit propagation","level":3,"id":"Unit_propagation_0"},{"heading":"Clause learning","level":3,"id":"Clause_learning_0"},{"heading":"Further improvement","level":3,"id":"Further_improvement_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159035,"modifiedTime":1737554783000,"sourceSize":7313,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/1. DPLL.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html":{"title":"2. Prenex Normal form","icon":"","description":"A way of writing first-order logic formulas in a standard form that simplifies the process of inference and proof. PNF is achieved by converting a first-order logic formula into a form where all quantifiers are at the beginning of the formula.To convert a formula to PNF, the following steps are typically performed:\nElimination of existential quantifiers: First, we eliminate all existential quantifiers by using Skolemization, which involves replacing each existential quantifier with a function that takes as input the universally quantified variables in the formula. The Skolem function must be constructed such that it satisfies any constraints specified by the existential quantifier.\nStandardization of variables: We then rename variables in the formula to avoid name clashes. This ensures that no two variables in the formula have the same name.\nMoving universal quantifiers to the front: Finally, we move all universal quantifiers to the beginning of the formula. Universal Instantiation (UI): If ∀x P(x) is true, then P(a) is true for any individual a in the domain of discourse.\nExistential Generalization (EG): If P(a) is true for some individual a in the domain of discourse, then ∃x P(x) is true.\nUniversal Generalization (UG): If P(a) is true for any individual a in the domain of discourse, then ∀x P(x) is true.\nExistential Instantiation (EI): If ∃x P(x) is true, then we can introduce a new individual constant a and replace x with a in P(x) to obtain P(a).\nQuantifier Negation Rules: ¬∀x P(x) is equivalent to ∃x ¬P(x)\n¬∃x P(x) is equivalent to ∀x ¬P(x) Quantifier Exchange Rule: ∀x∃y P(x,y) is equivalent to ∃y∀x P(x,y) Skolemization: Eliminating existential quantifiers by introducing Skolem functions or constants.\nQuantifier Distribution: Distributing a quantifier over a conjunction or disjunction, using the following rules: ∀x (P(x) ∧ Q(x)) is equivalent to (∀x P(x)) ∧ (∀x Q(x))\n∃x (P(x) ∧ Q(x)) is equivalent to (∃x P(x)) ∧ (∃x Q(x))\n∀x (P(x) ∨ Q(x)) is equivalent to (∀x P(x)) ∨ (∀x Q(x))\n∃x (P(x) ∨ Q(x)) is equivalent to (∃x P(x)) ∨ (∃x Q(x))\nis a technique used in first-order logic to eliminate existential quantifiers from a formula by introducing Skolem functions or Skolem constants. The resulting formula is logically equivalent to the original formula, but without any existential quantifiers.Skolemization is based on the idea that if a formula has an existential quantifier, then there must exist an object that satisfies the formula. Skolemization introduces a new function or constant symbol that represents this object, allowing us to eliminate the existential quantifier.There are two types of Skolemization: Skolem functions and Skolem constants. Skolem functions are introduced for formulas with free variables, while Skolem constants are introduced for closed formulas.To perform Skolemization using Skolem functions, we follow these steps:\nIdentify the innermost existential quantifier in the formula.\nIntroduce a new Skolem function symbol with the same arity as the quantified variable.\nReplace the quantified variable with the Skolem function applied to a tuple of the free variables in the quantifier's scope.\nRepeat the process for all remaining existential quantifiers in the formula.\nFor example, consider the formula ∃x P(x,y). To Skolemize this formula using a Skolem function, we introduce a new function symbol f and replace the existential quantifier with the function applied to y: P(f(y),y).To perform Skolemization using Skolem constants, we follow these steps:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Quantifiers in first-order logic:","level":3,"id":"Quantifiers_in_first-order_logic_0"},{"heading":"Skolemization","level":3,"id":"Skolemization_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159039,"modifiedTime":1737554783000,"sourceSize":4654,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/2. Prenex Normal form.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html":{"title":"3. Constraint Satisfaction Problem","icon":"","description":"A constraint satisfaction problem (CSP) is a computational problem that involves finding a solution to a set of constraints that restrict the possible values of variables in a problem domain. The goal of a CSP is to find a valid assignment of values to variables that satisfies all the constraints. CSPs are used in a variety of fields, such as artificial intelligence, operations research, and computer science. Some examples of CSPs include scheduling problems, routing problems, and puzzle-solving problems like Sudoku.\nInitialization: Start by initializing the variables and their domains. Variables are the entities that need to be assigned values, while the domains are the set of possible values that can be assigned to each variable.\nConstraint propagation: After initializing the variables and domains, perform constraint propagation to eliminate values from the domains that cannot satisfy the constraints. This is done by applying various techniques such as forward checking, arc consistency, and constraint propagation.\nVariable selection: Select a variable that has not been assigned a value yet. This variable should have the smallest domain (minimum remaining values) to minimize the branching factor of the search tree.\nValue selection: Select a value from the domain of the selected variable. This value should be consistent with the current assignment and should not violate any constraints.\nBacktracking: If a value cannot be assigned to the selected variable, backtrack to the previous variable and try a different value. If there are no more values to try for a variable, backtrack further to the previous variable until a valid assignment is found or all possible combinations of values have been tried.\nSolution checking: After assigning values to all variables, check if the solution satisfies all constraints. If not, backtrack and try a different combination of values.\nSolution output: If a valid solution is found, output the solution. If not, output that the problem is unsolvable.\nSimple backtracking:function pureBacktracking(csp): return backtrack({}, csp) function backtrack(assignment, csp): if assignment is complete: return assignment var = selectUnassignedVariable(csp, assignment) for value in orderDomainValues(var, assignment, csp): if isConsistent(var, value, assignment, csp): assignment[var] = value inferences = inference(var, value, assignment, csp) if inferences is not failure: assignment = assignment + inferences result = backtrack(assignment, csp) if result is not failure: return result del assignment[var] undoInferences(inferences, assignment, csp) return failure\nA technique used in constraint satisfaction problems to decide the order in which values are assigned to variables. The idea is to choose a variable and assign it a value from its domain that has the highest likelihood of leading to a solution, or the lowest likelihood of leading to a dead-end.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Steps involved in CSP search:","level":3,"id":"Steps_involved_in_CSP_search_0"},{"heading":"Backtracking","level":3,"id":"Backtracking_0"},{"heading":"Value ordering","level":3,"id":"Value_ordering_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159042,"modifiedTime":1737554783000,"sourceSize":10009,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/3. Constraint Satisfaction Problem.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html":{"title":"4. Constraint Learning","icon":"","description":"Constraint learning is a technique used in constraint programming to improve the efficiency of solving constraint satisfaction problems. This technique involves dynamically learning new constraints during the search process and adding them to the problem instance to further reduce the search space.The process of constraint learning involves examining the search history to identify situations where a constraint could have been applied but was not. These situations are known as constraint violations. Once a violation is identified, the constraint that could have been applied is extracted and added to the problem instance. This process is repeated until no more constraints can be learned, or until a solution is found. One common approach to constraint learning is to use an external solver to identify the violations and extract the constraints. These constraints can then be added to the problem instance and solved using the original solver. Another approach is to use a dedicated constraint learning solver that can both identify violations and solve the problem instance.When using constraint learning, it is important to balance the cost of learning constraints against the potential benefits. Learning too many constraints can lead to increased memory usage and overhead, while learning too few constraints may not significantly improve the search process. As a result, forgetting them can be a useful strategy. This means that the solver only considers a subset of the violations at any given time, ignoring the rest. This can help to reduce the search space and make the learning process more efficient.There are different approaches to forgetting violations. One common method is to use a priority queue to keep track of the violations, and only consider the highest-priority ones at each iteration. Another approach is to use a sliding window, where only a certain number of recent violations are considered.However, forgetting too many violations can also lead to suboptimal solutions, as important information may be lost. Therefore, it is important to find a balance between considering enough violations to learn useful information, and not considering too many that would slow down the learning process.Another important consideration is the choice of constraint learning algorithm. Different algorithms may be more effective for different types of problems, and it is important to experiment with different approaches to find the most effective one for a given problem.\nA constraint graph is a graphical representation of a set of constraints between variables.\nIn a constraint graph, each node represents a variable, and each edge represents a constraint between the two connected variables.\nConstraint graphs are useful for visualizing the structure of the problem and identifying relationships between variables.\nConstraint propagation algorithms can be applied to constraint graphs to reduce the domains of variables and find solutions to the problem. A tree-like constraint graph is a special type of constraint graph in which the graph forms a tree structure.\nIn a tree-like constraint graph, each variable is connected to only one other variable, forming a branching structure.\nTree-like constraint graphs are useful for problems in which the constraints between variables are hierarchical in nature.\nConstraint propagation algorithms can be applied to tree-like constraint graphs to reduce the domains of variables and find solutions to the problem.\nSome tips for using constraint graphs and tree-like constraint graphs:\nUse constraint graphs to visualize the structure of the problem and identify relationships between variables.\nUse tree-like constraint graphs for problems in which the constraints between variables are hierarchical in nature.\nApply constraint propagation algorithms to constraint graphs and tree-like constraint graphs to reduce the domains of variables and find solutions to the problem.\nBe aware that as the number of variables and constraints increases, the size and complexity of the constraint graph or tree-like constraint graph can become difficult to manage. In these cases, more specialized algorithms may be necessary.\na common issue in constraint satisfaction problems (CSPs). Symmetry arises when there are multiple equivalent solutions to a CSP. It can lead to redundant work when searching for solutions and can slow down the solution process. Symmetry can be eliminated using a variety of techniques, including symmetry-breaking constraints and variable ordering heuristics.Symmetry-breaking constraints are a type of constraint that is added to a CSP to break its symmetries. These constraints eliminate the equivalent solutions that result from symmetries, thereby reducing the search space and speeding up the solution process. A common type of symmetry-breaking constraint is a lexicographic ordering constraint, which imposes an ordering on the variables of the CSP. This ordering eliminates the equivalent solutions that result from permutations of the variables.Symmetry-breaking constraints can be difficult to identify and implement in large and complex CSPs. However, they are an effective way to reduce the search space and improve the performance of a CSP solver. Many CSP solvers include symmetry-breaking constraints as a standard feature, and they are widely used in practice to solve real-world problems.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Approach","level":3,"id":"Approach_0"},{"heading":"Forgetting Violations","level":4,"id":"Forgetting_Violations_0"},{"heading":"Constraint Graphs","level":3,"id":"Constraint_Graphs_0"},{"heading":"Tree-Like Constraint Graphs","level":4,"id":"Tree-Like_Constraint_Graphs_0"},{"heading":"Symmetry","level":3,"id":"Symmetry_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159046,"modifiedTime":1737554783000,"sourceSize":6489,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/4. Constraint Learning.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html":{"title":"5. Optimal Solving","icon":"","description":"Optimal solving in constraint satisfaction problems (CSPs) involves finding solutions that are optimal with respect to a given objective function. This is often useful in practical applications where there may be multiple solutions to a problem, but some solutions are better than others based on certain criteria.One approach to optimal solving in CSPs is to use complete search algorithms that systematically search the entire solution space for the optimal solution. These algorithms can be very effective for small problem instances, but they can become computationally infeasible for larger problems due to the exponential growth in the number of possible solutions.Another approach is to use heuristic search algorithms that use heuristics to guide the search towards the most promising solutions. These algorithms can be faster than complete search algorithms for larger problems, but they may not guarantee finding the optimal solution.Optimal solving in CSPs can also be done using optimization techniques such as linear programming or mixed-integer programming. These techniques formulate the CSP as an optimization problem and use optimization solvers to find the optimal solution. These techniques can be very powerful for large-scale problems, but they may require significant computational resources and may not be applicable to all types of CSPs.Symmetry breaking techniques can also be useful in optimal solving. Symmetry in CSPs can cause redundant computations and can make it difficult to find the optimal solution. By breaking symmetry, the search space can be reduced, and the search can be directed towards the most promising solutions.A search algorithm used in optimization problems to find the optimal solution by incrementally searching the solution space while keeping track of the best solution found so far. The algorithm uses a depth-first search strategy to explore the solution space, and employs a branch and bound technique to prune the search tree and eliminate suboptimal solutions.In the DFBB algorithm, the search begins at the root of the tree, and moves recursively down the tree, expanding nodes and branching out to explore new solutions. At each node, the algorithm computes the lower bound of the solution, which is an estimate of the minimum possible value of the objective function. If the lower bound is greater than the best solution found so far, the node is pruned and the search moves on to the next node. If the lower bound is less than or equal to the best solution found so far, the node is expanded and the search continues down the tree.The DFBB algorithm maintains a global best solution throughout the search, and updates it whenever a new better solution is found. The algorithm terminates when all nodes have been explored or when the best solution found cannot be improved further.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Depth-First Branch and Bound","level":3,"id":"Depth-First_Branch_and_Bound_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159049,"modifiedTime":1737554783000,"sourceSize":12492,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/5. Optimal Solving.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html":{"title":"6. Temporal Constraint Networks","icon":"","description":"Temporal Constraint Networks (TCN) are a class of constraint satisfaction problems where the variables represent time points and the constraints represent temporal relationships between the variables. TCNs are used to model and reason about temporal dependencies in a variety of fields, such as planning, scheduling, and resource allocation.In TCNs, each variable represents a time point and the domain of each variable consists of all possible time values. The constraints in a TCN specify temporal relationships between the variables, such as temporal precedence, duration, and resource usage. These constraints can be either soft or hard, depending on the problem being modeled.A TCN can be represented as a graph, where the nodes represent the variables (time points) and the edges represent the temporal constraints between the nodes. The constraints can be represented as functions or relations that map a set of time points to a set of valid temporal values.Solving a TCN involves finding a consistent assignment of time points to the variables that satisfies all of the constraints. This can be done using various algorithms, such as backtracking search and local search, with various heuristics and pruning techniques.In Temporal Constraint Networks (TCNs), temporal constraints are used to represent relationships between temporal variables. These constraints can be transitive, meaning if there is a constraint between variables A and B, and a constraint between variables B and C, then there is an implicit constraint between variables A and C. However, some temporal constraints may not be transitive, and can exhibit asymmetry, meaning if there is a constraint between variables A and B, there may not be a constraint between variables B and A.For example, consider the constraint \"Task A must be completed before Task B can begin.\" This constraint is transitive, as it implies that if Task B must be completed before Task C can begin, then Task A must also be completed before Task C can begin. However, this constraint is also asymmetric, as it implies that if Task B must be completed before Task A can begin, then Task A does not need to be completed before Task C can begin.Used to represent relationships between temporal variables. These constraints can be transitive, meaning if there is a constraint between variables A and B, and a constraint between variables B and C, then there is an implicit constraint between variables A and C. However, some temporal constraints may not be transitive, and can exhibit asymmetry, meaning if there is a constraint between variables A and B, there may not be a constraint between variables B and A.For example, consider the constraint \"Task A must be completed before Task B can begin.\" This constraint is transitive, as it implies that if Task B must be completed before Task C can begin, then Task A must also be completed before Task C can begin. However, this constraint is also asymmetric, as it implies that if Task B must be completed before Task A can begin, then Task A does not need to be completed before Task C can begin.\nBacktracking: This algorithm involves systematically searching through the space of possible solutions, backtracking when an inconsistency or dead-end is encountered. It involves a depth-first search of the solution space and is guided by a variable ordering heuristic, value ordering heuristic, and consistency checking.\nConstraint Propagation: This algorithm involves iteratively applying constraints to variables until no more constraints can be applied. It involves establishing arc consistency between pairs of variables, enforcing generalized arc consistency, or propagating time intervals.\nLocal Search: This algorithm involves iteratively improving a candidate solution by modifying a small subset of variables at each step. It involves defining a neighborhood of candidate solutions, moving to a neighboring solution, and accepting or rejecting the new solution based on an acceptance criterion.\nMixed Integer Programming (MIP): This algorithm involves formulating the TCN problem as a mixed-integer programming problem and solving it using an off-the-shelf MIP solver. It involves defining decision variables, objective function, and constraints, then using branch-and-bound, branch-and-cut, or column generation techniques to find an optimal solution.\nPetri Nets: This algorithm involves modeling the TCN problem as a Petri Net and analyzing its properties. It involves defining transitions, places, arcs, and markings, then simulating the behavior of the Petri Net to find a feasible or optimal solution.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Temporal constraints","level":3,"id":"Temporal_constraints_0"},{"heading":"Temporal constraints","level":3,"id":"Temporal_constraints_1"},{"heading":"Solving","level":3,"id":"Solving_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159057,"modifiedTime":1737554783000,"sourceSize":4965,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/6. Temporal Constraint Networks.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html":{"title":"Constraint Satisfaction 1","icon":"","description":"\n1\nConsider the following set Γ of first order formulae:Γ = { ∀x(¬Q(x) → P(x)), ¬∃y P(y), Q(a) → ∃x(R(x) ∧ ¬Q(x)) }\n(a) Use normal-forming moves to transform Γ into a set ∆ of first order clauses such that ∆ is satisfiable if and only if Γ is satisfiable.First get the quantifiers to the front (<a data-href=\"2. Prenex Normal form\" href=\"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2. Prenex Normal form</a>): ∀x(¬Q(x) → P(x)), ∀y ¬P(y), ∃x(Q(a) → (R(x) ∧ ¬Q(x)))\nNext remove the existential quantifier and use a name (skolem constant) instead: ∀x(¬Q(x) → P(x)), ∀y ¬P(y), Q(a) → (R(b) ∧ ¬Q(b))\nDelete the universal quantifiers, and put the propositional parts into clause form:∆ = { Q(x) ∨ P(x), ¬P(y), ¬Q(a) ∨ R(b) ¬Q(a) ∨ ¬Q(b) }\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":["artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html#_0"],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159066,"modifiedTime":1737554783000,"sourceSize":6334,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/1. Exercises/Constraint Satisfaction 1.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html":{"title":"Constraint Satisfaction 2","icon":"","description":"\nConsider the following constraint network γ = (V, D, C): Variables: V = {a, b, c}. Domains: For all v ∈ V : Dv = {1, 2, 3, 4, 5, 6, 7, 8, 9}. Constraints: a = 2b; a + 1 = c; b + c ≤ 6. (a) Draw a constraint graph for this constraint network. It may help to also label each constraint with its definition when completing the next exercise.“ac-3 constraint.png” could not be found.(b) Run the AC-3(γ) algorithm, as specified in the lecture. Precisely, for each iteration of the while-loop, give the content of M at the start of the iteration, give the pair (u, v) removed from M, give the domain of u after the call to Revise(γ, u, v), and give the pairs (w, u) added into M.(a) M content: {(a, b),(a, c),(b, a),(b, c),(c, a),(c, b)}; pair selected: (a, b); Da = {2, 4, 6, 8}; Candidate (c, a) already present in M. (b) M content: {(a, c),(b, a),(b, c),(c, a),(c, b)}; pair selected: (a, c); Da = {2, 4, 6, 8}; No change, so nothing added into M. (c) M content: {(b, a),(b, c),(c, a),(c, b)}; pair selected: (b, a); Db = {1, 2, 3, 4}; Candidate (c, b) already present in M. (d) M content: {(b, c),(c, a),(c, b)}; pair selected: (b, c); Db = {1, 2, 3, 4}; No change, so nothing added into M. (e) M content: {(c, a),(c, b)}; pair selected: (c, a); Dc = {3, 5, 7, 9}; Candidate (b, c) added into M. (f) M content: {(c, b),(b, c)}; pair selected: (c, b); Dc = {3, 5}; Candidate (a, c) added into M. (g) M content: {(b, c),(a, c)}; pair selected: (b, c); Db = {1, 2, 3}; Candidate (a, b) added into M. (h) M content: {(a, c),(a, b)}; pair selected: (a, c); Da = {2, 4}; Candidate (b, a) added into M. (i) M content: {(a, b),(b, a)}; pair selected: (a, b); Da = {2, 4}; No change, so nothing added into M. (j) M content: {(b, a)}; pair selected: (b, a); Db = {1, 2}; Candidate (c, b) added into M. (k) M content: {(c, b)}; pair selected: (c, b); Dc = {3, 5}; No change, so nothing added into M. (l) M empty; return modified γ with reduced domains. The Queens Problem is usually stated in terms of a standard 8 × 8 chessboard, probably because a board of that size looks familiar. However, it can be formulated similarly for boards of other sizes, including smaller ones. To the right, one solution to the 5 queens problem is shown (that is, 5 queens are placed on a 5×5 chessboard without any attacking each other). How many other solutions to the 5 queens problem can you find just by exploiting symmetries in the problem? Identify at least one additional constraint which would allow the solution to the right, but would remove some of the symmetries. Make sure they only remove symmetrical solutions. Specifically, if some solution violates the new constraints, there must be another solution symmetrical to it which does not.\nUse here the standard encoding that there are 5 variables {q1, q2, ..., q5} where the domain of each is {1, ..., 5}, and qi represents the queen in column (file) i, and the value of qi is which row (rank) that queen is in.Some example constraints that break some of the symmetry are: q1 &lt; q5 This constraint breaks symmetry both vertically and horizontally. Given a solution a and its reflection a', a' breaks constraint 1 if and only if a does not break the constraint. q1 ≤ 3 ∧ q1 = 3 → q2 &lt; 3 This constraint breaks horizontal symmetry. It specifies that if q1 is less than or equal to 3 and q1 equals 3, then q2 must be less than 3. q3 ≤ 3 ∧ q3 = 3 → q2 &lt; 3 This constraint is similar to constraint 2 but also breaks 180° symmetry when q3 is not equal to 3. Constraint problems involving time are often formalized as Temporal Constraint Networks. To define a constraint network, we need a set of time points H and intervals between them. The time points we will use are:\nWe will start by considering a simple version, using the following constraints:\nBoth dishes take 35-40 minutes to cook, meaning each dish must be put in the oven and taken out between 35 and 40 minutes later.\nd1 must cool for at least 10 minutes before being served.\nd1 requires 30 minutes of preparation time.\nYou must serve dinner by 7:30pm.\nYou cannot start preparation until 6:00pm.\nNeither dish can go into the oven for the first 15 minutes after 6:00pm today, while you wait for the oven to pre-heat.\nd1 must come out of the oven before d2 goes into the oven, meaning d2 must come out of the oven after d1 goes into the oven. Considering only constraint 2 with the serve and d1-out time points, and assuming that serve is at 7:30pm, what is the latest that d1-out could occur?\nBased on the constraint, d1-out can happen anywhere between 10 and ∞ minutes before serve. The latest it can be served is then 10 minutes before. Note that this does not depend on the lower bound at all, so if the constraint was, between 10 and 20 minutes before, the latest would still be 10 minutes before. Hence, the answer is 7:20pm.\nWhat if you take into account all the constraints between the time points d1-in, d1-out, d2-in, d2-out, serve? (i.e., leaving out start and prep-d1). You will need to work iteratively.\n6:55pm, because the constraint that it has to come out before d2 goes in supersedes constraint 2. Adding up the time differences in the upper bounds gives us a time difference of −35.\nConstruct the distance graph for the simplified temporal constraint network in part b. In this graph, find the shortest path (which might be negative) from serve to d1-out using only the time points from b. As you’re doing this, can you convince yourself that this is exactly the same as part b?\n“simple_distance_graph.png” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159069,"modifiedTime":1737554783000,"sourceSize":7251,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/1. Exercises/Constraint Satisfaction 2.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html":{"title":"0. Intro","icon":"","description":"Planner = solver over a class of problem Planning and decision-making are fundamental tasks in Artificial Intelligence, which involve selecting actions to achieve a desired goal or maximize some utility function. Planning involves reasoning about future events and determining the sequence of actions required to achieve a goal, while decision-making involves selecting the best action to take in a given situation. Fully observable environment: The planner has complete knowledge about the current state of the environment and can fully observe the effects of actions. There are no hidden or partially observable states. Deterministic environment: The outcome of an action is deterministic and known in advance. There is no randomness or uncertainty in the environment. Static environment: The environment does not change while the planner is executing the plan. There are no external factors that can influence the environment. Finite set of actions: The planner has a finite set of actions to choose from, and the set of actions does not change over time. Discrete time: The planner operates in discrete time steps, where the duration of an action is assumed to be negligible compared to the time steps. Perfect execution: The planner assumes that the actions are executed perfectly, without any errors or failures. Sequential: Solution is a seqence of actions Planning in Symbolic AI involves representing the world in terms of logical propositions and reasoning about the effects of actions. A planner uses a domain model to determine the preconditions and effects of each action, and a search algorithm to find a sequence of actions that achieves the goal.Some popular planning algorithms in Symbolic AI include:\nSTRIPS (Stanford Research Institute Problem Solver): a classical planner that uses a forward search to find a sequence of actions that achieves the goal.\nGraphplan: a planner that uses a backward search and a graph-based representation of the planning problem to generate a plan.\nSATPLAN: a planner that converts the planning problem into a propositional satisfiability problem and uses a SAT solver to find a plan.\nDecision-making in Symbolic AI involves representing the world in terms of logical propositions and reasoning about the expected outcomes of different actions. A decision maker uses a utility function to evaluate the desirability of different outcomes, and selects the action that maximizes the expected utility.Some popular decision-making algorithms in Symbolic AI include:\nValue iteration: an algorithm for finding the optimal policy for a Markov decision process (MDP) by iteratively computing the optimal value function.\nPolicy iteration: an algorithm for finding the optimal policy for an MDP by iteratively improving an initial policy.\nQ-learning: a model-free reinforcement learning algorithm that learns the optimal policy by estimating the Q-value function.\nPlanning and decision-making algorithms have a wide range of applications in Symbolic AI, including:\nRobotics: planning and decision-making algorithms are used to control the actions of robots and autonomous systems.\nLogistics: planning algorithms are used to optimize the scheduling and routing of resources in transportation and supply chain management.\nGame AI: decision-making algorithms are used to create intelligent agents in games and simulations.\nNatural language processing: planning and decision-making algorithms are used to generate coherent and fluent natural language texts.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Classical Planning Assumptions","level":3,"id":"Classical_Planning_Assumptions_0"},{"heading":"Planning in Symbolic AI","level":3,"id":"Planning_in_Symbolic_AI_0"},{"heading":"Decision Making in Symbolic AI","level":3,"id":"Decision_Making_in_Symbolic_AI_0"},{"heading":"Applications of Planning and Decision Making in Symbolic AI","level":3,"id":"Applications_of_Planning_and_Decision_Making_in_Symbolic_AI_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159263,"modifiedTime":1737554783000,"sourceSize":3961,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/0. Intro.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html":{"title":"1. Plans","icon":"","description":"Classical planning is a branch of AI concerned with developing agents that can make a sequence of decisions to achieve a goal. It involves constructing a plan, a sequence of actions that will achieve a particular goal or set of goals. The plan is executed in the environment, and the agent observes the resulting state of the environment and updates its plan accordingly.Classical planning is typically formulated as a search problem, where the goal is to find a sequence of actions that will transform the initial state into a goal state. The problem can be defined as a tuple (S, A, T, I, G), where:\nS is a set of states\nA is a set of actions\nT is a transition function that maps a state and an action to a new state: I is the initial state\nG is the set of goal states\nC is the cost function\nNote: if action a is not applicable in state s then T(s,a) is undefined\nAs a result, given the planning model (S,A,T,I,G,C), finding a sequence of actions &lt;a1,a2,a3 ... &gt; leading to the environment from I to G in min cost.A sequence of actions that take the initial state of the planning problem to the goal state. Each action in the plan is executed sequentially and the effects of the previous action become the preconditions of the next action. A linear plan can be obtained through search algorithms such as depth-first search, breadth-first search, or A* search, where the goal is to find a sequence of actions that leads from the initial state to the goal state. Once a linear plan is found, it can be executed to achieve the goal. However, in some cases, there may be multiple linear plans that achieve the same goal, and it is up to the planner to choose the most appropriate one based on various factors such as efficiency, safety, and cost.A plan that contains conditional or looping constructs, which allow for more complex and flexible planning than linear plans. They can include if-then statements, loops, and other control structures that allow the planner to change its course of action based on the state of the environment or the outcome of actions.For example, consider a robot trying to navigate through a maze. A non-linear plan for the robot might include the following steps:\nIf the robot is at a dead end, turn around and go back the way it came.\nOtherwise, if there is an open passage to the left, turn left and move forward.\nOtherwise, if there is an open passage straight ahead, move forward.\nOtherwise, if there is an open passage to the right, turn right and move forward.\nOtherwise, the robot is stuck and cannot proceed.\nIn this plan, the robot uses conditional statements to decide which direction to move in based on the available passages. This allows for a more flexible and adaptive planning approach that can handle a variety of different maze layouts and obstacles.Non-linear plans are typically represented as decision trees or flowcharts that show the different possible paths through the plan based on the current state and the outcomes of previous actions. They can be more complex to generate and execute than linear plans, but they offer greater flexibility and adaptability in dynamic and uncertain environments.A plan that allows for multiple actions to be executed simultaneously or in a concurrent fashion. In other words, the actions in a parallel plan can be executed independently of each other, without any ordering constraints. This is in contrast to a sequential plan, where the actions must be executed in a specific order.Parallel plans are useful in situations where there are multiple agents or resources available to execute actions simultaneously. For example, in a manufacturing plant, multiple machines could be used in parallel to produce different parts of a product simultaneously.One approach to generating parallel plans is to use partial-order planning, where the order of some actions is left unspecified, allowing for multiple actions to be executed in parallel. Another approach is to use temporal planning, which allows for the specification of temporal constraints between actions, such as start and end times, and can be used to generate plans that take advantage of parallelism.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Problem Formulation","level":2,"id":"Problem_Formulation_0"},{"heading":"Linear plan","level":3,"id":"Linear_plan_0"},{"heading":"Non-linear plan","level":3,"id":"Non-linear_plan_0"},{"heading":"Parallel plan","level":3,"id":"Parallel_plan_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159267,"modifiedTime":1737554783000,"sourceSize":4617,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/1. Plans.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html":{"title":"2. Planning Algorithm","icon":"","description":"Forward search is a simple and intuitive planning algorithm. It starts with the initial state and generates successor states by applying available actions. If a successor state is a goal state, the planning process terminates. Otherwise, the algorithm continues the search from the successor state.```python\nfrom collections import deque def forward_search(problem): frontier = deque() explored = set() # Create a node with the initial state and add it to the frontier start_node = Node(problem.initial_state, None, None) frontier.append(start_node) while frontier: current_node = frontier.popleft() # Check if the current state is a goal state if problem.is_goal_state(current_node.state): return construct_plan(current_node) explored.add(current_node.state) # Generate the next possible actions actions = problem.get_actions(current_node.state) for action in actions: next_state = problem.apply_action(current_node.state, action) # Create a new node for the next state next_node = Node(next_state, current_node, action) # Add the next node to the frontier if it hasn't been explored before if next_state not in explored: frontier.append(next_node) return None # No valid plan found def construct_plan(node): plan = [] while node.parent is not None: plan.append(node.action) node = node.parent plan.reverse() return plan domain-specific: search control rules, heuristics\ndomain-independent: heuristics extracted from the STRIPS problem description backward search: from the goal to the initial state\nBackward search is a planning algorithm that starts with the goal state and works backward to the initial state. It generates predecessor states by applying available actions and checks if the predecessor state is the initial state. If it is, the planning process terminates. Otherwise, the algorithm continues the search from the predecessor state.Heuristic search algorithms, such as A and IDA, use heuristic functions to guide the search process. The heuristic function estimates the cost of reaching a goal state from a given state. The search algorithm then chooses the state with the lowest estimated cost and expands it.Graphplan is a classical planning algorithm that uses a propositional logic representation of the planning problem. It constructs a planning graph that represents the set of all possible states and actions in the planning problem. The planning graph is then used to find a valid plan.class GraphPlanner(object): def __init__(self): self._layered_plan: LayeredPlan = LayeredPlan() self._mutex = {} def plan(self, gr: Graph, g: set): index = gr.num_of_levels - 1 if not g.issubset(gr.prop[index]): return None plan = self._extract(gr, g, index) if plan: return self._layered_plan if gr.fixed_point: n = 0 try: props_mutex = self._mutex[gr.num_of_levels - 1] except KeyError: props_mutex = None if props_mutex: n = len(props_mutex) else: n = 0 while True: index += 1 gr = PlanningGraph.expand(gr) plan = self._extract(gr, g, index) if plan: return self._layered_plan elif gr.fixed_point: try: props_mutex = self._mutex[gr.num_of_levels-1] except KeyError: props_mutex = None if props_mutex: if n == len(props_mutex): # this means that it has stabilised return None else: n = len(props_mutex)\nfunction Extract(i, gi, πi) returns a parallel plan, or failure if i = 0 then return πi if gi ≠ {} then select any p ∈ gi E ← {a ∈ Ai | p ∈ eff+(a) and ∀b ∈ πi {(a, b)} ∉ µAi} if E = {} then return failure for each a ∈ E do result ← Extract(i, gi \\ eff+(a), πi ∪ {a}) if result ≠ failure then return result return failure else π ← Extract(i - 1, ∪a ∈ πi Pre(a), {}) if π = failure then return failure return π.πi\nend call: Extract(k, g, {}) where k is the last layer in the graph\nSTRIPS (Stanford Research Institute Problem Solver) is a language and planning system for describing and solving problems in artificial intelligence. It was developed at SRI International in the 1960s and was one of the first automated planning systems.The STRIPS language consists of a set of states, actions, and initial and goal states. The actions describe how the states can be changed, and the initial and goal states describe the starting and ending points of the problem. The actions are specified in terms of preconditions, which must be true for the action to be performed, and effects, which describe the changes that occur to the states when the action is performed.STRIPS uses a forward-chaining inference algorithm to search for a plan that will take the system from the initial state to the goal state. The algorithm builds a search tree of possible plans, starting with the initial state and working forward through the actions until it reaches the goal state.PDDL stands for \"Planning Domain Definition Language\". It is a formal language used to describe planning problems and domains in artificial intelligence. PDDL was created to standardize the representation of planning problems and solutions, making it easier to share and compare different planning algorithms.PDDL is a declarative language, meaning that it describes the problem in terms of its properties and constraints, rather than specifying a set of steps to solve the problem. The language provides a way to define the initial state of a problem, the set of possible actions, and the goal state.PDDL is commonly used in AI research and development, and many planning systems and solvers are designed to work with PDDL. The language has evolved over time, with new versions adding additional features and capabilities to better represent more complex planning problems.The syntax of PDDL consists of a set of rules that define the elements of the planning problem, such as the objects, initial state, goal state, and actions. Here is a brief overview of the syntax:\nObjects: declared using the :objects keyword followed by a list of object names enclosed in parentheses.\nPredicates: declared using the :predicates keyword followed by a list of predicate names and their arguments enclosed in parentheses.\nInitial state: declared using the :init keyword followed by a list of ground literals that describe the initial state of the planning problem.\nGoal state: declared using the :goal keyword followed by a list of ground literals that describe the desired goal state of the planning problem.\nActions: declared using the :action keyword followed by the name of the action, a list of its parameters enclosed in parentheses, a list of its preconditions enclosed in parentheses after the keyword :precondition, and a list of its effects enclosed in parentheses after the keyword :effect.\nThe preconditions and effects of an action are defined in terms of predicates and literals that refer to objects and their states. The syntax of PDDL also includes several other keywords and constructs, such as types, functions, and axioms, which provide additional expressivity for specifying planning problems.(define (domain blocks) (:requirements :strips) (:predicates (on ?x - block ?y - block) (ontable ?x - block) (clear ?x - block) (holding ?x - block)) (:action pick-up :parameters (?x - block) :precondition (and (clear ?x) (ontable ?x) (not (holding ?x))) :effect (and (not (ontable ?x)) (not (clear ?x)) (holding ?x))) (:action put-down :parameters (?x - block) :precondition (holding ?x) :effect (and (ontable ?x) (clear ?x) (not (holding ?x)))) (:action stack :parameters (?x - block ?y - block) :precondition (and (holding ?x) (clear ?y)) :effect (and (not (holding ?x)) (not (clear ?y)) (on ?x ?y) (clear ?x))) (:action unstack :parameters (?x - block ?y - block) :precondition (and (on ?x ?y) (clear ?x) (not (holding ?x))) :effect (and (holding ?x) (clear ?y) (not (on ?x ?y)))))\n(define (problem blocks-4-0) (:domain blocks) (:objects a b c d - block) (:init (on a b) (on b c) (ontable c) (ontable d) (clear a) (clear d) (not (on a c)) (not (on b d)) (not (holding ?x))) (:goal (and (on a b) (on b c) (on c d))))\nAction Description Language, which is a formal language used to specify actions and their effects in AI planning. ADL is an extension of STRIPS (Stanford Research Institute Problem Solver) language and allows the representation of more complex actions with richer preconditions and effects.ADL representation includes several features such as conditional effects, negative preconditions, quantified preconditions, and disjunctive preconditions. Conditional effects enable the specification of effects that are dependent on the conditions of the action. Negative preconditions allow stating what must not hold true for an action to be applicable. Quantified preconditions and disjunctive preconditions enable the specification of a range of values for the preconditions.The ADL representation is used by many AI planning systems, including SHOP and C+ +, and provides a more expressive and flexible way of describing actions and their effects, making it easier to develop complex planning problems.A planning algorithm that was introduced by Avrim Blum and Merrick Furst in 1995. The algorithm works by constructing a planning graph, which is a bipartite directed graph that represents the state space of the planning problem. The nodes in the graph represent the propositions that are true in a given state, and the edges represent the actions that can be taken to change the state.Graphplan works by constructing the planning graph in layers, where each layer contains all of the propositions and actions that can be achieved from the propositions in the previous layer. This process continues until either a goal state is found or a fixed number of layers have been constructed without finding a solution.Once the planning graph has been constructed, Graphplan uses a backward search to find a plan that achieves the goal state. This search works by starting at the goal state and working backwards through the layers of the planning graph to the initial state, using a series of graph-based operations to construct a plan.Graphplan has been shown to be more efficient than previous planning algorithms in many cases, particularly in problems where the state space is large and the goals are complex. However, it has also been criticized for being too computationally expensive in some cases, and for not being able to handle some types of planning problems.A concept in Graphplan that is used to prevent multiple actions from executing on the same set of literals simultaneously. In other words, it ensures that no two actions that change the same set of literals can occur at the same time.This concept is achieved in Graphplan through the use of mutexes (short for mutual exclusions). A mutex is a binary relation between two actions that indicates that they cannot both be executed in any valid plan. Mutexes are computed as part of the Graphplan algorithm, and they are derived from the preconditions and effects of the actions.For example, consider the two actions \"drive to airport\" and \"drive to hotel\". These two actions are mutex because they both require the robot to be in the same location (i.e., in the car), but they have different goals (i.e., airport vs. hotel). Thus, these actions cannot both occur in the same plan.Mutexes can be used to prune the search space of possible plans and reduce the amount of search that must be done. By identifying mutexes, Graphplan can eliminate certain combinations of actions that are guaranteed to be invalid, and focus on finding valid plans instead.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"State-space planning","level":2,"id":"State-space_planning_0"},{"heading":"Forward Search (sound and complete)","level":3,"id":"Forward_Search_(sound_and_complete)_0"},{"heading":"Problems:","level":4,"id":"Problems_0"},{"heading":"Backward Search","level":3,"id":"Backward_Search_0"},{"heading":"Heuristic Search","level":3,"id":"Heuristic_Search_0"},{"heading":"Graphplan","level":3,"id":"Graphplan_0"},{"heading":"Plan Extraction","level":4,"id":"Plan_Extraction_0"},{"heading":"STRIPS","level":3,"id":"STRIPS_0"},{"heading":"PDDL","level":3,"id":"PDDL_0"},{"heading":"Syntax","level":4,"id":"Syntax_0"},{"heading":"Example","level":4,"id":"Example_0"},{"heading":"ADL","level":3,"id":"ADL_0"},{"heading":"Graphplan","level":3,"id":"Graphplan_1"},{"heading":"Mutual exclusion","level":3,"id":"Mutual_exclusion_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159271,"modifiedTime":1737554783000,"sourceSize":13213,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/2. Planning Algorithm.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html":{"title":"3. Heuristics","icon":"","description":"Domain-independent heuristics are heuristics that can be applied to any planning problem without specific domain knowledge. These heuristics provide estimates of the cost or distance to the goal state from a given state, helping guide the search algorithm towards more promising paths.These heuristics are called domain-independent because they do not rely on specific knowledge about the domain, such as the structure of the planning operators or the specific goals. Instead, they provide general estimation techniques that can be applied to any planning problem. However, their effectiveness may vary depending on the characteristics of the problem and the quality of the heuristic function.This heuristic counts the number of goal propositions that are not yet achieved in the current state. It assumes that each action will achieve at least one goal proposition and provides an estimate of the remaining distance to the goal.This heuristic constructs a relaxed version of the planning graph, where all delete effects of actions are ignored. It then estimates the cost to achieve the goal state in this relaxed graph, which provides a lower bound on the actual cost in the original problem. By ignoring the delete effects, the planning algorithm considers that once a proposition is achieved, it remains true throughout the plan execution. This simplification allows the planner to focus on achieving the positive effects and finding a sequence of actions that satisfies the goal conditions without explicitly handling the removal of unwanted propositions.function h_FF(current_state, goal_state): h = 0 for each proposition in goal_state: if proposition not in current_state: h = h + 1 return h\nThis heuristic computes the sum of the costs of individual actions needed to achieve each goal proposition independently. It assumes that actions can be executed in any order and their costs can be summed up.Relaxation is a common technique used in heuristic search to simplify the problem by ignoring certain aspects. The h max and h sum heuristics are two relaxation techniques often used in domain-independent planning.The h max heuristic is based on the idea of ignoring the negative effects of actions during the relaxation process. By considering only the positive effects of actions and ignoring any negative effects, the h max heuristic provides an optimistic estimate of the cost to reach the goal state from a given state. It assumes that all actions will have positive outcomes and no obstacles or conflicts will be encountered along the way. While the h max heuristic can overestimate the actual cost, it tends to be more informed than simpler heuristics like h 0 (zero heuristic) which assumes no action has any effect.# Initialization\nfor each proposition p: if p belongs to the current state s: H[p] = 0 else: H[p] = infinity # Fixed point iteration\nwhile not converged: for each action a: H[a] = max(H[p] for p in Pre(a)) for each proposition p in EFF+(a): H[p] = min(H[p], H[a] + cost(a)) # Calculate h_max heuristic\nh_max = max(H[p] for p in goal_state) # Return h_max value\nreturn h_max\nThe h sum heuristic takes relaxation a step further by ignoring the interactions between subgoals. In this heuristic, the problem is further simplified by considering each subgoal independently and summing up the estimated costs to achieve each subgoal. The h sum heuristic assumes that achieving each subgoal is independent of achieving the others and that there are no conflicts or dependencies among the subgoals. This approach can be useful when the subgoals are loosely connected or when the interactions between them do not significantly affect the overall cost. However, it can lead to suboptimal estimates when there are strong interactions or dependencies between the subgoals.Both the h max and h sum heuristics are quick to compute and can provide initial estimates for planning problems. However, they may not always accurately reflect the true cost of achieving the goal state due to the simplifications made during relaxation. To improve the effectiveness of these heuristics, domain-specific knowledge, advanced search techniques, abstraction, and pattern databases can be employed to tailor the heuristics to the problem at hand and provide more accurate estimates.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Domain-independent heuristics","level":3,"id":"Domain-independent_heuristics_0"},{"heading":"Goal counting heuristic","level":3,"id":"Goal_counting_heuristic_0"},{"heading":"Relaxed planning graph heuristic","level":3,"id":"Relaxed_planning_graph_heuristic_0"},{"heading":"Delete Relaxation (h FF):","level":4,"id":"Delete_Relaxation_(h_FF)_0"},{"heading":"Additive heuristic","level":3,"id":"Additive_heuristic_0"},{"heading":"h max and h sum heuristics","level":3,"id":"h_max_and_h_sum_heuristics_0"},{"heading":"h max heuristic","level":4,"id":"h_max_heuristic_0"},{"heading":"h sum heuristic","level":4,"id":"h_sum_heuristic_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159274,"modifiedTime":1737554783000,"sourceSize":10791,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/3. Heuristics.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html":{"title":"4. Regression","icon":"","description":"In planning, an action a is considered relevant for a goal g if it satisfies the following conditions:\nPositive Effect: The action a must make at least one of the propositions in g true. In other words, the intersection between the positive effects of a (eff+(a)) and g should not be an empty set ()\nNegative Effect: The action a must not make any of the propositions in g false. The intersection between the negative effects of a (eff-(a)) and g should be an empty set ).\nIf an action a is relevant for a goal g, the regression operator γ-1(g, a) is defined as follows:This means that the resulting state after applying action a to the goal g is obtained by removing the positive effects of a from g and adding the preconditions of a to the resulting state.By identifying the relevant actions for a goal, planners can focus on the actions that directly contribute to achieving the goal and avoid considering irrelevant actions. This relevance analysis helps streamline the planning process and improve the efficiency of finding a solution.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Relevance","level":3,"id":"Relevance_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159278,"modifiedTime":1737554783000,"sourceSize":1292,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/4. Regression.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html":{"title":"5. Plan space search","icon":"","description":"Plan-space search involves building a partial plan using the following components:\nMultiset O: A collection of operators {o1, ..., on}.\nSet &lt;: Ordering constraints oi &lt; oj, with transitivity built in.\nSet B: Binding constraints x = y, x ≠ y, x ∈ D, x ∉ D, representing substitutions.\nSet L: Causal links oi → oj, indicating that the effect p of oi establishes the precondition p of oj. The ordering constraints oi &lt; oj and binding constraints in B apply to the parameters of oi and oj appearing in p.\nThese components collectively define the structure of the plan and the relationships between operators, constraints, and causal links.Refers to a precondition of an action that is not satisfied in the current state. It means that there is a condition that must be true for the action to be applicable, but that condition is currently false or unknown.When planning, the goal is to find a sequence of actions that leads from an initial state to a desired goal state. During the search process, actions are considered and applied based on their preconditions. If an action has an open precondition, it means that the necessary condition for executing the action is not met, and the action cannot be applied in the current state.In planning, a threat refers to a situation where a partially constructed plan is at risk of being invalidated or unable to achieve the intended goal. More specifically, a threat occurs when a causal link between two actions in the plan is in danger of being broken.A causal link represents a dependency between two actions, where the effect of one action is a precondition for another action. The threat arises when there is a possibility that an action producing a precondition for another action may be undone or invalidated by a subsequent action.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Basic Idea","level":3,"id":"Basic_Idea_0"},{"heading":"Open Precondition","level":3,"id":"Open_Precondition_0"},{"heading":"Threats","level":3,"id":"Threats_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159282,"modifiedTime":1737554783000,"sourceSize":3312,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/5. Plan space search.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html":{"title":"Heuristics, Regression, and Partial-Order Planning","icon":"","description":"\nConsider the following delivery problem. Two driverless trucks (T1 and T2) can autonomously deliver the products that customers have requested. The products are initially at the depot (location A), and our trucks can reach the customers identified by their locations (B,C,D,E,F) by following suitable paths connecting adjacent locations. Just two out of these five customers (F and D) have to be served. Assume for simplicity that the two trucks have always enough products on-board. Ignore any cost that could be associated to the actions. A typed STRIPS description of the problem is as follows:\nTypes:\nLocations = {A, B, C, D, E, F}\nTrucks = {T1, T2}\nPropositions:\nat(t, x), where x ∈ Locations and t ∈ Trucks\ndelivered(x), where x ∈ Locations\nadjacent(x, y), where x, y ∈ Locations\nActions: go(t, x, y), where t ∈ Trucks, x, y ∈ Locations Preconditions: {at(t, x), adjacent(x, y)}\nAdd-list: {at(t, y)}\nDelete-list: {at(t, x)} deliver(t, x), where t ∈ Trucks, x ∈ Locations Preconditions: {at(t, x)}\nAdd-list: {delivered(x)}\nDelete-list: ∅ Initial state: {at(T1, A), at(T2, A)} ∪ {adjacent(x, y) | x, y ∈ Locations, there is an edge between x and y in the graph of Figure 1}Goal: {delivered(F), delivered(D), at(T1, A), at(T2, A)}\nExplain the concepts of the delete relaxation of a problem P and of a relaxed-plan for P.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159320,"modifiedTime":1737554783000,"sourceSize":7906,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/1. Exercises/Heuristics, Regression, and Partial-Order Planning.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html":{"title":"Planning Representations and Graph-Based Approaches","icon":"","description":"\nThe monkey-and-bananas domain features a monkey in a laboratory with some bananas hanging out of reach from the ceiling. A box is available that will enable the monkey to reach the bananas by pushing the box underneath them and climbing onto it. Initially, the monkey is at location A, the bananas are at location B, and the box is at location C. When the monkey climbs onto the box, the monkey’s height changes from floor to ceiling. The actions available are Go from one location to another, Push a pushable object from a location to another, ClimbUp onto or ClimbDown from a climbable object, and Grasping a graspable object. Grasping results in having the object if the monkey and object are at the same place and same height. Suppose that the goal of the monkey is to fool the scientists by getting the bananas but returning the box to its original place and then going back to its original location. Recall that STRIPS does not have negative preconditions nor negative goals. Describe the problem using the STRIPS representation (operators, initial state, goal).\n• Predicates: - climbable(o): o can be climbed on - pushable(o): o can be pushed - have(o): the monkey has o - graspable(o): o can be grasped - at(o,l,h): o is at location l and at height h • Initial state s0: {climbable(box), pushable(box), graspable(bananas), at(monkey,A,floor), at(bananas,B,ceiling), at(box,C,floor)} • Goal g: {have(bananas), at(box,C,floor), at(monkey,A,floor)} • Operators: - go(l1,l2) * pre: {at(monkey,l1,floor)} * eff-: {at(monkey,l1,floor)} * eff+: {at(monkey,l2,floor)} - push(o,l1,l2) * pre: {pushable(o), at(monkey,l1,floor), at(o,l1,floor)} * eff-: {at(monkey,l1,floor), at(o,l1,floor)} * eff+: {at(monkey,l2,floor), at(o,l2,floor)} - climbUp(o,l) * pre: {climbable(o), at(o,l,floor), at(monkey,l,floor)} * eff-: {at(monkey,l,floor)} * eff+: {at(monkey,l,ceiling)} - climbDown(o,l) * pre: {climbable(o), at(o,l,floor), at(monkey,l,ceiling)} * eff-: {at(monkey,l,ceiling)} * eff+: {at(monkey,l,floor)} - grasp(o,l,h) * pre: {graspable(o), at(o,l,h), at(monkey,l,h)} * eff-: {at(o,l,h)} * eff+: {have(o)} • Shortest plan: hgo(A,C), push(box,C,B), climbUp(box,B), grasp(bananas,B,ceiling), climbDown(box,B), push(box,B,C), go(C,A) Give the shortest sequential plan for this problem\nThe STRIPS rule for a state s and an action a = (pre(a), eff-(a), eff+(a)) is: s' = γ(s, a) = { (s - eff-(a)) ∪ eff+(a) if pre(a) ⊆ s undefined otherwise (action not executable)\n} We want to compute the result s' of applying go(A,C) in s0:\n- The precondition is verified: {at(monkey,A,floor)} ⊆ s0, hence the action is applicable.\n- The resulting state is: { climbable(box), pushable(box), graspable(bananas), at(monkey,C,floor), at(bananas,B,ceiling), at(box,C,floor) } ADL to STRIPS Consider the following action description, written in the ADL fragment of PDDL. (:action A\n:precondition (p)\n:effect (and (not (p))\n(when (q) (r))\n(q))\n)\nWrite down a set of equivalent plain STRIPS actions (i.e., without conditional effects, and without disjunction or negation in the preconditions)(\n:action A1 :precondition (and (p) (q)) :effect (and (not (p)) (r)) ) (\n:action A2 :precondition (and (p) (not (q))) :effect (and (not (p)) (q)) ) (\n:action A2 :precondition (and (p) (not-q)) :effect (and (not (p)) (q) (not (not-q))) ) Describe the changes that would need to be made to other actions in the domain description, the initial state and/or the goal in order to make the domain fall only within the STRIPS language.\nSince we have introduced a new proposition in the domain we need to ensure that: • the initial state description contains either (q) or (not (q)), since one of them will need to be true given the closed-world assumption • any other action adding (q) also deletes (not (q)) • any other action deleting (q) also adds (not (q)) This assumes of course that the other actions were already STRIPS compliant, otherwise they need to be transformed in the same way as A. If the goal wasn’t STRIPS compliant and mentioned (not q), then this needs to be transformed into (not-q) Graph-Based Planning\nConsider the following planning problem. You want a sheep and a goat. Using your credit card, you can buy a sheep from the Automatic Sheep Machine (ASM). Waving your magic wand turns a sheep into a goat. The planning operators are as follows, where a box represents an operator whose pre-conditions are on the left-hand side and effects are on the right-hand side of the box. Draw the planning graph until the first level leading to a plan. Include all mutex relations.\n“graph_plan.png” could not be found.The mutexes are as follows: • at level 2, W ave deletes haveS and is mutex with both ASM (inconsistence) and the maintenance action for haveS (interference + inconsistence) which have haveS as precondition (and effect for the maintenance action). • at level 2, haveS is mutex with haveG (inconsistent support caused by the action mutexes) • at level 3, W ave raemains mutex with ASM and the maintenance action for haveS for the same reasons (interference and inconsistence are static properties so these mutexes cannot disappear). • at level 3, the maintenance action for haveG is mutex with both W ave and the maintenance action for haveS because of competing needs (due to the mutex of haveS and haveG at level 2). • at level 3, the mutex between haveS and haveG has disappeared because haveG can now be produced by its maintenance action, which is not mutex with ASM. State at which levels Graphplan would attempt extraction.\nExtraction would only be attempted at level 3 because the goal ({haveS, haveG}) is mutex at level 2. 3.\nHighlight in the graph the plan extracted by Graphplan (include the maintenance actions chosen).\nThe actions, propositions and links that are part of the final plan are in bold face.\nGiven the built planning graph structure, provide at least two heuristics that can be computed from the graph and then used in an informed state space heuristic search framework (e.g., A*). Report the estimated distance to the goal in the initial state for each heuristic.\nA first heuristic can be computed by taking the index of the smallest level in which all the goal propositions have all appeared (h1). Another, more informed heuristic is the index of the first level in which all the goal propositions appear and are not mutex (h2). Therefore h1(s0, G) = 2 and h2(s0, G) = 3.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html","pathToRoot":"../../../..","attachments":[],"createdTime":1741084159325,"modifiedTime":1737554783000,"sourceSize":6841,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/1. Exercises/Planning Representations and Graph-Based Approaches.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/0.-data-lake/0.-introduction.html":{"title":"0. Introduction","icon":"","description":"A Data Lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to structure it first, and run different types of analytics—from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.Data Lakes can scale to handle massive volumes of data, accommodating petabytes and even exabytes of information. This is crucial for enterprises dealing with large-scale data from various sources.Data Lakes offer elastic scaling capabilities, allowing you to expand or reduce storage and compute resources based on your current needs, ensuring optimal resource utilization and cost management.Data Lakes support various types of data, including:\nStructured Data: Data organized in rows and columns, such as databases and spreadsheets.\nSemi-structured Data: Data with some organizational properties, like JSON, XML, and CSV files.\nUnstructured Data: Data without a predefined structure, such as text files, images, videos, and audio.\nUnlike traditional databases that require schema-on-write, Data Lakes use schema-on-read. This means you can apply the schema as you read the data, providing greater flexibility in handling diverse data types.Data Lakes are typically built on low-cost storage systems like Hadoop Distributed File System (HDFS), Amazon S3, and Azure Data Lake Storage, which offer affordable and scalable storage solutions.Many cloud-based Data Lakes operate on a pay-as-you-go pricing model, allowing you to only pay for the storage and compute resources you actually use, further optimizing costs.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Key Characteristics of Data Lakes","level":3,"id":"Key_Characteristics_of_Data_Lakes_0"},{"heading":"Scalability","level":4,"id":"Scalability_0"},{"heading":"Massive Data Handling","level":5,"id":"Massive_Data_Handling_0"},{"heading":"Elastic Scaling","level":5,"id":"Elastic_Scaling_0"},{"heading":"Flexibility","level":4,"id":"Flexibility_0"},{"heading":"Variety of Data Types","level":5,"id":"Variety_of_Data_Types_0"},{"heading":"Schema-on-Read","level":5,"id":"Schema-on-Read_0"},{"heading":"Cost-effectiveness","level":4,"id":"Cost-effectiveness_0"},{"heading":"Low-cost Storage","level":5,"id":"Low-cost_Storage_0"},{"heading":"Pay-as-you-go","level":5,"id":"Pay-as-you-go_0"},{"heading":"Decoupled Storage and Compute","level":4,"id":"Decoupled_Storage_and_Compute_0"},{"heading":"Independent Scaling","level":5,"id":"Independent_Scaling_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/0.-data-lake/0.-introduction.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641308,"modifiedTime":1737554783000,"sourceSize":8744,"sourcePath":"Big Data/0. Data Lake/0. Introduction.md","exportPath":"big-data/0.-data-lake/0.-introduction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/0.-data-lake/1.-architecture.html":{"title":"1. Architecture","icon":"","description":"Batch ingestion involves loading data in large chunks at scheduled intervals. This can be achieved using tools like:\nApache Sqoop: Used for transferring bulk data between Apache Hadoop and structured datastores such as relational databases.\nApache Flume: Designed for efficiently collecting, aggregating, and moving large amounts of log data.\nCustom ETL Jobs: Tailor-made Extract, Transform, Load (ETL) processes using various scripting and programming languages.\nReal-time ingestion handles continuous data streams to capture data in real-time. Tools commonly used include:\nApache Kafka: A distributed streaming platform capable of handling high throughput for real-time data feeds.\nAWS Kinesis: A service for real-time processing of streaming data at massive scale on AWS.\nAzure Event Hubs: A big data streaming platform and event ingestion service on Azure.\nDistributed file systems offer scalable storage solutions for large datasets. Commonly used systems include:\nHadoop Distributed File System (HDFS): A primary storage system for Hadoop applications, offering high throughput access to data.\nAmazon S3: Scalable object storage service provided by AWS.\nAzure Data Lake Storage (ADLS): Scalable and secure data lake storage for high-performance analytics workloads on Azure.\nObject storage is used for storing large amounts of unstructured data. S3-compatible storage solutions are widely used for their scalability and cost-effectiveness.Batch processing frameworks handle large-scale data processing jobs that run at scheduled intervals. Common tools include:\nApache Spark: Provides fast, in-memory data processing capabilities and supports batch processing.\nApache Flink: A stream-processing framework with support for batch processing.\nHadoop MapReduce: A programming model for processing large data sets with a distributed algorithm on a Hadoop cluster.\nStream processing frameworks handle continuous data streams for real-time analysis. Common tools include:\nApache Kafka Streams: A client library for building applications and microservices, where the input and output data are stored in Kafka clusters.\nApache Flink: Provides high-throughput, low-latency stream processing.\nApache Storm: A distributed real-time computation system for processing data streams.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Ingestion Layer","level":3,"id":"Ingestion_Layer_0"},{"heading":"Batch Ingestion","level":4,"id":"Batch_Ingestion_0"},{"heading":"Real-time Ingestion","level":4,"id":"Real-time_Ingestion_0"},{"heading":"Storage Layer","level":3,"id":"Storage_Layer_0"},{"heading":"Distributed File Systems","level":4,"id":"Distributed_File_Systems_0"},{"heading":"Object Storage","level":4,"id":"Object_Storage_0"},{"heading":"Processing Layer","level":3,"id":"Processing_Layer_0"},{"heading":"Batch Processing","level":4,"id":"Batch_Processing_0"},{"heading":"Stream Processing","level":4,"id":"Stream_Processing_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/0.-data-lake/1.-architecture.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641310,"modifiedTime":1737554783000,"sourceSize":4873,"sourcePath":"Big Data/0. Data Lake/1. Architecture.md","exportPath":"big-data/0.-data-lake/1.-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/0.-data-lake/2.-workflow.html":{"title":"2. Workflow","icon":"","description":"\nJob Preparation: Prepare a job request describing the specific data needed, the time range, the data format, and any special data processing requirements. Request Database and Table Permissions: Apply for permissions for the databases and tables required for the computations.\nCreate Tables: Create process and result tables in Hive for the computations. Directory Structure: Set up a directory structure to store and manage data, allowing for data processing and analysis within this directory. This structure can include multiple subdirectories, each representing a specific dataset or subject area. Upload Resource Files: Upload resource files used in calculations, such as jar files. In many big data frameworks like Apache Hadoop and Apache Spark, computations and transformations are often written in Java or Scala. Jar files are Java ARchive files that package compiled Java code, libraries, and resources into a single file. These jar files contain the necessary code and dependencies required to perform specific tasks in the workflow.\nConfigure Global Parameters: Set parameters that might be applied across nodes. Make the workflow general and configurable by setting global parameters like input data path, output result path, etc., which can be easily adjusted during execution to accommodate different datasets or requirements. (Example: location of the input data)\nCreate Nodes: Create task nodes within the workflow. Design multiple task nodes in the workflow designer, with each node representing a specific processing step. Workflows are typically composed of several tasks that need to be executed in a specific order. By creating individual nodes for each task, you can organize and manage the workflow effectively. Each node can represent steps like data extraction, transformation, aggregation, and loading into a target system.\nDebug Execution: Execute directly on nodes. Before running the entire workflow, debug to ensure each node executes as expected. Schedule Time: Specify the exact time when the task should start.\nFrequency: Define the repeat cycle of the task.\nDependencies: If the current task depends on the completion of other tasks, set these dependencies to ensure tasks execute in the correct order.\nTimeout Settings: Set the maximum runtime for the task to prevent it from indefinitely occupying resources.\nNotifications and Reports: Configure how to notify after task completion (e.g., email, SMS) and how to generate and store execution reports.\nFailure Handling: Specify actions to take if the task fails, such as retry attempts, wait times, or failure notifications. Complete Configuration: Define and configure all necessary task nodes, parameters, dependencies, and scheduling settings in the workflow design tool. This includes specifying each task's execution logic, input/output data, and any required resource files.\nCheck and Verify: Before submission, perform checks and verification to ensure the workflow configuration is correct. Validate parameters, confirm resource file paths, test connections, etc.\nSubmit Workflow: Click the \"Submit\" button to send the workflow to the \"Pending Release\" page. This action is similar to code submission in software engineering, indicating the current phase of work is complete and ready to move to the next stage.\nPending Release Page: On this page, the workflow waits for further review and scheduling. Administrators or authorized personnel can review the pending workflows, checking for compliance with organizational standards, potential performance issues, and data security requirements.\nReview and Schedule: Once approved, the workflow can be scheduled for execution. Based on the set scheduling parameters (e.g., execution time, frequency), the workflow will run automatically at the predetermined time.\nMonitoring and Optimization: After the workflow is published and executed, track its execution status in the monitoring interface. View execution logs, performance metrics, etc. If issues or performance bottlenecks are found, adjust and optimize the workflow before resubmitting.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Submitting a Job","level":3,"id":"Submitting_a_Job_0"},{"heading":"Creating a Project","level":3,"id":"Creating_a_Project_0"},{"heading":"Creating a Workflow","level":3,"id":"Creating_a_Workflow_0"},{"heading":"Configuring Schedule Time","level":3,"id":"Configuring_Schedule_Time_0"},{"heading":"Submitting the Workflow","level":3,"id":"Submitting_the_Workflow_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/0.-data-lake/2.-workflow.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641311,"modifiedTime":1737554783000,"sourceSize":4811,"sourcePath":"Big Data/0. Data Lake/2. Workflow.md","exportPath":"big-data/0.-data-lake/2.-workflow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/0.-data-lake/3.-datax.html":{"title":"3. DataX","icon":"","description":"DataX is an open-source data synchronization tool from Alibaba Group, designed to efficiently transfer data across various data sources and targets. It supports a wide range of data sources, including relational databases, NoSQL databases, and cloud storage services. DataX is highly extensible, easy to use, and optimized for performance.\nExtensibility: DataX has a modular structure that allows for easy customization and extension for different data sources.\nEase of Use: Configuration is straightforward through JSON files.\nHigh Performance: DataX is optimized for efficient data transfer and synchronization.\nRich Support: Supports various data sources like MySQL, Oracle, SQL Server, HDFS, and more. Core Engine: Manages the entire data synchronization process.\nReader Plugins: Read data from the source.\nWriter Plugins: Write data to the target. Prerequisites: Java: Ensure JDK 1.8+ is installed.\nPython: Ensure Python 2.7+ is installed. Download DataX:\ngit clone https://github.com/alibaba/DataX.git\ncd DataX DataX uses JSON configuration files to define data synchronization jobs. These files specify the data source (reader), the data destination (writer), and any necessary transformations.{ \"job\": { \"content\": [ { \"reader\": { \"name\": \"mysqlreader\", \"parameter\": { \"username\": \"your_username\", \"password\": \"your_password\", \"column\": [\"id\", \"name\", \"age\"], \"splitPk\": \"id\", \"connection\": [ { \"table\": [\"your_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_database\"] } ] } }, \"writer\": { \"name\": \"mysqlwriter\", \"parameter\": { \"writeMode\": \"insert\", \"username\": \"your_username\", \"password\": \"your_password\", \"column\": [\"id\", \"name\", \"age\"], \"connection\": [ { \"table\": [\"your_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_target_database\"] } ] } } } ], \"setting\": { \"speed\": { \"channel\": 3 } } }\n}\nUse the datax.py script to execute your job.python datax.py /path/to/your/job.json\nThis command runs the job defined in the JSON configuration file, transferring data from the source to the target as specified. Prepare the Environment: Ensure that both Java and Python are installed on your system.\nSet the JAVA_HOME and PYTHON_HOME environment variables if needed. Create the Configuration File: Write a JSON file (job.json) with the necessary configuration.\nSpecify the reader plugin with source details.\nSpecify the writer plugin with target details.\nDefine the data columns, connection details, and transfer settings. Execute the Job: Run the DataX script using the configuration file. Data Migration: Moving data between different databases or data stores.\nData Integration: Combining data from multiple sources into a single database.\nData Synchronization: Keeping data synchronized between source and target systems.\nThe DataX JSON configuration file is divided into several sections. Each section contains various parameters that control the behavior of the data synchronization job. { \"job\": { \"content\": [ { \"reader\": { \"name\": \"mysqlreader\", \"parameter\": { \"username\": \"your_username\", \"password\": \"your_password\", \"column\": [\"id\", \"name\", \"age\"], \"splitPk\": \"id\", \"connection\": [ { \"table\": [\"your_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_database\"] } ] } }, \"writer\": { \"name\": \"mysqlwriter\", \"parameter\": { \"writeMode\": \"insert\", \"username\": \"your_username\", \"password\": \"your_password\", \"column\": [\"id\", \"name\", \"age\"], \"connection\": [ { \"table\": [\"your_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_target_database\"] } ] } } } ], \"setting\": { \"speed\": { \"channel\": 3 } } }\n} job Object: Description: This is the root object that contains the entire job configuration.\nExample:\n{ \"job\": { ... }\n} content Array: Description: This array holds the content of the job, which includes the reader and writer configuration. It can contain multiple reader-writer pairs.\nExample:\n{ \"content\": [ { ... } ]\n} reader Object: Description: Defines the data source from which DataX reads data.\nExample:\n{ \"reader\": { ... }\n} name Attribute (under reader): Description: Specifies the type of reader plugin. In this example, mysqlreader indicates that the data source is a MySQL database.\nExample:\n{ \"name\": \"mysqlreader\"\n} parameter Object (under reader): Description: Contains the parameters required by the reader plugin to connect to and read from the data source.\nExample:\n{ \"parameter\": { ... }\n} username and password: Description: Credentials for connecting to the source database.\nExample:\n{ \"username\": \"your_username\", \"password\": \"your_password\"\n} column Array: Description: Specifies the columns to read from the source table.\nExample:\n{ \"column\": [\"id\", \"name\", \"age\"]\n} splitPk: Description: The primary key column used to split data for parallel processing. This enhances performance by allowing concurrent reads.\nExample:\n{ \"splitPk\": \"id\"\n} connection Array: Description: Contains connection details for the source database. Multiple connections can be specified.\nExample:\n{ \"connection\": [ { ... } ]\n} table Array (under connection): Description: Lists the tables from which to read data.\nExample:\n{ \"table\": [\"your_table\"]\n} jdbcUrl Array (under connection): Description: Specifies the JDBC URL for connecting to the source database.\nExample:\n{ \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_database\"]\n} writer Object: Description: Defines the data destination to which DataX writes data.\nExample:\n{ \"writer\": { ... }\n} name Attribute (under writer): Description: Specifies the type of writer plugin. In this example, mysqlwriter indicates that the data destination is a MySQL database.\nExample:\n{ \"name\": \"mysqlwriter\"\n} parameter Object (under writer): Description: Contains the parameters required by the writer plugin to connect to and write to the data destination.\nExample:\n{ \"parameter\": { ... }\n} writeMode: Description: Specifies the mode for writing data. Common modes include insert and replace.\nExample:\n{ \"writeMode\": \"insert\"\n} username and password (under writer): Description: Credentials for connecting to the target database.\nExample:\n{ \"username\": \"your_username\", \"password\": \"your_password\"\n} column Array (under writer): Description: Specifies the columns to write to the target table. These should match the columns read from the source.\nExample:\n{ \"column\": [\"id\", \"name\", \"age\"]\n} connection Array (under writer): Description: Contains connection details for the target database.\nExample:\n{ \"connection\": [ { ... } ]\n} table Array (under writer connection): Description: Lists the tables to which data will be written.\nExample:\n{ \"table\": [\"your_table\"]\n} jdbcUrl Array (under writer connection): Description: Specifies the JDBC URL for connecting to the target database.\nExample:\n{ \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_target_database\"]\n} setting Object: Description: Contains job settings that control aspects like speed and error handling.\nExample:\n{ \"setting\": { ... }\n} speed Object (under setting): Description: Specifies the speed settings for the job, such as the number of parallel channels.\nExample:\n{ \"speed\": { ... }\n} channel: Description: Defines the number of parallel channels to use for the job. More channels can increase throughput but require more system resources.\nExample:\n{ \"channel\": 3\n} Configuration File:\n{ \"job\": { \"content\": [ { \"reader\": { \"name\": \"mysqlreader\", \"parameter\": { \"username\": \"source_username\", \"password\": \"source_password\", \"column\": [\"id\", \"name\", \"age\"], \"splitPk\": \"id\", \"connection\": [ { \"table\": [\"source_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/source_database\"] } ] } }, \"writer\": { \"name\": \"mysqlwriter\", \"parameter\": { \"writeMode\": \"insert\", \"username\": \"target_username\", \"password\": \"target_password\", \"column\": [\"id\", \"name\", \"age\"], \"connection\": [ { \"table\": [\"target_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/target_database\"] } ] } } } ], \"setting\": { \"speed\": { \"channel\": 3 } } }\n} Run the Job:\npython datax.py /path/to/your/mysql_to_mysql_job.json Configuration File:\n{ \"job\": { \"content\": [ { \"reader\": { \"name\": \"mysqlreader\", \"parameter\": { \"username\": \"your_username\", \"password\": \"your_password\", \"column\": [\"id\", \"name\", \"age\"], \"splitPk\": \"id\", \"connection\": [ { \"table\": [\"your_table\"], \"jdbcUrl\": [\"jdbc:mysql://localhost:3306/your_database\"] } ] } }, \"writer\": { \"name\": \"hdfswriter\", \"parameter\": { \"defaultFS\": \"hdfs://your_hdfs_cluster\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/your_table\", \"fileName\": \"your_file\", \"column\": [ { \"name\": \"id\", \"type\": \"long\" }, { \"name\": \"name\", \"type\": \"string\" }, { \"name\": \"age\", \"type\": \"int\" } ], \"writeMode\": \"overwrite\", \"fieldDelimiter\": \"\\t\" } } } ], \"setting\": { \"speed\": { \"channel\": 3 } } }\n} Run the Job:\npython datax.py /path/to/your/mysql_to_hdfs_job.json Check Logs: DataX provides detailed logs for each job, which can help diagnose issues.\nValidate Configuration: Ensure that your JSON configuration file is correctly formatted and all necessary parameters are specified.\nPlugin Compatibility: Verify that the reader and writer plugins you are using are compatible with your data sources and targets.\nDataX supports custom plugins to extend its functionality for specific data sources or use cases. Follow the guidelines in the official documentation to develop and integrate custom plugins.\nChannel Configuration: Adjust the number of channels to optimize data transfer speed. More channels can increase throughput but may require more system resources.\nBatch Size: Tune the batch size for reading and writing operations to balance between throughput and system load. Credentials Management: Use environment variables or secure vaults to manage sensitive information like database credentials.\nNetwork Security: Ensure secure network configurations, especially when transferring data over public networks.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Key Features","level":3,"id":"Key_Features_0"},{"heading":"Components of DataX","level":3,"id":"Components_of_DataX_0"},{"heading":"Installation and Setup","level":3,"id":"Installation_and_Setup_0"},{"heading":"Configuration Files","level":3,"id":"Configuration_Files_0"},{"heading":"Example JSON Configuration","level":5,"id":"Example_JSON_Configuration_0"},{"heading":"Running a DataX Job","level":3,"id":"Running_a_DataX_Job_0"},{"heading":"Detailed Steps for a DataX Job","level":4,"id":"Detailed_Steps_for_a_DataX_Job_0"},{"heading":"Common Use Cases","level":3,"id":"Common_Use_Cases_0"},{"heading":"Detailed Explanation of DataX JSON Configuration File","level":3,"id":"Detailed_Explanation_of_DataX_JSON_Configuration_File_0"},{"heading":"Example Configuration File","level":4,"id":"Example_Configuration_File_0"},{"heading":"Breakdown of the Configuration","level":4,"id":"Breakdown_of_the_Configuration_0"},{"heading":"Examples","level":3,"id":"Examples_0"},{"heading":"Example 1: MySQL to MySQL","level":4,"id":"Example_1_MySQL_to_MySQL_0"},{"heading":"Example 2: MySQL to HDFS","level":4,"id":"Example_2_MySQL_to_HDFS_0"},{"heading":"Troubleshooting Tips","level":3,"id":"Troubleshooting_Tips_0"},{"heading":"Advanced Topics","level":3,"id":"Advanced_Topics_0"},{"heading":"Custom Plugins","level":4,"id":"Custom_Plugins_0"},{"heading":"Performance Tuning","level":4,"id":"Performance_Tuning_0"},{"heading":"Security Considerations","level":4,"id":"Security_Considerations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/0.-data-lake/3.-datax.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641312,"modifiedTime":1737554783000,"sourceSize":14671,"sourcePath":"Big Data/0. Data Lake/3. DataX.md","exportPath":"big-data/0.-data-lake/3.-datax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/1.-hadoop.html":{"title":"1. Hadoop","icon":"","description":"Hadoop is an open-source framework designed for distributed storage and processing of large datasets across clusters of commodity hardware. It provides a scalable and fault-tolerant solution for processing and analyzing vast amounts of data.Hadoop Distributed File System (HDFS): HDFS is a distributed file system that stores data across multiple machines in a cluster. It provides high-throughput access to large datasets.MapReduce: MapReduce is a programming model and processing framework for distributed data processing in Hadoop. It enables parallel processing of data by dividing tasks into map and reduce phases.YARN (Yet Another Resource Negotiator): YARN is the resource management layer of Hadoop. It allows multiple applications to run on the same cluster while efficiently managing resources.Scalability: Hadoop can handle large-scale data processing by distributing the workload across multiple nodes in a cluster, making it highly scalable.Fault Tolerance: Hadoop provides fault tolerance by automatically replicating data across multiple nodes. If a node fails, the data can still be accessed from other replicas.Cost-Effectiveness: Hadoop utilizes commodity hardware, making it a cost-effective solution for storing and processing large datasets compared to traditional storage and processing systems.Flexibility: Hadoop is flexible and can handle structured, semi-structured, and unstructured data, allowing organizations to work with diverse data types.Parallel Processing: Hadoop's MapReduce model enables parallel processing of data, leading to faster data processing and analysis.Batch Processing: Hadoop is well-suited for batch processing tasks such as log analysis, data cleansing, and large-scale data transformations.Data Warehousing: Hadoop can be used for building data warehouses that store and process large volumes of structured and semi-structured data for business intelligence and analytics.Data Lake: Hadoop's ability to handle diverse data types makes it suitable for building data lakes that serve as a centralized repository for storing raw and processed data for analytics purposes.Machine Learning: Hadoop can be integrated with machine learning frameworks to perform distributed training and processing of machine learning models on large datasets.\nHadoop has a vast ecosystem of tools and technologies that extend its capabilities, including Apache Hive, Apache Pig, Apache Spark, Apache HBase, and Apache Kafka.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction to Hadoop","level":3,"id":"Introduction_to_Hadoop_0"},{"heading":"Key Components of Hadoop","level":3,"id":"Key_Components_of_Hadoop_0"},{"heading":"Advantages of Hadoop","level":3,"id":"Advantages_of_Hadoop_0"},{"heading":"Use Cases of Hadoop","level":3,"id":"Use_Cases_of_Hadoop_0"},{"heading":"Hadoop Ecosystem","level":3,"id":"Hadoop_Ecosystem_0"},{"heading":"Apache Hive","level":4,"id":"Apache_Hive_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/1.-hadoop.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641314,"modifiedTime":1737554783000,"sourceSize":4526,"sourcePath":"Big Data/1. Hadoop/1. Hadoop.md","exportPath":"big-data/1.-hadoop/1.-hadoop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/2.-hdfs.html":{"title":"2. HDFS","icon":"","description":"\nHadoop HDFS is a distributed file system designed to store and manage large amounts of data across a cluster of commodity hardware.\nIt is one of the core components of the Apache Hadoop framework, which is widely used for big data processing and analytics. HDFS is designed to scale horizontally by adding more machines to the cluster, accommodating the storage and processing needs of massive datasets.\nExample: If a company's data grows from 100 TB to 500 TB, new nodes can be added to the HDFS cluster to handle the increased storage without any major reconfiguration. HDFS provides fault tolerance by replicating data across multiple nodes. If a node fails, data can be seamlessly recovered from the replicas.\nExample: If a DataNode storing a block of data fails, HDFS automatically retrieves the data from other replicas on different nodes, ensuring data availability. HDFS is optimized for sequential read/write operations, making it well-suited for large-scale data processing tasks.\nExample: Data-intensive applications like log processing and batch data analytics can achieve high throughput by processing large files in a sequential manner.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Key Features","level":3,"id":"Key_Features_0"},{"heading":"Scalability","level":4,"id":"Scalability_0"},{"heading":"Fault Tolerance","level":4,"id":"Fault_Tolerance_0"},{"heading":"High Throughput","level":4,"id":"High_Throughput_0"},{"heading":"Data Locality","level":4,"id":"Data_Locality_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/2.-hdfs.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641316,"modifiedTime":1737554783000,"sourceSize":5758,"sourcePath":"Big Data/1. Hadoop/2. HDFS.md","exportPath":"big-data/1.-hadoop/2.-hdfs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/3.-mapreduce-&-yarn.html":{"title":"3. MapReduce & Yarn","icon":"","description":"MapReduce is a programming model and processing framework for distributed data processing in Hadoop. It allows for parallel processing of large datasets by dividing tasks into two phases: Map and Reduce. Job Tracker: Manages resource allocation and job scheduling.\nAssigns tasks to nodes in the cluster.\nMonitors the progress of the tasks. Task Tracker: Executes tasks assigned by the Job Tracker.\nProcesses data and returns the results to the Job Tracker. Map Task: Processes input data and produces intermediate &lt;key, value&gt; pairs.\nEach mapper works on a split of data and applies a user-defined function to transform the input into intermediate outputs. Reduce Task: Merges all intermediate values associated with the same key.\nReduces the set of intermediate data to a smaller set of aggregated results. Consider a simple example where we want to count the occurrences of each word in a large collection of documents. Map Phase: Each mapper processes a split of the input data (documents).\nFor each word in the document, the mapper emits a key-value pair where the key is the word and the value is 1. Input Document 1: \"cat dog cat\"\nMapper Output: (\"cat\", 1), (\"dog\", 1), (\"cat\", 1) Input Document 2: \"dog mouse\"\nMapper Output: (\"dog\", 1), (\"mouse\", 1) Shuffle and Sort Phase: The framework groups all intermediate key-value pairs by key.\nIntermediate data is sorted and shuffled to be sent to reducers. Shuffled Output: (\"cat\", [1, 1]), (\"dog\", [1, 1]), (\"mouse\", [1]) Reduce Phase: Each reducer processes a list of values associated with a key.\nThe reducer sums up the values for each key to produce the final result. Reducer Output: (\"cat\", 2), (\"dog\", 2), (\"mouse\", 1) You have a large dataset of web server logs, and you want to determine which pages are the most visited.Each line in the log file represents a request and looks something like this:192.168.1.1 - - [08/Jul/2023:13:55:36 -0400] \"GET /index.html HTTP/1.1\" 200 1043\n192.168.1.2 - - [08/Jul/2023:13:55:37 -0400] \"GET /about.html HTTP/1.1\" 200 523\n192.168.1.1 - - [08/Jul/2023:13:55:38 -0400] \"GET /contact.html HTTP/1.1\" 200 643\n192.168.1.1 - - [08/Jul/2023:13:55:39 -0400] \"GET /index.html HTTP/1.1\" 200 1043\nMapper Input:\nEach line from the log file.\nMapper Output:\nKey: URL (the page visited)\nValue: 1 (indicating a single visit)\nMapper Function:def map_function(log_line): # Extract the URL from the log line parts = log_line.split(\" \") url = parts[6] # Assuming the URL is the 7th part of the log line return (url, 1)\nExample Mapper Output:(\"GET /index.html HTTP/1.1\", 1)\n(\"GET /about.html HTTP/1.1\", 1)\n(\"GET /contact.html HTTP/1.1\", 1)\n(\"GET /index.html HTTP/1.1\", 1) The framework groups all intermediate key-value pairs by key (URL).\nIntermediate data is sorted and shuffled to be sent to reducers.\nShuffled and Sorted Output:(\"GET /index.html HTTP/1.1\", [1, 1])\n(\"GET /about.html HTTP/1.1\", [1])\n(\"GET /contact.html HTTP/1.1\", [1])\nReducer Input:\nKey: URL\nValue: List of visit counts\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3. MapReduce &amp; Yarn","level":1,"id":"3._MapReduce_&_Yarn_0"},{"heading":"MapReduce","level":3,"id":"MapReduce_0"},{"heading":"Key Concepts","level":4,"id":"Key_Concepts_0"},{"heading":"Word Count Simple Example","level":4,"id":"Word_Count_Simple_Example_0"},{"heading":"Real-Life Example: Analyzing Web Server Logs","level":4,"id":"Real-Life_Example_Analyzing_Web_Server_Logs_0"},{"heading":"Problem Statement:","level":5,"id":"Problem_Statement_0"},{"heading":"Web Server Log Format:","level":5,"id":"Web_Server_Log_Format_0"},{"heading":"MapReduce Steps","level":5,"id":"MapReduce_Steps_0"},{"heading":"1. Map Phase","level":6,"id":"1._Map_Phase_0"},{"heading":"2. Shuffle and Sort Phase","level":6,"id":"2._Shuffle_and_Sort_Phase_0"},{"heading":"3. Reduce Phase","level":6,"id":"3._Reduce_Phase_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/3.-mapreduce-&-yarn.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641317,"modifiedTime":1737554783000,"sourceSize":7574,"sourcePath":"Big Data/1. Hadoop/3. MapReduce & Yarn.md","exportPath":"big-data/1.-hadoop/3.-mapreduce-&-yarn.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/3.-metadata.html":{"title":"3. Metadata","icon":"","description":"\nStructure: The file system is organized in a hierarchical structure similar to traditional file systems, with directories and subdirectories.\n/user/ ├── alice/ │ ├── data.txt │ └── report.pdf └── bob/ ├── image.png └── notes.txt Mapping: The NameNode maintains a mapping of file paths to metadata, allowing users to access files using standard file path syntax. Namespace Operations: Operations such as creating, deleting, and renaming files and directories modify the namespace and are tracked by the NameNode. Block Size: Files in HDFS are split into blocks (default is 128 MB). Larger files are divided into multiple blocks, while smaller files may occupy a single block.\nBlock ID: Each block has a unique identifier that is used to track its location and status across the DataNodes.\nBlock Location: The NameNode keeps a record of which DataNodes contain each block's replicas. This is essential for fault tolerance and load balancing.\nBlock 1: - Block ID: blk_0001 - Size: 128 MB - Locations: DataNode1, DataNode2 Block 2: - Block ID: blk_0002 - Size: 64 MB - Locations: DataNode2, DataNode3 Default Setting: The default replication factor is usually set to 3, meaning each block is replicated on three different DataNodes.\nCustomization: Users can set a different replication factor on a per-file basis during file creation. This allows for flexibility based on data importance and required reliability.\nReplica Management: The NameNode monitors the health of replicas and can re-replicate blocks if a DataNode fails or if the number of replicas falls below the desired count.\nReplica Locations: - blk_0001: DataNode1, DataNode2, DataNode3 - blk_0002: DataNode2, DataNode3, DataNode4 Ownership: Each file and directory has an associated owner and group. This information is crucial for access control.\nPermissions: HDFS supports a POSIX-like permission model, allowing for read, write, and execute permissions for user, group, and others.\nTimestamps: HDFS records three timestamps: Creation Time: When the file was created.\nModification Time: Last time the file's content was modified.\nAccess Time: Last time the file was accessed (though this feature can be disabled for performance reasons). Ownership: - Owner: alice - Group: users Permissions: - rw-r--r-- # Owner: read/write, Group: read, Others: read Timestamps: - Creation Time: 2024-09-01 08:30:00 - Modification Time: 2024-09-22 10:00:00 - Access Time: 2024-09-21 15:45:00 # (Access time tracking can be disabled for performance) Registration: DataNodes register themselves with the NameNode during startup. This process involves sending their storage capacity, available space, and other health metrics.\nHeartbeat Mechanism: DataNodes send heartbeat signals to the NameNode at regular intervals (typically every 3 seconds). If the NameNode stops receiving heartbeats from a DataNode, it considers that DataNode dead and initiates replication of its blocks.\nBlock Reports: DataNodes periodically send block reports to the NameNode, which contain information about all blocks stored on that DataNode. This ensures the NameNode has up-to-date block location information. Edit Logs: The NameNode writes all changes to the filesystem namespace to an edit log. This log is crucial for recovering from failures. Checkpointing: Periodically, the NameNode performs a checkpoint operation, which involves merging the edit log with the current namespace image to create a new snapshot of the filesystem. This helps in managing the size of the edit log. Image File: The namespace image is a binary file stored on the NameNode, containing the current state of the filesystem (directory structure and file metadata).\nRecovery Process: In the event of a NameNode failure, the namespace image, combined with the edit log, allows the NameNode to recover the filesystem to its last consistent state.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"File System Namespace","level":3,"id":"File_System_Namespace_0"},{"heading":"Block Metadata","level":3,"id":"Block_Metadata_0"},{"heading":"Replication Factor","level":3,"id":"Replication_Factor_0"},{"heading":"File Attributes","level":3,"id":"File_Attributes_0"},{"heading":"DataNode Information","level":3,"id":"DataNode_Information_0"},{"heading":"Transaction Logs","level":3,"id":"Transaction_Logs_0"},{"heading":"Namespace Image","level":3,"id":"Namespace_Image_0"},{"heading":"Heartbeats and Block Reports","level":3,"id":"Heartbeats_and_Block_Reports_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/3.-metadata.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641318,"modifiedTime":1737554783000,"sourceSize":5705,"sourcePath":"Big Data/1. Hadoop/3. Metadata.md","exportPath":"big-data/1.-hadoop/3.-metadata.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/4.-hdfs-shell.html":{"title":"4. HDFS Shell","icon":"","description":"The general syntax for Hadoop shell commands is:hadoop fs [options] [command] [path]\nWhere:\nhadoop is the command to run Hadoop.\nfs specifies that the command is related to the file system.\n[options] are global options applied to Hadoop commands.\n[command] is the specific action you want to perform.\n[path] is the HDFS path where the command will be applied. Description: Lists files and directories in the specified directory.\nSyntax: hadoop fs -ls [path] Description: Creates a directory in the specified location.\nSyntax: hadoop fs -mkdir [path]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"General Syntax","level":3,"id":"General_Syntax_0"},{"heading":"Common Hadoop File System Commands","level":3,"id":"Common_Hadoop_File_System_Commands_0"},{"heading":"1. ls","level":4,"id":"1._`ls`_0"},{"heading":"2. mkdir","level":4,"id":"2._`mkdir`_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/4.-hdfs-shell.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641319,"modifiedTime":1737554783000,"sourceSize":5749,"sourcePath":"Big Data/1. Hadoop/4. HDFS Shell.md","exportPath":"big-data/1.-hadoop/4.-hdfs-shell.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/5.-java-api.html":{"title":"5. Java API","icon":"","description":"Before you can use the Hadoop Java API, you need to include Hadoop's library in your project. If you are using Maven, you can add the Hadoop client dependency to your pom.xml.&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;YourHadoopVersion&lt;/version&gt;\n&lt;/dependency&gt; Creating a Configuration Object: The first step is to create a Configuration instance which holds the Hadoop cluster configuration.\nConfiguration configuration = new Configuration();\nconfiguration.set(\"fs.defaultFS\", \"hdfs://localhost:9000\"); Getting an Instance of FileSystem: FileSystem fs = FileSystem.get(configuration); Uploading a File:\nPath srcPath = new Path(\"file:///localpath/to/source/file.txt\");\nPath dstPath = new Path(\"/hdfs/destination/path/\");\nfs.copyFromLocalFile(srcPath, dstPath);\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Setup for Using Hadoop Java API","level":4,"id":"Setup_for_Using_Hadoop_Java_API_0"},{"heading":"Basic Operations with Hadoop Java API","level":4,"id":"Basic_Operations_with_Hadoop_Java_API_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/5.-java-api.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641321,"modifiedTime":1737554783000,"sourceSize":1552,"sourcePath":"Big Data/1. Hadoop/5. Java API.md","exportPath":"big-data/1.-hadoop/5.-java-api.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/6.-mapreduce-&-yarn.html":{"title":"6. MapReduce & Yarn","icon":"","description":"MapReduce is a programming model and processing framework for distributed data processing in Hadoop. It allows for parallel processing of large datasets by dividing tasks into two phases: Map and Reduce. Job Tracker: Manages resource allocation and job scheduling.\nAssigns tasks to nodes in the cluster.\nMonitors the progress of the tasks. Task Tracker: Executes tasks assigned by the Job Tracker.\nProcesses data and returns the results to the Job Tracker. Map Task: Processes input data and produces intermediate &lt;key, value&gt; pairs.\nEach mapper works on a split of data and applies a user-defined function to transform the input into intermediate outputs. Reduce Task: Merges all intermediate values associated with the same key.\nReduces the set of intermediate data to a smaller set of aggregated results. Consider a simple example where we want to count the occurrences of each word in a large collection of documents. Map Phase: Each mapper processes a split of the input data (documents).\nFor each word in the document, the mapper emits a key-value pair where the key is the word and the value is 1. Input Document 1: \"cat dog cat\"\nMapper Output: (\"cat\", 1), (\"dog\", 1), (\"cat\", 1) Input Document 2: \"dog mouse\"\nMapper Output: (\"dog\", 1), (\"mouse\", 1) Shuffle and Sort Phase: The framework groups all intermediate key-value pairs by key.\nIntermediate data is sorted and shuffled to be sent to reducers. Shuffled Output: (\"cat\", [1, 1]), (\"dog\", [1, 1]), (\"mouse\", [1]) Reduce Phase: Each reducer processes a list of values associated with a key.\nThe reducer sums up the values for each key to produce the final result. Reducer Output: (\"cat\", 2), (\"dog\", 2), (\"mouse\", 1) You have a large dataset of web server logs, and you want to determine which pages are the most visited.Each line in the log file represents a request and looks something like this:192.168.1.1 - - [08/Jul/2023:13:55:36 -0400] \"GET /index.html HTTP/1.1\" 200 1043\n192.168.1.2 - - [08/Jul/2023:13:55:37 -0400] \"GET /about.html HTTP/1.1\" 200 523\n192.168.1.1 - - [08/Jul/2023:13:55:38 -0400] \"GET /contact.html HTTP/1.1\" 200 643\n192.168.1.1 - - [08/Jul/2023:13:55:39 -0400] \"GET /index.html HTTP/1.1\" 200 1043\nMapper Input:\nEach line from the log file.\nMapper Output:\nKey: URL (the page visited)\nValue: 1 (indicating a single visit)\nMapper Function:def map_function(log_line): # Extract the URL from the log line parts = log_line.split(\" \") url = parts[6] # Assuming the URL is the 7th part of the log line return (url, 1)\nExample Mapper Output:(\"GET /index.html HTTP/1.1\", 1)\n(\"GET /about.html HTTP/1.1\", 1)\n(\"GET /contact.html HTTP/1.1\", 1)\n(\"GET /index.html HTTP/1.1\", 1) The framework groups all intermediate key-value pairs by key (URL).\nIntermediate data is sorted and shuffled to be sent to reducers.\nShuffled and Sorted Output:(\"GET /index.html HTTP/1.1\", [1, 1])\n(\"GET /about.html HTTP/1.1\", [1])\n(\"GET /contact.html HTTP/1.1\", [1])\nReducer Input:\nKey: URL\nValue: List of visit counts\nReducer Output:\nKey: URL\nValue: Total number of visits\nReducer Function:def reduce_function(url, visit_counts): total_visits = sum(visit_counts) return (url, total_visits)\nExample Reducer Output:(\"GET /index.html HTTP/1.1\", 2)\n(\"GET /about.html HTTP/1.1\", 1)\n(\"GET /contact.html HTTP/1.1\", 1)\n# Simulated log data\nlog_data = [ \"192.168.1.1 - - [08/Jul/2023:13:55:36 -0400] \\\"GET /index.html HTTP/1.1\\\" 200 1043\", \"192.168.1.2 - - [08/Jul/2023:13:55:37 -0400] \\\"GET /about.html HTTP/1.1\\\" 200 523\", \"192.168.1.1 - - [08/Jul/2023:13:55:38 -0400] \\\"GET /contact.html HTTP/1.1\\\" 200 643\", \"192.168.1.1 - - [08/Jul/2023:13:55:39 -0400] \\\"GET /index.html HTTP/1.1\\\" 200 1043\"\n] # Map phase\nmapped_data = []\nfor line in log_data: mapped_data.append(map_function(line)) # Shuffle and sort phase\nfrom collections import defaultdict\nshuffled_data = defaultdict(list)\nfor key, value in mapped_data: shuffled_data[key].append(value) # Reduce phase\nreduced_data = []\nfor key, value_list in shuffled_data.items(): reduced_data.append(reduce_function(key, value_list)) # Print the results\nfor url, total_visits in reduced_data: print(f\"{url}: {total_visits} visits\")\nIn this real-life example, the MapReduce framework processes a large set of web server logs to determine the most visited pages. The process involves:\nMapping: Extracting the URL from each log entry and emitting a key-value pair (&lt;URL, 1&gt;).\nShuffling and Sorting: Grouping and sorting the key-value pairs by URL.\nReducing: Summing the visit counts for each URL to get the total visits.\nYARN is the resource management layer of Hadoop that allows multiple applications to run on the same cluster while efficiently managing resources. ResourceManager: Manages the use of resources across the cluster.\nAllocates resources to various applications.\nKeeps track of available and used resources. NodeManager: Manages resource and task scheduling on a single node.\nMonitors the resource usage of containers on the node.\nReports the resource usage to the ResourceManager. ApplicationMaster: Manages the lifecycle of a single application.\nRequests resources from the ResourceManager.\nWorks with the NodeManager to execute and monitor tasks. Consider an example where we have a cluster with multiple applications running concurrently. ResourceManager: Receives requests for resources from different applications.\nAllocates resources based on availability and application priority. NodeManager: Monitors and manages resources on individual nodes.\nLaunches containers to execute tasks assigned by the ApplicationMaster.\nReports resource status and task progress to the ResourceManager. ApplicationMaster: Manages a specific application (e.g., a MapReduce job).\nNegotiates resources with the ResourceManager.\nCoordinates the execution of tasks on various NodeManagers. Two applications, App1 (MapReduce job) and App2 (Spark job), are submitted to the YARN cluster.\nResourceManager allocates resources to both applications based on their requirements and cluster availability.\nNodeManagers on different nodes launch containers to execute tasks for App1 and App2.\nApplicationMaster for App1 requests additional containers from the ResourceManager as needed to complete the MapReduce job.\nApplicationMaster for App2 coordinates the execution of Spark tasks on the allocated containers.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"6. MapReduce &amp; Yarn","level":1,"id":"6._MapReduce_&_Yarn_0"},{"heading":"MapReduce","level":3,"id":"MapReduce_0"},{"heading":"Key Concepts","level":4,"id":"Key_Concepts_0"},{"heading":"Word Count Simple Example","level":4,"id":"Word_Count_Simple_Example_0"},{"heading":"Real-Life Example: Analyzing Web Server Logs","level":4,"id":"Real-Life_Example_Analyzing_Web_Server_Logs_0"},{"heading":"Problem Statement:","level":5,"id":"Problem_Statement_0"},{"heading":"Web Server Log Format:","level":5,"id":"Web_Server_Log_Format_0"},{"heading":"MapReduce Steps","level":5,"id":"MapReduce_Steps_0"},{"heading":"1. Map Phase","level":6,"id":"1._Map_Phase_0"},{"heading":"2. Shuffle and Sort Phase","level":6,"id":"2._Shuffle_and_Sort_Phase_0"},{"heading":"3. Reduce Phase","level":6,"id":"3._Reduce_Phase_0"},{"heading":"Full MapReduce Job Example in Python (Pseudo-Code)","level":5,"id":"Full_MapReduce_Job_Example_in_Python_(Pseudo-Code)_0"},{"heading":"YARN (Yet Another Resource Negotiator)","level":3,"id":"YARN_(Yet_Another_Resource_Negotiator)_0"},{"heading":"Key Components","level":4,"id":"Key_Components_0"},{"heading":"Example","level":4,"id":"Example_0"},{"heading":"Scenario","level":4,"id":"Scenario_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/6.-mapreduce-&-yarn.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641327,"modifiedTime":1737554783000,"sourceSize":7574,"sourcePath":"Big Data/1. Hadoop/6. MapReduce & Yarn.md","exportPath":"big-data/1.-hadoop/6.-mapreduce-&-yarn.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html":{"title":"7. Single Node Deployment Guide - Tecent Cloud","icon":"","description":"sudo adduser hadoop\nsudo passwd hadoop sudo usermod -aG sudo hadoop\nsudo usermod -aG wheel hadoop su - hadoop\nsudo yum install java-1.8.0-openjdk-devel\n(download and hadoop-3.3.6.tar to root) sudo tar -xvf hadoop-3.3.6.tar.gz\nsudo mv hadoop-3.3.6 /usr/local/hadoop\nnano ~/.bashrc\nexport HADOOP_HOME=/usr/local/hadoop\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOMEea\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nsource ~/.bashrc\nsudo nano $HADOOP_HOME/etc/hadoop/core-site.xml\n&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;\n&lt;/configuration&gt;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641352,"modifiedTime":1737554783000,"sourceSize":3997,"sourcePath":"Big Data/1. Hadoop/7. Single Node Deployment Guide - Tecent Cloud.md","exportPath":"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/0.-comprehensive-guide.html":{"title":"0. Comprehensive Guide","icon":"","description":"Scala (/ˈskɑːlə, ˈskeɪlə/) is a multi-paradigm programming language designed to integrate features of object-oriented and functional programming. It was designed by Martin Odersky at the École Polytechnique Fédérale de Lausanne (EPFL) in 2001, based on his previous work on Funnel. Scala for the Java platform was released in late 2003/early 2004.\nRuns on the JVM and can coexist with existing programs.\nDirectly uses Java libraries.\nStatically typed, similar to Java.\nSyntax is similar to Java but more concise and expressive.\nSupports both object-oriented and functional programming.\nMore object-oriented than Java. Type inference, immutability, functional programming, advanced program constructs.\nConcurrency: actor model.\nInteroperability with existing Java code, comparisons, and trade-offs with Java. javac java\n.java --------&gt; .class ----------&gt; run on JVM\n.scala -------&gt; .class ----------&gt; run on JVM scalac scala <a data-tooltip-position=\"top\" aria-label=\"https://docs.scala-lang.org/zh-cn/cheatsheets/index.html\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://docs.scala-lang.org/zh-cn/cheatsheets/index.html\" target=\"_self\">Scala Syntax Quick Reference</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://docs.scala-lang.org/zh-cn/tour/tour-of-scala.html\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://docs.scala-lang.org/zh-cn/tour/tour-of-scala.html\" target=\"_self\">Scala Official Documentation - Tour of Scala</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Scala","level":3,"id":"Scala_0"},{"heading":"Features","level":3,"id":"Features_0"},{"heading":"Focus Areas","level":3,"id":"Focus_Areas_0"},{"heading":"Relationship with Java","level":3,"id":"Relationship_with_Java_0"},{"heading":"Resources","level":3,"id":"Resources_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/0.-comprehensive-guide.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641446,"modifiedTime":1737554783000,"sourceSize":5062,"sourcePath":"Big Data/2. Scala/0. Comprehensive Guide.md","exportPath":"big-data/2.-scala/0.-comprehensive-guide.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/1.-variables-and-data-types.html":{"title":"1. Variables and Data Types","icon":"","description":"\nSimilar to Java\n// for single-line comments\n/* */ for multi-line comments\n/** */ for documentation comments before methods or classes, useful for generating documentation with scaladoc.\nvar name [:VariableType] = value // variable\nval name [:ConstantType] = value // constant\nIn Scala, due to its functional programming elements, it is generally advised to use constants over variables whenever possible.\nThe type can be omitted during declaration, as the compiler can infer it.\nScala is statically typed, and once the type is determined, it cannot be changed.\nVariables and constants must have initial values upon declaration.\nVariables are mutable, while constants are immutable.\nFor reference type constants, you can change the fields of the object but not the reference itself.\nNo need to end statements with ; as Scala automatically recognizes the end of a statement. Can start with a letter or underscore, followed by letters, digits, or underscores, similar to C/C++/Java.\nCan start with an operator and only contain operators (e.g., +-*/#!), making operator overloading very flexible.\nCan use any string enclosed in backticks, even if it matches one of the 39 Scala keywords.\nvar _abc: String = \"hello\"\nval -+/%# = 10\nval `if` = 10\nprintln(_abc)\nprintln(-+/%#)\nprintln(`if`)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Comments","level":3,"id":"Comments_0"},{"heading":"Variables and Constants","level":3,"id":"Variables_and_Constants_0"},{"heading":"Identifier Naming Conventions","level":3,"id":"Identifier_Naming_Conventions_0"},{"heading":"Keywords","level":3,"id":"Keywords_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/1.-variables-and-data-types.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641447,"modifiedTime":1737554783000,"sourceSize":8099,"sourcePath":"Big Data/2. Scala/1. Variables and Data Types.md","exportPath":"big-data/2.-scala/1.-variables-and-data-types.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/2.-operators.html":{"title":"2. Operators","icon":"","description":"\nMostly similar to Java.\nArithmetic operators: + - * / %. The + operator can be used for unary positive, binary addition, and string concatenation. The modulus operator % can also be used with floating-point numbers. There are no increment and decrement operators (++ --).\nRelational operators: == != &lt; &gt; &lt;= &gt;=\nLogical operators: &amp;&amp; || !, with short-circuit evaluation supported.\nAssignment operators: = += -= *= /= %=\nBitwise operators: &amp; | ^ ~\nShift operators: &lt;&lt; &gt;&gt; &gt;&gt;&gt;, where &lt;&lt; &gt;&gt; are signed left and right shifts, and &gt;&gt;&gt; is an unsigned right shift.\nIn Scala, all operators are essentially methods of objects, allowing for more flexible operator overloading than in C++. In Scala, operators are methods, and any method with a single parameter can be used as an infix operator. For example, 10.+(1) is equivalent to 10 + 1.\nYou can define custom operators by using valid operator characters (only special symbols) as function names. When an expression uses multiple operators, the precedence is determined by the first character of the operator. Both built-in and custom operators follow the same precedence rules.\nOperator precedence (from highest to lowest):* / %\n+ -\n:\n= !\n&lt; &gt;\n&amp;\n^\n|\n(all letters, $, _) For example, the following two expressions are equivalent:\na + b ^? c ?^ d less a ==&gt; b | c\n((a + b) ^? (c ?^ d)) less ((a ==&gt; b) | c)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Custom Operators","level":3,"id":"Custom_Operators_0"},{"heading":"Operator Precedence","level":3,"id":"Operator_Precedence_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/2.-operators.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641457,"modifiedTime":1737554783000,"sourceSize":5866,"sourcePath":"Big Data/2. Scala/2. Operators.md","exportPath":"big-data/2.-scala/2.-operators.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/3.-control-flow.html":{"title":"3. Control Flow","icon":"","description":"if (condition) { // code block\n} else if (condition) { // code block\n} else { // code block\n} In Scala, the if-else statement can have a return value, making it an expression. The return value is the result of the last executed statement.\nYou can force the return type to be Unit to ignore the last expression's value, resulting in ().\nIf there are multiple return types, the target variable's type must be specified as a common superclass, or it can be automatically inferred.\nScala does not have a ternary conditional operator; use if (a) b else c instead of a ? b : c.\nNested conditions work similarly. Range iteration: for(i &lt;- 1 to 10) {}, where 1 to 10 is a method call on Int, returning a Range.\nThe range 1 to 10 includes the right boundary, while 1 until 10 excludes it. You can also use the Range class directly.\nRange with step: 1 to 10 by 2.\nThe range is a collection, so you can also iterate over regular collections: for(i &lt;- collection) {}.\nLoop guard: A condition to filter elements, similar to continue. Syntax: for (i &lt;- collection if condition) { // code block\n} Equivalent to: for (i &lt;- collection) { if (condition) { // code block }\n} Nested loops can be combined into one for comprehension: Standard syntax: for (i &lt;- 1 to 4) { for (j &lt;- 1 to 5) { println(s\"i = $i, j = $j\") }\n} Equivalent syntax: for (i &lt;- 1 to 4; j &lt;- 1 to 5) { println(s\"i = $i, j = $j\")\n} Example: Multiplication table for (i &lt;- 1 to 9; j &lt;- 1 to i) { print(s\"$j * $i = ${i * j} \\t\") if (j == i) println()\n} Introducing variables in the loop:\nfor (i &lt;- 1 to 10; j = 10 - i) { println(s\"i = $i, j = $j\")\n} Using {} syntax for loops: The above variable introduction example is equivalent to: for { i &lt;- 1 to 10 j = 10 - i\n} { println(s\"i = $i, j = $j\")\n} Loops can have return values, typically Unit, represented by ().\nLoops can also use yield to return a collection, pausing the loop, executing, and then continuing. Similar to Ruby/Python.\nval v = for (i &lt;- 1 to 10) yield i * i // default implementation is Vector, Vector(1, 4, 9, 16, 25, 36, 49, 64, 81, 100) For Java compatibility; not recommended as they return Unit.\nAvoid using as they require variable declaration outside the loop, affecting external variables.\nwhile (condition) { // code block\n} do { // code block\n} while (condition) Scala does not include break and continue keywords. Instead, use functional programming techniques. Use breakable structure to implement break and continue. Loop guards can replace continue to some extent. You can exit a loop by throwing and catching an exception to replace break.\ntry { for (i &lt;- 0 to 10) { if (i == 3) throw new RuntimeException println(i) }\n} catch { case _: Exception =&gt; // do nothing\n} Use Scala's Breaks class for the break method (which internally uses exception handling).\nimport scala.util.control.Breaks Breaks.breakable { for (i &lt;- 0 to 10) { if (i == 3) Breaks.break() println(i) }\n} ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<code>if-else</code> Statement","level":3,"id":"`if-else`_Statement_0"},{"heading":"<code>for</code> Loop (For Comprehensions)","level":3,"id":"`for`_Loop_(For_Comprehensions)_0"},{"heading":"<code>while</code> and <code>do while</code> Loops","level":3,"id":"`while`_and_`do_while`_Loops_0"},{"heading":"Loop Control","level":3,"id":"Loop_Control_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/3.-control-flow.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641458,"modifiedTime":1737554783000,"sourceSize":3739,"sourcePath":"Big Data/2. Scala/3. Control Flow.md","exportPath":"big-data/2.-scala/3.-control-flow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/4.-functional-programming.html":{"title":"4. Functional Programming","icon":"","description":"\nProcedural Programming: Solves problems step-by-step.\nObject-Oriented Programming (OOP): Decomposes objects, behaviors, and attributes, solving problems through object relationships and method calls. It offers low coupling, high reusability, and strong maintainability.\nFunctional Programming: Unlike procedural and OOP, which are imperative, functional programming focuses on data mapping. In pure functional programming, there are no variables; all quantities are constants. The computation is a series of expression evaluations, with each program segment having a return value. It abstracts away implementation details, making it easier to understand but more complex for compilers.\nAdvantages of Functional Programming: High programming efficiency due to immutability, consistent function outputs for specific inputs regardless of context. Functional programming has no side effects, which benefits parallel processing. Scala excels in big data processing, as seen in frameworks like Spark and Kafka.\ndef func(arg1: TypeOfArg1, arg2: TypeOfArg2): RetType = { // function body\n} In functional programming, functions are first-class citizens (can be assigned, passed as parameters, and returned), and can be defined in any code block.\nFunctions defined at the class or object level are called methods, while those defined inside methods are called functions. Broadly, they are all functions.\nUse return to return a value; if omitted, the last expression's value is used.\nFor Unit return type, use return, return (), or omit it entirely.\nAny return value must be a subclass of the specified return type. In Java, functions are referred to as class or instance methods, not generalized functions.\nFunctional programming's term \"function\" derives from the mathematical concept of a function, which emphasizes mapping between data.\nIn programming, a function, including those in Scala, refers to a subroutine that performs a specific task. Variable Arguments: Similar to Java, wrapped in an array.\ndef f4(str: String*): Unit = {} If there are other parameters, variable arguments must be last.\nUse as an array within the function. Default Parameter Values:\ndef f5(name: String = \"Alice\"): Unit = {} Like C++, default parameters must be at the end. Named Parameters: Specify parameter names during function calls.\ndef f6(name: String, age: Int = 20, loc: String = \"Beijing\"): Unit = { println(s\"name: $name, age: $age, location: $loc\")\n}\nf6(\"Bob\")\nf6(\"Alice\", loc = \"Xi'an\")\nf6(\"Michael\", 30) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Paradigm Comparison","level":3,"id":"Paradigm_Comparison_0"},{"heading":"Function Definition","level":3,"id":"Function_Definition_0"},{"heading":"Terminology","level":3,"id":"Terminology_0"},{"heading":"Function Parameters","level":3,"id":"Function_Parameters_0"},{"heading":"Simplifying Functions","level":3,"id":"Simplifying_Functions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/4.-functional-programming.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641459,"modifiedTime":1737554783000,"sourceSize":8659,"sourcePath":"Big Data/2. Scala/4. Functional Programming.md","exportPath":"big-data/2.-scala/4.-functional-programming.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/5.-package-management.html":{"title":"5. Package Management","icon":"","description":"\nScala's object-oriented (OO) concepts are derived from Java, with many similarities but additional features. Declaration: package name\nPurpose: Distinguish classes with the same name to avoid naming conflicts.\nManage large numbers of classes by organizing them into modules.\nControl access permissions. Naming: Package names must be valid identifiers (letters, digits, underscores, and cannot start with a digit). The dot (.) serves as a separator for different levels of the package hierarchy.\nNaming Convention: Typically follows the pattern com.company.projectname.modulename, though it can vary based on project guidelines. Java-Style Package Management: Declare a package at the top of the source file. The source file's location does not need to match the package hierarchy strictly, representing logical levels rather than physical file structure. However, it's common practice to align the file structure with the package hierarchy, similar to Java. Nested Package Definition:\npackage com { object Outer { var name = \"Outer\" } package inner { package scala { object Inner { def main(args: Array[String]): Unit = { println(Outer.name) Outer.name = \"Inner\" println(Outer.name) } } } }\n} Benefits of Nested Packages: A single source file can declare multiple top-level packages.\nClasses in a subpackage can access content in the parent package without import. However, the outer package cannot directly access the inner package without an import. Compiling Nested Packages: If testing nested packages in a single file using VSCode instead of an IDE, the package must be compiled with scalac and specify the entry class for execution. The compiled bytecode files will be organized in the package hierarchy. scalac PackageManagement.scala\nscala com.inner.scala.Inner Define a singleton package object for a Scala package. Members defined in the package object are shared among all classes and objects within the package and can be accessed directly without import.\nSyntax: package object The package object must be defined at the same hierarchy level as the package it corresponds to. For example, defining a package object for com.inner must be done in the com package as package object inner { ... }. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Package Management","level":2,"id":"Package_Management_0"},{"heading":"Object-Oriented Concepts in Scala","level":3,"id":"Object-Oriented_Concepts_in_Scala_0"},{"heading":"Packages","level":3,"id":"Packages_0"},{"heading":"Package Management in Scala","level":3,"id":"Package_Management_in_Scala_0"},{"heading":"Package Objects","level":3,"id":"Package_Objects_0"},{"heading":"Importing Packages","level":3,"id":"Importing_Packages_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/5.-package-management.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641465,"modifiedTime":1737554783000,"sourceSize":4583,"sourcePath":"Big Data/2. Scala/5. Package Management.md","exportPath":"big-data/2.-scala/5.-package-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/6.-object-oriented-programming.html":{"title":"6. Object-Oriented Programming","icon":"","description":"\nIn Java, if a class is declared public, it must match the file name, and only one public class per file is allowed. Default access allows multiple classes per file.\nIn Scala, there is no public keyword; the default access level is public, and multiple classes can be defined in a single file, not necessarily matching the file name.\n[descriptor] class classname { // body: fields &amp; methods [descriptor] var/val name: Type = _ [descriptor] def method(args: ArgsType): RetType = { // method body }\n} Access modifiers can be private, protected, private[packageName]. By default, members are public.\nUse @scala.beans.BeanProperty for Java Bean-style getters and setters.\nFields initialized with _ get default values: 0 for value types and null for reference types. Constants (val) cannot use _ as they can only be initialized once. Java uses private fields with getters and setters for encapsulation.\nScala simplifies this by treating public fields as private with automatic getter and setter methods: obj.field() and obj.field_=(value).\nFor Java Bean compatibility, use @scala.beans.BeanProperty. Java: private, protected, public, and default package access.\nScala: Default is public.\nprivate: accessible within the class and companion object.\nprotected: stricter than Java, accessible within the same class and subclasses, but not within the same package.\nprivate[packageName]: accessible within the specified package. Main and auxiliary constructors.\nclass ClassName [descriptor] [([descriptor][val/var] arg1: Arg1Type, [descriptor][val/var] arg2: ...)] { // main constructor def this(argsList1) { this(args) // call main constructor } def this(argsList2) { this(argsList1) // can call main or another constructor }\n}\nExample:object Constructor { def main(args: Array[String]): Unit = { val p: Person = new Person() p.Person() // call method val p1 = new Person(\"Alice\") val p2 = new Person(\"Bob\", 25) p1.Person() }\n} class Person { var name: String = _ var age: Int = _ println(\"call main constructor\") def this(name: String) { this() println(\"call auxiliary constructor 1\") this.name = name println(s\"Person: $name $age\") } def this(name: String, age: Int) { this(name) this.age = age println(\"call auxiliary constructor 2\") println(s\"Person: $name $age\") } // regular method, not a constructor def Person(): Unit = { println(\"call Person.Person() method\") }\n} Main Constructor: Written in the class definition.\nThe method body is part of the main constructor and executes in order. Auxiliary Constructors: Defined using this.\nMust call the main constructor directly or indirectly.\nOverloaded, so parameter lists must differ.\nCan define methods with the same name as the class. Parameter Modifiers: Unmodified: just a parameter, accessible within the class but not a member.\nvar/val: makes it a member variable or constant.\nMain constructor parameters can have access modifiers. Best Practices: Use var/val for main constructor parameters.\nDefine auxiliary constructors if multiple overloads are needed. class ChildClassName[(argList1)] extends BaseClassName[(args)] { body } Inherit properties and methods from the base class.\nCan call base class constructors but limited to one constructor type. In Java, properties are statically bound, while methods are dynamically bound.\nIn Scala, both properties and methods are dynamically bound.\nAll instance methods are virtual methods.\nUse override to override methods and properties.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Class Definition","level":3,"id":"Class_Definition_0"},{"heading":"Encapsulation","level":3,"id":"Encapsulation_0"},{"heading":"Access Modifiers","level":3,"id":"Access_Modifiers_0"},{"heading":"Constructors","level":3,"id":"Constructors_0"},{"heading":"Inheritance","level":3,"id":"Inheritance_0"},{"heading":"Polymorphism","level":3,"id":"Polymorphism_0"},{"heading":"Abstract Classes","level":3,"id":"Abstract_Classes_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/6.-object-oriented-programming.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641466,"modifiedTime":1737554783000,"sourceSize":7669,"sourcePath":"Big Data/2. Scala/6. Object-Oriented Programming.md","exportPath":"big-data/2.-scala/6.-object-oriented-programming.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/7.-collections.html":{"title":"7. Collections","icon":"","description":"\nThree main types: List, Set, and Map, with multiple implementations for each. Three main types: Seq (sequence), Set, and Map, all extending from Iterable.\nFor almost all collection classes, both mutable and immutable versions are provided. Immutable collections: scala.collection.immutable\nMutable collections: scala.collection.mutable\nThese two packages might contain types with the same name, so ensure you're using the correct version. Immutable collections cannot be modified directly. Any modification operation returns a new collection. scala.collection.immutable hierarchy:“Scala_immutable_collections_tree.jpg” could not be found. Common immutable collections include List, Set, Map, and Range. Mutable collections can be modified directly. scala.collection.mutable hierarchy:“Scala_mutable_collections_tree.jpg” could not be found. Common mutable collections include ArrayBuffer, ListBuffer, HashSet, and HashMap. Immutable collections cannot change size, but elements can be modified.\nMutable collections can change size and elements.\nWhen modifying collections, prefer using operators for immutable collections and methods for mutable collections.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Java Collections:","level":3,"id":"Java_Collections_0"},{"heading":"Scala Collections:","level":3,"id":"Scala_Collections_0"},{"heading":"Immutable Collections:","level":3,"id":"Immutable_Collections_0"},{"heading":"Mutable Collections:","level":3,"id":"Mutable_Collections_0"},{"heading":"Immutable vs. Mutable:","level":3,"id":"Immutable_vs._Mutable_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/7.-collections.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641467,"modifiedTime":1737554783000,"sourceSize":5552,"sourcePath":"Big Data/2. Scala/7. Collections.md","exportPath":"big-data/2.-scala/7.-collections.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/8.-pattern-matching.html":{"title":"8. Pattern Matching","icon":"","description":"\nReplaces the traditional switch-case structure found in languages like C/C++/Java, but with enhanced functionality.\nSyntax:\nvalue match { case caseVal1 =&gt; returnVal1 case caseVal2 =&gt; returnVal2 ... case _ =&gt; defaultVal\n} Each case is checked in order until one matches.\nYou can add guards to cases with conditions:\ndef abs(num: Int): Int = { num match { case i if i &gt;= 0 =&gt; i case i if i &lt; 0 =&gt; -i }\n} Supports matching literals of all types, including strings, characters, numbers, booleans, arrays, and lists.\nCan handle variables of type Any, matching against different types and values.\nAlways provide a default case (case _) to handle unexpected values.\nYou can also match on types using type patterns:\ndef describeType(x: Any) = x match { case i: Int =&gt; s\"Int $i\" case s: String =&gt; s\"String $s\" case list: List[_] =&gt; s\"List $list\" case array: Array[Int] =&gt; s\"Array[Int] ${array.mkString(\", \")}\" case _ =&gt; \"Something else\"\n} Different patterns for matching arrays, including elements count and specific values:\nval arr = Array(1, 2, 3)\narr match { case Array(0) =&gt; \"Array with one element 0\" case Array(x, y) =&gt; s\"Array with two elements: $x, $y\" case Array(1, _*) =&gt; \"Array starting with 1\" case _ =&gt; \"Other\"\n}\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Pattern Matching with <code>match-case</code>","level":3,"id":"Pattern_Matching_with_`match-case`_0"},{"heading":"Array Matching","level":3,"id":"Array_Matching_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/8.-pattern-matching.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641468,"modifiedTime":1737554783000,"sourceSize":4509,"sourcePath":"Big Data/2. Scala/8. Pattern Matching.md","exportPath":"big-data/2.-scala/8.-pattern-matching.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/9.-exception-handling.html":{"title":"9. Exception Handling","icon":"","description":"Scala's exception handling syntax and underlying mechanics are very similar to Java's.\nSurround the code that may throw an exception with a try block, and catch different exceptions with multiple catch blocks. The finally block contains code that executes regardless of whether an exception was caught.\ntry { int a = 0; int b = 0; int c = a / b;\n} catch (ArithmeticException e) { e.printStackTrace();\n} catch (Exception e) { e.printStackTrace();\n} finally { System.out.println(\"finally\");\n} Use try to surround the code, with catch containing all exception handling logic, and finally for code that executes regardless of exceptions. Scala does not have compile-time exceptions; all exceptions are handled at runtime.\nUse the throw keyword to throw exceptions, where all exceptions are subclasses of Throwable. The throw expression type is Nothing.\nThe @throws[ExceptionList] annotation in Scala is equivalent to Java's throws keyword, used to declare possible exceptions.\nobject ExceptionTest { def main(args: Array[String]): Unit = { // test of exceptions try { val n = 10 / 0 } catch { case e: ArithmeticException =&gt; { println(s\"ArithmeticException raised.\") } case e: Exception =&gt; { println(\"Normal Exceptions raised.\") } } finally { println(\"finally\") } }\n}\nImplicit conversions in Scala can simplify code and make it more expressive. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Exception Handling in Scala","level":3,"id":"Exception_Handling_in_Scala_0"},{"heading":"Exception Handling in Java:","level":3,"id":"Exception_Handling_in_Java_0"},{"heading":"Exception Handling in Scala:","level":3,"id":"Exception_Handling_in_Scala_1"},{"heading":"Implicit Conversions","level":2,"id":"Implicit_Conversions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/9.-exception-handling.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641475,"modifiedTime":1737554783000,"sourceSize":5625,"sourcePath":"Big Data/2. Scala/9. Exception Handling.md","exportPath":"big-data/2.-scala/9.-exception-handling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/2.-scala/10.-sbt.html":{"title":"10. Sbt","icon":"","description":"sbt (Simple Build Tool) is the standard build tool for Scala projects, similar to Maven or Gradle for Java projects.\nZero configuration for simple projects.\nUses Scala source code to manage project build.\nPrecise incremental compilation, saving time.\nLibrary management using Coursier.\nSupports mixed Scala and Java projects. sbt depends on Java; ensure JDK 1.8 or higher is installed.\nDownload the installer or archive from the official site.\nSet the SBT_HOME environment variable and add %SBT_HOME%\\bin to the PATH environment variable. Create a project directory and a build file:\nmkdir foo-build\ncd foo-build\ntouch build.sbt Start sbt:\nsbt Exit sbt shell:\nsbt:foo-build&gt; exit Compile the project:\nsbt:foo-build&gt; compile Create source files and directories:\nmkdir -p src/main/scala/example // src/main/scala/example/Hello.scala\npackage example object Hello extends App { println(\"Hello\")\n} Run the project:\nsbt:foo-build&gt; run ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview of sbt","level":3,"id":"Overview_of_sbt_0"},{"heading":"Key Features of sbt:","level":3,"id":"Key_Features_of_sbt_0"},{"heading":"Installation:","level":3,"id":"Installation_0"},{"heading":"Getting Started with sbt","level":3,"id":"Getting_Started_with_sbt_0"},{"heading":"Creating a Simple Project","level":4,"id":"Creating_a_Simple_Project_0"},{"heading":"sbt Commands and Operations","level":3,"id":"sbt_Commands_and_Operations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/2.-scala/10.-sbt.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641448,"modifiedTime":1737554783000,"sourceSize":6879,"sourcePath":"Big Data/2. Scala/10. Sbt.md","exportPath":"big-data/2.-scala/10.-sbt.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/0.-concept-&-architecture/0.-spark.html":{"title":"0. Spark","icon":"","description":"\nApache Spark is an open-source distributed computing system designed for big data processing and analytics.\nIt provides an efficient and unified platform for handling large-scale data processing tasks, including batch processing, real-time streaming, machine learning, and graph processing. Speed: Spark offers in-memory data processing, which significantly improves performance compared to traditional disk-based processing systems.\nEase of Use: Spark provides high-level APIs in Java, Scala, Python, and R, making it accessible to a wide range of developers and data scientists.\nFault Tolerance: Spark automatically recovers from failures and provides fault tolerance through its resilient distributed dataset (RDD) abstraction.\nScalability: Spark scales horizontally, allowing it to handle large datasets and distributed computing across clusters of machines.\nVersatility: Spark supports a wide range of data processing tasks, including batch processing, stream processing, machine learning, and graph processing. Spark Core: Spark Core is the foundation of the Spark framework, providing basic functionalities like task scheduling, memory management, and fault recovery.\nSpark SQL: Spark SQL enables working with structured and semi-structured data using SQL-like queries, making it easier to analyze structured data within Spark.\nSpark Streaming: Spark Streaming allows processing and analyzing real-time data streams, making it suitable for applications requiring real-time analytics or data transformations.\nSpark MLlib: MLlib is Spark's machine learning library, offering a wide range of algorithms and utilities for building machine learning models at scale.\nGraphX: GraphX is Spark's graph processing library, providing APIs for working with graphs and performing graph analytics and computations. Speed: Spark's in-memory processing and optimized execution engine make it significantly faster than traditional disk-based processing systems.\nEase of Use: Spark provides a user-friendly API and supports multiple programming languages, simplifying development and data analysis tasks.\nUnified Platform: Spark's unified platform allows users to perform various data processing tasks, such as batch processing, streaming, machine learning, and graph analytics, within a single framework.\nScalability: Spark scales horizontally by distributing data and computation across multiple nodes, enabling it to handle large-scale data processing.\nIntegration: Spark integrates well with other big data tools and frameworks, including Hadoop, Hive, HBase, and various data sources, enabling seamless data integration and interoperability. Big Data Processing: Spark is widely used for processing and analyzing large volumes of data in various domains, including financial services, e-commerce, healthcare, and telecommunications.\nReal-time Analytics: Spark Streaming enables real-time analytics on data streams, making it suitable for applications like fraud detection, social media analysis, and real-time monitoring.\nMachine Learning: Spark MLlib offers a rich set of machine learning algorithms and tools, empowering users to build and deploy scalable machine learning models.\nGraph Analytics: Spark GraphX facilitates graph processing and analysis, enabling applications in social network analysis, recommendation systems, and network analysis.\nSpark SQL is a module of Spark designed for processing structured data. It provides two programming abstractions: DataFrame and DataSet, and acts as a distributed SQL query engine.Initially, Hive SQL is transformed into MapReduce and then submitted to the cluster for execution, which significantly simplifies the complexity of writing MapReduce programs. However, due to the relatively slow execution efficiency of the MapReduce model, Spark SQL emerged. Spark SQL transforms SQL queries into RDDs (Resilient Distributed Datasets) and then submits them to the cluster for execution, resulting in much faster performance.\nSeamless Integration\nSeamlessly mix SQL queries with Spark programs.\nUnified Data Access\nConnect to any data source in the same way.\nHive Compatibility\nRun unmodified Hive queries on existing data.\nStandard Data Connectivity\nConnect through JDBC or ODBC.\nSimilar to RDDs, DataFrames are distributed data containers. However, DataFrames resemble traditional database tables more closely, recording data structure information (schema) alongside the data. Additionally, like Hive, DataFrames support nested data types (struct, array, and map). From an API usability perspective, the DataFrame API offers a higher-level set of relational operations, making it more user-friendly and accessible compared to the functional RDD API.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Apache Spark","level":3,"id":"Apache_Spark_0"},{"heading":"Introduction to Apache Spark","level":4,"id":"Introduction_to_Apache_Spark_0"},{"heading":"Key Features of Apache Spark","level":3,"id":"Key_Features_of_Apache_Spark_0"},{"heading":"Components of Apache Spark","level":3,"id":"Components_of_Apache_Spark_0"},{"heading":"Advantages of Apache Spark","level":3,"id":"Advantages_of_Apache_Spark_0"},{"heading":"Use Cases of Apache Spark","level":3,"id":"Use_Cases_of_Apache_Spark_0"},{"heading":"Spark SQL","level":3,"id":"Spark_SQL_0"},{"heading":"Definition","level":4,"id":"Definition_0"},{"heading":"Features","level":3,"id":"Features_0"},{"heading":"DataFrame","level":3,"id":"DataFrame_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/0.-concept-&-architecture/0.-spark.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641495,"modifiedTime":1737554783000,"sourceSize":6968,"sourcePath":"Big Data/3. Spark/0. Concept & Architecture/0. Spark.md","exportPath":"big-data/3.-spark/0.-concept-&-architecture/0.-spark.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html":{"title":"1. Concepts","icon":"","description":"Spark is a fast, in-memory, general-purpose, and scalable big data processing engine.It was developed by the AMP Lab (Algorithms, Machines, and People Lab) at UC Berkeley. The Spark ecosystem, also called BDAS, is an open-source platform designed to showcase large-scale data applications through integration across algorithms, machines, and people. Leveraging big data and cloud resources, AMP Lab developed Spark to transform massive data into useful information, helping people better understand the world. It was open-sourced in 2010, became an Apache incubator project in June 2013, and was promoted to an Apache top-level project in February 2014. The framework is written in Scala.Apache Spark performs computations in memory, using a DAG (Directed Acyclic Graph) execution engine to enable faster processing. Its in-memory operations make it approximately 100 times faster than Hadoop MapReduce, and 10 times faster on disk.The speed advantage of Spark over MapReduce is due to:\nReduced Disk I/O: Intermediate results are stored in memory instead of being written to disk as in MapReduce.\nIncreased Parallelism: Tasks in Spark are executed as threads and split into stages, allowing parallel execution.\nAvoided Redundant Computations: The DAG in Spark reduces redundant operations.\nSpark supports APIs for Java, Python, and Scala, along with over 80 high-level algorithms. Interactive shells for Python and Scala allow users to verify solutions on a Spark cluster quickly.Spark provides a wide range of libraries, such as Spark SQL, Spark Streaming, MLlib, and GraphX, enabling seamless integration within an application. For example, Spark SQL handles structured data, Spark Streaming handles real-time data, MLlib provides machine learning algorithms, and GraphX supports graph computations.Spark can integrate with other open-source tools, using Hadoop's YARN and Apache Mesos as resource managers. It supports data formats like HDFS and HBase, making it easy for users with existing Hadoop clusters to utilize Spark’s capabilities without data migration.The Spark framework includes the following modules:\nSpark Core: Implements fundamental functionalities like task scheduling, memory management, error recovery, and storage system interaction. It also defines the API for Resilient Distributed Datasets (RDDs).\nSpark SQL: A package for working with structured data, allowing queries in SQL or Hive SQL dialects. Supports sources like Hive tables, Parquet, and JSON.\nSpark Streaming: Provides streaming data processing capabilities, closely aligned with RDD APIs for seamless data processing.\nSpark MLlib: A machine learning library with classification, regression, clustering, and collaborative filtering functionalities.\nSpark GraphX: An API for graph processing, with optimized performance and a rich set of operators for complex graph algorithms on large datasets.\nStructured Streaming: A scalable, fault-tolerant streaming engine that processes structured streaming data in DataFrames. It operates on Spark SQL's engine, enabling real-time analysis.\nSpark can run in three main modes: Local, Cluster, and Cloud.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Features of Spark","level":3,"id":"Features_of_Spark_0"},{"heading":"Speed","level":4,"id":"Speed_0"},{"heading":"Ease of Use","level":4,"id":"Ease_of_Use_0"},{"heading":"Versatility","level":4,"id":"Versatility_0"},{"heading":"Compatibility","level":4,"id":"Compatibility_0"},{"heading":"Spark Components","level":3,"id":"Spark_Components_0"},{"heading":"Spark Operating Modes","level":3,"id":"Spark_Operating_Modes_0"},{"heading":"Operating Modes","level":4,"id":"Operating_Modes_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641496,"modifiedTime":1737554783000,"sourceSize":11896,"sourcePath":"Big Data/3. Spark/0. Concept & Architecture/1. Concepts.md","exportPath":"big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html":{"title":"2. Architecture and Execution","icon":"","description":"The core of the Spark framework is a computing engine that adopts a standard master-slave structure. The figure below illustrates the basic structure of Spark during execution. The Driver in the diagram represents the master, responsible for managing the job scheduling across the entire cluster. The Executors, represented as slaves, are responsible for executing the tasks.“sparkContext.jpg” could not be found.The Spark Driver node is used to execute the main method of a Spark job, responsible for the actual code execution. During Spark job execution, the Driver is mainly responsible for:\nConverting user programs into jobs\nScheduling tasks among Executors\nTracking the execution status of Executors\nDisplaying the query execution status through the UI\nIn essence, we cannot precisely define the Driver as there is no direct reference to it in the programming process. Simply put, the Driver is the program that drives the entire application, also known as the Driver class.A Spark Executor is a JVM process running on the worker nodes in the cluster, responsible for executing specific tasks (Tasks) in a Spark job. These tasks are independent of each other. When a Spark application starts, Executor nodes are simultaneously started and persist throughout the application's lifecycle. If an Executor node fails or crashes, the Spark application can continue running by rescheduling the tasks from the failed node to other Executor nodes.In an independent deployment environment of a Spark cluster, there is no need to rely on other resource scheduling frameworks as Spark itself implements resource scheduling functions. Therefore, the environment includes two other core components: Master and Worker. The Master is a process primarily responsible for resource scheduling and allocation and cluster monitoring, similar to the ResourceManager (RM) in a Yarn environment. The Worker is also a process running on a server in the cluster, allocated resources by the Master to process and compute data in parallel, similar to the NodeManager (NM) in a Yarn environment.When a Hadoop user submits an application to the YARN cluster, the submission program should include an ApplicationMaster, which is responsible for requesting resource containers from the resource scheduler to execute the task, running the user's program tasks, monitoring the task execution, tracking the task status, and handling task failures. Simply put, the decoupling between ResourceManager (resources) and Driver (computation) relies on ApplicationMaster.A Spark Executor is a JVM process running on worker nodes in the cluster, dedicated to computation. When submitting an application, parameters can be specified to determine the number of computing nodes and the corresponding resources. The resources generally refer to the memory size of the Executor and the number of virtual CPU cores used. The related startup parameters for the application are as follows:In a distributed computing framework, multiple tasks are executed simultaneously. Since tasks are distributed across different computing nodes, true parallel execution of multiple tasks can be achieved. Note that this is parallelism, not concurrency. We refer to the number of tasks executed in parallel across the entire cluster as parallelism. The level of parallelism for a job depends on the default configuration of the framework and can be dynamically modified during application runtime.“sparkDAG.jpg” could not be found.Big data computing engines are generally classified into four categories based on their usage. The first category includes MapReduce, supported by Hadoop, which divides computation into two stages: Map and Reduce. For higher-level applications, this requires breaking down algorithms and chaining multiple jobs to complete a full algorithm, such as iterative computations. This limitation led to the emergence of DAG-supported frameworks.Thus, DAG-supported frameworks are classified as second-generation computing engines, like Tez and higher-level Oozie. Tez and Oozie were mainly batch processing tasks at the time. Next came the third-generation computing engines represented by Spark. The main feature of the third-generation engines is the internal DAG support within a job (not across jobs) and real-time computation. The so-called directed acyclic graph here is not a true graph but a high-level abstract model of data flow directly mapped from a Spark program. Simply put, it visually represents the execution process of the entire program, making it easier to understand and can be used to represent the program's topology. A Directed Acyclic Graph (DAG) is a topological graph composed of points and lines, with direction and no cycles.The submission process refers to the workflow where developers submit applications to the Spark runtime environment for computation via the Spark client. This process is similar across different deployment environments but has subtle differences. We won't detail these differences here, but since deploying Spark to a Yarn environment is more common in domestic work, the submission process in this course is based on the Yarn environment. When a Spark application is submitted to a Yarn environment, there are generally two deployment modes: Client and Cluster. The main difference between these modes is the location of the Driver program.When a Spark application is submitted to a Yarn environment, there are generally two deployment modes: Client and Cluster. The main difference between these modes is the location of the Driver program.In Client mode, the Driver module for monitoring and scheduling runs on the client side, not within Yarn, so it's generally used for testing.\nThe Driver runs on the local machine where the task is submitted\nAfter starting, the Driver communicates with ResourceManager to request the start of ApplicationMaster\nResourceManager allocates a container and starts ApplicationMaster on an appropriate NodeManager, responsible for requesting Executor memory from ResourceManager\nResourceManager allocates a container upon receiving the ApplicationMaster's resource request, and ApplicationMaster starts Executor processes on the specified NodeManager\nAfter starting, Executor processes register back with the Driver. Once all Executors are registered, the Driver begins executing the main function\nWhen an Action operator is executed, a job is triggered. Based on wide dependencies, stages are divided, each stage generates a corresponding TaskSet, and tasks are then distributed to Executors for execution.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Spark Runtime Architecture","level":3,"id":"Spark_Runtime_Architecture_0"},{"heading":"Core Components","level":3,"id":"Core_Components_0"},{"heading":"Driver","level":4,"id":"Driver_0"},{"heading":"Executor","level":4,"id":"Executor_0"},{"heading":"Master &amp; Worker","level":4,"id":"Master_&_Worker_0"},{"heading":"ApplicationMaster","level":4,"id":"ApplicationMaster_0"},{"heading":"Core Concepts","level":3,"id":"Core_Concepts_0"},{"heading":"Executor and Core","level":4,"id":"Executor_and_Core_0"},{"heading":"Parallelism","level":3,"id":"Parallelism_0"},{"heading":"Directed Acyclic Graph (DAG)","level":3,"id":"Directed_Acyclic_Graph_(DAG)_0"},{"heading":"Submission Process","level":3,"id":"Submission_Process_0"},{"heading":"Yarn Client Mode","level":3,"id":"Yarn_Client_Mode_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641497,"modifiedTime":1737554783000,"sourceSize":8523,"sourcePath":"Big Data/3. Spark/0. Concept & Architecture/2. Architecture and Execution.md","exportPath":"big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/0.-rdd.html":{"title":"0. RDD","icon":"","description":"RDD (Resilient Distributed Dataset) is the most fundamental data processing model in Spark. It is an abstract class that represents a resilient, immutable, partitioned collection of elements that can be computed in parallel.\nResilient Storage Resilience: Automatic switching between memory and disk.\nFault Tolerance: Automatic recovery of lost data.\nComputation Resilience: Retry mechanism for failed computations.\nPartitioning Resilience: Ability to repartition as needed. Distributed: Data is stored across different nodes in a big data cluster.\nDataset: RDD encapsulates the computation logic but does not store the data.\nData Abstraction: RDD is an abstract class that requires subclass implementation.\nImmutable: RDDs encapsulate computation logic and cannot be changed. Any change results in the creation of a new RDD with the new computation logic.\nPartitioned and Parallel Computation Partition List:\nThe RDD data structure contains a list of partitions used for parallel computation, which is crucial for implementing distributed computing.\nPartition Computation Function:\nSpark uses a partition function to compute each partition individually.\nPartitioner:\nFor key-value type data, a partitioner can be set to customize data partitioning.\nPreferred Location:\nDuring data computation, different node locations can be chosen based on the state of computation nodes.\nFrom a computational perspective, data processing requires computing resources (memory &amp; CPU) and a computation model (logic). During execution, computing resources and the computation model must be coordinated and integrated. The Spark framework first requests resources and then decomposes the application's data processing logic into individual computing tasks. These tasks are sent to the allocated computing nodes to execute the specified computation model, ultimately yielding the computation results. RDD is the core model used for data processing in the Spark framework. Let's look at how RDD works in a Yarn environment:\nStart the Yarn cluster environment.\nSpark creates scheduling nodes and computing nodes by requesting resources.\nThe Spark framework partitions the computation logic into different tasks based on partitions.\nScheduling nodes send tasks to the corresponding computing nodes based on their state for computation.\nFrom the above process, it is clear that RDD mainly encapsulates the logic and generates Tasks to be sent to Executor nodes for computation.There are four ways to create RDDs in Spark: Creating RDD from a Collection (Memory)\nSpark provides two methods to create RDDs from a collection: parallelize and makeRDD.\nval sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")\nval sparkContext = new SparkContext(sparkConf)\nval rdd1 = sparkContext.parallelize(List(1,2,3,4))\nval rdd2 = sparkContext.makeRDD(List(1,2,3,4))\nrdd1.collect().foreach(println)\nrdd2.collect().foreach(println)\nsparkContext.stop() Internally, the makeRDD method is just a wrapper around the parallelize method.\ndef makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices)\n} Creating RDD from External Storage (File)\nCreating RDDs from external storage systems includes local file systems and all Hadoop-supported datasets like HDFS, HBase, etc.\nval sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")\nval sparkContext = new SparkContext(sparkConf)\nval fileRDD: RDD[String] = sparkContext.textFile(\"input\")\nfileRDD.collect().foreach(println)\nsparkContext.stop() Creating RDD from Other RDDs\nThis is mainly achieved by performing operations on an existing RDD to produce a new RDD. Refer to subsequent chapters for details. Directly Creating RDD (new)\nUsing the new keyword to directly construct an RDD is generally used by the Spark framework itself. When creating an RDD using parallelize or makeRDD in Apache Spark, we start with an in-memory collection (like a List or Array) because this collection represents the initial dataset that we want to distribute and process in parallel across the Spark cluster. Understanding the internal logic behind this process helps to appreciate how Spark manages data distribution and parallel computation. Here’s an explanation of the internal logic: Starting with a Collection: When you use parallelize or makeRDD, you provide a collection that contains the data you want to process. This collection is typically a List, Array, or other sequence in Scala.\nExample:\nval data = List(1, 2, 3, 4) Creating the RDD: Spark's parallelize method takes the collection and creates an RDD from it. Here’s a simplified view of what happens internally:\nval rdd = sparkContext.parallelize(data) Data Distribution: Spark splits the collection into partitions, distributing the data across the different nodes in the cluster. The number of partitions can be specified using the numSlices parameter. If not specified, Spark uses a default value based on the cluster configuration.\nEach partition contains a subset of the original data collection.\nInternally, this involves creating ParallelCollectionRDD objects. ParallelCollectionRDD: The ParallelCollectionRDD is a specialized RDD type used for collections created from in-memory datasets. It takes care of dividing the collection into partitions and distributing them.\nExample internal code (simplified):\nval rdd = new ParallelCollectionRDD(sparkContext, data, numSlices) Tasks and Partitioning: Each partition in the RDD is treated as a task that can be executed independently and in parallel by different nodes in the cluster.\nSpark's scheduler assigns these tasks to worker nodes, where they are processed concurrently. Execution: When an action (like collect) is called on the RDD, Spark's execution engine schedules and runs the tasks on the distributed data.\nExample:\nrdd.collect().foreach(println) This triggers the computation, and each node processes its partition of the data, returning the results to the driver program. Driver Program: The driver program (where you write your Spark application) initiates the creation of an RDD from a collection.\nExample:\nval sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")\nval sparkContext = new SparkContext(sparkConf)\nval data = List(1, 2, 3, 4)\nval rdd = sparkContext.parallelize(data, 2) // 2 partitions ParallelCollectionRDD: Spark constructs a ParallelCollectionRDD instance.\nThe collection is divided into 2 partitions (or the number you specify).\nInternal constructor (simplified):\nnew ParallelCollectionRDD(sparkContext, data, numSlices) Task Distribution: Each partition becomes a task.\nSpark's scheduler assigns these tasks to executors on worker nodes. Execution: When an action like collect is invoked:\nrdd.collect().foreach(println) Tasks are executed on the worker nodes, each processing its partition and sending the results back to the driver. By default, Spark can split a job into multiple tasks and send them to Executor nodes for parallel computation. The number of tasks that can be executed in parallel is referred to as parallelism. This number can be specified when constructing an RDD. The number of tasks executed in parallel is not the same as the number of partitions.val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"spark\")\nval sparkContext = new SparkContext(sparkConf)\nval dataRDD: RDD[Int] = sparkContext.makeRDD(List(1, 2, 3, 4), 4)\nval fileRDD: RDD[String] = sparkContext.textFile(\"input\", 2)\nfileRDD.collect().foreach(println)\nsparkContext.stop() When reading data from memory, the data can be partitioned according to the specified parallelism. The core Spark source code for data partitioning is as follows:\ndef positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = { (0 until numSlices).iterator.map { i =&gt; val start = ((i * length) / numSlices).toInt val end = (((i + 1) * length) / numSlices).toInt (start, end) }\n} When reading data from a file, the data is partitioned according to Hadoop's file reading rules. There are some differences between the slicing rules and the data reading rules. The core Spark source code is as follows:\npublic InputSplit[] getSplits(JobConf job, int numSplits) throws IOException { long totalSize = 0; // compute total size for (FileStatus file : files) { // check we have valid files if (file.isDirectory()) { throw new IOException(\"Not a file: \" + file.getPath()); } totalSize += file.getLen(); } long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits); long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input. FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize); // Further implementation details ... for (FileStatus file : files) { ... if (isSplitable(fs, path)) { long blockSize = file.getBlockSize(); long splitSize = computeSplitSize(goalSize, minSize, blockSize); ... } } protected long computeSplitSize(long goalSize, long minSize, long blockSize) { return Math.max(minSize, Math.min(goalSize, blockSize)); }\nIn the above code:\npositions function is used for partitioning data read from memory.\ngetSplits method is used for partitioning data read from files according to Hadoop's file reading rules, considering factors like block size and minimum split size.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"RDD","level":3,"id":"RDD_0"},{"heading":"Core Properties","level":3,"id":"Core_Properties_0"},{"heading":"Execution Principle","level":3,"id":"Execution_Principle_0"},{"heading":"Basic Programming","level":3,"id":"Basic_Programming_0"},{"heading":"RDD Creation","level":4,"id":"RDD_Creation_0"},{"heading":"Internal Logic","level":4,"id":"Internal_Logic_0"},{"heading":"Detailed Steps in Code","level":4,"id":"Detailed_Steps_in_Code_0"},{"heading":"RDD Parallelism and Partitioning","level":3,"id":"RDD_Parallelism_and_Partitioning_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/0.-rdd.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641498,"modifiedTime":1737554783000,"sourceSize":10388,"sourcePath":"Big Data/3. Spark/1. RDD/0. RDD.md","exportPath":"big-data/3.-spark/1.-rdd/0.-rdd.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/1.-transformations.html":{"title":"1. Transformations","icon":"","description":"RDD transformations in Spark are broadly categorized into Value types, Double Value types, and Key-Value types.\nFunction Signature:\ndef map[U: ClassTag](f: T =&gt; U): RDD[U] Function Description:\nMaps each element of the dataset to a new element, potentially changing the element's type or value.\nval dataRDD: RDD[Int] = sparkContext.makeRDD(List(1, 2, 3, 4))\nval dataRDD1: RDD[Int] = dataRDD.map(num =&gt; num * 2)\nval dataRDD2: RDD[String] = dataRDD1.map(num =&gt; s\"$num\") Example:\nExtract user request URL paths from server log data apache.log. Function Signature:\ndef mapPartitions[U: ClassTag]( f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] Function Description:\nProcesses data by partition, sending each partition to a computation node for processing. This can include any arbitrary processing, even filtering data.\nval dataRDD1: RDD[Int] = dataRDD.mapPartitions(datas =&gt; datas.filter(_ == 2)) Example:\nRetrieve the maximum value from each data partition.\nDifference between map and mapPartitions:\nData Processing Perspective: map processes one element at a time within a partition, similar to serial operations.\nmapPartitions processes data by partition, handling the entire partition in batch. Functionality Perspective: map transforms and changes the data source without changing the number of elements.\nmapPartitions can change the number of elements, as it accepts and returns an iterator, allowing for addition or reduction of data. Performance Perspective: map is similar to serial operations, thus performance is lower.\nmapPartitions is similar to batch processing, thus performance is higher, but it may occupy memory for a long time, leading to potential memory overflow errors. In limited memory scenarios, map is recommended. Function Signature:\ndef mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] Function Description:\nProcesses data by partition, allowing arbitrary processing, even filtering data. Additionally, it provides the current partition index.\nval dataRDD1 = dataRDD.mapPartitionsWithIndex( (index, datas) =&gt; datas.map(data =&gt; (index, data))\n) Example:\nRetrieve data from the second partition. Function Signature:\ndef flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] Function Description:\nFlattens the data and then applies a mapping transformation, thus also known as a flat map.\nval dataRDD = sparkContext.makeRDD(List(List(1, 2), List(3, 4)), 1)\nval dataRDD1 = dataRDD.flatMap(list =&gt; list)\nThis example flattens the list of lists into a single list: List(1, 2, 3, 4).\nExample:\nFlatten the list List(List(1,2),3,List(4,5)). Function Signature:\ndef glom(): RDD[Array[T]] Function Description:\nTransforms the data in the same partition into an array of the same type, keeping partitions unchanged.\nval dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 1)\nval dataRDD1: RDD[Array[Int]] = dataRDD.glom() Example:\nSum the maximum values of all partitions (take the maximum value in each partition, and sum these maximum values). Function Signature:\ndef groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] Function Description:\nGroups data according to the specified rule. The partitions remain unchanged, but data is shuffled and reorganized. This operation can result in data being grouped in the same partition.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"RDD Transformations","level":3,"id":"RDD_Transformations_0"},{"heading":"Value Types","level":3,"id":"Value_Types_0"},{"heading":"map","level":4,"id":"map_0"},{"heading":"mapPartitions","level":4,"id":"mapPartitions_0"},{"heading":"mapPartitionsWithIndex","level":4,"id":"mapPartitionsWithIndex_0"},{"heading":"flatMap","level":4,"id":"flatMap_0"},{"heading":"glom","level":4,"id":"glom_0"},{"heading":"groupBy","level":4,"id":"groupBy_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/1.-transformations.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641499,"modifiedTime":1737554783000,"sourceSize":17006,"sourcePath":"Big Data/3. Spark/1. RDD/1. Transformations.md","exportPath":"big-data/3.-spark/1.-rdd/1.-transformations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/2.-actions.html":{"title":"2. Actions","icon":"","description":"\nFunction Signature:\ndef reduce(f: (T, T) =&gt; T): T Function Description:\nAggregates all the elements in the RDD, first combining elements within partitions, and then combining results across partitions.\nval rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n// Aggregate data\nval reduceResult: Int = rdd.reduce(_ + _) Function Signature:\ndef collect(): Array[T] Function Description:\nReturns all the elements of the dataset as an array to the driver program.\nval rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n// Collect data to the driver\nrdd.collect().foreach(println) Function Signature:\ndef count(): Long Function Description:\nReturns the number of elements in the RDD.\nval rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n// Return the number of elements in the RDD\nval countResult: Long = rdd.count() Function Signature:\ndef first(): T Function Description:\nReturns the first element in the RDD.\nval rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n// Return the first element in the RDD\nval firstResult: Int = rdd.first()\nprintln(firstResult) Function Signature:\ndef take(num: Int): Array[T] Function Description:\nReturns an array with the first num elements of the RDD.\nval rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))\n// Return the first `num` elements in the RDD\nval takeResult: Array[Int] = rdd.take(2)\nprintln(takeResult.mkString(\",\")) Function Signature:\ndef takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] Function Description:\nReturns the first num elements of the RDD after sorting.\nval rdd: RDD[Int] = sc.makeRDD(List(1, 3, 2, 4))\n// Return the first `num` elements of the sorted RDD\nval result: Array[Int] = rdd.takeOrdered(2)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"reduce","level":3,"id":"reduce_0"},{"heading":"collect","level":3,"id":"collect_0"},{"heading":"count","level":3,"id":"count_0"},{"heading":"first","level":3,"id":"first_0"},{"heading":"take","level":3,"id":"take_0"},{"heading":"takeOrdered","level":3,"id":"takeOrdered_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/2.-actions.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641500,"modifiedTime":1737554783000,"sourceSize":4070,"sourcePath":"Big Data/3. Spark/1. RDD/2. Actions.md","exportPath":"big-data/3.-spark/1.-rdd/2.-actions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html":{"title":"3. Serialization & Dependencies & Persistence","icon":"","description":" Closure Check\nFrom a computational perspective, code outside operators runs on the Driver, while code inside operators runs on Executors. In Scala's functional programming, this often leads to closures where operators use external variables. If these external variables cannot be serialized, errors occur because values cannot be sent to Executors. Therefore, before task execution, the closure is checked to ensure that all objects within it are serializable. This process is called closure checking. Note that in Scala 2.12, closure compilation has changed. Serialization Methods and Properties\nFrom a computational perspective, code outside operators runs on the Driver, while code inside operators runs on Executors. Consider the following code:\nobject SerializableExample { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf().setAppName(\"SparkCoreTest\").setMaster(\"local[*]\") val sc: SparkContext = new SparkContext(conf) val rdd: RDD[String] = sc.makeRDD(Array(\"hello world\", \"hello spark\", \"hive\", \"atguigu\")) val search = new Search(\"hello\") search.getMatch1(rdd).collect().foreach(println) search.getMatch2(rdd).collect().foreach(println) sc.stop() }\n} class Search(query: String) extends Serializable { def isMatch(s: String): Boolean = { s.contains(query) } // Function serialization example def getMatch1(rdd: RDD[String]): RDD[String] = { rdd.filter(isMatch) } // Property serialization example def getMatch2(rdd: RDD[String]): RDD[String] = { rdd.filter(_.contains(query)) }\n} Kryo Serialization Framework\nRefer to: <a data-tooltip-position=\"top\" aria-label=\"https://github.com/EsotericSoftware/kryo\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://github.com/EsotericSoftware/kryo\" target=\"_self\">Kryo</a>. Java serialization can serialize any class but is heavy and results in large serialized objects. For performance reasons, Spark 2.0+ supports the Kryo serialization mechanism, which is ten times faster than Java serialization. While Kryo serialization is used internally by Spark for simple data types, arrays, and strings, you still need to implement the Serializable interface even when using Kryo.\nobject KryoSerializationExample { def main(args: Array[String]): Unit = { val conf: SparkConf = new SparkConf() .setAppName(\"SerDemo\") .setMaster(\"local[*]\") .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .registerKryoClasses(Array(classOf[Searcher])) val sc: SparkContext = new SparkContext(conf) val rdd: RDD[String] = sc.makeRDD(Array(\"hello world\", \"hello atguigu\", \"atguigu\", \"hahah\"), 2) val searcher = new Searcher(\"hello\") val result: RDD[String] = searcher.getMatchedRDD1(rdd) result.collect.foreach(println) }\n} case class Searcher(query: String) { def isMatch(s: String): Boolean = s.contains(query) def getMatchedRDD1(rdd: RDD[String]): RDD[String] = rdd.filter(isMatch) def getMatchedRDD2(rdd: RDD[String]): RDD[String] = { val q = query rdd.filter(_.contains(q)) }\n} RDD Lineage\nRDD only supports coarse-grained transformations, i.e., operations performed on large sets of records. It records the lineage of operations to reconstruct lost partitions. The lineage captures metadata and transformation behavior, allowing reconstruction of lost data partitions based on these records.\nval fileRDD: RDD[String] = sc.textFile(\"input/1.txt\")\nprintln(fileRDD.toDebugString)\nprintln(\"----------------------\")\nval wordRDD: RDD[String] = fileRDD.flatMap(_.split(\" \"))\nprintln(wordRDD.toDebugString)\nprintln(\"----------------------\")\nval mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))\nprintln(mapRDD.toDebugString)\nprintln(\"----------------------\")\nval resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)\nprintln(resultRDD.toDebugString)\nresultRDD.collect() RDD Dependencies\nDependencies refer to the relationships between adjacent RDDs.\nval sc: SparkContext = new SparkContext(conf)\nval fileRDD: RDD[String] = sc.textFile(\"input/1.txt\")\nprintln(fileRDD.dependencies)\nprintln(\"----------------------\")\nval wordRDD: RDD[String] = fileRDD.flatMap(_.split(\" \"))\nprintln(wordRDD.dependencies)\nprintln(\"----------------------\")\nval mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))\nprintln(mapRDD.dependencies)\nprintln(\"----------------------\")\nval resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)\nprintln(resultRDD.dependencies)\nresultRDD.collect() Narrow Dependencies\nNarrow dependencies occur when each parent RDD partition is used by at most one child RDD partition.\nclass OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) Wide Dependencies\nWide dependencies occur when multiple child RDD partitions depend on a single parent RDD partition, causing a shuffle.\nclass ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] RDD Stage Division\nDAG (Directed Acyclic Graph) is a directed graph without cycles, used to represent the stages of RDD computations. RDD Stage Division Source Code\ntry { finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)\n} catch { case e: Exception =&gt; logWarning(\"Creating new stage failed due to exception - job: \" + jobId, e) listener.jobFailed(e) return\n} private def createResultStage( rdd: RDD[_], func: (TaskContext, Iterator[_]) =&gt; _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage = { val parents = getOrCreateParentStages(rdd, jobId) val id = nextStageId.getAndIncrement() val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite) stageIdToStage(id) = stage updateJobIdStageIdMaps(jobId, stage) stage\n} private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = { getShuffleDependencies(rdd).map { shuffleDep =&gt; getOrCreateShuffleMapStage(shuffleDep, firstJobId) }.toList\n} private[scheduler] def getShuffleDependencies(rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] = { val parents = new HashSet[ShuffleDependency[_, _, _]] val visited = new HashSet[RDD[_]] val waitingForVisit = new Stack[RDD[_]] waitingForVisit.push(rdd) while (waitingForVisit.nonEmpty) { val toVisit = waitingForVisit.pop() if (!visited(toVisit)) { visited += toVisit toVisit.dependencies.foreach { case shuffleDep: ShuffleDependency[_, _, _] =&gt; parents += shuffleDep case dependency =&gt; waitingForVisit.push(dependency.rdd) } } } parents\n} RDD Task Division\nTasks in RDD are divided into Application, Job, Stage, and Task. Application: Initiating a SparkContext creates an Application.\nJob: An Action operator generates a Job.\nStage: The number of wide dependencies (ShuffleDependencies) plus one.\nTask: The number of partitions in the final RDD of a stage equals the number of tasks. val tasks: Seq[Task[_]] = try { stage match { case stage: ShuffleMapStage =&gt; partitionsToCompute.map { id =&gt; val locs = taskIdToLocations(id) val part = stage.rdd.partitions(id) new ShuffleMapTask(stage.id, stage.latestInfo.attemptId, taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId) } case stage: ResultStage =&gt; partitionsToCompute.map { id =&gt; val p: Int = stage.partitions(id) val part = stage.rdd.partitions(p) val locs = taskIdToLocations(id) new ResultTask(stage.id, stage.latestInfo.attemptId, taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId) } } val partitionsToCompute: Seq[Int] = stage.findMissingPartitions() override def findMissingPartitions(): Seq[Int] = { mapOutputTrackerMaster.findMissingPartitions(shuffleDep.shuffleId).getOrElse(0 until numPartitions) } }\n### RDD Persistence 1) **RDD Cache**\nRDDs can cache or persist results to avoid recomputation. By default, data is cached in the JVM heap memory. This caching occurs when an action operator is triggered, storing the RDD in memory for reuse.\n```scala\nprintln(wordToOneRdd.toDebugString)\nwordToOneRdd.cache()\n// Change storage level\n// mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2) Storage Levels:object StorageLevel { val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1)\n} Cache data may be lost or evicted from memory if memory is insufficient. RDD's fault tolerance ensures correct computation even if cached data is lost, by recomputing lost partitions. While Spark automatically persists some shuffle data, it's recommended to use persist or cache for reusable data. RDD Checkpoint\nCheckpointing writes intermediate results to disk, breaking the lineage to reduce fault tolerance costs. After checkpointing, if a node fails, it can recompute data from the checkpoint. Checkpointing is triggered by an Action operation.\nsc.setCheckpointDir(\"./checkpoint1\")\nval lineRdd: RDD[String] = sc.textFile(\"input/1.txt\")\nval wordRdd: RDD[String] = lineRdd.flatMap(_.split(\" \"))\nval wordToOneRdd: RDD[(String, Long)] = wordRdd.map(word =&gt; (word, System.currentTimeMillis()))\nwordToOneRdd.cache()\nwordToOneRdd.checkpoint()\nwordToOneRdd.collect().foreach(println) Cache vs Checkpoint Cache stores data without breaking lineage; Checkpoint breaks lineage.\nCache data is stored in memory/disk with low reliability; Checkpoint data is stored in fault-tolerant file systems like HDFS.\nIt's recommended to cache checkpointed RDDs to avoid recomputation. Spark supports HashPartitioner and RangePartitioner, and allows custom partitioners. HashPartitioner is the default.\nOnly Key-Value RDDs have partitioners; non Key-Value RDDs have None as their partitioner.\nEach RDD's partition ID ranges from 0 to (numPartitions - 1), determining which partition a value belongs to. HashPartitioner\nComputes the hashCode of the key and mods by the number of partitions.\nclass HashPartitioner(partitions: Int) extends Partitioner { require(partitions &gt;= 0, s\"Number of partitions ($partitions) cannot be negative.\") def numPartitions: Int = partitions def getPartition(key: Any): Int = key match { case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) } override def equals(other: Any): Boolean = other match { case h: HashPartitioner =&gt; h.numPartitions == numPartitions case _ =&gt; false } override def hashCode: Int = numPartitions\n} RangePartitioner\nMaps data within a range to a partition, aiming for even distribution and ordered partitions.\nclass RangePartitioner[K: Ordering: ClassTag, V]( partitions: Int, rdd: RDD[_ &lt;: Product2[K, V]], private var ascending: Boolean = true) extends Partitioner { require(partitions &gt;= 0, s\"Number of partitions cannot be negative but found $partitions.\") private var ordering = implicitly[Ordering[K]] private var rangeBounds: Array[K] = { ... } def numPartitions: Int = rangeBounds.length + 1 private var binarySearch: ((Array[K], K) =&gt; Int) = CollectionsUtils.makeBinarySearch[K] def getPartition(key: Any): Int = { val k = key.asInstanceOf[K] var partition = 0 if (rangeBounds.length &lt;= 128) { while (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) { partition += 1 } } else { partition = binarySearch(rangeBounds, k) if (partition &lt; 0) partition = -partition - 1 if (partition &gt; rangeBounds.length) partition = rangeBounds.length } if (ascending) partition else rangeBounds.length - partition } override def equals(other: Any): Boolean = { ... } override def hashCode(): Int = { ... } @throws(classOf[IOException]) private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { ... } @throws(classOf[IOException]) private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException { ... }\n} ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3. Serialization &amp; Dependencies &amp; Persistence","level":1,"id":"3._Serialization_&_Dependencies_&_Persistence_0"},{"heading":"RDD Serialization","level":3,"id":"RDD_Serialization_0"},{"heading":"RDD Dependencies","level":3,"id":"RDD_Dependencies_0"},{"heading":"RDD Partitioners","level":3,"id":"RDD_Partitioners_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641501,"modifiedTime":1737554783000,"sourceSize":13997,"sourcePath":"Big Data/3. Spark/1. RDD/3. Serialization & Dependencies & Persistence.md","exportPath":"big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/4.-collaboration.html":{"title":"4. Collaboration","icon":"","description":"An RDD (Resilient Distributed Dataset) is divided into multiple partitions. Each partition is a subset of the data and can be processed independently. This partitioning is the key to Spark's ability to perform distributed and parallel processing efficiently.Each partition of an RDD is stored on different workers (or sometimes the same worker if there are fewer workers than partitions). This means that an RDD's data is distributed across the cluster, allowing Spark to leverage the processing power of multiple nodes for parallel computation.\nWhen an RDD is created (e.g., from a file or other data source), Spark automatically divides it into partitions. For example, if you read a large text file, Spark may divide the file into multiple partitions, with each partition containing a chunk of lines from the file. The Driver schedules tasks for each partition. Each task is responsible for processing one partition of the RDD. Each worker node has one or more executors. An executor is responsible for executing the tasks assigned to its worker node.\nEach executor processes its assigned partitions in parallel with executors on other worker nodes. For example, if there are three partitions, Spark might assign these partitions to three different executors, potentially on different worker nodes. The executors perform the transformations on their respective partitions. For instance, if the RDD transformation involves mapping and reducing operations, each executor will apply these operations to the partitions it holds. When an action like collect() is called, it triggers the execution of the transformations that have been defined on the RDD. collect() is an action that gathers the results from all partitions and returns them to the driver. Each executor processes its partitions and sends the results back to the Driver.\nThe Driver then aggregates these results if needed (depending on the action performed).\nlines = sc.textFile(\"hdfs://path/to/textfile.txt\")\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Partitioning of RDDs","level":3,"id":"Partitioning_of_RDDs_0"},{"heading":"Storage of Partitions on Workers","level":3,"id":"Storage_of_Partitions_on_Workers_0"},{"heading":"Execution Flow in Detail","level":3,"id":"Execution_Flow_in_Detail_0"},{"heading":"1. RDD Creation and Partitioning","level":4,"id":"1._RDD_Creation_and_Partitioning_0"},{"heading":"2. Task Scheduling","level":4,"id":"2._Task_Scheduling_0"},{"heading":"3. Parallel Processing on Workers","level":4,"id":"3._Parallel_Processing_on_Workers_0"},{"heading":"4. Local Computation","level":4,"id":"4._Local_Computation_0"},{"heading":"5. Action Trigger","level":4,"id":"5._Action_Trigger_0"},{"heading":"6. Result Collection","level":4,"id":"6._Result_Collection_0"},{"heading":"Example Walkthrough:","level":3,"id":"Example_Walkthrough_0"},{"heading":"1. Read Text File into an RDD","level":4,"id":"1._Read_Text_File_into_an_RDD_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/4.-collaboration.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641502,"modifiedTime":1737554783000,"sourceSize":6263,"sourcePath":"Big Data/3. Spark/1. RDD/4. Collaboration.md","exportPath":"big-data/3.-spark/1.-rdd/4.-collaboration.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html":{"title":"5. Reading and Saving RDD Files","icon":"","description":"Spark's data reading and saving capabilities can be categorized along two dimensions: file formats and file systems.\nFile formats include: text files, CSV files, sequence files, and object files;\nFile systems include: local file system, HDFS, HBase, and databases.// Reading input file\nval inputRDD: RDD[String] = sc.textFile(\"input/1.txt\")\n// Saving data\ninputRDD.saveAsTextFile(\"output\")\nA SequenceFile is a flat file consisting of binary key-value pairs designed by Hadoop. In SparkContext, you can call sequenceFile[keyClass, valueClass](path).// Saving data as SequenceFile\ndataRDD.saveAsSequenceFile(\"output\")\n// Reading SequenceFile\nsc.sequenceFile[Int, Int](\"output\").collect().foreach(println)\nObject files are files where objects are serialized and saved, using Java's serialization mechanism. You can use the objectFile[T: ClassTag](path) function to read object files and return the corresponding RDD. Similarly, you can call saveAsObjectFile() to output object files. Since serialization is involved, the type must be specified.// Saving data\ndataRDD.saveAsObjectFile(\"output\")\n// Reading data\nsc.objectFile[Int](\"output\").collect().foreach(println)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Text Files","level":4,"id":"Text_Files_0"},{"heading":"Sequence Files","level":4,"id":"Sequence_Files_0"},{"heading":"Object Files","level":4,"id":"Object_Files_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641535,"modifiedTime":1737554783000,"sourceSize":1268,"sourcePath":"Big Data/3. Spark/1. RDD/5. Reading and Saving RDD Files.md","exportPath":"big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html":{"title":"6. Accumulators & Broadcast Variables","icon":"","description":"Accumulators are used to aggregate information from Executor variables to the Driver. Variables defined in the Driver program will have a separate copy in each Task on the Executor side. Each Task updates these copies, which are then merged back into the Driver.val rdd = sc.makeRDD(List(1, 2, 3, 4, 5))\n// Declare accumulator\nvar sum = sc.longAccumulator(\"sum\")\nrdd.foreach(num =&gt; { // Use accumulator sum.add(num)\n})\n// Get accumulator value\nprintln(\"sum = \" + sum.value)\n// Custom accumulator\n// 1. Extend AccumulatorV2 and specify the generic types\n// 2. Override the abstract methods of the accumulator\nclass WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Long]] { var map: mutable.Map[String, Long] = mutable.Map() // Check if the accumulator is in its initial state override def isZero: Boolean = { map.isEmpty } // Copy the accumulator override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = { new WordCountAccumulator } // Reset the accumulator override def reset(): Unit = { map.clear() } // Add data to the accumulator (In) override def add(word: String): Unit = { // Check if the map contains the word // If it does, increment the word count // If not, add the word to the map map(word) = map.getOrElse(word, 0L) + 1L } // Merge accumulators override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = { val map1 = map val map2 = other.value // Merge two maps map = map1.foldLeft(map2) { (innerMap, kv) =&gt; { innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2 innerMap } } } // Return the result of the accumulator (Out) override def value: mutable.Map[String, Long] = map\n}\nBroadcast variables are used to efficiently distribute large objects. They send a large read-only value to all worker nodes to be used by one or more Spark operations. For example, if your application needs to send a large read-only lookup table to all nodes, broadcast variables are useful. Spark will send the variable separately for each task.val rdd1 = sc.makeRDD(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"d\", 4)), 4)\nval list = List((\"a\", 4), (\"b\", 5), (\"c\", 6), (\"d\", 7))\n// Declare broadcast variable\nval broadcast: Broadcast[List[(String, Int)]] = sc.broadcast(list)\nval resultRDD: RDD[(String, (Int, Int))] = rdd1.map { case (key, num) =&gt; { var num2 = 0 // Use broadcast variable for ((k, v) &lt;- broadcast.value) { if (k == key) { num2 = v } } (key, (num, num2)) }\n} Partitions: Each RDD is split into partitions. Each partition can be processed independently and in parallel.\nWorkers and Executors: Each worker node runs executors that process the partitions of the RDD. The executors perform the transformations on their local partitions.\nDriver: The Driver schedules tasks for the partitions and collects the results after execution. RDD Creation:\nval lines = sc.textFile(\"hdfs://path/to/textfile.txt\") This creates an RDD[String] and automatically partitions it. Transformations:\nval words = lines.flatMap(line =&gt; line.split(\" \"))\nval wordPairs = words.map(word =&gt; (word, 1))\nval wordCounts = wordPairs.reduceByKey((a, b) =&gt; a + b) Each transformation is applied to the partitions of the RDD. Action:\nval results = wordCounts.collect() The collect() action triggers execution of transformations. Assume three worker nodes, each handling one partition: Driver (Main Program) | SparkContext | Schedules Tasks | ------------------- | | | Worker 1 Worker 2 Worker 3 (Executor) (Executor) (Executor) | | |\nPartition 1 Partition 2 Partition 3 | | | Tasks to process each partition | | | Processes Processes Processes Partition 1 Partition 2 Partition 3 | | | Local Results Local Results Local Results | | | Collects Results from all Partitions | Final Output\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"6. Accumulators &amp; Broadcast Variables","level":1,"id":"6._Accumulators_&_Broadcast_Variables_0"},{"heading":"Accumulators","level":3,"id":"Accumulators_0"},{"heading":"System Accumulators","level":4,"id":"System_Accumulators_0"},{"heading":"Custom Accumulators","level":4,"id":"Custom_Accumulators_0"},{"heading":"Broadcast Variables","level":3,"id":"Broadcast_Variables_0"},{"heading":"Basic Programming","level":4,"id":"Basic_Programming_0"},{"heading":"Enhanced Explanations:","level":3,"id":"Enhanced_Explanations_0"},{"heading":"Key Points:","level":4,"id":"Key_Points_0"},{"heading":"Example Walkthrough (Enhanced):","level":4,"id":"Example_Walkthrough_(Enhanced)_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641570,"modifiedTime":1737554783000,"sourceSize":4520,"sourcePath":"Big Data/3. Spark/1. RDD/6. Accumulators & Broadcast Variables.md","exportPath":"big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/2.-spark-sql/0.-sparksql.html":{"title":"0. SparkSQL","icon":"","description":"SparkSQL evolved from Shark, which provided a tool for those familiar with RDBMS but not with MapReduce, allowing them to get up to speed quickly. Hive was the early SQL-on-Hadoop tool running on Hadoop. However, the extensive I/O operations during MapReduce computations, due to intermediate data being written to disk, significantly lowered performance. To improve SQL-on-Hadoop efficiency, many SQL-on-Hadoop tools emerged, with notable ones being:\nDrill\nImpala\nShark\nShark, developed at the Berkeley Lab as part of the Spark ecosystem, was based on Hive and improved three modules: memory management, physical plan, and execution. It ran on the Spark engine and offered a performance boost of 10-100 times over Hive.“shark.png” could not be found.\nHowever, as Spark evolved, Shark's heavy reliance on Hive (such as using Hive's syntax parser and query optimizer) limited the Spark team's goal of integrating all components under one stack. This led to the SparkSQL project, which abandoned Shark's codebase but retained some of its advantages like in-memory columnar storage and Hive compatibility, and redeveloped the SparkSQL code. Freed from Hive's dependencies, SparkSQL improved data compatibility, performance optimization, and component extension, making it highly versatile.\nData Compatibility: SparkSQL is compatible with Hive and can fetch data from RDDs, Parquet files, JSON files, and future versions may support data from RDBMS and NoSQL databases like Cassandra.\nPerformance Optimization: Besides adopting optimization techniques like in-memory columnar storage and byte-code generation, it will introduce cost models to dynamically evaluate queries for the best physical plan.\nComponent Extension: SQL syntax parser, analyzer, and optimizer can all be redefined and extended.\nOn June 1, 2014, Shark and SparkSQL project lead Reynold Xin announced the cessation of Shark development, focusing all resources on SparkSQL. This marked the end of Shark but led to the development of two branches: SparkSQL and Hive on Spark.SparkSQL, as part of the Spark ecosystem, continued to evolve independently of Hive while maintaining compatibility. Hive on Spark is a development plan where Hive can use Spark as one of its underlying engines, alongside MapReduce and Tez.For developers, SparkSQL simplifies RDD development, enhances development efficiency, and provides fast execution. SparkSQL introduces two programming abstractions similar to Spark Core's RDD:\nDataFrame\nDataSet\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Hive and SparkSQL","level":3,"id":"Hive_and_SparkSQL_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/2.-spark-sql/0.-sparksql.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641572,"modifiedTime":1737554783000,"sourceSize":2910,"sourcePath":"Big Data/3. Spark/2. Spark SQL/0. SparkSQL.md","exportPath":"big-data/3.-spark/2.-spark-sql/0.-sparksql.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html":{"title":"1. DataFrame & DataSet","icon":"","description":"In Spark, a DataFrame is a distributed dataset based on RDDs, similar to a two-dimensional table in traditional databases. The main difference between DataFrame and RDD is that DataFrame carries schema information (metadata), meaning each column in the DataFrame has a name and type. This allows Spark SQL to leverage more structural information, optimizing transformations applied to DataFrame and significantly improving runtime efficiency. In contrast, RDD lacks internal structure details, leading Spark Core to perform generic stage-level optimizations. Like Hive, DataFrame supports nested data types (struct, array, and map). From an API usability perspective, DataFrame API provides a higher-level relational operation set, more user-friendly than the functional RDD API.A DataFrame provides a schema view of data. It can be treated as a table in a database and is lazily evaluated but offers better performance due to an optimized execution plan through the Spark Catalyst optimizer. For instance, joining two DataFrames followed by filtering could be inefficient if executed directly. The optimizer pushes the filter operation below the join, reducing the dataset size before joining, thus shortening execution time.A DataSet is a distributed data collection introduced in Spark 1.6, extending DataFrame. It combines RDD advantages (strong typing and lambda functions) with Spark SQL's optimized execution engine. DataSet supports functional transformations (map, flatMap, filter, etc.).\nDataSet: An extension of DataFrame API, SparkSQL's latest data abstraction.\nUser-Friendly API: Type-safe checks and query optimization characteristics of DataFrame.\nSchema Information: Uses case classes for defining data structure, mapping each attribute name directly to a DataSet field name.\nStrong Typing: Allows DataSet[Car], DataSet[Person], etc.\nDataFrame and DataSet Relationship: DataFrame is a special case of DataSet, where DataFrame = DataSet[Row]. You can convert a DataFrame to DataSet using the as method. Row is a type representing table structure information, similar to Car or Person types, with data retrieval requiring specified order.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. DataFrame &amp; DataSet","level":1,"id":"1._DataFrame_&_DataSet_0"},{"heading":"DataFrame","level":3,"id":"DataFrame_0"},{"heading":"DataSet","level":3,"id":"DataSet_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641574,"modifiedTime":1737554783000,"sourceSize":2306,"sourcePath":"Big Data/3. Spark/2. Spark SQL/1. DataFrame & DataSet.md","exportPath":"big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/2.-spark-sql/2.-coding.html":{"title":"2. Coding","icon":"","description":"In older versions, SparkSQL provided two starting points for SQL queries: SQLContext for Spark's own SQL queries and HiveContext for connecting to Hive.SparkSession is the new entry point for SQL queries in Spark. Essentially, it combines SQLContext and HiveContext, so any APIs available in SQLContext and HiveContext can also be used in SparkSession. Internally, SparkSession encapsulates SparkContext, meaning the actual computation is done by SparkContext.In Spark SQL, SparkSession is the entry point for creating DataFrames and executing SQL queries. There are three ways to create DataFrames: through Spark data sources, converting from an existing RDD, or querying from a Hive table. Creating from Spark Data Sources List available file formats for Spark data sources:\nscala&gt; spark.read.\ncsv format jdbc json load option options orc parquet schema table text textFile Reading a JSON file to create a DataFrame:\nscala&gt; val df = spark.read.json(\"/opt/module/spark/examples/src/main/resources/people.json\")\ndf: org.apache.spark.sql.DataFrame = [age: bigint, name: string] Displaying the result:\nscala&gt; df.show\n+----+-------+\n| age| name|\n+----+-------+\n|null|Michael|\n| 30| Andy|\n| 19| Justin|\n+----+-------+ Creating a DataFrame:\nscala&gt; val df = spark.read.json(\"/opt/module/spark/examples/src/main/resources/people.json\")\ndf: org.apache.spark.sql.DataFrame = [age: bigint, name: string] Creating a temporary table from the DataFrame:\nscala&gt; df.createOrReplaceTempView(\"people\") Querying the table using SQL:\nscala&gt; val sqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string] Displaying the result:\nscala&gt; sqlDF.show\n+----+-------+\n| age| name|\n+----+-------+\n|null|Michael|\n| 30| Andy|\n| 19| Justin|\n+----+-------+ Note: Temporary tables are session-scoped and will be invalidated once the session ends.c If you need a table valid across sessions, you can use global temporary views, which require full path access like global_temp.people. Creating a global temporary table:\nscala&gt; df.createGlobalTempView(\"people\") Querying the global table using SQL:\nscala&gt; spark.sql(\"SELECT * FROM global_temp.people\").show()\n+----+-------+\n| age| name|\n+----+-------+\n|null|Michael|\n| 30| Andy|\n| 19| Justin|\n+----+-------+ scala&gt; spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n+----+-------+\n| age| name|\n+----+-------+\n|null|Michael|\n| 30| Andy|\n| 19| Justin|\n+----+-------+ DataFrame provides a domain-specific language (DSL) for managing structured data. DSL can be used in Scala, Java, Python, and R. Using DSL syntax eliminates the need to create temporary views. Creating a DataFrame:\nscala&gt; val df = spark.read.json(\"data/user.json\")\ndf: org.apache.spark.sql.DataFrame = [age: bigint, name: string] Viewing the Schema of the DataFrame:\nscala&gt; df.printSchema\nroot |-- age: Long (nullable = true) |-- name: string (nullable = true) Viewing only the \"name\" column:\nscala&gt; df.select(\"name\").show()\n+--------+\n| name |\n+--------+\n|zhangsan|\n| lisi |\n| wangwu |\n+--------+ Viewing the \"name\" column and \"age + 1\" column:\nNote: When performing operations, each column must use $, or use the quote expression: single quote + field name.\nscala&gt; df.select($\"name\", $\"age\" + 1).show()\nscala&gt; df.select('name, 'age + 1).show()\nscala&gt; df.select('name, ('age + 1).as(\"new_age\")).show()\n+--------+---------+\n| name | new_age |\n+--------+---------+\n|zhangsan| 21|\n| lisi | 31|\n| wangwu | 41|\n+--------+---------+ Filtering rows where \"age\" is greater than 30:\nscala&gt; df.filter($\"age\" &gt; 30).show()\n+---+--------+\n|age| name|\n+---+--------+\n| 40| wangwu |\n+---+--------+ Grouping by \"age\" and counting the number of rows in each group:\nscala&gt; df.groupBy(\"age\").count().show()\n+---+-----+\n|age|count|\n+---+-----+\n| 20| 1|\n| 30| 1|\n| 40| 1|\n+---+-----+ When developing a program in an IDE like IntelliJ IDEA, if you need to convert between RDD and DataFrame or DataSet, you must import import spark.implicits._. Here, spark is not a package name in Scala, but the variable name of the created SparkSession object. Therefore, you must create a SparkSession object before importing. The spark object cannot be declared with var because Scala only supports importing objects declared with val.\nIn spark-shell, no import is needed as it is done automatically.scala&gt; val idRDD = sc.textFile(\"data/id.txt\")\nscala&gt; idRDD.toDF(\"id\").show()\n+---+\n| id|\n+---+\n| 1|\n| 2|\n| 3|\n| 4|\n+---+\nIn practical development, it is common to use case classes to convert RDD to DataFrame.scala&gt; case class User(name: String, age: Int)\ndefined class User\nscala&gt; sc.makeRDD(List((\"zhangsan\", 30), (\"lisi\", 40))).map(t =&gt; User(t._1, t._2)).toDF().show()\n+--------+---+\n| name|age|\n+--------+---+\n|zhangsan| 30|\n| lisi| 40|\n+--------+---+\nA DataFrame is essentially an RDD with schema information, so you can directly get the underlying RDD.scala&gt; val df = sc.makeRDD(List((\"zhangsan\", 30), (\"lisi\", 40))).map(t =&gt; User(t._1, t._2)).toDF()\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\nscala&gt; val rdd = df.rdd\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at &lt;console&gt;:25\nscala&gt; val array = rdd.collect()\narray: Array[org.apache.spark.sql.Row] = Array([zhangsan,30], [lisi,40])\nNote: The resulting RDD has elements of type Row.scala&gt; array(0)\nres28: org.apache.spark.sql.Row = [zhangsan,30]\nscala&gt; array(0)(0)\nres29: Any = zhangsan\nscala&gt; array(0).getAs[String](\"name\")\nres30: String = zhangsan\nA DataSet is a strongly-typed collection of data that requires corresponding type information. Using Case Class Sequence to Create DataSet:\nscala&gt; case class Person(name: String, age: Long)\ndefined class Person\nscala&gt; val caseClassDS = Seq(Person(\"zhangsan\", 2)).toDS()\ncaseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]\nscala&gt; caseClassDS.show()\n+---------+---+\n| name|age|\n+---------+---+\n| zhangsan| 2|\n+---------+---+ Using a Sequence of Basic Types to Create DataSet:\nscala&gt; val ds = Seq(1, 2, 3, 4, 5).toDS\nds: org.apache.spark.sql.Dataset[Int] = [value: int]\nscala&gt; ds.show()\n+-----+\n|value|\n+-----+\n| 1|\n| 2|\n| 3|\n| 4|\n| 5|\n+-----+ Note: In practical use, it is rare to convert a sequence into a DataSet directly; it is more common to obtain a DataSet from an RDD.SparkSQL can automatically convert an RDD containing case classes to a DataSet. The case class defines the structure of the table, and the attributes of the case class become the columns of the table through reflection. Case classes can include complex structures like Seq or Array.scala&gt; case class User(name: String, age: Int)\ndefined class User\nscala&gt; sc.makeRDD(List((\"zhangsan\", 30), (\"lisi\", 49))).map(t =&gt; User(t._1, t._2)).toDS()\nres11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]\nA DataSet is essentially an RDD with additional schema information, so you can directly get the underlying RDD.scala&gt; case class User(name: String, age: Int)\ndefined class User\nscala&gt; val ds = sc.makeRDD(List((\"zhangsan\", 30), (\"lisi\", 49))).map(t =&gt; User(t._1, t._2)).toDS()\nds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]\nscala&gt; val rdd = ds.rdd\nrdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at &lt;console&gt;:25\nscala&gt; rdd.collect\nres12: Array[User] = Array(User(zhangsan, 30), User(lisi, 49))\nDataFrame is a special case of DataSet, so they can be converted to each other. Converting DataFrame to DataSet:\nscala&gt; case class User(name: String, age: Int)\ndefined class User\nscala&gt; val df = sc.makeRDD(List((\"zhangsan\", 30), (\"lisi\", 49))).toDF(\"name\", \"age\")\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\nscala&gt; val ds = df.as[User]\nds: org.apache.spark.sql.Dataset[User] = [name: string, age: int] Converting DataSet to DataFrame:\nscala&gt; val ds = df.as[User]\nds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]\nscala&gt; val df = ds.toDF()\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int] ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"SparkSession","level":3,"id":"SparkSession_0"},{"heading":"DataFrame","level":3,"id":"DataFrame_0"},{"heading":"Creation","level":4,"id":"Creation_0"},{"heading":"SQL Style Syntax","level":4,"id":"SQL_Style_Syntax_0"},{"heading":"DSL Syntax","level":4,"id":"DSL_Syntax_0"},{"heading":"Converting RDD to DataFrame","level":4,"id":"Converting_RDD_to_DataFrame_0"},{"heading":"Converting DataFrame to RDD","level":4,"id":"Converting_DataFrame_to_RDD_0"},{"heading":"DataSet","level":3,"id":"DataSet_0"},{"heading":"Creating DataSet","level":4,"id":"Creating_DataSet_0"},{"heading":"Converting RDD to DataSet","level":4,"id":"Converting_RDD_to_DataSet_0"},{"heading":"Converting DataSet to RDD","level":4,"id":"Converting_DataSet_to_RDD_0"},{"heading":"Converting DataFrame to DataSet","level":4,"id":"Converting_DataFrame_to_DataSet_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/2.-spark-sql/2.-coding.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641591,"modifiedTime":1737554783000,"sourceSize":9653,"sourcePath":"Big Data/3. Spark/2. Spark SQL/2. Coding.md","exportPath":"big-data/3.-spark/2.-spark-sql/2.-coding.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html":{"title":"3. RDD & DataFrame & DataSet","icon":"","description":"\nRDD, DataFrame, and DataSet are all distributed elastic datasets in the Spark platform, facilitating the processing of large-scale data.\nAll three exhibit lazy evaluation, meaning that transformations (like map) are not executed immediately. They only start processing when an action (like foreach) is invoked.\nThey share many common functions such as filter, sort, etc.\nOperations on DataFrame and DataSet often require the import import spark.implicits._ (import this after creating the SparkSession object).\nAll three automatically cache computations based on Spark's memory availability, preventing memory overflow even with large data volumes.\nThey all have the concept of partitions.\nDataFrame and DataSet can use pattern matching to access the values and types of individual fields. RDD RDD is commonly used with Spark MLlib.\nRDD does not support SparkSQL operations. DataFrame Unlike RDD and DataSet, each row in a DataFrame has a fixed type of Row, and the values of each column cannot be directly accessed without parsing to get the values of individual fields.\nDataFrame and DataSet both support SparkSQL operations like select, groupBy, and can register temporary tables/views for SQL operations.\nDataFrame and DataSet support convenient saving methods, such as saving as CSV with headers, making column names clear at a glance. DataSet DataSet and DataFrame have identical member functions; the only difference is the data type of each row. A DataFrame is actually a special case of DataSet: type DataFrame = Dataset[Row].\nDataFrame can also be called Dataset[Row]. Each row's type is Row, and without parsing, the specific fields and their types cannot be known. You can use the getAs method or pattern matching to extract specific fields.\nIn DataSet, the type of each row is not fixed. After defining a case class, you can freely access the information of each row. “rddCompare.png” could not be found.In practice, development is usually done using IntelliJ IDEA.&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3. RDD &amp; DataFrame &amp; DataSet","level":1,"id":"3._RDD_&_DataFrame_&_DataSet_0"},{"heading":"Commonalities Among RDD, DataFrame, and DataSet","level":3,"id":"Commonalities_Among_RDD,_DataFrame,_and_DataSet_0"},{"heading":"Differences Between RDD, DataFrame, and DataSet","level":3,"id":"Differences_Between_RDD,_DataFrame,_and_DataSet_0"},{"heading":"Converting Between RDD, DataFrame, and DataSet","level":3,"id":"Converting_Between_RDD,_DataFrame,_and_DataSet_0"},{"heading":"Developing SparkSQL in IntelliJ IDEA","level":3,"id":"Developing_SparkSQL_in_IntelliJ_IDEA_0"},{"heading":"Adding Dependencies","level":4,"id":"Adding_Dependencies_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641592,"modifiedTime":1737554783000,"sourceSize":4342,"sourcePath":"Big Data/3. Spark/2. Spark SQL/3. RDD & DataFrame & DataSet.md","exportPath":"big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html":{"title":"4. User-Defined Functions","icon":"","description":"Users can add custom functions using the spark.udf functionality to achieve custom behavior. Create DataFrame:\nscala&gt; val df = spark.read.json(\"data/user.json\")\ndf: org.apache.spark.sql.DataFrame = [age: bigint, username: string] Register UDF:\nscala&gt; spark.udf.register(\"addName\", (x: String) =&gt; \"Name:\" + x)\nres9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;, StringType, Some(List(StringType))) Create Temporary Table:\nscala&gt; df.createOrReplaceTempView(\"people\") Apply UDF:\nscala&gt; spark.sql(\"SELECT addName(username), age FROM people\").show() Both strongly typed Dataset and weakly typed DataFrame provide built-in aggregate functions such as count(), countDistinct(), avg(), max(), and min(). In addition, users can define their own custom aggregate functions by extending UserDefinedAggregateFunction for weakly typed UDAFs. From Spark 3.0, UserDefinedAggregateFunction is deprecated, and users are encouraged to use strongly typed Aggregator. Using RDD:\nval conf: SparkConf = new SparkConf().setAppName(\"app\").setMaster(\"local[*]\")\nval sc: SparkContext = new SparkContext(conf)\nval res: (Int, Int) = sc.makeRDD(List((\"zhangsan\", 20), (\"lisi\", 30), (\"wangwu\", 40))).map { case (name, age) =&gt; (age, 1)\n}.reduce { (t1, t2) =&gt; (t1._1 + t2._1, t1._2 + t2._2)\n}\nprintln(res._1 / res._2)\n// Close connection\nsc.stop() Using Accumulator:\nclass MyAC extends AccumulatorV2[Int, Int] { var sum: Int = 0 var count: Int = 0 override def isZero: Boolean = sum == 0 &amp;&amp; count == 0 override def copy(): AccumulatorV2[Int, Int] = { val newMyAC = new MyAC newMyAC.sum = this.sum newMyAC.count = this.count newMyAC } override def reset(): Unit = { sum = 0 count = 0 } override def add(v: Int): Unit = { sum += v count += 1 } override def merge(other: AccumulatorV2[Int, Int]): Unit = { other match { case o: MyAC =&gt; { sum += o.sum count += o.count } case _ =&gt; } } override def value: Int = sum / count\n} Using UDAF - Weakly Typed:\n// Define a class extending UserDefinedAggregateFunction and override its methods\nclass MyAverageUDAF extends UserDefinedAggregateFunction { // Data type of the input parameter for the aggregate function def inputSchema: StructType = StructType(Array(StructField(\"age\", IntegerType))) // Data type of the buffer schema (sum, count) def bufferSchema: StructType = StructType(Array(StructField(\"sum\", LongType), StructField(\"count\", LongType))) // Data type of the return value def dataType: DataType = DoubleType // Whether this function is deterministic (returns the same output for the same input) def deterministic: Boolean = true // Initialize the buffer def initialize(buffer: MutableAggregationBuffer): Unit = { buffer(0) = 0L // Sum of ages buffer(1) = 0L // Count of ages } // Update the buffer with an input row def update(buffer: MutableAggregationBuffer, input: Row): Unit = { if (!input.isNullAt(0)) { buffer(0) = buffer.getLong(0) + input.getInt(0) buffer(1) = buffer.getLong(1) + 1 } } // Merge two buffers def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) } // Calculate the final result def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)\n} // Create and register the UDAF\nval myAverage = new MyAverageUDAF\nspark.udf.register(\"avgAge\", myAverage)\nspark.sql(\"SELECT avgAge(age) FROM user\").show() Using UDAF - Strongly Typed:\n// Input data type\ncase class User01(username: String, age: Long) // Buffer schema\ncase class AgeBuffer(var sum: Long, var count: Long) // Define a class extending org.apache.spark.sql.expressions.Aggregator and override its methods\nclass MyAverageUDAF1 extends Aggregator[User01, AgeBuffer, Double] { override def zero: AgeBuffer = AgeBuffer(0L, 0L) override def reduce(b: AgeBuffer, a: User01): AgeBuffer = { b.sum += a.age b.count += 1 b } override def merge(b1: AgeBuffer, b2: AgeBuffer): AgeBuffer = { b1.sum += b2.sum b1.count += b2.count b1 } override def finish(buff: AgeBuffer): Double = buff.sum.toDouble / buff.count // Encoders for serialization, fixed format override def bufferEncoder: Encoder[AgeBuffer] = Encoders.product override def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n} // Create and register the UDAF\nval myAverageUDAF1 = new MyAverageUDAF1\nval col: TypedColumn[User01, Double] = myAverageUDAF1.toColumn // Create Dataset and perform aggregation\nval ds: Dataset[User01] = df.as[User01]\nds.select(col).show() // In Spark 3.0, use strong typed Aggregator instead of UserDefinedAggregateFunction\nval udaf = new MyAverageUDAF1\nspark.udf.register(\"avgAge\", functions.udaf(udaf))\nspark.sql(\"SELECT avgAge(age) FROM user\").show() // Case class for buffer\ncase class Buff(var sum: Long, var cnt: Long) // Define strong typed UDAF\nclass MyAvgAgeUDAF extends Aggregator[Long, Buff, Double] { override def zero: Buff = Buff(0, 0) override def reduce(b: Buff, a: Long): Buff = { b.sum += a b.cnt += 1 b } override def merge(b1: Buff, b2: Buff): Buff = { b1.sum += b2.sum b1.cnt += b2.cnt b1 } override def finish(reduction: Buff): Double = reduction.sum.toDouble / reduction.cnt override def bufferEncoder: Encoder[Buff] = Encoders.product override def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n} // Register and use strong typed UDAF\nval udaf = new MyAvgAgeUDAF\nspark.udf.register(\"avgAge\", functions.udaf(udaf))\nspark.sql(\"SELECT avgAge(age) FROM user\").show() ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"User-Defined Functions","level":3,"id":"User-Defined_Functions_0"},{"heading":"UDF (User-Defined Function)","level":4,"id":"UDF_(User-Defined_Function)_0"},{"heading":"UDAF (User-Defined Aggregate Function)","level":4,"id":"UDAF_(User-Defined_Aggregate_Function)_0"},{"heading":"Example Use Case: Calculate Average Age","level":5,"id":"Example_Use_Case_Calculate_Average_Age_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641594,"modifiedTime":1737554783000,"sourceSize":6684,"sourcePath":"Big Data/3. Spark/2. Spark SQL/4. User-Defined Functions.md","exportPath":"big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html":{"title":"5. Data Loading and Saving","icon":"","description":"SparkSQL provides general methods for loading and saving data. The term \"general\" here means using the same API to read and save data in different formats based on the parameters provided. The default file format for reading and saving in SparkSQL is Parquet. Loading Data\nThe spark.read.load method is a general way to load data.\nscala&gt; spark.read.\ncsv\ntable format\ntext jdbc\njson textFile\nload option\noptions orc\nparquet schema To read data in different formats, you can set different parameters for each data format.\nspark.read.format(\"…\")[.option(\"…\")].load(\"…\") format(\"…\"): Specifies the data format to load, including \"csv\", \"jdbc\", \"json\", \"orc\", \"parquet\", and \"textFile\".\nload(\"…\"): The path to load data in the specified format.\noption(\"…\"): For \"jdbc\" format, necessary JDBC parameters like url, user, password, and dbtable. Typically, we use the read API to load a file into a DataFrame and then query it. However, we can also query the file directly using format. and the file path.\nscala&gt; spark.sql(\"SELECT * FROM json.`/opt/module/data/user.json`\").show() Saving Data\nThe df.write.save method is a general way to save data.\ndf.write.format(\"…\")[.option(\"…\")].save(\"…\") format(\"…\"): Specifies the data format to save, including \"csv\", \"jdbc\", \"json\", \"orc\", \"parquet\", and \"textFile\".\nsave(\"…\"): The path to save data in the specified format.\noption(\"…\"): For \"jdbc\" format, necessary JDBC parameters like url, user, password, and dbtable. Save operations can use SaveMode to specify how to handle existing data, set using the mode() method. These SaveMode settings are not atomic operations and do not involve locking. df.write.mode(\"append\").json(\"/opt/module/data/output\") Parquet is the default data source in Spark SQL, an efficient columnar storage format for nested data. With Parquet files, Spark SQL can perform all operations easily without needing to specify the format. Loading Data\nval df = spark.read.parquet(\"path/to/parquet/file\") Saving Data\ndf.write.parquet(\"path/to/output\") Spark SQL can automatically infer the schema of a JSON dataset and load it as a Dataset[Row]. Use SparkSession.read.json() to load JSON files. Note that Spark expects each line in the JSON file to be a valid JSON object. Import Implicit Conversions\nimport spark.implicits._ Load JSON File\nval df = spark.read.json(\"data/user.json\") Create Temporary Table\ndf.createOrReplaceTempView(\"people\") Query Data\nval teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\nteenagerNamesDF.show()\n+------+\n| name|\n+------+\n|Justin|\n+------+ ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"General Methods for Loading and Saving Data","level":3,"id":"General_Methods_for_Loading_and_Saving_Data_0"},{"heading":"Parquet","level":3,"id":"Parquet_0"},{"heading":"JSON","level":3,"id":"JSON_0"},{"heading":"CSV","level":3,"id":"CSV_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641595,"modifiedTime":1737554783000,"sourceSize":11623,"sourcePath":"Big Data/3. Spark/2. Spark SQL/5. Data Loading and Saving.md","exportPath":"big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/3.-streaming/0.-spark-streaming.html":{"title":"0. Spark Streaming","icon":"","description":"Spark Streaming makes it easier to build scalable and fault-tolerant streaming applications.Spark Streaming is used for processing streaming data. It supports various data input sources such as Kafka, Flume, Twitter, ZeroMQ, and simple TCP sockets. Once data is input, it can be processed using Spark's high-level operations like map, reduce, join, and window. The results can be stored in many places, including HDFS, databases, etc. Similar to how Spark operates on RDDs, Spark Streaming uses discretized streams (DStream) as its abstraction, representing a sequence of data over time. Internally, the data received in each time interval is stored as an RDD, and a DStream is a sequence of these RDDs (hence the term \"discretized\"). Simply put, a DStream is a wrapper around RDDs for real-time data processing.\nEase of Use\nFault Tolerance\nSeamless Integration into the Spark Ecosystem\n“SparkStreaming.png” could not be found.In versions before Spark 1.5, users could limit the rate at which the Receiver receives data by setting a static configuration parameter spark.streaming.receiver.maxRate. While this could prevent memory overflow by matching the data receiving rate to the processing capacity, it introduced other issues. For example, if the data production rate from the producer exceeds maxRate and the cluster's processing capacity is also higher than maxRate, it could lead to underutilized resources. To better coordinate the data receiving rate and processing capacity, starting from version 1.5, Spark Streaming can dynamically control the data receiving rate to match the cluster's processing capacity.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Streaming","level":3,"id":"Streaming_0"},{"heading":"Features of Spark Streaming","level":4,"id":"Features_of_Spark_Streaming_0"},{"heading":"Spark Streaming Architecture","level":4,"id":"Spark_Streaming_Architecture_0"},{"heading":"Backpressure Mechanism","level":4,"id":"Backpressure_Mechanism_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/3.-streaming/0.-spark-streaming.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641623,"modifiedTime":1737554783000,"sourceSize":6251,"sourcePath":"Big Data/3. Spark/3. Streaming/0. Spark Streaming.md","exportPath":"big-data/3.-spark/3.-streaming/0.-spark-streaming.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/3.-streaming/1.-kafka-data-source.html":{"title":"1. Kafka Data Source","icon":"","description":"Receiver API: Requires a dedicated Executor to receive data, then forwards it to other Executors for computation. The issue with this approach is that the speed of the receiving Executor and the computing Executors can differ. Particularly, when the receiving Executor's speed exceeds that of the computing Executors, it can lead to memory overflow in the computation nodes. This method was provided in earlier versions but is no longer suitable in current versions.Direct API: The computing Executors actively consume data from Kafka, controlling the speed themselves. Requirement: Read data from Kafka using Spark Streaming, perform simple calculations on the data, and print the results to the console. Dependencies: &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt;\n&lt;/dependency&gt; Code Example:\npackage com.atguigu.kafka; import org.apache.spark.SparkConf;\nimport org.apache.spark.streaming.dstream.ReceiverInputDStream;\nimport org.apache.spark.streaming.kafka.KafkaUtils;\nimport org.apache.spark.streaming.{Seconds, StreamingContext}; public class ReceiverAPI { public static void main(String[] args) { // 1. Create SparkConf SparkConf sparkConf = new SparkConf().setAppName(\"ReceiverWordCount\").setMaster(\"local[*]\"); // 2. Create StreamingContext StreamingContext ssc = new StreamingContext(sparkConf, Seconds(3)); // 3. Read Kafka data to create DStream (based on Receiver mode) ReceiverInputDStream&lt;(String, String)&gt; kafkaDStream = KafkaUtils.createStream( ssc, \"linux1:2181,linux2:2181,linux3:2181\", \"atguigu\", Map&lt;String, Integer&gt;(\"atguigu\" -&gt; 1) ); // 4. Compute WordCount kafkaDStream.map((key, value) -&gt; (value, 1)) .reduceByKey((a, b) -&gt; a + b) .print(); // 5. Start the task ssc.start(); ssc.awaitTermination(); }\n} Requirement: Read data from Kafka using Spark Streaming, perform simple calculations on the data, and print the results to the console. Dependencies: ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Version Selection","level":3,"id":"Version_Selection_0"},{"heading":"Kafka 0-8 Receiver Mode","level":3,"id":"Kafka_0-8_Receiver_Mode_0"},{"heading":"Kafka 0-10 Direct Mode","level":3,"id":"Kafka_0-10_Direct_Mode_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/3.-streaming/1.-kafka-data-source.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641624,"modifiedTime":1737554783000,"sourceSize":4788,"sourcePath":"Big Data/3. Spark/3. Streaming/1. Kafka Data Source.md","exportPath":"big-data/3.-spark/3.-streaming/1.-kafka-data-source.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/3.-streaming/2.-dstream-transformations.html":{"title":"2. DStream Transformations","icon":"","description":"Operations on DStream are similar to those on RDD and are divided into Transformations and Output Operations. Additionally, there are some special primitives in transformation operations, such as updateStateByKey(), transform(), and various window-related primitives.Stateless transformations apply simple RDD transformations to each batch, i.e., transforming each RDD within a DStream. Note that for key-value DStream transformations (such as reduceByKey()), you need to import StreamingContext._ in Scala to use them. It's important to remember that even though these functions appear to act on the entire stream, each DStream internally consists of many RDDs (batches), and stateless transformations are applied to each RDD individually. For example, reduceByKey() will reduce data within each time interval but will not reduce data across different intervals.The transform operation allows arbitrary RDD-to-RDD functions to be executed on DStreams. Even if these functions are not exposed in the DStream API, transform can conveniently extend the Spark API. This function is scheduled once per batch, essentially applying a transformation to the RDDs within the DStream.object Transform { def main(args: Array[String]): Unit = { // Create SparkConf val sparkConf: SparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"WordCount\") // Create StreamingContext val ssc = new StreamingContext(sparkConf, Seconds(3)) // Create DStream val lineDStream: ReceiverInputDStream[String] = ssc.socketTextStream(\"linux1\", 9999) // RDD transformation val wordAndCountDStream: DStream[(String, Int)] = lineDStream.transform { rdd =&gt; val words: RDD[String] = rdd.flatMap(_.split(\" \")) val wordAndOne: RDD[(String, Int)] = words.map((_, 1)) val value: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _) value } // Print results wordAndCountDStream.print() // Start the computation ssc.start() ssc.awaitTermination() }\n}\nJoining two streams requires the batch sizes of both streams to be the same to trigger computation simultaneously. The join operation processes the RDDs in the current batch of both streams, similar to joining two RDDs.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"DStream Transformations","level":3,"id":"DStream_Transformations_0"},{"heading":"Stateless Transformations","level":4,"id":"Stateless_Transformations_0"},{"heading":"Transform","level":4,"id":"Transform_0"},{"heading":"Join","level":4,"id":"Join_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/3.-streaming/2.-dstream-transformations.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641651,"modifiedTime":1737554783000,"sourceSize":7886,"sourcePath":"Big Data/3. Spark/3. Streaming/2. DStream Transformations.md","exportPath":"big-data/3.-spark/3.-streaming/2.-dstream-transformations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/3.-streaming/3.-dstream-output.html":{"title":"3. DStream Output","icon":"","description":"Output operations specify the actions to be performed on the data after it has been transformed through various operations (e.g., pushing the results to an external database or outputting to the screen). Similar to the lazy evaluation of RDDs, if a DStream and its derived DStreams do not execute an output operation, they will not be evaluated. If no output operation is set in the StreamingContext, the entire context will not start.\nThe output operations are as follows: print(): Prints the first 10 elements of each batch of the DStream on the driver node running the streaming application. This is used for development and debugging. In the Python API, the same operation is called print(). saveAsTextFiles(prefix, [suffix]): Saves the content of the DStream as text files. Each batch's storage file name is based on the prefix and suffix parameters, forming \"prefix-Time_IN_MS[.suffix]\". saveAsObjectFiles(prefix, [suffix]): Saves the data in the DStream as SequenceFiles in a Java object serialization format. Each batch's storage file name is based on the prefix and suffix parameters, forming \"prefix-TIME_IN_MS[.suffix]\". Currently unavailable in Python. saveAsHadoopFiles(prefix, [suffix]): Saves the data in the DStream as Hadoop files. Each batch's storage file name is based on the prefix and suffix parameters, forming \"prefix-TIME_IN_MS[.suffix]\". Currently unavailable in Python. foreachRDD(func): The most general output operation that applies a function func to each RDD generated from the stream. The function func should push the data in each RDD to an external system, such as saving the RDD to a file or writing it to a database over the network. The general output operation foreachRDD() is used to run arbitrary computations on the RDDs in a DStream. This is similar to transform(), as it allows us to access any RDD. In foreachRDD(), you can reuse all the action operations available in Spark. For instance, a common use case is writing data to an external database like MySQL.\nNotes:\nConnections should not be written at the driver level (serialization issues).\nIf written inside foreach, a connection will be created for each data item in every RDD, which is inefficient.\nUse foreachPartition to create connections within each partition (improving efficiency).\nStreaming tasks need to run 24/7, but sometimes you need to stop the program for code upgrades. Since it is a distributed program, killing each process individually is impractical, so configuring a graceful shutdown becomes crucial. Use an external file system to control the internal program shutdown.\nMonitorStop\nimport java.net.URI;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.{FileSystem, Path};\nimport org.apache.spark.streaming.{StreamingContext, StreamingContextState}; class MonitorStop(ssc: StreamingContext) extends Runnable { override def run(): Unit = { val fs: FileSystem = FileSystem.get(new URI(\"hdfs://linux1:9000\"), new Configuration(), \"atguigu\"); while (true) { try { Thread.sleep(5000); } catch { case e: InterruptedException =&gt; e.printStackTrace(); } val state: StreamingContextState = ssc.getState; val bool: Boolean = fs.exists(new Path(\"hdfs://linux1:9000/stopSpark\")); if (bool) { if (state == StreamingContextState.ACTIVE) { ssc.stop(stopSparkContext = true, stopGracefully = true); System.exit(0); } } } }\n} SparkTest\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Output operations","level":3,"id":"Output_operations_0"},{"heading":"Graceful Shutdown","level":3,"id":"Graceful_Shutdown_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/3.-streaming/3.-dstream-output.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641652,"modifiedTime":1737554783000,"sourceSize":5237,"sourcePath":"Big Data/3. Spark/3. Streaming/3. DStream Output.md","exportPath":"big-data/3.-spark/3.-streaming/3.-dstream-output.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/4.-core/0.-spark-kernel.html":{"title":"0. Spark Kernel","icon":"","description":"The Spark kernel generally refers to the core operating mechanism of Spark, including the running mechanism of Spark's core components, Spark's task scheduling mechanism, Spark's memory management mechanism, and the operating principles of Spark's core functions. Mastering the principles of the Spark kernel can help us better design Spark code and accurately identify the root cause of problems that occur during the execution of projects.The Spark driver node is used to execute the main method in Spark tasks and is responsible for the actual execution of the code.The Driver is primarily responsible for the following during the execution of a Spark job:\nConverting user programs into jobs (Jobs).\nScheduling tasks (Tasks) among Executors.\nTracking the execution status of Executors.\nDisplaying query execution status through the UI.\nThe Spark Executor object is responsible for running specific tasks in a Spark job, with each task being independent of others. When a Spark application starts, the ExecutorBackend node is launched simultaneously and remains throughout the lifecycle of the Spark application. If an ExecutorBackend node fails or crashes, the Spark application can continue executing by rescheduling the tasks on the failed node to other Executor nodes.Executors have two core functions:\nRunning the tasks that make up a Spark application and returning the results to the Driver.\nProviding in-memory storage for RDDs that are requested to be cached by the user program through their own Block Manager. RDDs are directly cached within the Executor process, allowing tasks to utilize cached data to speed up computation during runtime.\nThe core steps of the basic submission process for deploying a Spark application are as follows:\nAfter submitting a task, the Driver program starts first.\nThe Driver then registers the application with the cluster manager.\nSubsequently, the cluster manager allocates and starts Executors based on the configuration file of the task.\nThe Driver begins executing the main function. Spark queries are lazily executed, and when an Action operator is executed, a reverse calculation begins, dividing Stages based on wide dependencies. Each Stage corresponds to a TaskSet, which contains multiple Tasks, and the available Executor resources are located and scheduled.\nFollowing the principle of data locality, Tasks are distributed to specified Executors for execution. During task execution, Executors continuously communicate with the Driver, reporting the execution status of the tasks.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Core Components","level":4,"id":"Core_Components_0"},{"heading":"Driver","level":5,"id":"Driver_0"},{"heading":"Executor","level":5,"id":"Executor_0"},{"heading":"General Execution Process","level":4,"id":"General_Execution_Process_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/4.-core/0.-spark-kernel.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641654,"modifiedTime":1737554783000,"sourceSize":2681,"sourcePath":"Big Data/3. Spark/4. Core/0. Spark Kernel.md","exportPath":"big-data/3.-spark/4.-core/0.-spark-kernel.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/4.-core/1.-deployment.html":{"title":"1. Deployment","icon":"","description":"Spark supports various cluster managers, including: Standalone:\nThe standalone mode is Spark's built-in simple cluster manager. It comes with complete services and can be deployed independently to a cluster without relying on any other resource management system. Using Standalone, it is easy to set up a cluster. Hadoop YARN:\nA unified resource management mechanism that can run multiple computing frameworks such as MapReduce (MR), Storm, etc. Depending on the location of the Driver in the cluster, it is divided into yarn-client (outside the cluster) and yarn-cluster (inside the cluster). Apache Mesos:\nA powerful distributed resource management framework that allows multiple different frameworks, including Yarn, to be deployed on it. Kubernetes (K8S):\nA containerized deployment environment. In addition to these general cluster managers, Spark also provides a local cluster deployment mode and a Windows environment for user testing and learning. Since Hadoop YARN is the most commonly used cluster manager in production environments, our focus is on Spark cluster deployment in Hadoop YARN mode.\nExecute a script to submit the task, which actually starts a JVM process of SparkSubmit.\nThe main method in the SparkSubmit class reflects and calls the main method of YarnClusterApplication.\nYarnClusterApplication creates a Yarn client and then sends an execution command to the Yarn server: bin/java ApplicationMaster.\nAfter receiving the command, the Yarn framework starts the ApplicationMaster on the specified NodeManager (NM).\nApplicationMaster starts the Driver thread and executes the user's job.\nApplicationMaster registers with the ResourceManager (RM) and requests resources.\nAfter obtaining resources, ApplicationMaster sends a command to NodeManager: bin/java CoarseGrainedExecutorBackend.\nThe CoarseGrainedExecutorBackend process receives the message, communicates with the Driver, registers the started Executor, and then starts the computing object Executor to wait for receiving tasks.\nThe Driver thread continues to execute to complete job scheduling and task execution.\nThe Driver assigns tasks and monitors task execution.\nNote: SparkSubmit, ApplicationMaster, and CoarseGrainedExecutorBackend are independent processes; Driver is an independent thread; Executor and YarnClusterApplication are objects.“yarnCluster.png” could not be found.\nExecute a script to submit the task, which actually starts a JVM process of SparkSubmit.\nThe main method in the SparkSubmit class reflects and calls the main method of the user code.\nThe Driver thread starts, executes the user's job, and creates ScheduleBackend.\nYarnClientSchedulerBackend sends a command to ResourceManager: bin/java ExecutorLauncher.\nAfter receiving the command, the Yarn framework starts ExecutorLauncher on the specified NodeManager (actually still calling the main method of ApplicationMaster).\nApplicationMaster registers with ResourceManager and requests resources.\nAfter obtaining resources, ApplicationMaster sends a command to NodeManager: bin/java CoarseGrainedExecutorBackend.\nThe CoarseGrainedExecutorBackend process receives the message, communicates with the Driver, registers the started Executor, and then starts the computing object Executor to wait for receiving tasks.\nThe Driver assigns tasks and monitors task execution.\nNote: SparkSubmit, ApplicationMaster, and YarnCoarseGrainedExecutorBackend are independent processes; Executor and Driver are objects.“yarnClient.png” could not be found.The Standalone cluster has two important components: Master (ResourceManager):\nA process responsible for resource scheduling and allocation, as well as monitoring the cluster. Worker (NodeManager):\nA process that runs on a server in the cluster. It has two main responsibilities: storing partitions of RDDs in its memory and starting other processes and threads (Executors) to perform parallel processing and computation on RDD partitions. In Standalone Cluster mode, after submitting the task, the Master finds a Worker to start the Driver.\nThe Driver registers the application with the Master.\nThe Master allocates resources based on the submit script's requirements and starts Executors on the Worker nodes.\nAfter starting, Executors register back to the Driver.\nAfter all Executors are registered, the Driver starts executing the main function.\nWhen an Action operator is executed, stages are divided, and each stage generates a corresponding TaskSet.\nTasks are distributed to each Executor for execution.\nIn Standalone Client mode, the Driver runs on the local machine where the task is submitted.\nThe Driver registers the application with the Master.\nThe Master allocates resources based on the submit script's requirements and starts Executors on the Worker nodes.\nAfter starting, Executors register back to the Driver.\nAfter all Executors are registered, the Driver starts executing the main function.\nWhen an Action operator is executed, stages are divided, and each stage generates a corresponding TaskSet.\nTasks are distributed to each Executor for execution.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Spark Deployment Modes","level":3,"id":"Spark_Deployment_Modes_0"},{"heading":"YARN Mode Operation Mechanism","level":3,"id":"YARN_Mode_Operation_Mechanism_0"},{"heading":"YARN Cluster Mode","level":4,"id":"YARN_Cluster_Mode_0"},{"heading":"YARN Client Mode","level":4,"id":"YARN_Client_Mode_0"},{"heading":"Standalone Mode Operation Mechanism","level":4,"id":"Standalone_Mode_Operation_Mechanism_0"},{"heading":"Standalone Cluster Mode","level":5,"id":"Standalone_Cluster_Mode_0"},{"heading":"Standalone Client Mode","level":5,"id":"Standalone_Client_Mode_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/4.-core/1.-deployment.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641655,"modifiedTime":1737554783000,"sourceSize":5523,"sourcePath":"Big Data/3. Spark/4. Core/1. Deployment.md","exportPath":"big-data/3.-spark/4.-core/1.-deployment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/4.-core/2.-spark-communication-architecture.html":{"title":"2. Spark Communication Architecture","icon":"","description":"The evolution of the communication framework in Spark:\nIn the early versions of Spark, Akka was used as the internal communication component.\nIn Spark 1.3, the Netty communication framework was introduced to solve the problem of large data transmission during shuffles.\nIn Spark 1.6, both Akka and Netty could be configured for use. Netty fully implemented the functions of Akka within Spark.\nIn the Spark 2.x series, Akka was abandoned, and Netty was used exclusively.\nThe Spark 2.x versions use the Netty communication framework as the internal communication component. Spark's new RPC framework, based on Netty, was inspired by Akka's design and is based on the Actor model, as shown in the figure below:“actor.png” could not be found.In the Spark communication framework, each component (Client/Master/Worker) can be considered an independent entity. These entities communicate with each other through messages. The relationship between the components is shown in the diagram below:“sparkMessage.png” could not be found.Each Endpoint (Client/Master/Worker) has one InBox and N OutBoxes (N &gt;= 1, where N depends on how many other endpoints the current endpoint communicates with; each endpoint it communicates with corresponds to one OutBox). Messages received by the endpoint are written into the InBox, and messages to be sent are written into the OutBox and sent to the InBox of the other endpoint.\nDriver:\nclass DriverEndpoint extends IsolatedRpcEndpoint Executor:\nclass CoarseGrainedExecutorBackend extends IsolatedRpcEndpoint\nThe Spark communication architecture is shown in the diagram below:“SparkCommunicationArchitecture.png” could not be found. RpcEndpoint: The RPC communication endpoint. Each node (Client/Master/Worker) in Spark is called an RPC endpoint, implementing the RpcEndpoint interface. It designs different messages and business logic based on the needs of each endpoint. If it needs to send a message (inquiry), it calls the Dispatcher. In Spark, all endpoints have a lifecycle: Constructor\nonStart\nreceive*\nonStop RpcEnv: The RPC context environment that each RPC endpoint runs within is called RpcEnv. In the current version of Spark, NettyRpcEnv is used. Dispatcher: The message dispatcher, responsible for dispatching messages sent from or received by remote RPC endpoints to the appropriate inboxes (or outboxes). If the recipient of the message is the endpoint itself, the message is stored in the inbox; otherwise, it is placed in the outbox. Inbox: The message inbox. Each local RpcEndpoint corresponds to one inbox. When the Dispatcher inserts a message into the inbox, it adds the corresponding EndpointData to the internal ReceiverQueue. Additionally, when the Dispatcher is created, a separate thread is started to poll the ReceiverQueue to consume the inbox messages. RpcEndpointRef: A reference to a remote RpcEndpoint. When we need to send a message to a specific RpcEndpoint, we typically obtain a reference to that RpcEndpoint and send the message through that reference. OutBox: The message outbox. For the current RpcEndpoint, each target RpcEndpoint corresponds to an outbox. If messages need to be sent to multiple target RpcEndpoints, there will be multiple OutBoxes. Once a message is placed in the outbox, it is immediately sent out using the TransportClient. The process of placing the message in the outbox and sending it is handled within the same thread. RpcAddress: Represents the address of the remote RpcEndpointRef, consisting of a host and port. TransportClient: The Netty communication client. Each outbox corresponds to a TransportClient, which continuously polls the outbox and, based on the receiver information of the outbox message, requests the corresponding remote TransportServer. TransportServer: The Netty communication server. Each RpcEndpoint corresponds to a TransportServer, which receives remote messages and calls the Dispatcher to distribute messages to the appropriate inbox or outbox. In production environments, the deployment mode of a Spark cluster is generally YARN-Cluster mode. For subsequent kernel analysis, we assume the cluster is deployed in YARN-Cluster mode. The Driver thread mainly initializes the SparkContext object, prepares the required context for operation, maintains an RPC connection with the ApplicationMaster, and requests resources from the ApplicationMaster. Meanwhile, it begins scheduling tasks based on user business logic, dispatching tasks to available Executors.When the ResourceManager returns Container resources to the ApplicationMaster, the ApplicationMaster attempts to start Executor processes in the corresponding Containers. Once the Executor process is up, it registers back to the Driver. After successful registration, it maintains a heartbeat with the Driver while waiting for the Driver to dispatch tasks. Once the tasks are dispatched and executed, the task status is reported back to the Driver.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Spark Communication Endpoints","level":3,"id":"Spark_Communication_Endpoints_0"},{"heading":"Analysis of Spark Communication Architecture","level":4,"id":"Analysis_of_Spark_Communication_Architecture_0"},{"heading":"Spark Task Scheduling Mechanism","level":3,"id":"Spark_Task_Scheduling_Mechanism_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/4.-core/2.-spark-communication-architecture.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641656,"modifiedTime":1737554783000,"sourceSize":5340,"sourcePath":"Big Data/3. Spark/4. Core/2. Spark Communication Architecture.md","exportPath":"big-data/3.-spark/4.-core/2.-spark-communication-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/4.-core/3.-task-scheduling.html":{"title":"3. Task Scheduling","icon":"","description":"In production environments, Spark clusters are generally deployed in YARN-Cluster mode. For the subsequent kernel analysis, we will assume the cluster is deployed in YARN-Cluster mode. The Driver thread primarily initializes the SparkContext object, prepares the necessary context for operation, maintains an RPC connection with the ApplicationMaster to request resources, and starts scheduling tasks based on user business logic, dispatching tasks to available Executors.When the ResourceManager returns Container resources to the ApplicationMaster, the ApplicationMaster attempts to start the Executor processes in the corresponding Containers. Once the Executor process is up, it registers back to the Driver. After successful registration, it maintains a heartbeat with the Driver while waiting for the Driver to dispatch tasks. After the dispatched tasks are executed, the task status is reported back to the Driver.When the Driver starts, it prepares tasks according to the user program logic and gradually distributes tasks based on the available Executor resources. Before detailing task scheduling, let's explain a few concepts in Spark: a Spark application includes Job, Stage, and Task.\nJob: Triggered by an Action method, a new Job is created whenever an Action method is encountered.\nStage: A subset of a Job, divided by RDD wide dependencies (i.e., Shuffle). A new Stage is created whenever a Shuffle is encountered.\nTask: A subset of a Stage, measured by parallelism (number of partitions). The number of partitions determines the number of tasks.\nSpark's task scheduling is divided into two main paths: Stage-level scheduling and Task-level scheduling. The overall scheduling process is illustrated below:RDDs in Spark form a lineage (dependency) graph through their transformations, creating a DAG. An Action triggers a Job and schedules its execution, leading to the creation of two schedulers: DAGScheduler and TaskScheduler.\nDAGScheduler: Responsible for Stage-level scheduling, splitting Jobs into Stages and packaging each Stage into a TaskSet, which is then passed to TaskScheduler.\nTaskScheduler: Responsible for Task-level scheduling, distributing the TaskSet from DAGScheduler to Executors according to a specified scheduling strategy. The SchedulerBackend provides available resources, with multiple implementations interfacing with different resource management systems.\nDuring the initialization of the SparkContext, DAGScheduler, TaskScheduler, SchedulerBackend, and HeartbeatReceiver are initialized, and the SchedulerBackend and HeartbeatReceiver are started.The SchedulerBackend requests resources through the ApplicationMaster and continuously obtains suitable tasks from the TaskScheduler to dispatch to Executors. The HeartbeatReceiver receives heartbeat information from Executors, monitors their status, and notifies the TaskScheduler.Spark's task scheduling starts with DAG splitting, primarily handled by the DAGScheduler. When an Action operation is encountered, it triggers the calculation of a Job and submits it to the DAGScheduler. The following diagram illustrates the Job submission process:\nJob: Consists of the final RDD and the Action method.\nSparkContext: Submits the Job to the DAGScheduler, which splits the Job into several Stages based on the RDD's lineage graph (DAG). The split strategy is to trace back from the final RDD through dependencies, identifying wide dependencies (i.e., Shuffles) and splitting Stages accordingly. Narrow dependencies are included in the same Stage for pipelined computation. There are two types of Stages: ResultStage: The final Stage in the DAG, determined by the Action method.\nShuffleMapStage: Prepares data for downstream Stages. “shuffleMap.png” could not be found.The Job triggered by saveAsTextFile consists of RDD-3 and the saveAsTextFile method. Based on the dependencies between RDDs, the process starts from RDD-3 and traces back to RDD-0, splitting Stages at wide dependencies. RDD-3 is included in the final Stage (ResultStage), while RDD-0, RDD-1, and RDD-2 (narrow dependencies) are included in the same Stage for pipelined computation. This forms the ShuffleMapStage.The submission of a Stage depends on the execution of its parent Stage(s). A Stage is submitted only after its parent Stage(s) have completed execution. The Task information (partition and method details) is serialized and packaged into a TaskSet, which is then submitted to the TaskScheduler. The TaskScheduler monitors the Stage's execution status, rescheduling failed Stages if necessary (due to Executor loss or Fetch failures). Other types of Task failures are retried within the TaskScheduler.The TaskScheduler handles Task-level scheduling. The DAGScheduler packages Stages into TaskSets and submits them to the TaskScheduler, which encapsulates them into TaskSetManager and adds them to the scheduling queue. The structure of TaskSetManager is illustrated below.The TaskSetManager monitors and manages Tasks within the same Stage. The TaskScheduler schedules tasks based on TaskSetManager.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Task Scheduling Mechanism","level":3,"id":"Task_Scheduling_Mechanism_0"},{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Stage-Level Scheduling","level":3,"id":"Stage-Level_Scheduling_0"},{"heading":"Task-Level Scheduling","level":3,"id":"Task-Level_Scheduling_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/4.-core/3.-task-scheduling.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641658,"modifiedTime":1737554783000,"sourceSize":10236,"sourcePath":"Big Data/3. Spark/4. Core/3. Task Scheduling.md","exportPath":"big-data/3.-spark/4.-core/3.-task-scheduling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html":{"title":"4. Spark Shuffle Analysis","icon":"","description":"When dividing stages, the last stage is called the finalStage, which is essentially a ResultStage object. All preceding stages are referred to as ShuffleMapStage. The completion of a ShuffleMapStage is accompanied by the writing of shuffle files to disk.A ResultStage generally corresponds to the action operator in the code, meaning it applies a function to the dataset of each partition in the RDD, indicating the end of a job's execution.Let's clarify an assumption: each Executor has only one CPU core, meaning that regardless of how many task threads are allocated to an Executor, only one task thread can execute at a time.In the diagram below, there are three Reducers. Each Task calculates its hash (partitioner: hash/numReduce) and categorizes the data into three different categories. Each Task divides its data into three categories, and Reducers collect data from each Task that belongs to their respective categories, aggregating them into a large collection of the same category. Each Task outputs three local files. With four Mapper Tasks, a total of 4 Tasks x 3 categorized files = 12 local small files are produced.“hashShuffle.png” could not be found.The optimized HashShuffle process introduces a consolidation mechanism, which reuses buffers. This optimization is enabled by setting spark.shuffle.consolidateFiles to true (default is false). It is generally recommended to enable this option when using HashShuffleManager.There are still four Tasks, with data categorized into three types by the hash algorithm based on their keys. Regardless of how many tasks are in the same process, the same key will be placed in the same buffer. The data in the buffer is then written to local files based on the number of cores (one core holds data of only one type of key). In each process with one Task, three local files are written. With four Mapper Tasks, the total output is 2 Cores x 3 categorized files = 6 local small files.“optimizedHashShuffle.png” could not be found.In this mode, data is first written to a data structure. For reduceByKey, data is written to a Map, where it undergoes local aggregation while being written to memory. For Join operations, data is written to an ArrayList and directly to memory. If the threshold is reached, the data in the memory structure is written to disk, and the memory structure is cleared.Before spilling to disk, data is sorted by key. The sorted data is then written to disk files in batches, with the default batch size being 10,000 entries. Data is written to disk files using buffer overflow, where each overflow creates a disk file. This means a single Task process can generate multiple temporary files.Finally, in each Task, all temporary files are merged in the merge process. This involves reading all temporary files and writing them once to the final file, meaning all data for a Task is in one file. An index file is also written separately, indicating the start and end offsets of each Task's data in the file.“sortShuffle.png” could not be found.The conditions for triggering the bypass mechanism are as follows:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"ShuffleMapStage and ResultStage","level":3,"id":"ShuffleMapStage_and_ResultStage_0"},{"heading":"HashShuffle Analysis","level":3,"id":"HashShuffle_Analysis_0"},{"heading":"Unoptimized HashShuffle","level":4,"id":"Unoptimized_HashShuffle_0"},{"heading":"Optimized HashShuffle","level":4,"id":"Optimized_HashShuffle_0"},{"heading":"SortShuffle Analysis","level":3,"id":"SortShuffle_Analysis_0"},{"heading":"Regular SortShuffle","level":4,"id":"Regular_SortShuffle_0"},{"heading":"Bypass SortShuffle","level":4,"id":"Bypass_SortShuffle_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641659,"modifiedTime":1737554783000,"sourceSize":4646,"sourcePath":"Big Data/3. Spark/4. Core/4. Spark Shuffle Analysis.md","exportPath":"big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/5.-pyspark/0.-installation.html":{"title":"0. Installation","icon":"","description":"# Spark - The default distribution uses Hadoop 3.3 and Hive 2.3\npip install pyspark\n# Spark SQL\npip install pyspark[sql]\n# pandas API on Spark\npip install pyspark[pandas_on_spark] plotly # to plot your data, you can install plotly together.\n# Spark Connect\npip install pyspark[connect]\n# manually choose the mirror for faster downloading\nPYSPARK_RELEASE_MIRROR=http://mirror.apache-kr.org PYSPARK_HADOOP_VERSION=3 pip install # use -v option in pip to track the installation and download status\nPYSPARK_HADOOP_VERSION=3 pip install pyspark -v\n# create a new conda environment from your terminal and activate it\nconda create -n pyspark_env\nconda activate pyspark_env # Spark\nconda install -c conda-forge pyspark # can also add \"python=3.8 some_package [etc.]\" here\nNote that PySpark for conda is maintained separately by the community; while new versions generally get packaged quickly, the availability through conda(-forge) is not directly in sync with the PySpark release cycle.PySpark is included in the distributions available at the Apache Spark website. You can download a distribution you want from the site. tar xzvf spark-3.5.3-bin-hadoop3.tgz\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Using PyPI","level":3,"id":"Using_PyPI_0"},{"heading":"PySpark with/without a specific Hadoop version","level":3,"id":"PySpark_with/without_a_specific_Hadoop_version_0"},{"heading":"Using Conda","level":3,"id":"Using_Conda_0"},{"heading":"Manually Downloading","level":3,"id":"Manually_Downloading_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/5.-pyspark/0.-installation.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641660,"modifiedTime":1737554783000,"sourceSize":2905,"sourcePath":"Big Data/3. Spark/5. Pyspark/0. Installation.md","exportPath":"big-data/3.-spark/5.-pyspark/0.-installation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/5.-pyspark/1.-dataframe.html":{"title":"1. DataFrame","icon":"","description":"PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later.When actions such as collect() are explicitly called, the computation starts.PySpark applications start with initializing SparkSession which is the entry point of PySpark as below.from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nA PySpark DataFrame can be created via pyspark.sql. Create a PySpark DataFrame from a list of rows.from datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row df = spark.createDataFrame([ Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)), Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)), Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf\nCreate a PySpark DataFrame with an explicit schema.df = spark.createDataFrame([ (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)), (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)), (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\nCreate a PySpark DataFrame from a pandas DataFramepandas_df = pd.DataFrame({ 'a': [1, 2, 3], 'b': [2., 3., 4.], 'c': ['string1', 'string2', 'string3'], 'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)], 'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf\nThe DataFrames created above all have the same results and schema.# All DataFrames above result same.\ndf.show()\ndf.printSchema()\n+---+---+-------+----------+-------------------+\n| a| b| c| d| e|\n+---+---+-------+----------+-------------------+\n| 1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n| 2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n| 3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+ root |-- a: long (nullable = true) |-- b: double (nullable = true) |-- c: string (nullable = true) |-- d: date (nullable = true) |-- e: timestamp (nullable = true)\nEnable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter.spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\nThe rows can also be shown vertically.df.show(1, vertical=True)\n-RECORD 0------------------ a | 1 b | 2.0 c | string1 d | 2000-01-01 e | 2000-01-01 12:00:00\nonly showing top 1 row\nDataFrame’s schemadf.columns\n# ['a', 'b', 'c', 'd', 'e'] df.printSchema()\n# root\n# |-- a: long (nullable = true)\n# |-- b: double (nullable = true)\n# |-- c: string (nullable = true)\n# |-- d: date (nullable = true)\n# |-- e: timestamp (nullable = true) df.select(\"a\", \"b\", \"c\").describe().show()\n# +-------+---+---+-------+\n# |summary| a| b| c|\n# +-------+---+---+-------+\n# | count| 3| 3| 3|\n# | mean|2.0|3.0| null|\n# | stddev|1.0|1.0| null|\n# | min| 1|2.0|string1|\n# | max| 3|4.0|string3|\n# +-------+---+---+-------+ df.collect() # avoid throwing an out-of-memory exception\n# [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n# Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n# Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))] df.take(1)\n# [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))] df.toPandas()\n# conversion back to a pandas\nPySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance.df.a\n# Column&lt;b'a'&gt;\nColumnsfrom pyspark.sql import Column\nfrom pyspark.sql.functions import upper type(df.c) == type(upper(df.c)) == type(df.c.isNull())\n# True df.select(df.c).show()\n# +-------+\n# | c|\n# +-------+\n# |string1|\n# |string2|\n# |string3|\n# +-------+\nAssign new Column instancedf.withColumn('upper_c', upper(df.c)).show()\n# +---+---+-------+----------+-------------------+-------+\n# | a| b| c| d| e|upper_c|\n# +---+---+-------+----------+-------------------+-------+\n# | 1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n# | 2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n# | 3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n# +---+---+-------+----------+-------------------+-------+ df.filter(df.a == 1).show()\n# +---+---+-------+----------+-------------------+\n# | a| b| c| d| e|\n# +---+---+-------+----------+-------------------+\n# | 1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n# +---+---+-------+----------+-------------------+\nPySpark supports various UDFs and APIs to allow users to execute Python native functions.import pandas as pd\nfrom pyspark.sql.functions import pandas_udf @pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -&gt; pd.Series: # Simply plus one by using pandas Series. return series + 1 df.select(pandas_plus_one(df.a)).show() # +------------------+\n# |pandas_plus_one(a)|\n# +------------------+\n# | 2|\n# | 3|\n# | 4|\n# +------------------+ def pandas_filter_func(iterator): for pandas_df in iterator: yield pandas_df[pandas_df.a == 1] df.mapInPandas(pandas_filter_func, schema=df.schema).show() # +---+---+-------+----------+-------------------+\n# | a| b| c| d| e|\n# +---+---+-------+----------+-------------------+\n# | 1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n# +---+---+-------+----------+-------------------+\nsplit-apply-combine strategydf = spark.createDataFrame([ ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30], ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60], ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()\n# +-----+------+---+---+\n# |color| fruit| v1| v2|\n# +-----+------+---+---+\n# | red|banana| 1| 10|\n# | blue|banana| 2| 20|\n# | red|carrot| 3| 30|\n# | blue| grape| 4| 40|\n# | red|carrot| 5| 50|\n# |black|carrot| 6| 60|\n# | red|banana| 7| 70|\n# | red| grape| 8| 80|\n# +-----+------+---+---+ df.groupby('color').avg().show()\n# +-----+-------+-------+\n# |color|avg(v1)|avg(v2)|\n# +-----+-------+-------+\n# | red| 4.8| 48.0|\n# |black| 6.0| 60.0|\n# | blue| 3.0| 30.0|\n# +-----+-------+-------+\nApply a Python native function against each group by using pandas API.def plus_mean(pandas_df): return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean()) df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show() # +-----+------+---+---+\n# |color| fruit| v1| v2|\n# +-----+------+---+---+\n# | red|banana| -3| 10|\n# | red|carrot| -1| 30|\n# | red|carrot| 0| 50|\n# | red|banana| 2| 70|\n# | red| grape| 3| 80|\n# |black|carrot| 0| 60|\n# | blue|banana| -1| 20|\n# | blue| grape| 1| 40|\n# +-----+------+---+---+\nCo-grouping and applying a functiondf1 = spark.createDataFrame( [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)], ('time', 'id', 'v1')) df2 = spark.createDataFrame( [(20000101, 1, 'x'), (20000101, 2, 'y')], ('time', 'id', 'v2')) def merge_ordered(l, r): return pd.merge_ordered(l, r) df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas( merge_ordered, schema='time int, id int, v1 double, v2 string').show()\n+--------+---+---+---+\n| time| id| v1| v2|\n+--------+---+---+---+\n|20000101| 1|1.0| x|\n|20000102| 1|3.0| x|\n|20000101| 2|2.0| y|\n|20000102| 2|4.0| y|\n+--------+---+---+---+\nCSVdf.write.csv('foo.csv', header=True)\nspark.read.csv('foo.csv', header=True).show()\n# +-----+------+---+---+\n# |color| fruit| v1| v2|\n# +-----+------+---+---+\n# | red|banana| 1| 10|\n# | blue|banana| 2| 20|\n# | red|carrot| 3| 30|\n# | blue| grape| 4| 40|\n# | red|carrot| 5| 50|\n# |black|carrot| 6| 60|\n# | red|banana| 7| 70|\n# | red| grape| 8| 80|\n# +-----+------+---+---+\nParquetdf.write.parquet('bar.parquet')\nspark.read.parquet('bar.parquet').show()\n# +-----+------+---+---+\n# |color| fruit| v1| v2|\n# +-----+------+---+---+\n# | red|banana| 1| 10|\n# | blue|banana| 2| 20|\n# | red|carrot| 3| 30|\n# | blue| grape| 4| 40|\n# | red|carrot| 5| 50|\n# |black|carrot| 6| 60|\n# | red|banana| 7| 70|\n# | red| grape| 8| 80|\n# +-----+------+---+---+\nORCdf.write.orc('zoo.orc')\nspark.read.orc('zoo.orc').show()\n# +-----+------+---+---+\n# |color| fruit| v1| v2|\n# +-----+------+---+---+\n# | red|banana| 1| 10|\n# | blue|banana| 2| 20|\n# | red|carrot| 3| 30|\n# | blue| grape| 4| 40|\n# | red|carrot| 5| 50|\n# |black|carrot| 6| 60|\n# | red|banana| 7| 70|\n# | red| grape| 8| 80|\n# +-----+------+---+---+\nDataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly.df.createOrReplaceTempView(\"tableA\")\nspark.sql(\"SELECT count(*) from tableA\").show()\n# +--------+\n# |count(1)|\n# +--------+\n# | 8|\n# +--------+\nUDFs can be registered and invoked in SQL out of the box@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -&gt; pd.Series: return s + 1 spark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(v1) FROM tableA\").show()\n# +-----------+\n# |add_one(v1)|\n# +-----------+\n# | 2|\n# | 3|\n# | 4|\n# | 5|\n# | 6|\n# | 7|\n# | 8|\n# | 9|\n# +-----------+\nSQL expressions be mixed and used as PySpark columnsfrom pyspark.sql.functions import expr df.selectExpr('add_one(v1)').show()\ndf.select(expr('count(*)') &gt; 0).show()\n+-----------+\n|add_one(v1)|\n+-----------+\n| 2|\n| 3|\n| 4|\n| 5|\n| 6|\n| 7|\n| 8|\n| 9|\n+-----------+ +--------------+\n|(count(1) &gt; 0)|\n+--------------+\n| true|\n+--------------+\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"DataFrame Creation","level":3,"id":"DataFrame_Creation_0"},{"heading":"Viewing Data","level":3,"id":"Viewing_Data_0"},{"heading":"Selecting and Accessing Data¶","level":3,"id":"Selecting_and_Accessing_Data¶_0"},{"heading":"Applying a Function","level":3,"id":"Applying_a_Function_0"},{"heading":"Grouping Data","level":3,"id":"Grouping_Data_0"},{"heading":"Getting Data In/Out","level":3,"id":"Getting_Data_In/Out_0"},{"heading":"Working with SQL","level":3,"id":"Working_with_SQL_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/5.-pyspark/1.-dataframe.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641698,"modifiedTime":1737554783000,"sourceSize":10672,"sourcePath":"Big Data/3. Spark/5. Pyspark/1. DataFrame.md","exportPath":"big-data/3.-spark/5.-pyspark/1.-dataframe.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/5.-pyspark/2.-spark-connect.html":{"title":"2. Spark Connect","icon":"","description":"Spark Connect introduced a decoupled client-server architecture for Spark that allows remote connectivity to Spark clusters using the DataFrame API.%%bash\nsource ~/.profile # Make sure environment variables are loaded.\n$HOME/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION\nLaunch the server configured Spark to run as localhost:15002from pyspark.sql import SparkSession SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\nCreate a remote Spark sessionspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\nOnce the remote Spark session is created successfully, it can be used the same way as a regular Spark session. Therefore, you can create a DataFrame with the following command.from datetime import datetime, date\nfrom pyspark.sql import Row df = spark.createDataFrame([ Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)), Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)), Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()\n+---+---+-------+----------+-------------------+\n| a| b| c| d| e|\n+---+---+-------+----------+-------------------+\n| 1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n| 2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n| 4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Launch Spark server with Spark Connect","level":3,"id":"Launch_Spark_server_with_Spark_Connect_0"},{"heading":"Connect to Spark Connect server","level":3,"id":"Connect_to_Spark_Connect_server_0"},{"heading":"Create DataFrame","level":3,"id":"Create_DataFrame_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/5.-pyspark/2.-spark-connect.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641699,"modifiedTime":1737554783000,"sourceSize":1629,"sourcePath":"Big Data/3. Spark/5. Pyspark/2. Spark Connect.md","exportPath":"big-data/3.-spark/5.-pyspark/2.-spark-connect.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html":{"title":"3. Pandas API on Spark","icon":"","description":"import pandas as pd\nimport numpy as np\nimport pyspark.pandas as ps\nfrom pyspark.sql import SparkSession\ns = ps.Series([1, 3, 5, np.nan, 6, 8])\npsdf = ps.DataFrame( {'a': [1, 2, 3, 4, 5, 6], 'b': [100, 200, 300, 400, 500, 600], 'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]}, index=[10, 20, 30, 40, 50, 60])\nCreating a pandas DataFrame by passing a numpy array, with a datetime index and labeled columnsdates = pd.date_range('20130101', periods=6)\npdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Object Creation","level":3,"id":"Object_Creation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641701,"modifiedTime":1737554783000,"sourceSize":613,"sourcePath":"Big Data/3. Spark/5. Pyspark/3. Pandas API on Spark.md","exportPath":"big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/3.-spark/6.-source-code/0.-spark-storage.html":{"title":"0. Spark Storage","icon":"","description":"In Spark, a Block represents the smallest unit of data for reading and writing. In the source code, the implementation of Block is represented by BlockData. BlockData abstracts the storage method of a Block and provides various ways to read the underlying block data. After using a Block, the caller will invoke the dispose() method. The source code implementation is as follows:private[spark] trait BlockData { def toInputStream(): InputStream def toNetty(): Object def toChunkedByteBuffer(allocator: Int =&gt; ByteBuffer): ChunkedByteBuffer def toByteBuffer(): ByteBuffer def size: Long def dispose(): Unit\n}\nBlockData is a trait that defines a series of operations on a Block. It has three concrete implementation classes, with the inheritance structure.BlockId is an abstraction used to uniquely identify a Block. The source code for BlockId and its implementation are as follows:sealed abstract class BlockId { def name: String def asRDDId: Option[RDDBlockId] = if (isRDD) Some(asInstanceOf[RDDBlockId]) else None def isRDD: Boolean = isInstanceOf[RDDBlockId] def isShuffle: Boolean = isInstanceOf[ShuffleBlockId] || isInstanceOf[ShuffleBlockBatchId] def isBroadcast: Boolean = isInstanceOf[BroadcastBlockId] override def toString: String = name\n} case class RDDBlockId(rddId: Int, splitIndex: Int) extends BlockId { override def name: String = \"rdd_\" + rddId + \"_\" + splitIndex\n} case class ShuffleBlockId(shuffleId: Int, mapId: Long, reduceId: Int) extends BlockId { override def name: String = \"shuffle_\" + shuffleId + \"_\" + mapId + \"_\" + reduceId\n} case class ShuffleBlockBatchId( shuffleId: Int, mapId: Long, startReduceId: Int, endReduceId: Int) extends BlockId { override def name: String = { \"shuffle_\" + shuffleId + \"_\" + mapId + \"_\" + startReduceId + \"_\" + endReduceId }\n} case class ShuffleDataBlockId(shuffleId: Int, mapId: Long, reduceId: Int) extends BlockId { override def name: String = \"shuffle_\" + shuffleId + \"_\" + mapId + \"_\" + reduceId + \".data\"\n} case class ShuffleIndexBlockId(shuffleId: Int, mapId: Long, reduceId: Int) extends BlockId { override def name: String = \"shuffle_\" + shuffleId + \"_\" + mapId + \"_\" + reduceId + \".index\"\n} case class BroadcastBlockId(broadcastId: Long, field: String = \"\") extends BlockId { override def name: String = \"broadcast_\" + broadcastId + (if (field == \"\") \"\" else \"_\" + field)\n} case class TaskResultBlockId(taskId: Long) extends BlockId { override def name: String = \"taskresult_\" + taskId\n} case class StreamBlockId(streamId: Int, uniqueId: Long) extends BlockId { override def name: String = \"input-\" + streamId + \"-\" + uniqueId\n} private[spark] case class TempLocalBlockId(id: UUID) extends BlockId { override def name: String = \"temp_local_\" + id\n} private[spark] case class TempShuffleBlockId(id: UUID) extends BlockId { override def name: String = \"temp_shuffle_\" + id\n} private[spark] case class TestBlockId(id: String) extends BlockId { override def name: String = \"test_\" + id\n} class UnrecognizedBlockId(name: String) extends SparkException(s\"Failed to parse $name into a block ID\") object BlockId { val RDD = \"rdd_([0-9]+)_([0-9]+)\".r val SHUFFLE = \"shuffle_([0-9]+)_([0-9]+)_([0-9]+)\".r val SHUFFLE_BATCH = \"shuffle_([0-9]+)_([0-9]+)_([0-9]+)_([0-9]+)\".r val SHUFFLE_DATA = \"shuffle_([0-9]+)_([0-9]+)_([0-9]+).data\".r val SHUFFLE_INDEX = \"shuffle_([0-9]+)_([0-9]+)_([0-9]+).index\".r val BROADCAST = \"broadcast_([0-9]+)([_A-Za-z0-9]*)\".r val TASKRESULT = \"taskresult_([0-9]+)\".r val STREAM = \"input-([0-9]+)-([0-9]+)\".r val TEMP_LOCAL = \"temp_local_([-A-Fa-f0-9]+)\".r val TEMP_SHUFFLE = \"temp_shuffle_([-A-Fa-f0-9]+)\".r val TEST = \"test_(.*)\".r def apply(name: String): BlockId = name match { case RDD(rddId, splitIndex) =&gt; RDDBlockId(rddId.toInt, splitIndex.toInt) case SHUFFLE(shuffleId, mapId, reduceId) =&gt; ShuffleBlockId(shuffleId.toInt, mapId.toLong, reduceId.toInt) case SHUFFLE_BATCH(shuffleId, mapId, startReduceId, endReduceId) =&gt; ShuffleBlockBatchId(shuffleId.toInt, mapId.toLong, startReduceId.toInt, endReduceId.toInt) case SHUFFLE_DATA(shuffleId, mapId, reduceId) =&gt; ShuffleDataBlockId(shuffleId.toInt, mapId.toLong, reduceId.toInt) case SHUFFLE_INDEX(shuffleId, mapId, reduceId) =&gt; ShuffleIndexBlockId(shuffleId.toInt, mapId.toLong, reduceId.toInt) case BROADCAST(broadcastId, field) =&gt; BroadcastBlockId(broadcastId.toLong, field.stripPrefix(\"_\")) case TASKRESULT(taskId) =&gt; TaskResultBlockId(taskId.toLong) case STREAM(streamId, uniqueId) =&gt; StreamBlockId(streamId.toInt, uniqueId.toLong) case TEMP_LOCAL(uuid) =&gt; TempLocalBlockId(UUID.fromString(uuid)) case TEMP_SHUFFLE(uuid) =&gt; TempShuffleBlockId(UUID.fromString(uuid)) case TEST(value) =&gt; TestBlockId(value) case _ =&gt; throw new UnrecognizedBlockId(name) }\n}\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Block","level":3,"id":"Block_0"},{"heading":"BlockData","level":3,"id":"BlockData_0"},{"heading":"BlockId","level":3,"id":"BlockId_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/3.-spark/6.-source-code/0.-spark-storage.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641705,"modifiedTime":1737554783000,"sourceSize":5140,"sourcePath":"Big Data/3. Spark/6. Source Code/0. Spark Storage.md","exportPath":"big-data/3.-spark/6.-source-code/0.-spark-storage.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/4.-hive/0.-hive.html":{"title":"0. Hive","icon":"","description":"Hive is a data warehouse infrastructure tool built on top of Hadoop for providing data summarization, query, and analysis. It facilitates reading, writing, and managing large datasets residing in distributed storage using SQL.Hive is a data warehouse software that facilitates querying and managing large datasets residing in distributed storage using SQL. It is built on top of Hadoop and provides an abstraction for the Hadoop MapReduce framework. Here are the key components of Hive:The MetaStore is a critical component of Hive, responsible for storing metadata about Hive databases, tables, columns, partitions, and other objects. It provides:\nMetadata Access: Interfaces for the Hive CLI and HiveServer2 to access metadata.\nMetadata Storage: Typically uses an RDBMS such as MySQL or PostgreSQL for storing metadata.\nMetadata Consistency: Ensures consistency and availability of metadata across different components of Hive.\nThe Query Compiler translates HiveQL queries into a directed acyclic graph (DAG) of MapReduce jobs. It includes the following phases:\nParse Phase: Converts the HiveQL query into an Abstract Syntax Tree (AST).\nAnalyze Phase: Validates the query semantics and creates a logical plan.\nOptimize Phase: Applies optimization rules to the logical plan to improve query performance.\nGenerate Phase: Transforms the optimized logical plan into a physical plan consisting of MapReduce jobs.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Key Components of Hive","level":3,"id":"Key_Components_of_Hive_0"},{"heading":"MetaStore","level":4,"id":"MetaStore_0"},{"heading":"Query Compiler","level":4,"id":"Query_Compiler_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/4.-hive/0.-hive.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641707,"modifiedTime":1737554783000,"sourceSize":5720,"sourcePath":"Big Data/4. Hive/0. Hive.md","exportPath":"big-data/4.-hive/0.-hive.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/4.-hive/1.-data-definition-language-(ddl).html":{"title":"1. Data Definition Language (DDL)","icon":"","description":"To create a new database in Hive:CREATE DATABASE [IF NOT EXISTS] database_name\n[COMMENT 'database comment']\n[LOCATION hdfs_path]\n[WITH DBPROPERTIES (property_name=property_value, ...)];\nShow the existing databases:SHOW DATABASES [LIKE 'identifier_with_wildcards']; LIKE is a keyword where * means any character and | denotes the OR operation.\nDescribe a specific database:DESCRIBE DATABASE [EXTENDED] db_name; Adding EXTENDED to this query shows detailed information about the selected database.\nTo alter the properties or location of an existing database:ALTER DATABASE database_name SET DBPROPERTIES (property_name=property_value, ...); ALTER DATABASE database_name SET LOCATION hdfs_path; ALTER DATABASE database_name SET OWNER USER username;\nTo drop a database:DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE]; RESTRICT (default): Fails if the database is not empty.\nCASCADE: Deletes all contents if the database is not empty.\nTo select a database for use:USE database_name;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Create Database","level":3,"id":"Create_Database_0"},{"heading":"Select Database","level":3,"id":"Select_Database_0"},{"heading":"Modify Database","level":3,"id":"Modify_Database_0"},{"heading":"Drop Database","level":3,"id":"Drop_Database_0"},{"heading":"Use Database","level":3,"id":"Use_Database_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/4.-hive/1.-data-definition-language-(ddl).html","pathToRoot":"../..","attachments":[],"createdTime":1737584641708,"modifiedTime":1737554783000,"sourceSize":7840,"sourcePath":"Big Data/4. Hive/1. Data Definition Language (DDL).md","exportPath":"big-data/4.-hive/1.-data-definition-language-(ddl).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/4.-hive/2.-data-manipulation-language-(dml).html":{"title":"2. Data Manipulation Language (DML)","icon":"","description":"The LOAD statement loads data from a file into a Hive table.LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2, ...)];\nKeywords Explanation:\nLOCAL: Loads data from the local filesystem. Without this keyword, data is loaded from HDFS.\nOVERWRITE: Overwrites the existing data in the table. Without this, data is appended to the table.\nPARTITION: Specifies the partition to load the data into. If the target is a partitioned table, the partition must be specified.\nExamples: Create a table:\nCREATE TABLE student ( id INT, name STRING\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'; Load data from a local file into the Hive table:\nLOAD DATA LOCAL INPATH '/opt/module/datas/student.txt' INTO TABLE student; Load data from HDFS into the Hive table: Upload the file to HDFS:\nhadoop fs -put /opt/module/datas/student.txt /user/atguigu Load the HDFS data into the Hive table:\nLOAD DATA INPATH '/user/atguigu/student.txt' INTO TABLE student; Overwrite existing data in the Hive table: Upload the file to HDFS:\nhadoop fs -put /opt/module/datas/student.txt /user/atguigu; Overwrite the data in the Hive table:\nLOAD DATA INPATH '/user/atguigu/student.txt' OVERWRITE INTO TABLE student; INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2, ...)] select_statement;\nKeywords Explanation:\nINTO: Appends the results to the target table.\nOVERWRITE: Overwrites the existing data in the target table.\nExamples: Create a table:\nCREATE TABLE student1 ( id INT, name STRING\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '\\t'; Insert data from a query result:\nINSERT OVERWRITE TABLE student3 SELECT * FROM student; INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2, ...)] VALUES (values_row [, values_row, ...]);\nExample:INSERT INTO TABLE student1 VALUES (1, 'wangwu'), (2, 'zhaoliu');\nINSERT OVERWRITE [LOCAL] DIRECTORY directory\n[ROW FORMAT row_format] [STORED AS file_format] select_statement;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Load","level":3,"id":"Load_0"},{"heading":"Insert","level":3,"id":"Insert_0"},{"heading":"Insert Query Results into a Table","level":4,"id":"Insert_Query_Results_into_a_Table_0"},{"heading":"Insert Given Values into a Table","level":4,"id":"Insert_Given_Values_into_a_Table_0"},{"heading":"Write Query Results to a Directory","level":4,"id":"Write_Query_Results_to_a_Directory_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/4.-hive/2.-data-manipulation-language-(dml).html","pathToRoot":"../..","attachments":[],"createdTime":1737584641709,"modifiedTime":1737554783000,"sourceSize":6947,"sourcePath":"Big Data/4. Hive/2. Data Manipulation Language (DML).md","exportPath":"big-data/4.-hive/2.-data-manipulation-language-(dml).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html":{"title":"3. Single Node Deployment Guide - Tecent Cloud","icon":"","description":"(this part same as <a data-href=\"7. Single Node Deployment Guide - Tecent Cloud\" href=\"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">7. Single Node Deployment Guide - Tecent Cloud</a> in Hadoop)sudo adduser hadoop\nsudo passwd hadoop\nsudo usermod -aG sudo hadoop\nsudo usermod -aG wheel hadoop\nsu - hadoop\nsudo yum install java-1.8.0-openjdk-devel Download Hadoop 3.3.6 and place it under the user hadoop.\nsudo tar -xvf hadoop-3.3.6.tar.gz\nsudo mv hadoop-3.3.6 /usr/local/hadoop\nEdit the .bashrc file to set up environment variables.nano ~/.bashrc\nAdd the following:export HADOOP_HOME=/usr/local/hadoop\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nApply the changes:source ~/.bashrc core-site.xml:\nsudo nano $HADOOP_HOME/etc/hadoop/core-site.xml\nAdd the following:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Setup Hadoop","level":3,"id":"1._Setup_Hadoop_0"},{"heading":"Create a Hadoop User","level":4,"id":"Create_a_Hadoop_User_0"},{"heading":"Install Java","level":4,"id":"Install_Java_0"},{"heading":"Download and Install Hadoop","level":4,"id":"Download_and_Install_Hadoop_0"},{"heading":"Configure Hadoop Environment","level":4,"id":"Configure_Hadoop_Environment_0"},{"heading":"Configure Hadoop XML Files","level":4,"id":"Configure_Hadoop_XML_Files_0"}],"links":["big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html#_0"],"author":"","coverImageURL":"","fullURL":"big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641710,"modifiedTime":1737554783000,"sourceSize":8310,"sourcePath":"Big Data/4. Hive/3. Single Node Deployment Guide - Tecent Cloud.md","exportPath":"big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/0.-hbase.html":{"title":"0. Hbase","icon":"","description":"HBase is a distributed, scalable NoSQL database designed for storing large amounts of data. It is based on Google's Bigtable design and runs on top of Hadoop HDFS, offering efficient random read and write capabilities while supporting operations on large datasets.Logically, HBase's data model is similar to that of a relational database, where data is stored in a table with rows and columns. However, from the perspective of HBase's underlying physical storage structure (Key-Value), HBase resembles a multi-dimensional map.HBase's physical storage structure relies on HDFS. Each table is divided into multiple regions, with each region storing a subset of the table. A RegionServer is responsible for managing multiple regions. Data is stored in HFiles by column families, and these HFiles are stored on HDFS.A namespace is similar to a database concept in relational databases, containing multiple tables. HBase has two built-in namespaces: hbase and default. The hbase namespace stores HBase's built-in tables, while default is the default namespace for user tables.Similar to the concept of tables in relational databases. However, when defining a table in HBase, you only need to declare column families, not specific columns. This means that when writing data to HBase, fields can be dynamically specified as needed. Thus, compared to relational databases, HBase can easily handle schema changes.Each row in an HBase table is composed of a RowKey and multiple columns. Data is stored in the dictionary order of the RowKey, and data retrieval can only be done based on the RowKey, making the design of the RowKey crucial.Each column in HBase is defined by a Column Family and a Column Qualifier, for example, info:name, info:age. When creating a table, only the column family needs to be specified; column qualifiers do not need to be pre-defined. Column families are the basic unit of storage, and data from different column families is stored separately, each with independent storage and retrieval mechanisms.Used to identify different versions of data, each data entry is given a timestamp when written to HBase if not specified. This timestamp is the time when the data is written to HBase. This allows HBase to store multiple versions of the same cell.A cell is uniquely identified by {rowkey, column family:column qualifier, timestamp}. The data within a cell is stored in bytecode format without any specific type.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Data Model","level":3,"id":"Data_Model_0"},{"heading":"Physical Storage Structure","level":3,"id":"Physical_Storage_Structure_0"},{"heading":"Data Model","level":3,"id":"Data_Model_1"},{"heading":"1. Namespace","level":4,"id":"1._Namespace_0"},{"heading":"2. Table","level":4,"id":"2._Table_0"},{"heading":"3. Row","level":4,"id":"3._Row_0"},{"heading":"4. Column Family and Column","level":4,"id":"4._Column_Family_and_Column_0"},{"heading":"5. Timestamp","level":4,"id":"5._Timestamp_0"},{"heading":"6. Cell","level":4,"id":"6._Cell_0"},{"heading":"Features of HBase","level":3,"id":"Features_of_HBase_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/0.-hbase.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641712,"modifiedTime":1737554783000,"sourceSize":3973,"sourcePath":"Big Data/5. Hbase/0. Hbase.md","exportPath":"big-data/5.-hbase/0.-hbase.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/2.-shell.html":{"title":"2. Shell","icon":"","description":"\nEnter the HBase Shell:\n[a@hadoop102 hbase]$ bin/hbase shell View Help Command:\nhbase(main):001:0&gt; help List Tables:\nhbase(main):002:0&gt; list Create Table:\nhbase(main):002:0&gt; create 'student', 'info' Insert Data into Table:\nhbase(main):003:0&gt; put 'student', '1001', 'info:sex', 'male'\nhbase(main):004:0&gt; put 'student', '1001', 'info:age', '18'\nhbase(main):005:0&gt; put 'student', '1002', 'info:name', 'Janna'\nhbase(main):006:0&gt; put 'student', '1002', 'info:sex', 'female'\nhbase(main):007:0&gt; put 'student', '1002', 'info:age', '20' Scan Table Data:\nhbase(main):008:0&gt; scan 'student'\nhbase(main):009:0&gt; scan 'student', {STARTROW =&gt; '1001', STOPROW =&gt; '1001'}\nhbase(main):010:0&gt; scan 'student', {STARTROW =&gt; '1001'} Describe Table Structure:\nhbase(main):011:0&gt; describe 'student' Update Specific Field Data:\nhbase(main):012:0&gt; put 'student', '1001', 'info:name', 'Nick'\nhbase(main):013:0&gt; put 'student', '1001', 'info:age', '100' Get Data of a Specific Row or Column:\nhbase(main):014:0&gt; get 'student', '1001'\nhbase(main):015:0&gt; get 'student', '1001', 'info:name' Count Rows in Table:\nhbase(main):021:0&gt; count 'student' Delete Data: Delete all data of a row:\nhbase(main):016:0&gt; deleteall 'student', '1001' Delete specific column data of a row:\nhbase(main):017:0&gt; delete 'student', '1002', 'info:sex' Truncate Table Data:\nhbase(main):018:0&gt; truncate 'student' Note: To truncate a table, first disable it, then truncate it. Drop Table: First, disable the table:\nhbase(main):019:0&gt; disable 'student' Then, drop the table:\nhbase(main ):020:0&gt; drop 'student' Note: If you directly drop the table, an error will occur: ERROR: Table student is enabled. Disable it first. Alter Table Information: Store 3 versions of data in the info column family:\nhbase(main):022:0&gt; alter 'student', {NAME=&gt;'info', VERSIONS=&gt;3}\nhbase(main):022:0&gt; get 'student', '1001', {COLUMN=&gt;'info:name', VERSIONS=&gt;3} ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Basic Operations","level":3,"id":"Basic_Operations_0"},{"heading":"Table Operations","level":3,"id":"Table_Operations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/2.-shell.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641715,"modifiedTime":1737554783000,"sourceSize":2507,"sourcePath":"Big Data/5. Hbase/2. Shell.md","exportPath":"big-data/5.-hbase/2.-shell.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/3.-processes.html":{"title":"3. Processes","icon":"","description":"“HBaseArchitecture.png” could not be found. StoreFile The physical file that stores the actual data. StoreFiles are stored as HFiles in HDFS. Each store will have one or more StoreFiles (HFiles), and the data in each StoreFile is ordered. MemStore The write cache. Since data in HFiles must be ordered, data is first stored in MemStore, sorted, and then flushed to HFiles when the flush trigger is reached. Each flush creates a new HFile. WAL (Write-Ahead Log) Since data must be ordered in MemStore before being flushed to HFiles, storing data only in memory has a high risk of data loss. To address this, data is first written to a Write-Ahead Log (WAL) file and then written to MemStore. In case of system failure, data can be reconstructed from this log file. Client Access to Zookeeper: The client first accesses Zookeeper to determine which Region Server hosts the hbase:meta table.\nAccess Region Server: The client then accesses the corresponding Region Server to query the hbase:meta table, identifying which Region Server and Region the target data is located in based on the namespace:table/rowkey. The region information and meta table location are cached in the client's meta cache for future requests.\nCommunicate with Target Region Server: The client communicates with the target Region Server.\nWrite Data to WAL: Data is sequentially written (appended) to the WAL.\nWrite Data to MemStore: Data is then written to the corresponding MemStore, where it is sorted.\nSend Acknowledgment: An acknowledgment is sent back to the client.\nFlush Data to HFile: When the MemStore flush trigger is reached, data is flushed to HFiles.\n“MemStoreFlush.png” could not be found. MemStore Size Limit: When the size of a MemStore reaches hbase.hregion.memstore.flush.size (default 128MB), all MemStores in the same Region will be flushed. When the size of a MemStore reaches hbase.hregion.memstore.flush.size (default 128MB) * hbase.hregion.memstore.block.multiplier (default 4), further writes to that MemStore will be blocked. RegionServer MemStore Size Limit: When the total size of MemStores in a RegionServer reaches java_heapsize * hbase.regionserver.global.memstore.size (default 0.4) * hbase.regionserver.global.memstore.size.lower.limit (default 0.95), regions will be flushed in descending order of MemStore size until the total MemStore size drops below the limit. When the total MemStore size in a RegionServer reaches java_heapsize * hbase.regionserver.global.memstore.size (default 0.4), further writes to all MemStores will be blocked. Automatic Flush Time: MemStores will also be flushed when the automatic flush time interval is reached. WAL File Limit: When the number of WAL files exceeds hbase.regionserver.max.logs, regions will be flushed in chronological order until the number of WAL files drops below the limit (this property name has been deprecated, and manual setting is no longer required; the maximum value is 32). ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture Principles","level":3,"id":"Architecture_Principles_0"},{"heading":"Write Process","level":3,"id":"Write_Process_0"},{"heading":"MemStore Flush Triggers:","level":3,"id":"MemStore_Flush_Triggers_0"},{"heading":"Read Process","level":2,"id":"Read_Process_0"},{"heading":"HBase Read Process","level":3,"id":"HBase_Read_Process_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/3.-processes.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641716,"modifiedTime":1737554783000,"sourceSize":5921,"sourcePath":"Big Data/5. Hbase/3. Processes.md","exportPath":"big-data/5.-hbase/3.-processes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/4.-api.html":{"title":"4. API","icon":"","description":"After creating a new project, add the following dependencies in the pom.xml:&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;\n&lt;/dependency&gt;\npublic static Configuration conf; static { // Instantiate using HBaseConfiguration singleton method conf = HBaseConfiguration.create(); conf.set(\"hbase.zookeeper.quorum\", \"192.166.9.102\"); conf.set(\"hbase.zookeeper.property.clientPort\", \"2181\");\n}\npublic static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException { // Creating HBaseAdmin object to manage and access tables in HBase // Connection connection = ConnectionFactory.createConnection(conf); // HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName);\n}\npublic static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException { HBaseAdmin admin = new HBaseAdmin(conf); // Check if the table exists if (isTableExist(tableName)) { System.out.println(\"Table \" + tableName + \" already exists\"); // System.exit(0); } else { // Create table descriptor and convert table name to bytes HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); // Create multiple column families for (String cf : columnFamily) { descriptor.addFamily(new HColumnDescriptor(cf)); } // Create table based on configuration admin.createTable(descriptor); System.out.println(\"Table \" + tableName + \" created successfully!\"); }\n}\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Environment Setup","level":3,"id":"Environment_Setup_0"},{"heading":"Configuration Object","level":3,"id":"Configuration_Object_0"},{"heading":"Checking if a Table Exists","level":3,"id":"Checking_if_a_Table_Exists_0"},{"heading":"Creating a Table","level":3,"id":"Creating_a_Table_0"},{"heading":"Deleting a Table","level":3,"id":"Deleting_a_Table_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/4.-api.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641724,"modifiedTime":1737554783000,"sourceSize":6112,"sourcePath":"Big Data/5. Hbase/4. API.md","exportPath":"big-data/5.-hbase/4.-api.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/5.-mapreduce.html":{"title":"5. MapReduce","icon":"","description":"By using HBase's Java API, we can implement MapReduce operations that interact with HBase, such as importing data from the local file system into an HBase table or performing data analysis with MapReduce after reading raw data from HBase.\nView HBase's MapReduce tasks execution\n$ bin/hbase mapredcp Import environment variables Temporary import (effective only for the current session):\n$ export HBASE_HOME=/opt/module/hbase\n$ export HADOOP_HOME=/opt/module/hadoop-2.7.2\n$ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp` Permanent import: add to /etc/profile\nexport HBASE_HOME=/opt/module/hbase\nexport HADOOP_HOME=/opt/module/hadoop-2.7.2 Also configure in hadoop-env.sh (after the for loop):\nexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/* Run official MapReduce tasks Example 1: Count the number of rows in the Student table\n$ /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter student Example 2: Import local data into HBase using MapReduce Create a local TSV file: fruit.tsv\n1001 Apple Red\n1002 Pear Yellow\n1003 Pineapple Yellow Create HBase table\nHbase(main):001:0&gt; create 'fruit','info' Create input_fruit folder in HDFS and upload fruit.tsv file\n$ /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/\n$ /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/ Execute MapReduce to import data into HBase fruit table\n$ /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar importtsv \\\n-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\\nhdfs://hadoop102:9000/input_fruit Use the scan command to view the imported results\nHbase(main):001:0&gt; scan 'fruit' Goal: Migrate part of the data from the fruit table to the fruit_mr table using MapReduce.Steps to implement: Construct ReadFruitMapper class to read data from fruit table\npackage com.a; import java.io.IOException;\nimport org.apache.hadoop.hbase.Cell;\nimport org.apache.hadoop.hbase.CellUtil;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Result;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableMapper;\nimport org.apache.hadoop.hbase.util.Bytes; public class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; { @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException { // Extract 'name' and 'color' from 'fruit' and put into Put object Put put = new Put(key.get()); // Traverse and add columns for (Cell cell : value.rawCells()) { // Add/clone column family 'info' if (\"info\".equals(Bytes.toString(CellUtil.cloneFamily(cell)))) { // Add/clone column 'name' if (\"name\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) { put.add(cell); // Add/clone column 'color' } else if (\"color\".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) { put.add(cell); } } } // Write each row's data read from 'fruit' to context as map output context.write(key, put); }\n} Construct WriteFruitMRReducer class to write data read from fruit table to fruit_mr table\npackage com.atguigu.Hbase_mr; import java.io.IOException;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableReducer;\nimport org.apache.hadoop.io.NullWritable; public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; { @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException { // Write each row's data to 'fruit_mr' table for (Put put : values) { context.write(NullWritable.get(), put); } }\n} Construct Fruit2FruitMRRunner extends Configured implements Tool to assemble and run the Job\n// Assemble Job\npublic int run(String[] args) throws Exception { // Get Configuration Configuration conf = this.getConf(); // Create Job Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); // Configure Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); // Set Mapper TableMapReduceUtil.initTableMapperJob( \"fruit\", // Table name scan, // Scan controller ReadFruitMapper.class, // Mapper class ImmutableBytesWritable.class, // Mapper output key type Put.class, // Mapper output value type job // Job ); // Set Reducer TableMapReduceUtil.initTableReducerJob(\"fruit_mr\", WriteFruitMRReducer.class, job); // Set Reduce tasks job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if (!isSuccess) { throw new IOException(\"Job running with error\"); } return isSuccess ? 0 : 1;\n} Call and run the Job task in the main function\npublic static void main(String[] args) throws Exception { Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args); System.exit(status);\n} Package and run the task\n$ /opt/module/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/Hbase-0.0.1-SNAPSHOT.jar com.z.Hbase.mr1.Fruit2FruitMRRunner Tips:\nBefore running the task, ensure that the table to import data into exists. If not, create it in advance.\nMaven packaging command: -P local clean package or -P dev clean package install (to package third-party jars, use the maven-shade-plugin).\nGoal: Write data from HDFS into an HBase table.Steps to implement: Construct ReadFruitFromHDFSMapper to read file data from HDFS\npackage com.atguigu; import java.io.IOException;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper; public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; { @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException { // Read data from HDFS String lineValue = value.toString(); // Split each line's data using \\t, store in String array String[] values = lineValue.split(\"\\t\"); // Get values based on data meaning String rowKey = values[0]; String name = values[1]; String color = values[2]; // Initialize rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); // Initialize put object Put put = new Put(Bytes.toBytes(rowKey)); // Parameters: column family, column, value put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"name\"), Bytes.toBytes(name)); put.add(Bytes.toBytes(\"info\"), Bytes.toBytes(\"color\"), Bytes.toBytes(color)); context.write(rowKeyWritable, put); }\n} Construct WriteFruitMRFromTxtReducer class\npackage com.z.Hbase.mr2 ;import java.io.IOException;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.TableReducer;\nimport org.apache.hadoop.io.NullWritable; public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; { @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException { // Write each row's data to 'fruit_hdfs' table for (Put put : values) { context.write(NullWritable.get(), put); } }\n}\n``` Create Txt2FruitRunner to assemble Job\npublic int run(String[] args) throws Exception { // Get Configuration Configuration conf = this.getConf(); // Create Job Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Txt2FruitRunner.class); Path inPath = new Path(\"hdfs://hadoop102:9000/input_fruit/fruit.tsv\"); FileInputFormat.addInputPath(job, inPath); // Set Mapper job.setMapperClass(ReadFruitFromHDFSMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); // Set Reducer TableMapReduceUtil.initTableReducerJob(\"fruit_mr\", WriteFruitMRFromTxtReducer.class, job); // Set Reduce tasks job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if (!isSuccess) { throw new IOException(\"Job running with error\"); } return isSuccess ? 0 : 1;\n} Call and execute the Job\npublic static void main(String[] args) throws Exception { Configuration conf = HBaseConfiguration.create(); int status = ToolRunner.run(conf, new Txt2FruitRunner(), args); System.exit(status);\n} Package and run the task\n$ /opt/module/hadoop-2.7.2/bin/yarn jar hbase-0.0.1-SNAPSHOT.jar com.atguigu.hbase.mr2.Txt2FruitRunner ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"MapReduce","level":3,"id":"MapReduce_0"},{"heading":"Official HBase-MapReduce","level":4,"id":"Official_HBase-MapReduce_0"},{"heading":"4.3.2 Custom HBase-MapReduce1","level":4,"id":"4.3.2_Custom_HBase-MapReduce1_0"},{"heading":"Custom HBase-MapReduce2","level":4,"id":"Custom_HBase-MapReduce2_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/5.-mapreduce.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641725,"modifiedTime":1737554783000,"sourceSize":10917,"sourcePath":"Big Data/5. Hbase/5. MapReduce.md","exportPath":"big-data/5.-hbase/5.-mapreduce.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/6.-integration-with-hive.html":{"title":"6. Integration with Hive","icon":"","description":" Hive Data Warehouse: Essentially, Hive creates a bi-directional relationship between files stored in HDFS and MySQL to facilitate the use of HQL for management and querying.\nUsed for Data Analysis and Cleaning: Suitable for offline data analysis and cleaning, but it has higher latency.\nBased on HDFS and MapReduce: Data stored in Hive remains on DataNodes, and HQL statements are ultimately converted into MapReduce jobs for execution. HBase Database: A non-relational database that stores data in column families.\nUsed for Storing Structured and Unstructured Data: Suitable for single table non-relational data storage, but not for complex queries like JOINs.\nBased on HDFS: Data is stored persistently as HFiles in DataNodes and managed by RegionServers in the form of regions.\nLow Latency for Online Business Use: Handles large amounts of enterprise data efficiently and provides high-speed data access. Note: Integration between HBase and Hive is incompatible in the some versions. Therefore, need to recompile hive-hbase-handler.Environment Setup:\nSince Hive operations might affect HBase, Hive needs the necessary HBase JAR files. We can copy or create soft links for these JAR files.export HBASE_HOME=/opt/module/hbase\nexport HIVE_HOME=/opt/module/hive\nln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar $HIVE_HOME/lib/hbase-common-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbase-server-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it-1.3.1.jar\nln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar\nln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar\nModify the ZooKeeper properties in hive-site.xml:&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;\n&lt;/property&gt;\n&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;\n&lt;/property&gt;\nExample 1: Create a Hive table, associate it with an HBase table, and insert data into the Hive table, which will affect the HBase table.Steps to Implement:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Comparison between HBase and Hive","level":3,"id":"Comparison_between_HBase_and_Hive_0"},{"heading":"Using HBase and Hive Together","level":3,"id":"Using_HBase_and_Hive_Together_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/6.-integration-with-hive.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641749,"modifiedTime":1737554783000,"sourceSize":5330,"sourcePath":"Big Data/5. Hbase/6. Integration with Hive.md","exportPath":"big-data/5.-hbase/6.-integration-with-hive.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/5.-hbase/7.-optimization.html":{"title":"7. Optimization","icon":"","description":"In HBase, HMaster is responsible for monitoring the lifecycle of HRegionServer and balancing the load of RegionServer. If HMaster goes down, the entire HBase cluster will be in an unhealthy state and will not remain operational for long. Therefore, HBase supports high availability configuration for HMaster. Shut down the HBase cluster (skip this step if it is not running)\n[a@hadoop102 hbase]$ bin/stop-hbase.sh Create the backup-masters file in the conf directory\n[au@hadoop102 hbase]$ touch conf/backup-masters Configure high availability HMaster nodes in the backup-masters file\n[a@hadoop102 hbase]$ echo hadoop103 &gt; conf/backup-masters Copy the entire conf directory to other nodes\n[a@hadoop102 hbase]$ scp -r conf/ hadoop103:/opt/module/hbase/\n[a@hadoop102 hbase]$ scp -r conf/ hadoop104:/opt/module/hbase/ Open the web page to verify\nhttp://hadooo102:16010 Each region maintains a StartRow and EndRow. If the data being added falls within the RowKey range maintained by a specific region, that data is handled by that region. Following this principle, we can pre-plan the regions where the data will be placed to improve HBase performance. Manual pre-splitting\nHbase&gt; create 'staff1', 'info', 'partition1', SPLITS =&gt; ['1000', '2000', '3000', '4000'] Generate hexadecimal sequence pre-splitting\ncreate 'staff2', 'info', 'partition2', {NUMREGIONS =&gt; 15, SPLITALGO =&gt; 'HexStringSplit'} Pre-split according to rules set in a file Create a file splits.txt with the following content:\naaaa\nbbbb\ncccc\ndddd Execute:\ncreate 'staff3', 'partition3', SPLITS_FILE =&gt; 'splits.txt' Use Java API to create pre-split regions\n// Custom algorithm to generate a series of hash values stored in a two-dimensional array\nbyte[][] splitKeys = ... // some hash function\n// Create HBaseAdmin instance\nHBaseAdmin hAdmin = new HBaseAdmin(HbaseConfiguration.create());\n// Create HTableDescriptor instance\nHTableDescriptor tableDesc = new HTableDescriptor(tableName);\n// Create HBase table with pre-split regions using HTableDescriptor and hash values\nhAdmin.createTable(tableDesc, splitKeys); A RowKey uniquely identifies a piece of data. The partition where this data is stored depends on the RowKey's range within a pre-split region. The main purpose of designing RowKey is to ensure data is evenly distributed across all regions, preventing data skew to some extent. Here are common RowKey design schemes: Generate random numbers, hashes, and scatter values Example: Original RowKey 1001 becomes dd01903921ea24941c26a48f2cec24e0bb0e8cc7 after SHA1.\nOriginal RowKey 3001 becomes 49042c54de64a1e9bf0b33e00245660ef92dc7bd after SHA1.\nOriginal RowKey 5001 becomes 7b61dec07e02c188790670af43e717f0f46e8913 after SHA1. Usually, we extract samples from the dataset to decide what type of RowKey to hash as a critical value for each partition. String reversal Convert 20170524000001 to 10000042507102\nConvert 20170524000002 to 20000042507102\nThis can scatter gradually put data to some extent. String concatenation 20170524000001_a12e\n20170524000001_93i7 ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"High Availability","level":3,"id":"High_Availability_0"},{"heading":"Pre-Splitting Regions","level":3,"id":"Pre-Splitting_Regions_0"},{"heading":"RowKey Design","level":3,"id":"RowKey_Design_0"},{"heading":"Memory Optimization","level":3,"id":"Memory_Optimization_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/5.-hbase/7.-optimization.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641774,"modifiedTime":1737554783000,"sourceSize":8403,"sourcePath":"Big Data/5. Hbase/7. Optimization.md","exportPath":"big-data/5.-hbase/7.-optimization.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/6.-flink/0.-key-features-of-flink.html":{"title":"0. Key Features of Flink","icon":"","description":"Event-driven applications are stateful applications that extract data from one or more event streams and trigger computations, state updates, or other external actions based on incoming events. A typical example is message queues, such as Kafka, which are almost all event-driven applications.Event-Driven:\n“EventDrivenFlink.png” could not be found.Batch processing is characterized by being bounded, persistent, and large-scale, making it suitable for computations that require accessing the complete set of records, typically used for offline analytics. Stream processing, on the other hand, is unbounded and real-time, operating on each data item as it flows through the system, typically used for real-time analytics.In Spark's worldview, everything consists of batches: offline data is a large batch, while real-time data is composed of an infinite series of small batches.In Flink's worldview, everything consists of streams: offline data is a bounded stream, and real-time data is an unbounded stream. This is known as bounded and unbounded streams.\nUnbounded Streams: Unbounded streams have a start but no end. They do not terminate and provide data continuously, requiring immediate processing of each event upon arrival. We cannot wait for all data to arrive because the input is unbounded and will never complete. Processing unbounded streams often requires processing events in a specific order (e.g., in the order they occurred) to ensure result completeness.\nBounded Streams: Bounded streams have a clearly defined start and end. They can be processed by acquiring all data before performing any computations, and they do not need to be processed in order. Bounded stream processing is also known as batch processing.\nThis stream-oriented architecture offers the significant benefit of extremely low latency.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Event-Driven","level":3,"id":"Event-Driven_0"},{"heading":"Worldview of Stream and Batch","level":3,"id":"Worldview_of_Stream_and_Batch_0"},{"heading":"Layered API","level":3,"id":"Layered_API_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/6.-flink/0.-key-features-of-flink.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641776,"modifiedTime":1737554783000,"sourceSize":4869,"sourcePath":"Big Data/6. Flink/0. Key Features of Flink.md","exportPath":"big-data/6.-flink/0.-key-features-of-flink.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/6.-flink/1.-deployment-and-startup.html":{"title":"1. Deployment and Startup","icon":"","description":"Flink supports multiple deployment modes tailored for different environments and use cases:\nLocal Mode:\nRuns Flink on a single machine for quick experimentation or debugging. It is rarely used in production.\nStandalone Mode:\nDeploys Flink using its built-in cluster manager. Commonly used for development and testing environments where Hadoop YARN is not available.\nYARN Mode:\nUtilizes Hadoop YARN for resource management, ensuring effective utilization of cluster resources. This mode is preferred in enterprise production environments where multiple workloads coexist on the same cluster. Note: For installation guidelines, refer to the official documentation or community resources.\nThe deployment of Flink on YARN involves the following steps: YARN Client Initialization: Fetches Hadoop configuration using environment variables: YARN_CONF_DIR, HADOOP_CONF_DIR, or HADOOP_CONF_PATH.\nDefaults to $HADOOP_HOME/etc/hadoop if the above variables are not set. Resource Request and Preparation: The client verifies resource availability (e.g., CPU, memory).\nConfiguration files (e.g., HDFS and Flink-specific settings) and Flink JARs are uploaded to HDFS. Application Launch: The client sends a request to YARN's ResourceManager to start an application.\nResourceManager coordinates with NodeManagers to provision containers and download necessary files. Application Master (AM) Initialization: The ApplicationMaster launches the JobManager.\nJobManager’s address is written to HDFS for later use. TaskManager Provisioning: The ApplicationMaster requests additional containers for TaskManagers.\nTaskManagers download the JARs and configuration files from HDFS. Execution and Monitoring: TaskManagers execute tasks and report status to the JobManager.\nApplicationMaster manages task states, including restarting failed tasks. Job Completion: Upon task completion, the ApplicationMaster deregisters with the ResourceManager and shuts down. Modify Hadoop Configuration: Edit etc/hadoop/yarn-site.xml to disable strict virtual memory checks:\n&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt; This setting prevents tasks from being killed if they exceed memory limits. Update Environment Variables: Add the following to /etc/profile:\nexport HADOOP_CONF_DIR=/export/servers/hadoop/etc/hadoop Ensure YARN_CONF_DIR or HADOOP_CONF_DIR is correctly set for accessing YARN and HDFS configurations. Start Required Services: Start HDFS, Zookeeper (if applicable), and the YARN cluster. Submit Jobs: Use the YARN session or cluster mode (explained below) to submit Flink jobs. Flink on YARN supports two primary modes of operation: Overview:\nA persistent session is established where JobManager and TaskManager remain active to accept jobs continuously. Best suited for small, frequent jobs. Starting a YARN Session:\nbin/yarn-session.sh -n 2 -tm 800 -jm 800 -s 1 -d -n: Number of containers.\n-tm: Memory allocated per TaskManager (in MB).\n-jm: Memory allocated to the JobManager (in MB).\n-s: Task slots per container.\n-d: Runs the session in the background. Submitting a Job:\nbin/flink run examples/batch/WordCount.jar Terminating the Session:\nyarn application -kill application_id Overview:\nA job is directly submitted to YARN, and resources are allocated dynamically. The session terminates once the job is completed. Ideal for larger, resource-intensive jobs. Submitting a Job:\nbin/flink run -m yarn-cluster -yn 2 -yjm 800 -ytm 800 /path/to/WordCount.jar -yn: Number of TaskManagers.\n-yjm: Memory allocated to the JobManager (in MB).\n-ytm: Memory allocated to each TaskManager (in MB). Dynamic Property Configuration:\nUse -D to override specific configurations temporarily:\n-Dfs.overwrite-files=true -Dtaskmanager.network.numberOfBuffers=16368 To switch from YARN session mode to standalone mode:\nDelete the /tmp/.yarn-properties-root file, which contains session details from the previous YARN session.\nStart the standalone cluster or configure for another mode as needed.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Deployment Modes in Flink","level":3,"id":"Deployment_Modes_in_Flink_0"},{"heading":"Flink on YARN Deployment Architecture","level":3,"id":"Flink_on_YARN_Deployment_Architecture_0"},{"heading":"Preparing for Cluster Startup","level":3,"id":"Preparing_for_Cluster_Startup_0"},{"heading":"YARN Deployment Modes","level":3,"id":"YARN_Deployment_Modes_0"},{"heading":"<strong>1. YARN Session Mode</strong>","level":4,"id":"**1._YARN_Session_Mode**_0"},{"heading":"<strong>2. YARN Cluster Mode</strong>","level":4,"id":"**2._YARN_Cluster_Mode**_0"},{"heading":"Switching Deployment Modes","level":3,"id":"Switching_Deployment_Modes_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/6.-flink/1.-deployment-and-startup.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641777,"modifiedTime":1737554783000,"sourceSize":4833,"sourcePath":"Big Data/6. Flink/1. Deployment and Startup.md","exportPath":"big-data/6.-flink/1.-deployment-and-startup.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/6.-flink/2.-architecture.html":{"title":"2. Architecture","icon":"","description":"Flink programs are built using two fundamental constructs: streams and transformations.\nStreams: Continuous flows of data records, which can be finite (batch) or infinite (streaming).\nTransformations: Operations that process one or more input streams to produce one or more output streams.\nA Flink program consists of the following key components: Source: Defines the data input. Flink provides various types of sources: Local collection-based sources: Load data from in-memory collections.\nFile-based sources: Read data from files.\nNetwork socket-based sources: Consume data from network sockets.\nCustom sources: Integrate with systems like Kafka, RabbitMQ, or user-defined data sources. Transformation: Operations applied to data streams to process or transform the data. Common transformations include: map(), flatMap(), filter(), keyBy(), reduce(), window(), union()\nAdvanced operations: window join(), split(), select(), etc. Sink: Defines the data output. Flink supports various sinks, such as: Writing to files or printing to the console.\nSending data to sockets.\nCustom sinks, including Kafka, RabbitMQ, MySQL, Elasticsearch, Cassandra, Hadoop FileSystem, and more. When a Flink program is executed, it maps to a Streaming Dataflow, comprising streams and transformation operators. The dataflow begins with one or more source operators and ends with one or more sink operators.“Z-Archive/Pictures/Big Data/flinkOperator.png” could not be found.\nStream Partitions: A stream may consist of multiple partitions, allowing parallel processing.\nOperator Subtasks: Each operator can have multiple subtasks, running independently in separate threads. Subtasks can be executed on different machines or containers.\nParallelism: The parallelism of a stream matches the parallelism of the operator generating the stream.\nAdjusting operator parallelism allows scalable and efficient execution. One-to-One Mode: Maintains both partitioning and order of elements between connected operators.\nExample: A connection from a Source to a Map operator retains element order. Redistributing Mode: Alters partitioning of the data. Examples: keyBy(): Repartitions the stream based on a key's hash value.\nbroadcast(): Sends all data to all operator subtasks.\nrebalance(): Randomly redistributes data across subtasks. In Flink, every operation is implemented as an operator. During execution, Flink optimizes operators by merging them into an operator chain. An operator chain:\nRepresents an execution unit that runs as a single thread on a TaskManager.\nImproves efficiency by reducing inter-operator communication overhead.\n“taskOperatorChain.png” could not be found.\nThe Flink execution process involves the following steps: Generating the DAG: Flink converts the program into a Directed Acyclic Graph (DAG) of tasks and operators. Submission to JobManager: The JobClient submits the DAG to the JobManager. Task Scheduling: The JobManager assigns tasks to available TaskManagers based on resource availability. Execution: Tasks run on TaskManagers, exchanging data as needed. Resource Monitoring: The JobManager monitors TaskManager heartbeats to ensure smooth execution and manage resource allocation. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Flink Program Structure","level":3,"id":"Flink_Program_Structure_0"},{"heading":"Components of a Flink Program","level":3,"id":"Components_of_a_Flink_Program_0"},{"heading":"Parallel Data Streams in Flink","level":3,"id":"Parallel_Data_Streams_in_Flink_0"},{"heading":"Key Concepts","level":4,"id":"Key_Concepts_0"},{"heading":"Data Transmission Modes Between Operators","level":4,"id":"Data_Transmission_Modes_Between_Operators_0"},{"heading":"Task and Operator Chain","level":3,"id":"Task_and_Operator_Chain_0"},{"heading":"Task Scheduling and Execution","level":3,"id":"Task_Scheduling_and_Execution_0"},{"heading":"Key Components","level":4,"id":"Key_Components_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/6.-flink/2.-architecture.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641778,"modifiedTime":1737554783000,"sourceSize":5305,"sourcePath":"Big Data/6. Flink/2. Architecture.md","exportPath":"big-data/6.-flink/2.-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/6.-flink/3.-flink-operators.html":{"title":"3. Flink Operators","icon":"","description":"Flink, like Spark, is a unified processing framework that supports both batch processing (DataSet) and stream processing (DataStream). This guide divides Flink operators into two main categories: DataSet Operators (for batch processing) and DataStream Operators (for stream processing).Reads data from a local collection.val env = ExecutionEnvironment.getExecutionEnvironment\nval textDataSet: DataSet[String] = env.fromCollection( List(\"1,张三\", \"2,李四\", \"3,王五\", \"4,赵六\")\n)\nReads data from a file.val textDataSet: DataSet[String] = env.readTextFile(\"/data/a.txt\")\nReads all files in a directory, including subdirectories.val parameters = new Configuration\nparameters.setBoolean(\"recursive.file.enumeration\", true)\nval file = env.readTextFile(\"/data\").withParameters(parameters)\nFlink can automatically recognize and decompress certain file types (e.g., .gz, .bz2).val file = env.readTextFile(\"/data/file.gz\")\nval env = ExecutionEnvironment.getExecutionEnvironment\nval textDataSet: DataSet[String] = env.fromCollection( List(\"张三,1\", \"李四,2\", \"王五,3\", \"张三,4\")\n)\nTransforms each element in the DataSet.case class User(name: String, id: String)\nval userDataSet: DataSet[User] = textDataSet.map { text =&gt; val fieldArr = text.split(\",\") User(fieldArr(0), fieldArr(1))\n}\nuserDataSet.print()\nTransforms each element into 0...n elements.val result = textDataSet.flatMap(line =&gt; line) .groupBy(0) // Group by the first element .sum(1) // Aggregate the second element\nresult.print()\nApplies a transformation to all elements in a partition.val result: DataSet[User] = textDataSet.mapPartition { lines =&gt; lines.map(index =&gt; User(index._1, index._2))\n}\nresult.print()\nFilters elements that meet certain conditions.val source: DataSet[String] = env.fromElements(\"java\", \"scala\", \"java\")\nval filter: DataSet[String] = source.filter(line =&gt; line.contains(\"java\"))\nfilter.print()\nAggregates elements within a DataSet or group.val source = env.fromElements((\"java\", 1), (\"scala\", 1), (\"java\", 1))\nval reduceData = source.groupBy(0).reduce((x, y) =&gt; (x._1, x._2 + y._2))\nreduceData.print()\nOptimized version of reduce for grouped aggregation.val result: DataSet[(String, Int)] = source.groupBy(0).reduceGroup { (in: Iterator[(String, Int)], out: Collector[(String, Int)]) =&gt; val tuple = in.reduce((x, y) =&gt; (x._1, x._2 + y._2)) out.collect(tuple)\n}\nresult.print()\nSelects elements with the minimum or maximum value.val result = textDataSet .groupBy(0) // Group by name .minBy(1) // Select the minimum value\nPerforms aggregation operations (e.g., MAX, MIN) on tuples.val value = input.groupBy(1).aggregate(Aggregations.MAX, 2)\nvalue.print()\nRemoves duplicate elements from a DataSet.val value: DataSet[(Int, String, Double)] = input.distinct(1)\nvalue.print()\nSelects the first N elements.input.first(2)\nJoins two DataSets based on specific conditions.val joinData = s1.join(s2).where(0).equalTo(0) { (s1, s2) =&gt; (s1._1, s1._2, s2._2, s1._3)\n}\nPerforms a left outer join.text1.leftOuterJoin(text2).where(0).equalTo(0).apply((first, second) =&gt; { if (second == null) (first._1, first._2, \"null\") else (first._1, first._2, second._2)\n}).print()\nPerforms a Cartesian product.val cross = input1.cross(input2) { (input1, input2) =&gt; (input1._1, input1._2, input1._3, input2._2)\n}\ncross.print()\nCombines two or more DataSets without deduplication.val unionData: DataSet[String] = elements1.union(elements2)\nRedistributes data evenly to avoid data skew.val rebalance = filterData.rebalance()\nPartitions data using a hash key.val unique = collection.partitionByHash(1).mapPartition { line =&gt; line.map(x =&gt; (x._1, x._2, x._3))\n}\nunique.writeAsText(\"hashPartition\", WriteMode.NO_OVERWRITE)\nenv.execute()\nPartitions data by range based on a key.val unique = collection.partitionByRange(x =&gt; x._1).mapPartition { line =&gt; line.map(x =&gt; (x._1, x._2, x._3))\n}\nunique.writeAsText(\"rangePartition\", WriteMode.OVERWRITE)\nenv.execute()\nSorts partitions by a specified field.val result = ds.sortPartition(1, Order.DESCENDING).collect()\nprintln(result)\nOutputs data to a local collection.result.collect()\nOutputs data to a file. Flink supports:\nVarious storage devices: local files, HDFS, etc.\nDifferent file formats: text files, CSV files, etc.\nExample:// Write data to a local file\nresult.writeAsText(\"/data/a\", WriteMode.OVERWRITE) // Write data to HDFS\nresult.writeAsText(\"hdfs://node01:9000/data/a\", WriteMode.OVERWRITE)\nFlink allows adding data sources to your program using StreamExecutionEnvironment.addSource(source). There are four main categories of sources:\nCollection-based source: Reads data from local collections.\nFile-based source: Reads text files that follow TextInputFormat and returns them as strings.\nSocket-based source: Reads from sockets; elements can be split using delimiters.\nCustom source: Create a custom source by implementing SourceFunction (non-parallel) or ParallelSourceFunction (parallel) interfaces.\nAdd the dependency:&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt;\n&lt;/dependency&gt;\nWrite Kafka data into Flink:val properties = new Properties()\nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\")\nproperties.setProperty(\"group.id\", \"consumer-group\")\nproperties.setProperty(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\nproperties.setProperty(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\nproperties.setProperty(\"auto.offset.reset\", \"latest\") val source = env.addSource(new FlinkKafkaConsumer011[String](\"sensor\", new SimpleStringSchema(), properties))\nval source = env.socketTextStream(\"IP\", PORT)\nTransforms each element in the stream.dataStream.map { x =&gt; x * 2 }\nGenerates zero, one, or more elements for each input element (e.g., splitting sentences into words).dataStream.flatMap { str =&gt; str.split(\" \") }\nFilters elements based on a condition.dataStream.filter { _ != 0 }\nPartitions the stream logically into non-overlapping partitions based on keys.dataStream.keyBy(0)\nApplies a rolling reduce operation on keyed data streams.keyedStream.reduce { _ + _ }\nApplies a rolling fold operation with an initial value.val result: DataStream[String] = keyedStream.fold(\"start\") { (str, i) =&gt; str + \"-\" + i\n}\nPerforms rolling aggregations (min, max, minBy, maxBy).keyedStream.sum(0)\nkeyedStream.min(0)\nkeyedStream.max(0)\nGroups keyed streams based on time or count.dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5)))\nGroups all events across the stream into one window.dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5)))\nApplies a user-defined function to each window.windowedStream.apply { WindowFunction }\nApplies a reduce function to a window.windowedStream.reduce { _ + _ }\nCombines multiple streams into one.dataStream.union(otherStream1, otherStream2, ...)\nJoins two streams over a shared window and keys.dataStream.join(otherStream) .where(&lt;key selector&gt;).equalTo(&lt;key selector&gt;) .window(TumblingEventTimeWindows.of(Time.seconds(3))) .apply(new JoinFunction() {...})\nJoins streams based on time intervals.keyedStream.intervalJoin(otherKeyedStream) .between(Time.milliseconds(-2), Time.milliseconds(2)) .process(new IntervalJoinFunction() {...})\nConnects two streams, maintaining their types.val connectedStreams = someStream.connect(otherStream)\nSplits a stream based on custom criteria.val split = someDataStream.split { num =&gt; if (num % 2 == 0) List(\"even\") else List(\"odd\")\n}\nSelects one or more streams from a split stream.val even = split.select(\"even\")\nval odd = split.select(\"odd\")\nval sinkTopic = \"test\"\ncase class Student(id: Int, name: String, addr: String, sex: String) val studentStream: DataStream[String] = dataStream.map { student =&gt; toJsonString(student)\n} val prop = new Properties()\nprop.setProperty(\"bootstrap.servers\", \"node01:9092\") val myProducer = new FlinkKafkaProducer011[String]( sinkTopic, new KeyedSerializationSchemaWrapper[String](new SimpleStringSchema()), prop\n)\nstudentStream.addSink(myProducer)\nstudentStream.print()\nenv.execute(\"Flink add sink\")\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"DataSet Operators (Batch Processing)","level":3,"id":"DataSet_Operators_(Batch_Processing)_0"},{"heading":"1. Source Operators","level":4,"id":"1._**Source_Operators**_0"},{"heading":"1.1 <code>fromCollection</code>","level":5,"id":"1.1_`fromCollection`_0"},{"heading":"<code>readTextFile</code>","level":5,"id":"`readTextFile`_0"},{"heading":"Recursive File Enumeration","level":5,"id":"Recursive_File_Enumeration_0"},{"heading":"Reading Compressed Files","level":5,"id":"Reading_Compressed_Files_0"},{"heading":"Transformation Operators","level":4,"id":"Transformation_Operators_0"},{"heading":"Setup Example","level":5,"id":"Setup_Example_0"},{"heading":"<code>map</code>","level":5,"id":"`map`_0"},{"heading":"<code>flatMap</code>","level":5,"id":"`flatMap`_0"},{"heading":"<code>mapPartition</code>","level":5,"id":"`mapPartition`_0"},{"heading":"<code>filter</code>","level":5,"id":"`filter`_0"},{"heading":"<code>reduce</code>","level":5,"id":"`reduce`_0"},{"heading":"<code>reduceGroup</code>","level":5,"id":"`reduceGroup`_0"},{"heading":"<code>minBy</code> and <code>maxBy</code>","level":5,"id":"`minBy`_and_`maxBy`_0"},{"heading":"<code>Aggregate</code>","level":5,"id":"`Aggregate`_0"},{"heading":"<code>distinct</code>","level":5,"id":"`distinct`_0"},{"heading":"<code>first</code>","level":5,"id":"`first`_0"},{"heading":"<code>join</code>","level":5,"id":"`join`_0"},{"heading":"<code>leftOuterJoin</code>","level":5,"id":"`leftOuterJoin`_0"},{"heading":"<code>cross</code>","level":5,"id":"`cross`_0"},{"heading":"<code>union</code>","level":5,"id":"`union`_0"},{"heading":"<code>rebalance</code>","level":5,"id":"`rebalance`_0"},{"heading":"<code>partitionByHash</code>","level":5,"id":"`partitionByHash`_0"},{"heading":"<code>partitionByRange</code>","level":5,"id":"`partitionByRange`_0"},{"heading":"<code>sortPartition</code>","level":5,"id":"`sortPartition`_0"},{"heading":"Sink Operators","level":4,"id":"Sink_Operators_0"},{"heading":"1. collect","level":5,"id":"1._**`collect`**_0"},{"heading":"2. writeAsText","level":5,"id":"2._**`writeAsText`**_0"},{"heading":"DataStream Operators","level":3,"id":"DataStream_Operators_0"},{"heading":"1. Source Operators","level":4,"id":"1._**Source_Operators**_1"},{"heading":"Example: Adding Kafka as a Source","level":5,"id":"Example_Adding_Kafka_as_a_Source_0"},{"heading":"Example: Socket-based Source","level":5,"id":"Example_Socket-based_Source_0"},{"heading":"2. Transformation Operators","level":4,"id":"2._**Transformation_Operators**_0"},{"heading":"1. map","level":5,"id":"1._**`map`**_0"},{"heading":"2. flatMap","level":5,"id":"2._**`flatMap`**_0"},{"heading":"3. filter","level":5,"id":"3._**`filter`**_0"},{"heading":"4. keyBy","level":5,"id":"4._**`keyBy`**_0"},{"heading":"5. reduce","level":5,"id":"5._**`reduce`**_0"},{"heading":"6. fold","level":5,"id":"6._**`fold`**_0"},{"heading":"7. Aggregations","level":5,"id":"7._**Aggregations**_0"},{"heading":"8. Window","level":5,"id":"8._**Window**_0"},{"heading":"9. WindowAll","level":5,"id":"9._**WindowAll**_0"},{"heading":"10. Window Apply","level":5,"id":"10._**Window_Apply**_0"},{"heading":"11. Window Reduce","level":5,"id":"11._**Window_Reduce**_0"},{"heading":"12. Union","level":5,"id":"12._**Union**_0"},{"heading":"13. Window Join","level":5,"id":"13._**Window_Join**_0"},{"heading":"14. Interval Join","level":5,"id":"14._**Interval_Join**_0"},{"heading":"15. Connect","level":5,"id":"15._**Connect**_0"},{"heading":"16. Split","level":5,"id":"16._**Split**_0"},{"heading":"17. Select","level":5,"id":"17._**Select**_0"},{"heading":"Kafka Sink Example","level":4,"id":"Kafka_Sink_Example_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/6.-flink/3.-flink-operators.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641779,"modifiedTime":1737554783000,"sourceSize":10185,"sourcePath":"Big Data/6. Flink/3. Flink Operators.md","exportPath":"big-data/6.-flink/3.-flink-operators.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/6.-flink/4.-time-and-window-in-stream-processing.html":{"title":"4. Time and Window in Stream Processing","icon":"","description":"Flink is a stream-oriented and real-time computation engine. Two key concepts in Flink are streaming and real-time: Streaming: Data continuously flows in without boundaries. However, computations need to operate on bounded data, which requires defining boundaries. These boundaries can be set based on time intervals or data volume. Real-time: Data is processed as soon as it arrives, producing results immediately. There are two types of computations: Within boundaries: E.g., counting the number of news articles read by users in the last five minutes.\nCombining with external data: E.g., identifying the regions where users reading news in the last five minutes come from by joining with an external table. Time plays a crucial role when defining boundaries in Flink. There are three types of time in Flink:\nEvent Time: The time when an event is created. For example, logs often include timestamps representing their creation time. Event time is used for accurate event processing.\nIngestion Time: The time when data enters Flink.\nProcessing Time: The local system time of the operator performing the computation. This is the default time attribute in Flink.\nFor example, if a log enters Flink at 2021-01-22 10:00:00.123 and reaches a window operator at 2021-01-22 10:00:01.234, the event time might be 2021-01-06 18:37:15.624. To analyze logs within a specific time range, event time is the most meaningful.A window defines boundaries for computations over unbounded data streams by splitting the stream into finite \"buckets\". This is crucial for processing infinite data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Time","level":3,"id":"Time_0"},{"heading":"Window","level":3,"id":"Window_0"},{"heading":"Window Types","level":4,"id":"Window_Types_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/6.-flink/4.-time-and-window-in-stream-processing.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641805,"modifiedTime":1737554783000,"sourceSize":5387,"sourcePath":"Big Data/6. Flink/4. Time and Window in Stream Processing.md","exportPath":"big-data/6.-flink/4.-time-and-window-in-stream-processing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/7.-kafka/0.-kafka.html":{"title":"0. Kafka","icon":"","description":"Kafka is a distributed message queue based on the publish/subscribe model, primarily used in the field of real-time big data processing.Benefits of Using Message Queues: Decoupling:\nMessage queues allow you to independently scale or modify processing on either side, as long as both sides adhere to the same interface constraints. Recoverability:\nWhen a component of the system fails, it doesn’t affect the entire system. Message queues reduce the coupling between processes, so even if a process handling messages crashes, the messages in the queue can still be processed when the system recovers. Buffering:\nIt helps to control and optimize the flow of data through the system, resolving discrepancies in the processing speed between producing and consuming messages. Flexibility &amp; Peak Handling Capacity:\nApplications need to remain functional even under a sudden surge in traffic, although such traffic spikes are often rare. Allocating resources to handle peak traffic at all times would be wasteful. Using a message queue allows critical components to withstand peak traffic without collapsing due to sudden overloads. Asynchronous Communication:\nOften, users do not want or need to process messages immediately. Message queues provide an asynchronous processing mechanism, allowing users to place a message in the queue without processing it immediately. You can place as many messages in the queue as needed and process them later. Point-to-Point Model (One-to-One, Consumer Actively Pulls Data, Message is Cleared After Receipt): The message producer sends messages to a Queue.\nThe message consumer retrieves and consumes messages from the Queue.\nOnce consumed, the message is removed from the Queue, meaning other consumers cannot consume the same message.\nThe Queue supports multiple consumers, but each message is only consumed by one consumer. Publish/Subscribe Model (One-to-Many, Consumers Do Not Clear Messages After Consumption): The message producer (publisher) publishes messages to a Topic.\nMultiple message consumers (subscribers) consume the message.\nUnlike the point-to-point model, messages published to a Topic are consumed by all subscribers. “Z-Archive/Pictures/Big Data/kafka architecture.png” could not be found.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Message Queue","level":3,"id":"Message_Queue_0"},{"heading":"Application Scenarios of Traditional Message Queues","level":4,"id":"Application_Scenarios_of_Traditional_Message_Queues_0"},{"heading":"Two Modes of Message Queues","level":4,"id":"Two_Modes_of_Message_Queues_0"},{"heading":"Kafka Basic Architecture","level":3,"id":"Kafka_Basic_Architecture_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/7.-kafka/0.-kafka.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641838,"modifiedTime":1737554783000,"sourceSize":4423,"sourcePath":"Big Data/7. Kafka/0. Kafka.md","exportPath":"big-data/7.-kafka/0.-kafka.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/7.-kafka/1.-deployment-&-commands.html":{"title":"1. Deployment & Commands","icon":"","description":"You can download Kafka from the official website:\n<a data-tooltip-position=\"top\" aria-label=\"http://kafka.apache.org/downloads.html\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://kafka.apache.org/downloads.html\" target=\"_self\">Kafka Downloads</a> Extract the Installation Package:\n[a@hadoop102 software]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/ Rename the Extracted Directory:\n[a@hadoop102 module]$ mv kafka_2.11-0.11.0.0/ kafka Create a logs Directory in /opt/module/kafka:\n[a@hadoop102 kafka]$ mkdir logs Modify the Configuration File:\n[a@hadoop102 kafka]$ cd config/\n[a@hadoop102 config]$ vi server.properties Add or modify the following configuration: # Globally unique broker ID, must not be duplicated broker.id=0 # Enable topic deletion delete.topic.enable=true # Number of threads handling network requests num.network.threads=3 # Number of threads handling disk I/O num.io.threads=8 # Size of the send socket buffer socket.send.buffer.bytes=102400 # Size of the receive socket buffer socket.receive.buffer.bytes=102400 # Maximum size of the request socket buffer socket.request.max.bytes=104857600 # Path for Kafka log storage log.dirs=/opt/module/kafka/logs # Number of partitions for the topic on this broker num.partitions=1 # Number of threads for data recovery and cleanup num.recovery.threads.per.data.dir=1 # Maximum retention time for segment files, after which they are deleted log.retention.hours=168 # Zookeeper cluster connection addresses zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181 Set Environment Variables:\n[a@hadoop102 module]$ sudo vi /etc/profile Add the following lines:\n# KAFKA_HOME\nexport KAFKA_HOME=/opt/module/kafka\nexport PATH=$PATH:$KAFKA_HOME/bin Apply the changes:\n[a@hadoop102 module]$ source /etc/profile Distribute the Installation Package:\n[a@hadoop102 module]$ xsync kafka/ Note: After distribution, remember to configure environment variables on the other machines. Modify the Configuration on hadoop103 and hadoop104: On hadoop103, set broker.id=1.\nOn hadoop104, set broker.id=2.\nNote: The broker.id must be unique and not duplicated. Start the Cluster: Start Kafka on each node (hadoop102, hadoop103, hadoop104):\n[a@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties\n[a@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties\n[a@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties Stop the Cluster: Stop Kafka on each node:\n[a@hadoop102 kafka]$ bin/kafka-server-stop.sh stop\n[a@hadoop103 kafka]$ bin/kafka-server-stop.sh stop\n[a@hadoop104 kafka]$ bin/kafka-server-stop.sh stop Kafka Cluster Start Script: You can use the following script to start Kafka on all nodes:\nfor i in hadoop102 hadoop103 hadoop104\ndo echo \"========== $i ==========\" ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'\ndone List All Topics on the Current Server:\n[a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list Create a Topic:\n[a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first Options: --topic: Defines the topic name.\n--replication-factor: Defines the number of replicas.\n--partitions: Defines the number of partitions. Delete a Topic:\n[a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first Note: The server.properties file must have delete.topic.enable=true set, otherwise the topic will only be marked for deletion. Send a Message:\n[a@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first\n&gt;hello world\n&gt;a a Consume Messages:\n[a@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first Option: --from-beginning: Reads all past data from the topic. View Details of a Specific Topic:\n[a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first Change the Number of Partitions:\n[a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6 ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Deployment &amp; Commands","level":1,"id":"1._Deployment_&_Commands_0"},{"heading":"Installation and Deployment","level":3,"id":"Installation_and_Deployment_0"},{"heading":"Cluster Planning","level":4,"id":"Cluster_Planning_0"},{"heading":"Downloading the JAR Package","level":4,"id":"Downloading_the_JAR_Package_0"},{"heading":"Cluster Deployment","level":4,"id":"Cluster_Deployment_0"},{"heading":"Kafka Command Line Operations","level":3,"id":"Kafka_Command_Line_Operations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/7.-kafka/1.-deployment-&-commands.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641839,"modifiedTime":1737554783000,"sourceSize":5381,"sourcePath":"Big Data/7. Kafka/1. Deployment & Commands.md","exportPath":"big-data/7.-kafka/1.-deployment-&-commands.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/7.-kafka/2.-architecture.html":{"title":"2. Architecture","icon":"","description":"In Kafka, messages are categorized by topics. Producers produce messages, and consumers consume messages, both targeting specific topics. A topic is a logical concept, whereas a partition is a physical concept. Each partition corresponds to a log file where the data produced by the producer is stored. The producer's data is continuously appended to the end of this log file, and each piece of data has its own offset. Each consumer in a consumer group records the offset of the last consumed message to resume consumption from that point in case of a failure.To prevent log files from becoming too large and inefficient for data location, Kafka uses sharding and indexing mechanisms, dividing each partition into multiple segments. Each segment corresponds to two files: a .index file and a .log file. These files are located in a folder named after the topic and partition number. For example, if the topic first has three partitions, the corresponding folders would be first-0, first-1, and first-2.00000000000000000000.index\n00000000000000000000.log\n00000000000000170410.index\n00000000000000170410.log\n00000000000000239430.index\n00000000000000239430.log\nThe .index file stores index information, while the .log file stores the actual data. Metadata in the index file points to the physical offset of the message in the corresponding log file. Below is a diagram illustrating the structure of the index and log files. Reasons for Partitioning: Scalability: Each partition can be adjusted to fit the machine it resides on, and a topic can consist of multiple partitions, allowing the cluster to handle any amount of data.\nIncreased Concurrency: Partitioning allows read and write operations at the partition level, increasing throughput. Partitioning Principles:\nWhen a producer sends data, it is encapsulated into a ProducerRecord object. If a partition is specified, the specified value is used as the partition value.\nIf no partition is specified but a key is provided, the partition is determined by taking the hash of the key modulo the number of partitions in the topic.\nIf neither a partition nor a key is specified, a random integer is generated during the first call, incremented on subsequent calls, and then modulo the total number of available partitions to get the partition value (round-robin algorithm). To ensure that the data sent by the producer is reliably delivered to the designated topic, each partition of the topic must send an acknowledgment (ack) to the producer after receiving the data. If the producer receives the ack, it proceeds to the next round of sending; otherwise, it retransmits the data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Workflow and File Storage Mechanism","level":3,"id":"Workflow_and_File_Storage_Mechanism_0"},{"heading":"Kafka Producer","level":3,"id":"Kafka_Producer_0"},{"heading":"Partition Strategy","level":4,"id":"Partition_Strategy_0"},{"heading":"Data Reliability Assurance","level":4,"id":"Data_Reliability_Assurance_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/7.-kafka/2.-architecture.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641840,"modifiedTime":1737554783000,"sourceSize":14235,"sourcePath":"Big Data/7. Kafka/2. Architecture.md","exportPath":"big-data/7.-kafka/2.-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/7.-kafka/3.-api.html":{"title":"3. API","icon":"","description":"Kafka's Producer sends messages asynchronously. The message-sending process involves two threads—main thread and Sender thread—as well as a shared variable—RecordAccumulator. The main thread sends messages to the RecordAccumulator, and the Sender thread continuously pulls messages from the RecordAccumulator and sends them to the Kafka broker.Related Parameters:\nbatch.size: The sender sends data only after accumulating data up to batch.size.\nlinger.ms: If the data does not reach batch.size, the sender will send the data after waiting for linger.ms. Import Dependencies:\n&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;\n&lt;/dependency&gt; Write Code:\nRequired Classes: KafkaProducer: Creates a producer object to send data.\nProducerConfig: Retrieves a series of configuration parameters.\nProducerRecord: Encapsulates each piece of data into a ProducerRecord object. 1. API without Callback Function:\npackage com.a.kafka; import org.apache.kafka.clients.producer.*;\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); // Kafka cluster, broker-list props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 1); // Retry count props.put(\"batch.size\", 16384); // Batch size props.put(\"linger.ms\", 1); // Wait time props.put(\"buffer.memory\", 33554432); // RecordAccumulator buffer size props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;&gt;(\"first\", Integer.toString(i), Integer.toString(i))); } producer.close(); }\n} 2. API with Callback Function:\nThe callback function is called when the producer receives an ack. It is an asynchronous call with two parameters, RecordMetadata and Exception. If Exception is null, the message was sent successfully; otherwise, it failed.\nNote: If the message fails to send, it will automatically retry, so no need to manually retry in the callback.\npackage com.a.kafka; import org.apache.kafka.clients.producer.*;\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); // Kafka cluster, broker-list props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 1); // Retry count props.put(\"batch.size\", 16384); // Batch size props.put(\"linger.ms\", 1); // Wait time props.put(\"buffer.memory\", 33554432); // RecordAccumulator buffer size props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;&gt;(\"first\", Integer.toString(i), Integer.toString(i)), new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (exception == null) { System.out.println(\"Success -&gt; \" + metadata.offset()); } else { exception.printStackTrace(); } } }); } producer.close(); }\n} Synchronous sending means that after a message is sent, the current thread is blocked until an ack is received. Since the send method returns a Future object, you can achieve synchronous sending by calling the get method on the Future object.package com.a.kafka; import org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties;\nimport java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); // Kafka cluster, broker-list props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 1); // Retry count props.put(\"batch.size\", 16384); // Batch size props.put(\"linger.ms\", 1); // Wait time props.put(\"buffer.memory\", 33554432); // RecordAccumulator buffer size props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;&gt;(\"first\", Integer.toString(i), Integer.toString(i))).get(); } producer.close(); }\n}\nWhen consuming data, Kafka's persistence ensures that data reliability is easily guaranteed, so there is no need to worry about data loss. However, since consumers may experience failures, such as power outages, during consumption, they need to resume from where they left off after recovery. Therefore, offset maintenance is a crucial consideration when consuming data. Import Dependencies:\n&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;\n&lt;/dependency&gt; Write Code:\nRequired Classes: KafkaConsumer: Creates a consumer object to consume data.\nConsumerConfig: Retrieves a series of configuration parameters.\nConsumerRecord: Encapsulates each piece of data into a ConsumerRecord object. Kafka provides an automatic offset committing feature to allow you to focus on your business logic.\nRelated Parameters for Automatic Offset Committing: enable.auto.commit: Enables or disables automatic offset committing.\nauto.commit.interval.ms: The time interval for automatically committing the offset. Code for Automatic Offset Committing:\npackage com.a.kafka; import org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays;\nimport java.util.Properties; public class CustomConsumer { public static void main(String[] args) { Properties props = new Properties(); // Kafka cluster props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"true\"); props.put(\"auto.commit.interval.ms\", \"1000\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\")); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); } } }\n} Although automatic offset committing is simple and convenient, it commits offsets based on time, making it difficult for developers to control the timing. Therefore, Kafka also provides a manual offset committing API. There are two methods for manual offset committing: commitSync (synchronous commit) and commitAsync (asynchronous commit). Both methods commit the highest offset of the polled batch of data; the difference is that commitSync blocks the current thread until the commit succeeds and automatically retries in case of failure, while commitAsync does not retry, which may result in a failed commit.1. Synchronous Offset Committing:\nSince synchronous offset committing has a retry mechanism, it is more reliable. Below is an example of synchronous offset committing.package com.a.kafka.consumer; import org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients .consumer.KafkaConsumer; import java.util.Arrays;\nimport java.util.Properties; public class CustomConsumer { public static void main(String[] args) { Properties props = new Properties(); // Kafka cluster props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"false\"); // Disable automatic offset committing props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\")); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); } consumer.commitSync(); // Synchronous commit, blocks current thread until offset is committed } }\n}\n2. Asynchronous Offset Committing:\nAlthough synchronous offset committing is more reliable, it blocks the current thread until the commit succeeds, significantly affecting throughput. Therefore, asynchronous offset committing is often preferred. Code for Asynchronous Offset Committing:package com.a.kafka.consumer; import org.apache.kafka.clients.consumer.*;\nimport org.apache.kafka.common.TopicPartition; import java.util.Arrays;\nimport java.util.Map;\nimport java.util.Properties; public class CustomConsumer { public static void main(String[] args) { Properties props = new Properties(); // Kafka cluster props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"false\"); // Disable automatic offset committing props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\")); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); } consumer.commitAsync(new OffsetCommitCallback() { @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) { if (exception != null) { System.err.println(\"Commit failed for \" + offsets); } } }); } }\n}\n3. Analyzing Data Loss and Duplication:\nWhether using synchronous or asynchronous offset committing, there is a risk of data loss or duplication. Committing the offset before consumption may result in data loss, while consuming before committing the offset may result in data duplication.Before Kafka version 0.9, offsets were stored in Zookeeper. Starting from version 0.9, offsets are stored in an internal Kafka topic by default. Kafka also allows custom offset storage.Maintaining offsets is quite complex because it requires consideration of consumer rebalancing. Rebalancing occurs when a new consumer joins a consumer group, an existing consumer leaves, or the partitions of the subscribed topic change, triggering partition reassignment. After rebalancing, the partitions each consumer consumes may change. Therefore, consumers must first obtain the partitions assigned to them and locate the most recently committed offset for each partition to resume consumption.To implement custom offset storage, the ConsumerRebalanceListener interface is used. Below is an example where the methods for committing and retrieving offsets need to be implemented according to the chosen offset storage system.package com.a.kafka.consumer; import org.apache.kafka.clients.consumer.*;\nimport org.apache.kafka.common.TopicPartition; import java.util.*; public class CustomConsumer { private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;(); public static void main(String[] args) { Properties props = new Properties(); // Kafka cluster props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"group.id\", \"test\"); props.put(\"enable.auto.commit\", \"false\"); // Disable automatic offset committing props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(\"first\"), new ConsumerRebalanceListener() { // Called before rebalance @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) { commitOffset(currentOffset); } // Called after rebalance @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) { currentOffset.clear(); for (TopicPartition partition : partitions) { consumer.seek(partition, getOffset(partition)); // Resume consumption from the most recently committed offset } } }); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value()); currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset()); } commitOffset(currentOffset); // Asynchronous commit } } // Retrieve the most recent offset for a partition private static long getOffset(TopicPartition partition) { // Implement offset retrieval logic here return 0; } // Commit the offsets for all partitions of this consumer private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) { // Implement offset commit logic here }\n}\nProducer interceptors were introduced in Kafka version 0.10, primarily used for implementing client-side custom control logic. For producers, interceptors allow users to make custom modifications to messages before they are sent and before the producer's callback logic is triggered. Producers can specify multiple interceptors to act sequentially on the same message, forming an interceptor chain. The ProducerInterceptor interface is defined in org.apache.kafka.clients.producer.ProducerInterceptor and includes the following methods:\nconfigure(configs): Called when configuration information is obtained and data is initialized.\nonSend(ProducerRecord): This method is encapsulated within the KafkaProducer.send method and runs in the user main thread. The producer ensures that this method is called before the message is serialized and the partition is computed. Users can perform any operations on the message in this method, but it is recommended not to modify the message's topic and partition, as this would affect the target partition's computation.\nonAcknowledgement(RecordMetadata, Exception): Called after the message is successfully sent to the Kafka broker from the RecordAccumulator, or in case of a failure during sending. This method usually runs before the producer's callback logic is triggered. onAcknowledgement runs in the producer's IO thread, so heavy logic should not be included in this method as it may slow down the producer's message-sending efficiency.\nclose(): Closes the interceptor, mainly for resource cleanup.\nAs mentioned, interceptors may run in multiple threads, so users need to ensure thread safety in their implementation. Additionally, if multiple interceptors are specified, the producer will call them in the specified order and only capture and log any exceptions thrown by the interceptors without propagating them. This behavior should be carefully considered during usage. Requirement: Implement a simple interceptor chain consisting of two interceptors. The first interceptor adds a timestamp to the beginning of the message value before sending, and the second interceptor updates the count of successfully sent and failed messages after sending. Implementation:\n1. Add Timestamp Interceptor:\npackage com.a.kafka.interceptor; import org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata; public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; { @Override public void configure(Map&lt;String, ?&gt; configs) { } @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { // Create a new record, adding the timestamp to the beginning of the message value return new ProducerRecord&lt;&gt;(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + \",\" + record.value()); } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { } @Override public void close() { }\n} 2. Count Successful and Failed Message Sends:\npackage com.a.kafka.interceptor; import org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata; public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt; { private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) {\n} @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { return record; } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { // Count successful and failed sends if (exception == null) { successCounter++; } else { errorCounter++; } } @Override public void close() { // Save results System.out.println(\"Successfully sent: \" + successCounter); System.out.println(\"Failed sends: \" + errorCounter); } }\n**3. Producer Main Program:**\n```java\npackage com.a.kafka.interceptor; import org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord; import java.util.ArrayList;\nimport java.util.List;\nimport java.util.Properties; public class InterceptorProducer { public static void main(String[] args) throws Exception { // 1. Set configuration information Properties props = new Properties(); props.put(\"bootstrap.servers\", \"hadoop102:9092\"); props.put(\"acks\", \"all\"); props.put(\"retries\", 3); props.put(\"batch.size\", 16384); props.put(\"linger.ms\", 1); props.put(\"buffer.memory\", 33554432); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 2. Build interceptor chain List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(\"com.a.kafka.interceptor.TimeInterceptor\"); interceptors.add(\"com.a.kafka.interceptor.CounterInterceptor\"); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = \"first\"; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 3. Send messages for (int i = 0; i &lt; 10; i++) { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, \"message\" + i); producer.send(record); } // 4. Close producer to trigger interceptor's close method producer.close(); }\n} Testing: Start a consumer on Kafka and then run the client Java program. [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh \\\n--bootstrap-server hadoop102:9092 --from-beginning --topic first Example output:\n1501904047034,message0\n1501904047225,message1\n1501904047230,message2\n1501904047234,message3\n1501904047236,message4\n1501904047240,message5\n1501904047243,message6\n1501904047246,message7\n1501904047249,message8\n1501904047252,message9 ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Producer API","level":3,"id":"Producer_API_0"},{"heading":"Message Sending Workflow","level":4,"id":"Message_Sending_Workflow_0"},{"heading":"Asynchronous Sending API","level":4,"id":"Asynchronous_Sending_API_0"},{"heading":"Synchronous Sending API","level":4,"id":"Synchronous_Sending_API_0"},{"heading":"Consumer API","level":3,"id":"Consumer_API_0"},{"heading":"Automatic Offset Committing","level":4,"id":"Automatic_Offset_Committing_0"},{"heading":"Manual Offset Committing","level":4,"id":"Manual_Offset_Committing_0"},{"heading":"Custom Offset Storage","level":4,"id":"Custom_Offset_Storage_0"},{"heading":"Custom Interceptor","level":3,"id":"Custom_Interceptor_0"},{"heading":"Interceptor Principle","level":4,"id":"Interceptor_Principle_0"},{"heading":"Interceptor Case Study","level":4,"id":"Interceptor_Case_Study_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/7.-kafka/3.-api.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641853,"modifiedTime":1737554783000,"sourceSize":23541,"sourcePath":"Big Data/7. Kafka/3. API.md","exportPath":"big-data/7.-kafka/3.-api.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/7.-kafka/4.-monitoring.html":{"title":"4. Monitoring","icon":"","description":" Modify Kafka Startup Command:\nModify the kafka-server-start.sh command:\nif [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then export KAFKA_HEAP_OPTS=\"-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70\" export JMX_PORT=\"9999\" #export KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\"\nfi Note: After modification, distribute the changes to other nodes before starting Kafka. Upload the Kafka Eagle Package:\nUpload the compressed package kafka-eagle-bin-1.3.7.tar.gz to the cluster's /opt/software directory. Extract the Package:\n[a@hadoop102 software]$ tar -zxvf kafka-eagle-bin-1.3.7.tar.gz Navigate to the Extracted Directory:\n[a@hadoop102 kafka-eagle-bin-1.3.7]$ ll You should see the following files:\ntotal 82932\n-rw-rw-r--. 1 a a 84920710 Aug 13 23:00 kafka-eagle-web-1.3.7-bin.tar.gz Extract kafka-eagle-web-1.3.7-bin.tar.gz to /opt/module:\n[a@hadoop102 kafka-eagle-bin-1.3.7]$ tar -zxvf kafka-eagle-web-1.3.7-bin.tar.gz -C /opt/module/ Rename the Extracted Directory:\n[a@hadoop102 module]$ mv kafka-eagle-web-1.3.7/ eagle Grant Execution Permissions to the Startup Script:\n[a@hadoop102 eagle]$ cd bin/\n[a@hadoop102 bin]$ chmod 777 ke.sh Modify the Configuration File:\nEdit the necessary configuration in the configuration file:\n######################################\n# Multi Zookeeper &amp; Kafka Cluster List\n######################################\nkafka.eagle.zk.cluster.alias=cluster1\ncluster1.zk.list=hadoop102:2181,hadoop103:2181,hadoop104:2181 ######################################\n# Kafka Offset Storage\n######################################\ncluster1.kafka.eagle.offset.storage=kafka ######################################\n# Enable Kafka Metrics\n######################################\nkafka.eagle.metrics.charts=true\nkafka.eagle.sql.fix.error=false ######################################\n# Kafka JDBC Driver Address\n######################################\nkafka.eagle.driver=com.mysql.jdbc.Driver\nkafka.eagle.url=jdbc:mysql://hadoop102:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull\nkafka.eagle.username=root\nkafka.eagle.password=000000 Add Environment Variables:\nEdit the environment profile to add Kafka Eagle:\nexport KE_HOME=/opt/module/eagle\nexport PATH=$PATH:$KE_HOME/bin Note: Don't forget to reload the profile:\nsource /etc/profile Start Kafka Eagle:\n[a@hadoop102 eagle]$ bin/ke.sh start The output should confirm a successful start:\n*******************************************************************\n* Kafka Eagle Service has started successfully.\n* Welcome, now you can visit 'http://192.168.9.102:8048/ke'\n* Account: admin, Password: 123456\n*******************************************************************\n* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;\n* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;\n******************************************************************* Note: Ensure that Zookeeper and Kafka are started before launching Kafka Eagle. Access the Monitoring Dashboard:\nOpen your browser and visit http://192.168.9.102:8048/ke to view the monitoring data. Kafka Eagle Features and Functionalities: Cluster Monitoring: Real-time monitoring of Kafka clusters, including metrics on brokers, topics, and partitions.\nTopic Management: Manage Kafka topics through the interface, view statistics, and modify configurations.\nConsumer Monitoring: Track consumer status, offset, and lag to ensure efficient message consumption.\nAlerting: Configure alerts for events like high consumer lag or decreased message throughput.\nLog and System Analysis: Analyze broker and message logs directly from the Kafka Eagle interface.\nSQL Interface: Query Kafka data using a SQL-like syntax, facilitating complex queries on streaming data.\nWeb Interface: User-friendly web interface for easy management and visualization of Kafka data and configurations. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Kafka Eagle","level":3,"id":"Kafka_Eagle_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/7.-kafka/4.-monitoring.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641855,"modifiedTime":1737554783000,"sourceSize":4519,"sourcePath":"Big Data/7. Kafka/4. Monitoring.md","exportPath":"big-data/7.-kafka/4.-monitoring.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/7.-kafka/5.-flume-integration.html":{"title":"5. Flume Integration","icon":"","description":" Configure Flume (flume-kafka.conf):\n# Define source, sink, and channel\na1.sources = r1\na1.sinks = k1\na1.channels = c1 # Source configuration\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F -c +0 /opt/module/data/flume.log\na1.sources.r1.shell = /bin/bash -c # Sink configuration\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092\na1.sinks.k1.kafka.topic = first\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1 # Channel configuration\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100 # Bind source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1 This configuration sets up Flume to collect data from a log file and send it to a Kafka topic. The exec source runs a shell command to tail a log file, ensuring real-time data capture. The data is then routed through an in-memory channel to a Kafka sink, which pushes the data to the Kafka cluster. Start Kafka Consumer in IDEA:\nStart a Kafka consumer in your development environment (IDEA) to monitor the incoming data. This step is crucial for validating that the Kafka cluster is correctly receiving and storing the data sent from Flume. Start Flume from the Flume Root Directory:\n$ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf Launching the Flume agent with the specified configuration allows Flume to begin processing data. This step initiates the data flow from the log file through Flume to Kafka. Append Data to flume.log and Check Kafka Consumer:\n$ echo hello &gt;&gt; /opt/module/data/flume.log By appending data to flume.log, you simulate new log entries. Check the Kafka consumer to see if the data appears as expected, indicating successful integration and data flow. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/7.-kafka/5.-flume-integration.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641856,"modifiedTime":1737554783000,"sourceSize":2068,"sourcePath":"Big Data/7. Kafka/5. Flume Integration.md","exportPath":"big-data/7.-kafka/5.-flume-integration.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/8.-doris/0.-doris.html":{"title":"0. Doris","icon":"","description":"Apache Doris is an MPP (Massively Parallel Processing) analytical database optimized for real-time analytics. Originally developed in 2008, Doris was engineered to address the limitations of existing data storage and computation solutions. At that time, systems like HBase were only capable of importing approximately 2,000 records per second, insufficient for the demands of Baidu's advertising system, Fengchao. To meet these high-performance needs, Doris was conceived and successfully integrated within the Baidu Fengchao system.\nSub-second query performance: Enables rapid response times, facilitating effective real-time data analysis.\nScalability: Designed to scale horizontally, accommodating large datasets with ease.\nVersatility: Supports various analytical operations, from generating fixed historical reports to performing dynamic real-time analyses.\nTo successfully compile Apache Doris, ensure the installation of the following dependencies:\nGCC 5.3.1+\nOracle JDK 1.8+\nPython 2.7+\nApache Maven 3.5+\nCMake 3.11+ Update System and Install Required Dependencies\nroot@doris1:/opt/software# apt-get -y update\nroot@doris1:/opt/software# apt-get install build-essential openjdk-11-jdk maven cmake byacc flex automake libtool-bin bison binutils-dev libiberty-dev zip unzip libncurses5-dev Download and Extract Apache Doris Source Code\nroot@doris1:/opt/software# wget http://www.apache.org/distr/doris/apache-doris-0.12.0-incubating-src.tar.gz\nroot@doris1:/opt/software# tar -zxvf apache-doris-0.12.0-incubating-src.tar.gz -C /opt/module/\nroot@doris1:/opt/software# cd /opt/module/apache-doris-0.12.0-incubating-src/ Prepare the Build Environment Create a src directory within the thirdparty folder.\nroot@doris1:/opt/module/apache-doris-0.12.0-incubating-src/thirdparty# mkdir src Switch from Dash to Bash (the default shell on Ubuntu might affect scripting).\nroot@doris1:~# ls -al /bin/sh\nroot@doris1:/opt/module/apache-doris-0.12.0-incubating-src# dpkg-reconfigure dash When prompted, select No. Set JAVA_HOME and update the system path.\nroot@doris1:/opt/module/apache-doris-0.12.0-incubating-src# vim /etc/profile\n# Add the following lines:\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\nexport PATH=${JAVA_HOME}/bin:$PATH\n# Reload the profile:\nroot@doris1:/opt/module/apache-doris-0.12.0-incubating-src# source /etc/profile Configure Maven for Faster Dependency Downloads Use the Aliyun mirror to accelerate Maven operations.\nroot@doris1:~# cd /etc/maven/\nroot@doris1:/etc/maven# vim settings.xml\n# Add this configuration to use the Aliyun mirror:\n&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;\n&lt;/mirror&gt; Compile Apache Doris\nroot@doris1:/opt/module/apache-doris-0.12.0-incubating-src# sh build.sh ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Key Features:","level":4,"id":"Key_Features_0"},{"heading":"Compilation of Apache Doris","level":3,"id":"Compilation_of_Apache_Doris_0"},{"heading":"Prerequisites","level":4,"id":"Prerequisites_0"},{"heading":"Installation Steps","level":4,"id":"Installation_Steps_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/8.-doris/0.-doris.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641865,"modifiedTime":1737554783000,"sourceSize":3359,"sourcePath":"Big Data/8. Doris/0. Doris.md","exportPath":"big-data/8.-doris/0.-doris.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/8.-doris/1.-installation.html":{"title":"1. Installation","icon":"","description":"Notes:\nFE's disk is primarily used for storing metadata, including logs and images, typically ranging from a few hundred MB to several GB.\nBE's disk is mainly used for storing user data. The total disk space should be calculated as the total user data * 3 (for three replicas), with an additional 40% reserved for background compaction and storing intermediate data.\nMultiple BE instances can be deployed on one machine, but only one FE instance can be deployed per machine. For three replicas, at least three machines should each host one BE instance.\nFE roles include Follower and Observer, with the Leader being elected from the Followers.\nThere should be at least one FE node. To achieve high availability (HA), deploy one Follower and one Observer.\nThe number of Followers must be odd, while the number of Observers is flexible.\nAccording to official documentation and past experience, for high cluster availability, deploy 3 Followers and 1-3 Observers. For offline business, deploy 1 Follower and 1-3 Observers.\nBroker deployment: The Broker is used to access external data sources, typically HDFS. Usually, deploy one Broker instance per machine.\nroot@doris1:~# vim /etc/hosts\n172.26.16.60 doris1 doris1\n172.26.16.61 doris2 doris2\n172.26.16.62 doris3 doris3\nroot@doris1:~# scp /etc/hosts doris2:/etc\nroot@doris1:~# scp /etc/hosts doris3:/etc\nroot@doris1:~# apt-get -y update\nroot@doris1:~# apt-get install openjdk-11-jdk\nroot@doris1:~# vim /etc/profile\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/\nexport PATH=${JAVA_HOME}/bin:$PATH\nroot@doris1:~# source /etc/profile\nRepeat the same steps on Doris2 and Doris3, then distribute the /etc/profile to the other nodes:root@doris1:~# scp /etc/profile doris2:/etc/\nroot@doris1:~# scp /etc/profile doris3:/etc/\nroot@doris2:~# source /etc/profile\nroot@doris1:~# ls -al /bin/sh\nroot@doris1:~# dpkg-reconfigure dash\nroot@doris1:~# mkdir /opt/module\nroot@doris2:~# mkdir /opt/module\nroot@doris3:~# mkdir /opt/module\nroot@doris1:~# cd /opt/module/\nroot@doris1:/opt/module# scp -r doris doris2:/opt/module\nroot@doris1:/opt/module# scp -r doris doris3:/opt/module\nroot@doris1:/opt/module# cd doris/fe/conf/\nroot@doris1:/opt/module/doris/fe/conf# vim fe.conf meta_dir = /opt/module/doris-meta\nroot@doris1:/opt/module/doris/fe/conf# mkdir /opt/module/doris-meta\nroot@doris1:/opt/module/doris/fe# sh bin/start_fe.sh --daemon\nroot@doris1:/opt/module/doris/fe# jps\nExpected output:15878 PaloFe\n15983 Jps\nConfigure the storage_root_path to specify the storage directories. You can use ; to specify multiple directories, and each directory can be followed by a size limit (default in GB).root@doris1:/opt/module/doris/be/conf# vim be.conf\nstorage_root_path = /opt/module/doris_storage1,10;/opt/module/doris_storage2\nCreate the storage directories:root@doris1:/opt/module/doris/be/conf# mkdir /opt/module/doris_storage1\nroot@doris1:/opt/module/doris/be/conf# mkdir /opt/module/doris_storage2\nRepeat the above steps on Doris2 and Doris3.root@doris1:/opt/software# dpkg -i mysql-common_5.7.31-1ubuntu18.04_amd64.deb root@doris1:/opt/software# dpkg -i mysql-community-client_5.7.31-1ubuntu18.04_amd64.deb root@doris1:/opt/software# dpkg -i mysql-community-client-dbgsym_5.7.31-1ubuntu18.04_amd64.deb root@doris1:/opt/software# dpkg -i libmysqlclient20-dbgsym_5.7.31-1ubuntu18.04_amd64.deb root@doris1:/opt/software# dpkg -i libmysqlclient-dev_5.7.31-1ubuntu18.04_amd64.deb root@doris1:/opt/software# dpkg -i mysql-client_5.7.31-1ubuntu18.04_amd64.deb\nroot@doris1:/opt/software# mysql -hdoris1 -P 9030 -uroot\nmysql&gt; ALTER SYSTEM ADD BACKEND \"doris1:9050\";\nmysql&gt; ALTER SYSTEM ADD BACKEND \"doris2:9050\";\nmysql&gt; ALTER SYSTEM ADD BACKEND \"doris3:9050\";\nroot@doris1:~# cd /opt/module/doris/be/\nroot@doris1:/opt/module/doris/be# sh bin/start_be.sh --daemon Repeat the above steps on Doris2 and Doris3.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Software and Hardware Requirements","level":3,"id":"Software_and_Hardware_Requirements_0"},{"heading":"1. Linux Operating System Requirements","level":4,"id":"1._Linux_Operating_System_Requirements_0"},{"heading":"2. Software Requirements","level":4,"id":"2._Software_Requirements_0"},{"heading":"3. Development and Testing Environment","level":3,"id":"3._Development_and_Testing_Environment_0"},{"heading":"4. Production Environment","level":4,"id":"4._Production_Environment_0"},{"heading":"5. Default Ports","level":4,"id":"5._Default_Ports_0"},{"heading":"Cluster Deployment","level":3,"id":"Cluster_Deployment_0"},{"heading":"Step 1: Prepare Three Machines and Configure Hostnames","level":4,"id":"Step_1_Prepare_Three_Machines_and_Configure_Hostnames_0"},{"heading":"Step 2: Install JDK","level":4,"id":"Step_2_Install_JDK_0"},{"heading":"Step 3: Switch from Dash to Bash on All Machines","level":4,"id":"Step_3_Switch_from_Dash_to_Bash_on_All_Machines_0"},{"heading":"Step 4: Manually Deploy and Configure FE","level":4,"id":"Step_4_Manually_Deploy_and_Configure_FE_0"},{"heading":"Step 5: Deploy BE Nodes","level":4,"id":"Step_5_Deploy_BE_Nodes_0"},{"heading":"Step 6: Install MySQL Client and Upload Required Packages","level":4,"id":"Step_6_Install_MySQL_Client_and_Upload_Required_Packages_0"},{"heading":"Step 7: Access Doris via MySQL Client and Add BE Nodes","level":4,"id":"Step_7_Access_Doris_via_MySQL_Client_and_Add_BE_Nodes_0"},{"heading":"Step 8: Start BE Nodes","level":4,"id":"Step_8_Start_BE_Nodes_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/8.-doris/1.-installation.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641867,"modifiedTime":1737554783000,"sourceSize":10404,"sourcePath":"Big Data/8. Doris/1. Installation.md","exportPath":"big-data/8.-doris/1.-installation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/8.-doris/2.-usage.html":{"title":"2. Usage","icon":"","description":" Create a test user\nroot@doris1:~# mysql -hdoris1 -P 9030 -uroot\nmysql&gt; create user 'test' identified by 'test'; Log in using the test user\nmysql&gt; exit;\nroot@doris1:~# mysql -hdoris1 -P 9030 -utest -ptest mysql&gt; create database test_db;\nGrant the test_db database to the test user to give them read and write permissions.mysql&gt; grant all on test_db to test;\nSwitch to the database:mysql&gt; use test_db; TINYINT: 1 byte, Range: -2^7 + 1 ~ 2^7 - 1\nSMALLINT: 2 bytes, Range: -2^15 + 1 ~ 2^15 - 1\nBIGINT: 8 bytes, Range: -2^63 + 1 ~ 2^63 - 1\nLARGEINT: 16 bytes, Range: -2^127 + 1 ~ 2^127 - 1\nFLOAT: 4 bytes, supports scientific notation\nDOUBLE: 12 bytes, supports scientific notation\nDECIMAL[(precision, scale)]: 16 bytes, exact decimal type. Default is DECIMAL(10, 0) precision: 1 ~ 27\nscale: 0 ~ 9\nInteger part: 1 ~ 18\nDoes not support scientific notation DATE: 3 bytes, Range: 0000-01-01 ~ 9999-12-31\nDATETIME: 8 bytes, Range: 0000-01-01 00:00:00 ~ 9999-12-31 23:59:59\nCHAR[(length)]: Fixed-length string. Length range: 1 ~ 255. Default is 1\nVARCHAR[(length)]: Variable-length string. Length range: 1 ~ 65533\nHLL: 1~16385 bytes, HLL column type, does not require length or default value specification. HLL columns can only be queried or used with functions like hll_union_agg, Hll_cardinality, hll_hash.\nBITMAP: Bitmap column type, does not require length or default value specification. Represents a set of integers, with a maximum element of 2^64 - 1.\nagg_type: Aggregation type, if not specified, the column is a key column. Otherwise, it is a value column. Options: SUM, MAX, MIN, REPLACE.\nDoris supports single partition and composite partition table creation.In composite partitioning:\nThe first level is Partition, where the user specifies a column (currently only integer and time columns are supported) as the partition column and defines the value range for each partition.\nThe second level is Distribution, where the user can specify one or more columns and the number of buckets for hash distribution.\nRecommended scenarios for using composite partitions:\nIf you have a time dimension or a similar ordered value dimension, you can use this dimension column as the partition column. The partition granularity can be evaluated based on the import frequency and partition data volume.\nHistorical data deletion: If there is a need to delete historical data (e.g., keeping only the last N days of data), composite partitioning can achieve this by deleting historical partitions. Alternatively, DELETE statements can be sent within a specified partition.\nSolving data skew: Each partition can specify the number of buckets individually. For example, in daily partitioning, if there is a significant difference in daily data volume, you can plan the data of different partitions reasonably by specifying the number of buckets for each partition. It's recommended to select a column with high distinction for bucketing.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Usage","level":3,"id":"Usage_0"},{"heading":"Creating Users","level":4,"id":"Creating_Users_0"},{"heading":"Creating Tables","level":4,"id":"Creating_Tables_0"},{"heading":"Creating a Database","level":5,"id":"Creating_a_Database_0"},{"heading":"Granting Permissions","level":5,"id":"Granting_Permissions_0"},{"heading":"Creating a Table","level":5,"id":"Creating_a_Table_0"},{"heading":"Field Types","level":6,"id":"Field_Types_0"},{"heading":"Partitioning","level":5,"id":"Partitioning_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/8.-doris/2.-usage.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641868,"modifiedTime":1737554783000,"sourceSize":29342,"sourcePath":"Big Data/8. Doris/2. Usage.md","exportPath":"big-data/8.-doris/2.-usage.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/8.-doris/3.-monitoring-and-alerting.html":{"title":"3. Monitoring and Alerting","icon":"","description":"Doris can use Prometheus and Grafana for monitoring and data collection. Doris exposes its monitoring data through the FE and BE HTTP interfaces. The monitoring data is presented as key-value text. Each key may have different labels to distinguish it. After setting up Doris, you can access the monitoring data in the browser via the following interfaces:\nFrontend: fe_host:fe_http_port/metrics\nBackend: be_host:be_web_server_port/metrics Upload and extract Prometheus\nroot@doris1:/opt/software# tar -zxvf prometheus-2.20.1.linux-amd64.tar.gz -C /opt/module/ Configure prometheus.yml\nConfigure two targets for FE and BE, and define labels and groups to specify groups. If there are multiple clusters, add the -job_name tag for the same configuration.\nroot@doris1:/opt/software# cd /opt/module/prometheus-2.20.1.linux-amd64/\nroot@doris1:/opt/module/prometheus-2.20.1.linux-amd64# vim prometheus.yml Example configuration:\nscrape_configs: - job_name: 'prometheus_doris' static_configs: - targets: ['doris1:8030','doris2:8030','doris3:8030'] labels: group: fe - targets: ['doris1:8040','doris2:8040','doris3:8040'] labels: group: be Start Prometheus\nroot@doris1:/opt/module/prometheus-2.20.1.linux-amd64# nohup ./prometheus --web.listen-address=\"doris1:8081\"&amp; Access Prometheus via browser Check status Go to Status -&gt; Target, and if the status is up, everything is running fine. Stop Prometheus Simply use kill -9 to stop it. Upload and extract Grafana\nroot@doris1:/opt/software# tar -zxvf grafana-7.1.5.linux-amd64.tar.gz -C /opt/module/ Configure defaults.ini\nroot@doris1:/opt/software# cd /opt/module/grafana-7.1.5/conf\nroot@doris1:/opt/module/grafana-7.1.5/conf# vim defaults.ini Example configuration: data = /opt/module/grafana-data/tmp logs = /opt/module/grafana-data/data/log http_addr = doris1 protocol = http http_port = 8182 Start Grafana\nroot@doris1:/opt/module/grafana-7.1.5/conf# cd ..\nroot@doris1:/opt/module/grafana-7.1.5# nohup bin/grafana-server &amp; Access Grafana via browser The default username and password are both admin. Configure Data Source Save the configuration Add a Dashboard Upload the prepared doris-overview_rev4.json Download link: <a data-tooltip-position=\"top\" aria-label=\"https://grafana.com/grafana/dashboards/9734/revisions\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://grafana.com/grafana/dashboards/9734/revisions\" target=\"_self\">Doris Overview Dashboard</a> Final dashboard configuration completed pprof is a powerful performance analysis tool that can capture multidimensional runtime data and convert it into human-readable formats such as PDF, SVG, and text. Install Go\nroot@doris1:/opt/module# wget https://dl.google.com/go/go1.13.1.linux-amd64.tar.gz\nroot@doris1:/opt/module# tar -zxvf go1.13.1.linux-amd64.tar.gz root@doris1:~# vim /etc/profile\nexport GOPATH=/opt/module/pprof\nexport GOROOT=/opt/module/go\nexport PATH=$PATH:${GOROOT}/bin\nroot@doris1:~# source /etc/profile\nroot@doris1:/opt/module# go version Install pprof\nroot@doris1:/opt/module/go/bin# apt install git\nroot@doris1:~# cd /opt/module/\ngo get -u github.com/google/pprof Set HEAPPROFILE\nroot@doris1:/opt/module# cd doris/be/conf/\nroot@doris1:/opt/module/doris/be/conf# vim be.conf export HEAPPROFILE=/tmp/doris_be.hprof Restart the cluster\nroot@doris1:/opt/module# sh doris/be/bin/start_be.sh --daemon\nroot@doris1:/opt/module# sh doris/fe/bin/start_fe.sh --daemon\nroot@doris1:/opt/module# sh apache_hdfs_broker/bin/start_broker.sh --daemon\nroot@doris2:/opt/module# sh doris/be/bin/start_be.sh --daemon\nroot@doris2:/opt/module# sh doris/fe/bin/start_fe.sh --daemon\nroot@doris2:/opt/module# sh apache_hdfs_broker/bin/start_broker.sh --daemon Dump memory usage when HEAPPROFILE conditions are met The memory usage will be written to the specified file path. You can then analyze the output using the pprof tool. Without this library, pprof can only convert data into text format, which is difficult to read. Once this library is installed, pprof can convert data into SVG, PDF, etc. Install Graphviz\nroot@doris1:~# apt install graphviz Check memory usage (use with caution as it can impact performance) Check CPU usage\nroot@doris1:/opt/module/pprof/bin# ./pprof --svg --seconds=60 http://doris1:8040/pprof/profile &gt;be.svg Wait for a minute, then download be.svg and open it in a browser This is a widely used CPU analysis method. Compared to pprof, this method requires login access to the target machine. However, it offers more flexibility as perf can collect stack information based on different events.\nPerf: A performance analysis tool built into the Linux kernel.\nFlamegraph: A visualization tool used to create flame graphs from perf outputs. Install Perf\nroot@doris1:/opt/module# apt install linux-tools-common\nroot@doris1:/opt/module# apt install linux-tools-4.15.0-112-generic\nroot@doris1:/opt/module# apt install linux-cloud-tools-4.15.0-112-generic Use Perf to collect data based on PID\nroot@doris1:/opt/module# ps -ef |grep be\nroot@doris1:/opt/module# perf record -g -p 5592 -- sleep 60 After a minute, perf.data will be created. Run the analysis command\nroot@doris1:/opt/module# perf report Upload FlameGraph-master.zip and extract it\nroot@doris1:/opt/module# unzip FlameGraph-master.zip Generate a visual graph\nroot@doris1:/opt/module# perf script | ./FlameGraph-master/stackcollapse-perf.pl | ./FlameGraph-master/flamegraph.pl &gt;se.svg Download se.svg and open it in a browser ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Monitoring and Alerting","level":3,"id":"Monitoring_and_Alerting_0"},{"heading":"Prometheus","level":3,"id":"Prometheus_0"},{"heading":"Grafana","level":3,"id":"Grafana_0"},{"heading":"Debugging Tools","level":3,"id":"Debugging_Tools_0"},{"heading":"pprof","level":4,"id":"pprof_0"},{"heading":"Graphviz","level":4,"id":"Graphviz_0"},{"heading":"Perf + Flamegraph","level":3,"id":"Perf_+_Flamegraph_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/8.-doris/3.-monitoring-and-alerting.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641869,"modifiedTime":1737554783000,"sourceSize":6419,"sourcePath":"Big Data/8. Doris/3. Monitoring and Alerting.md","exportPath":"big-data/8.-doris/3.-monitoring-and-alerting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/9.-elastic-search/0.-overview.html":{"title":"0. Overview","icon":"","description":"Elasticsearch is a distributed, open-source search and analytics engine built on top of Apache Lucene. It is designed for scalability, speed, and flexibility, allowing users to efficiently perform full-text searches, store and analyze large volumes of data, and run near real-time queries. Elasticsearch is particularly popular in use cases such as search engines, log and event data analysis, and e-commerce product search. Distributed Architecture\nElasticsearch operates in a distributed manner, meaning it divides data across multiple nodes in a cluster. This ensures that as the data grows, Elasticsearch can scale horizontally by adding more nodes to handle increased load. Each index is divided into smaller units called shards, and each shard can have replicas for fault tolerance. Example:\nIf you have a dataset too large to fit on a single server, Elasticsearch will automatically split the index into multiple shards and spread them across different nodes. If a node goes down, replicas on other nodes ensure the data is still available. Real-time Search\nElasticsearch provides near real-time (NRT) search capabilities. Once a document is indexed, it is almost immediately searchable within a fraction of a second. This makes Elasticsearch highly suitable for applications where fresh data needs to be available for search as soon as it arrives. Example:\nIn a logging system using the ELK stack (Elasticsearch, Logstash, Kibana), logs generated by servers are streamed and indexed into Elasticsearch in near real-time. You can search for specific error logs almost immediately after they occur. RESTful API\nElasticsearch interacts primarily through a RESTful API, using standard HTTP methods like GET, POST, PUT, DELETE. This makes it easy for developers to integrate Elasticsearch with a variety of programming languages. Example:\nTo search for all documents in an index called products, you can simply send a GET request: GET /products/_search The API responds with a JSON object containing the search results, which can be integrated into your application. Schema-free\nWhile Elasticsearch is schema-free (which means it automatically detects and adjusts to data structures), it also allows for mappings, where users can define specific data types for fields. This flexibility means that Elasticsearch can store and search heterogeneous data without requiring a predefined schema. Example:\nYou can index documents with varying structures:\n{ \"name\": \"Laptop\", \"price\": 999.99, \"in_stock\": true }\n{ \"name\": \"Smartphone\", \"price\": 699.99, \"release_date\": \"2024-01-01\" } Full-text Search\nElasticsearch excels at full-text search, providing a range of powerful search features like phrase matching, fuzzy searches, wildcard searches, and more. These features are built into the underlying Lucene engine, allowing it to handle complex queries efficiently. Example:\nA full-text query to search for a term in multiple fields:\nGET /products/_search\n{ \"query\": { \"multi_match\": { \"query\": \"laptop\", \"fields\": [\"name\", \"description\"] } }\n} This query searches for \"laptop\" in both the name and description fields, returning all matching documents. Analytics and Aggregations\nElasticsearch supports aggregations, which are used to perform analytics and summarize data in meaningful ways. You can calculate metrics such as averages, totals, min/max, percentiles, and more, making it ideal for real-time analytics. Example:\nIf you want to calculate the average price of products in your index:\nGET /products/_search\n{ \"size\": 0, \"aggs\": { \"average_price\": { \"avg\": { \"field\": \"price\" } } }\n} This returns the average price of all products in the index without returning any documents. High Availability\nElasticsearch ensures high availability by replicating shards across multiple nodes. If a node containing a shard goes down, its replica shard on another node will take over seamlessly, ensuring that the system remains operational. Example:\nIf a cluster consists of 3 nodes and each index is set up with 1 replica, even if one node fails, the cluster continues functioning using the replica shards stored on the remaining nodes. Horizontal Scalability\nOne of Elasticsearch’s biggest strengths is its ability to scale horizontally. This is achieved by adding more nodes to a cluster, which automatically balances the data and query load across those nodes. Example:\nIf your data grows and you start handling more searches, you can add new nodes to your cluster. Elasticsearch will distribute shards and data evenly across these nodes, ensuring that the system can handle the increased traffic without performance degradation. Document\nThe basic unit of data in Elasticsearch is a JSON document. Each document contains fields (key-value pairs) and belongs to an index. Documents in an index don't have to follow the same structure, but defining a mapping can provide consistency.\nExample: { \"name\": \"Smartphone\", \"price\": 699.99, \"release_date\": \"2024-01-01\"\n} Index\nAn index is a collection of documents that share similar characteristics. You can think of it as a table in a relational database. Each index can be split into multiple shards for distributed storage. Shard\nA shard is a self-contained instance of Lucene, the underlying search engine. Elasticsearch automatically splits an index into shards and replicates them to improve performance and availability. Node\nA node is a single running instance of Elasticsearch. Multiple nodes can work together to form a cluster, which distributes data and load for horizontal scaling. Cluster\nA cluster is a group of nodes that work together to hold the data and provide search and indexing capabilities. The cluster is identified by a unique name, and nodes join the cluster based on this name. Search Engine\nElasticsearch is widely used to build search engines for websites and applications. It offers rich search functionality like autocomplete, spell correction, relevance ranking, and more. Example:\nAn e-commerce website can use Elasticsearch to provide product search for its users. It can show relevant products instantly as users type, using features like autocomplete or phrase search. Log and Event Data\nCombined with Logstash and Kibana, Elasticsearch forms the popular ELK stack (Elasticsearch, Logstash, Kibana). This stack is used for monitoring, log management, and real-time data analysis by indexing logs from servers and applications. Example:\nA system administrator can use ELK to monitor server logs, detect anomalies, and generate visual reports to analyze system performance. Analytics Platforms\nElasticsearch’s aggregation capabilities make it a powerful tool for performing real-time analytics. Businesses can extract insights from their data streams and monitor key metrics in real time. Example:\nAn e-commerce company can use Elasticsearch to monitor sales trends, calculate the average order value, and track user behavior to optimize marketing strategies. E-commerce Search\nElasticsearch enhances product search for e-commerce platforms, supporting features like fuzzy search (for handling typos), sorting by relevance, and providing search filters like price ranges or categories. Example:\nA customer searching for \"iphon\" can still find relevant products like \"iPhone\" using Elasticsearch’s fuzzy matching capabilities. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Elasticsearch Overview","level":3,"id":"Elasticsearch_Overview_0"},{"heading":"Key Features of Elasticsearch","level":4,"id":"Key_Features_of_Elasticsearch_0"},{"heading":"Core Concepts","level":4,"id":"Core_Concepts_0"},{"heading":"Common Use Cases","level":4,"id":"Common_Use_Cases_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/9.-elastic-search/0.-overview.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641871,"modifiedTime":1737554783000,"sourceSize":8205,"sourcePath":"Big Data/9. Elastic Search/0. Overview.md","exportPath":"big-data/9.-elastic-search/0.-overview.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/9.-elastic-search/1.-index-&-shard.html":{"title":"1. Index & Shard","icon":"","description":"An index in Elasticsearch is a collection of documents that share common characteristics and is the primary structure where data is stored. Conceptually, it is similar to a table in a relational database like MySQL or PostgreSQL, but with key differences that reflect Elasticsearch’s focus on distributed systems and full-text search. Document Storage\nEvery piece of data in Elasticsearch is stored in the form of a document, which is a JSON object containing fields (key-value pairs). All documents within an index typically represent similar types of data. For instance, in an e-commerce application, you might have an index called products that stores documents related to product details, or an index called customers to store customer-related information.\nExample:\n{ \"name\": \"Smartphone\", \"brand\": \"TechCo\", \"price\": 699.99, \"release_date\": \"2024-01-01\"\n} Mappings\nWhile Elasticsearch is schema-free, it offers the option to define mappings for an index. A mapping defines the structure of the documents in the index, specifying the data types for each field (e.g., text, keyword, integer, date). Mappings are important for optimizing search and query performance.\nExample Mapping:\n{ \"mappings\": { \"properties\": { \"name\": { \"type\": \"text\" }, \"brand\": { \"type\": \"keyword\" }, \"price\": { \"type\": \"float\" }, \"release_date\": { \"type\": \"date\" } } }\n} This mapping ensures that name is analyzed for full-text search, brand is stored as a keyword (for exact matches), and price and release_date are treated as numeric and date fields, respectively. Indexing and Querying\nAn index allows you to efficiently index (store) and query documents. When documents are indexed, Elasticsearch creates an inverted index, a data structure optimized for fast full-text searches. Elasticsearch also supports rich queries, including full-text searches, filtering, and sorting, all of which are executed on the documents within an index.\nExample Query:\nA query to find all products in the products index where the name contains the word \"smartphone\":\nGET /products/_search\n{ \"query\": { \"match\": { \"name\": \"smartphone\" } }\n} Multi-index Queries\nElasticsearch allows querying across multiple indices simultaneously. This is useful when your data is partitioned into different indices but you need to search across them.\nExample:\nYou can search across products, customers, and orders indices in a single query:\nGET /products,customers,orders/_search\n{ \"query\": { \"match_all\": {} }\n} Index Lifecycle\nElasticsearch also supports Index Lifecycle Management (ILM), which automates managing indices over time, especially as they grow in size. ILM can automatically move indices through different phases such as hot, warm, cold, and delete, optimizing performance and storage usage. A shard is a fundamental component of Elasticsearch’s distributed architecture. It is a self-contained and independent instance of Apache Lucene, the underlying search engine used by Elasticsearch. Each index is divided into one or more primary shards, which can be further replicated across nodes to ensure high availability and fault tolerance. Primary Shards\nWhen you create an index in Elasticsearch, you define the number of primary shards. These shards contain the actual data of the index and handle read and write requests. By default, an index in Elasticsearch has 1 primary shard, but this can be configured based on your needs.\nExample:\nWhen creating an index, you can specify the number of primary shards:\nPUT /products\n{ \"settings\": { \"number_of_shards\": 3 }\n} This will create the products index with 3 primary shards. The data will be distributed evenly across these shards. Replica Shards\nTo ensure fault tolerance and high availability, Elasticsearch allows you to create replica shards. These replicas are exact copies of the primary shards and are stored on different nodes in the cluster to avoid data loss in case of a node failure.\nExample:\nBy default, each primary shard has 1 replica, but this can be increased:\nPUT /products\n{ \"settings\": { \"number_of_replicas\": 1 }\n} This creates 1 replica for each primary shard. If a node containing a primary shard fails, Elasticsearch automatically promotes the replica to a primary shard to continue handling requests. Shard Allocation and Rebalancing\nElasticsearch automatically manages shard allocation across nodes. When new nodes are added to the cluster, Elasticsearch will rebalance the shards, distributing them across all available nodes to ensure even data and query load distribution.\nExample:\nIf you add a new node to a cluster, Elasticsearch will move shards from the existing nodes to the new node to balance the cluster. This ensures that no single node is overloaded with too much data or too many queries. Parallelization\nShards are key to Elasticsearch’s ability to scale horizontally. By dividing data across shards, Elasticsearch allows queries to run in parallel. Each shard can process a portion of the query independently, and the results are merged before being returned to the user.\nExample:\nSuppose you search for documents in an index that is split into 5 shards. Elasticsearch will route the query to all 5 shards in parallel, gather the results, and return them as one aggregated response. This parallelism ensures that queries can be handled efficiently, even with large datasets. Shard Failover and Replication\nElasticsearch monitors the health of all shards in the cluster. If a shard becomes unavailable (e.g., due to node failure), Elasticsearch automatically promotes a replica to a primary shard and reallocates it to another node. This ensures no data loss and minimal downtime.\nExample:\nSuppose your cluster has 3 nodes, and each shard has 1 replica. If one node goes down, Elasticsearch automatically reassigns the replica shards to the remaining two nodes, ensuring continued availability. Shard Sizing\nWhile Elasticsearch can handle large data volumes, it is important to ensure that shards are sized correctly. Overly large shards can lead to memory issues, slow recovery times, and reduced query performance. The recommended size for a shard is typically between 10GB and 50GB, but this depends on your use case and the nature of your data. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1. Index &amp; Shard","level":1,"id":"1._Index_&_Shard_0"},{"heading":"Index","level":3,"id":"Index_0"},{"heading":"Key Aspects of an Index:","level":4,"id":"Key_Aspects_of_an_Index_0"},{"heading":"Shard","level":3,"id":"Shard_0"},{"heading":"Key Aspects of Shards:","level":4,"id":"Key_Aspects_of_Shards_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/9.-elastic-search/1.-index-&-shard.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641872,"modifiedTime":1737554783000,"sourceSize":8494,"sourcePath":"Big Data/9. Elastic Search/1. Index & Shard.md","exportPath":"big-data/9.-elastic-search/1.-index-&-shard.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/10.-clickhouse/0.-introduction-and-installation.html":{"title":"0. Introduction and Installation","icon":"","description":"ClickHouse is a columnar database management system (DBMS) developed by Yandex, Russia, and open-sourced in 2016. Written in C++, it is primarily used for Online Analytical Processing (OLAP) and allows SQL queries to generate real-time analytical reports.\nOLAP (On-Line Analytical Processing): Focuses on analytical processing and querying data in databases.\nOLTP (On-Line Transaction Processing): Focuses on transaction processing, involving operations like insert, update, and delete.\nUsing the following table as an example: Row-Oriented Storage: Data is organized in rows on disk. This structure is efficient for fetching all attributes of a single entity but inefficient for querying specific attributes (e.g., retrieving ages). Columnar Storage: Data is organized by columns on disk. This structure is efficient for retrieving specific attributes, such as ages, without scanning unnecessary data. Advantages of Columnar Storage: Superior for operations like aggregation, counting, and summation.\nEasier to compress due to homogeneous data types in columns, leading to improved compression ratios.\nSaves disk space and optimizes cache usage. Supports most standard SQL syntax, including DDL and DML.\nProvides user and permission management, as well as data backup and recovery.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"OLAP and OLTP","level":4,"id":"OLAP_and_OLTP_0"},{"heading":"Features","level":3,"id":"Features_0"},{"heading":"Columnar Storage","level":4,"id":"Columnar_Storage_0"},{"heading":"DBMS Features","level":4,"id":"DBMS_Features_0"},{"heading":"Diverse Engines","level":4,"id":"Diverse_Engines_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/10.-clickhouse/0.-introduction-and-installation.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641354,"modifiedTime":1737554783000,"sourceSize":4733,"sourcePath":"Big Data/10. ClickHouse/0. Introduction and Installation.md","exportPath":"big-data/10.-clickhouse/0.-introduction-and-installation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/10.-clickhouse/1.-data-types.html":{"title":"1. Data Types","icon":"","description":"Fixed-length integers, which can be either signed or unsigned.\nSigned Integer Range: Types: Int8, Int16, Int32, Int64 Unsigned Integer Range: Types: UInt8, UInt16, UInt32, UInt64 Types: Float32, Float64\nPrecision Loss Issue in Floating-Point Calculations: Example:\nSELECT 1.0 - 0.9; Result:\n┌──────minus(1., 0.9)─┐\n│ 0.09999999999999998 │\n└─────────────────────┘ ClickHouse does not have a dedicated Boolean type. Use UInt8 instead, with values restricted to 0 (false) or 1 (true).\nDecimal32(s) is equivalent to Decimal(9-s, s)\nDecimal64(s) is equivalent to Decimal(18-s, s)\nDecimal128(s) is equivalent to Decimal(38-s, s)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Integer Types","level":3,"id":"Integer_Types_0"},{"heading":"Floating-Point Numbers","level":3,"id":"Floating-Point_Numbers_0"},{"heading":"Boolean Type","level":3,"id":"Boolean_Type_0"},{"heading":"Decimal Type","level":3,"id":"Decimal_Type_0"},{"heading":"String Types","level":3,"id":"String_Types_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/10.-clickhouse/1.-data-types.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641355,"modifiedTime":1737554783000,"sourceSize":2695,"sourcePath":"Big Data/10. ClickHouse/1. Data Types.md","exportPath":"big-data/10.-clickhouse/1.-data-types.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/10.-clickhouse/2.-database-engines.html":{"title":"2. Database Engines","icon":"","description":"\nThe default database engine is Ordinary, which allows tables of any engine type.\nThe most commonly used table engine in production is the MergeTree series, which is the primary engine recommended by ClickHouse. MergeTree: The base engine with features like primary key indexing, data partitioning, data replication, data sampling, and support for delete and update operations.\nReplacingMergeTree: Adds deduplication capabilities.\nSummingMergeTree: Adds summarization and summation capabilities.\nAggregatingMergeTree: Supports aggregation functions.\nCollapsingMergeTree: Provides collapsing and deletion capabilities.\nVersionedCollapsingMergeTree: Supports version-based collapsing.\nGraphiteMergeTree: Specializes in compressed summarization.\nAdditional enhancements:\nCombine with Replicated for replication.\nCombine with Distributed for distributed storage. Integration engines are used to connect external data sources, such as Hadoop and MySQL.\nMaterializeMySQL is a database engine specifically for integrating with MySQL.\nMaterializeMySQL was introduced in ClickHouse version 20.8.2.3. This engine:\nMaps a MySQL database to a ClickHouse database.\nAutomatically creates corresponding ReplacingMergeTree tables in ClickHouse.\nReads MySQL binlogs to handle DDL and DML queries.\nActs as a MySQL replica, enabling real-time synchronization of business databases via MySQL binlog. Supports full and incremental synchronization. Initial full synchronization of MySQL tables and data.\nSubsequent incremental synchronization via binlogs. Automatically adds _sign and _version fields to ReplacingMergeTree tables: _version: Increments globally for each INSERT, UPDATE, or DELETE event.\n_sign: Marks rows as active (1) or deleted (-1). Supported binlog events:\nMYSQL_WRITE_ROWS_EVENT: _sign = 1, _version++.\nMYSQL_DELETE_ROWS_EVENT: _sign = -1, _version++.\nMYSQL_UPDATE_ROWS_EVENT: New data _sign = 1.\nMYSQL_QUERY_EVENT: Supports DDL operations like CREATE TABLE, DROP TABLE, RENAME TABLE. MySQL DDL queries are converted to ClickHouse equivalents (ALTER, CREATE, DROP, RENAME).\nUnsupported DDL queries in ClickHouse are ignored.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Database Engines and Table Engines","level":3,"id":"Database_Engines_and_Table_Engines_0"},{"heading":"MergeTree Variants","level":4,"id":"MergeTree_Variants_0"},{"heading":"Integration Engines","level":4,"id":"Integration_Engines_0"},{"heading":"<strong>MaterializeMySQL</strong>","level":3,"id":"**MaterializeMySQL**_0"},{"heading":"Overview","level":4,"id":"Overview_0"},{"heading":"Features","level":4,"id":"Features_0"},{"heading":"Usage Guidelines","level":3,"id":"Usage_Guidelines_0"},{"heading":"DDL Queries","level":4,"id":"DDL_Queries_0"},{"heading":"Data Manipulation","level":4,"id":"Data_Manipulation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/10.-clickhouse/2.-database-engines.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641356,"modifiedTime":1737554783000,"sourceSize":5859,"sourcePath":"Big Data/10. ClickHouse/2. Database Engines.md","exportPath":"big-data/10.-clickhouse/2.-database-engines.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/10.-clickhouse/3.-table-engines.html":{"title":"3. Table Engines","icon":"","description":"A table engine determines:\nHow data is stored and where it is read or written.\nSupported queries and their execution methods.\nConcurrent data access.\nIndex usage (if applicable).\nMultithreaded request execution capability.\nData replication parameters.\nThe engine must be explicitly defined when creating a table, along with any parameters. Engine names are case-sensitive.\nStorage: Columnar files on disk, no indexing or concurrency control.\nUse Case: Suitable for small tables with minimal data, mainly for testing and learning purposes.\nLimitations: Limited utility in production environments. Storage: Data is stored in memory in an uncompressed form. Data is lost upon server restart.\nPerformance: Extremely high performance (&gt;10GB/s) for simple queries, but lacks indexing.\nUse Case: Scenarios requiring ultra-high performance with small data volumes (up to ~100 million rows) or for testing.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Usage of Table Engines","level":3,"id":"Usage_of_Table_Engines_0"},{"heading":"Common Table Engines","level":3,"id":"Common_Table_Engines_0"},{"heading":"<strong>TinyLog</strong>","level":4,"id":"**TinyLog**_0"},{"heading":"<strong>Memory</strong>","level":4,"id":"**Memory**_0"},{"heading":"<strong>MergeTree (🌟)</strong>","level":4,"id":"**MergeTree_(🌟)**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/10.-clickhouse/3.-table-engines.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641357,"modifiedTime":1737554783000,"sourceSize":6253,"sourcePath":"Big Data/10. ClickHouse/3. Table Engines.md","exportPath":"big-data/10.-clickhouse/3.-table-engines.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/11.-data-migration/0.-sqoop.html":{"title":"0. Sqoop","icon":"","description":"Sqoop is a tool designed for transferring bulk data between Hadoop and structured data stores, such as relational databases. It stands for SQL-to-Hadoop and is commonly used for importing data from relational databases (RDBMS) like MySQL, Oracle, or PostgreSQL into HDFS or exporting data from HDFS to RDBMS. It is an essential component of the Hadoop ecosystem, facilitating seamless data transfers. Data Import: Sqoop can import data from relational databases into HDFS, Hive, or HBase. This operation is typically used for large-scale data transfers into Hadoop for processing. Data Export: It can also export data from HDFS back into relational databases. Incremental Imports: Sqoop supports importing only the new or modified data since the last import, optimizing performance for frequent data transfers. Parallel Processing: Sqoop uses parallel processing for data imports and exports, allowing you to handle large datasets efficiently. Schema Mapping: Sqoop handles automatic schema mapping, ensuring that the data types from the relational database are compatible with Hadoop storage systems like HDFS, Hive, and HBase. The basic syntax for importing data is as follows:sqoop import --connect jdbc:mysql://localhost:3306/database_name \\\n--username root --password password --table employees \\\n--target-dir /user/hadoop/employees_data\nThis command imports the employees table from a MySQL database into HDFS.\n--connect: Specifies the JDBC connection string to the source database.\n--username: Database username.\n--password: Database password.\n--table: Specifies the table to import.\n--target-dir: Specifies the destination directory in HDFS where the data will be stored.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Sqoop in Big Data","level":3,"id":"Sqoop_in_Big_Data_0"},{"heading":"Key Functions of Sqoop","level":3,"id":"Key_Functions_of_Sqoop_0"},{"heading":"Sqoop Import Example","level":3,"id":"Sqoop_Import_Example_0"},{"heading":"Importing Data with Custom Delimiters","level":4,"id":"Importing_Data_with_Custom_Delimiters_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/11.-data-migration/0.-sqoop.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641359,"modifiedTime":1737554783000,"sourceSize":6934,"sourcePath":"Big Data/11. Data Migration/0. Sqoop.md","exportPath":"big-data/11.-data-migration/0.-sqoop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/11.-data-migration/1.-datax.html":{"title":"1. DataX","icon":"","description":"DataX is an open-source, highly extensible, and scalable data integration tool developed by Alibaba. It is widely used in the big data ecosystem for batch data synchronization between various data sources, such as databases, data lakes, and data warehouses. With its easy-to-use framework and support for heterogeneous environments, DataX has become an essential component in the big data era.Key Features of DataX:\nData Migration: Facilitates efficient data movement between different storage systems, whether relational (e.g., MySQL, PostgreSQL) or non-relational (e.g., HDFS, Kafka, Elasticsearch).\nFault Tolerance: Ensures fault tolerance with automatic retries for failed tasks, ensuring smooth execution in a production environment.\nExtensibility: Allows for custom plugins and extension of connectors to support new data sources or specific use cases.\nScalability: Handles high-throughput data movements and is capable of scaling across multiple nodes for parallel execution.\nDataX operates on a reader-writer model, where the source system is accessed via a reader, and the data is written to the destination system using a writer. It supports a wide variety of data sources and destinations, including traditional RDBMS, NoSQL systems, cloud storage (e.g., HDFS, S3), and messaging queues (e.g., Kafka).\nReader: A module responsible for reading data from a source.\nWriter: A module responsible for writing data to a destination.\nJob: A task in DataX that defines the data synchronization process, including configurations for readers, writers, and other operations.\nDataX's job configuration can be defined in JSON format, where various parameters for reading and writing data, such as the database connection details and table names, are specified.\nBatch Data Transfer: DataX is primarily designed to handle batch data transfers. It supports full-load and incremental load of data, which means that it can either transfer all data at once or just the data that has changed since the last operation.\nData Transformation: During the data transfer process, DataX can apply transformations on the data (such as field mapping, type conversions, etc.) to ensure that it is compatible with the target schema.\nScheduling and Orchestration: DataX provides scheduling functionality, allowing jobs to be executed periodically, such as for nightly data loads. Integration with other orchestration tools (like Oozie or Apache Airflow) is also possible.\nParallel Processing: DataX uses parallelism to speed up data transfer. It allows tasks to be divided into sub-tasks that are executed concurrently, making it highly efficient for handling large datasets.\nData Validation: DataX offers various built-in validation checks (such as row count comparison) to ensure data integrity during transfers.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Introduction","level":3,"id":"Introduction_0"},{"heading":"Concepts","level":3,"id":"Concepts_0"},{"heading":"Functionality","level":3,"id":"Functionality_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/11.-data-migration/1.-datax.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641360,"modifiedTime":1737554783000,"sourceSize":8056,"sourcePath":"Big Data/11. Data Migration/1. DataX.md","exportPath":"big-data/11.-data-migration/1.-datax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/12.-schedule/1.-cron-expressions.html":{"title":"1. Cron Expressions","icon":"","description":"Cron expressions are strings that represent a schedule in a time-based job-scheduling system. They are typically used in Unix-like operating systems to automate the execution of scripts, commands, or software. A cron expression is composed of five or six fields separated by space, each representing a unit of time.A cron expression consists of the following fields:* * * * * [command to execute] Minute (0 - 59)\nHour (0 - 23)\nDay of the month (1 - 31)\nMonth (1 - 12 or JAN-DEC)\nDay of the week (0 - 6 or SUN-SAT)\nCommand to execute (optional in some systems)\nEach field can contain a value, a wildcard (*), a range (e.g., 1-3), a list (e.g., 1,2,3), or a step value (e.g., */2).\nAsterisk (*): Represents \"every\" time unit. For example, * in the minute field means \"every minute.\"\nComma (,): Specifies a list of values. For example, 1,3,5 in the day-of-week field means \"every Monday, Wednesday, and Friday.\"\nHyphen (-): Defines a range of values. For example, 10-12 in the hour field means \"10 AM, 11 AM, and 12 PM.\"\nSlash (/): Specifies increments. For example, */10 in the minute field means \"every 10 minutes.\" 0 0 * * * Description: Executes a command at midnight every day.\nBreakdown: Minute: 0 (at the 0th minute)\nHour: 0 (at 0 AM, which is midnight)\nDay of the month: * (every day)\nMonth: * (every month)\nDay of the week: * (every day of the week) 30 3 1 * * Description: Runs a command at 3:30 AM on the first day of every month.\nBreakdown: Minute: 30 (at the 30th minute)\nHour: 3 (at 3 AM)\nDay of the month: 1 (on the first day of the month)\nMonth: * (every month)\nDay of the week: * (every day of the week) 45 6 * * 1 Description: Executes a command at 6:45 AM every Monday.\nBreakdown: Minute: 45 (at the 45th minute)\nHour: 6 (at 6 AM)\nDay of the month: * (every day)\nMonth: * (every month)\nDay of the week: 1 (Monday) 0 22 * * 1-5 Description: Runs a command at 10 PM on weekdays (Monday through Friday).\nBreakdown: Minute: 0 (at the 0th minute)\nHour: 22 (at 10 PM)\nDay of the month: * (every day)\nMonth: * (every month)\nDay of the week: 1-5 (from Monday to Friday) */10 * * * * Description: Executes a command every 10 minutes.\nBreakdown: Minute: */10 (every 10 minutes)\nHour: * (every hour)\nDay of the month: * (every day)\nMonth: * (every month)\nDay of the week: * (every day of the week) ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Format of a Cron Expression","level":3,"id":"Format_of_a_Cron_Expression_0"},{"heading":"Special Characters","level":3,"id":"Special_Characters_0"},{"heading":"Examples of Cron Expressions","level":3,"id":"Examples_of_Cron_Expressions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/12.-schedule/1.-cron-expressions.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641394,"modifiedTime":1737554783000,"sourceSize":2861,"sourcePath":"Big Data/12. Schedule/1. Cron Expressions.md","exportPath":"big-data/12.-schedule/1.-cron-expressions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/12.-schedule/2.-quartz-cron-expressions.html":{"title":"2. Quartz Cron Expressions","icon":"","description":"Quartz cron expressions are similar to Unix/Linux cron expressions but have a few key differences and additional features that make them more versatile. They are used within the Quartz Scheduler to schedule jobs. A Quartz cron expression is a string composed of six or seven fields, each representing a time unit or a special character specifying repeating intervals.A Quartz cron expression has the following format:second minute hour day_of_month month day_of_week [year] Second (0 - 59)\nMinute (0 - 59)\nHour (0 - 23)\nDay of the month (1 - 31)\nMonth (1 - 12 or JAN-DEC)\nDay of the week (1 - 7 or SUN-SAT, where 1 is Sunday)\nYear (optional field) Asterisk (*): Represents all possible values for a field. For example, * in the minute field means \"every minute.\"\nQuestion Mark (?): Used to specify no specific value and is allowed in the day_of_month and day_of_week fields. It is useful when you need to specify something in one of these fields but not the other.\nHyphen (-): Indicates a range of values, such as 10-12 in the hour field to trigger an event from 10 AM to 12 PM.\nComma (,): Lists additional values. For instance, MON,WED,FRI in the day_of_week field means \"on Monday, Wednesday, and Friday.\"\nSlash (/): Specifies increments. 0/15 in the seconds field means \"at seconds 0, 15, 30, and 45 of each minute.\"\nL: Stands for \"last,\" but its meaning can vary based on the field. For example, L in the day_of_month field means \"the last day of the month.\"\nW: Represents the nearest weekday to a given day in the month. For example, 15W is interpreted as \"the nearest weekday to the 15th of the month.\" 0 0 12 * * ? Description: Fires at 12 PM (noon) every day. 0 15 10 ? * MON-FRI Description: Fires at 10:15 AM every Monday, Tuesday, Wednesday, Thursday, and Friday. 0 0/5 14 * * ? Description: Fires every 5 minutes starting at 2 PM and ending at 2:55 PM, every day. 0 0-5 14 * * ? Description: Fires every minute starting at 2 PM and ending at 2:05 PM, every day. 0 0/30 8-10 * * ? Description: Fires every 30 minutes between 8 AM and 10:30 AM, every day. 0 0 12 1W * ? Description: Fires at 12 PM (noon) on the nearest weekday to the first of the month. 0 0 12 LW * ? Description: Fires at 12 PM (noon) on the last weekday of the month. 0 0 12 ? JAN,FEB,MAR * Description: Fires at 12 PM (noon) on any day of the month, only in January, February, and March. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Format of a Quartz Cron Expression","level":3,"id":"Format_of_a_Quartz_Cron_Expression_0"},{"heading":"Special Characters in Quartz","level":3,"id":"Special_Characters_in_Quartz_0"},{"heading":"Examples of Quartz Cron Expressions","level":3,"id":"Examples_of_Quartz_Cron_Expressions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/12.-schedule/2.-quartz-cron-expressions.html","pathToRoot":"../..","attachments":[],"createdTime":1737584641395,"modifiedTime":1737554783000,"sourceSize":2757,"sourcePath":"Big Data/12. Schedule/2. Quartz Cron Expressions.md","exportPath":"big-data/12.-schedule/2.-quartz-cron-expressions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/0.-data-management.html":{"title":"0. Data Management","icon":"","description":"Data Management is the planning, policy-setting, regulation, and practice activities conducted throughout the entire lifecycle of data and information assets to deliver, control, protect, and enhance their value. The process includes execution and supervision.The responsibility of managing data must be jointly assumed by both business personnel and IT personnel. These two fields need to collaborate to ensure that the organization possesses high-quality data that meets strategic needs.The main driver of data management is to enable organizations to derive value from their data assets.\nUnderstanding and supporting the information needs of the business and its stakeholders (including customers, employees, and business partners).\nAcquiring, storing, protecting data, and ensuring the integrity of data assets.\nEnsuring the quality of data and information.\nEnsuring the privacy and confidentiality of stakeholder data.\nPreventing unauthorized or improper access, manipulation, and use of data and information.\nEnsuring data effectively serves the goal of business value addition.\nData is both an interpretation of the object it represents and an object that requires interpretation.Recognizing the need to prepare data and information for different purposes will form a core principle of data management: both data and information need to be managed. When the use of both and the needs of customers are managed together, the quality should be higher.An asset is an economic resource that can be owned or controlled, held, or create value. Assets can be converted into money.Many organizations define themselves as \"data-driven.\" To maintain competitiveness, businesses must stop making decisions based on intuition or feeling and instead use event triggers and applied analytics to gain actionable insights.Data management shares common characteristics with other forms of asset management, as illustrated in Figure 1-1. It involves understanding what data an organization has and what can be accomplished with it, then determining the best way to utilize data assets to achieve organizational goals. Like other management processes, data management must balance strategic and operational needs. This balance is best followed by a set of principles that guide data management practices.Data is an asset, but it differs from other assets in some aspects of its management. Unlike financial and physical assets, data assets do not deplete through use.Calling data an asset implies it has value. While there are technical means to measure the quantity and quality of data, no standards have yet been established to measure its value. Organizations wishing to make better decisions about their data should develop consistent methods to quantify that value. They should also measure the costs of low-quality data and the benefits of high-quality data.Ensuring that data meets the requirements of its applications is the primary goal of data management. To manage quality, organizations must understand stakeholders' quality requirements and measure data accordingly.Managing any asset first requires having data about that asset (e.g., number of employees, account numbers). Data used for managing and how data is used are called metadata. Because data cannot be physically handled or touched, understanding what it is and how to use it requires defining this knowledge in the form of metadata. Metadata arises from a variety of processes related to data creation, processing, and use, including architecture, modeling, management, governance, data quality management, system development, IT, business operations, and analytics.Even small organizations may have complex blueprints for technology and business processes. Data is created in multiple places and needs to move among many storage locations due to usage, thus requiring coordination to maintain consistency. This needs planning from an architectural and process perspective.Data and data management are closely integrated with information technology and IT management. Managing data requires a method to ensure technology serves rather than drives the organization's strategic data.Data management requires a range of skills and expertise, so no single team can manage all an organization's data. Data management needs technical ability, non-technical skills, and collaboration capabilities.While there are many specialized applications for data management, it must be effectively applied across the entire enterprise. This is one reason why data management and data governance are intertwined.Data is fluid, and data management must continuously evolve to keep up with the ways data is created, applied, and changed by consumers.Data has a lifecycle, thus data management needs to manage its lifecycle. Because data generates more data, the data lifecycle itself can be quite complex. Data management practices need to consider the entire lifecycle of data. Different types of data have different lifecycle characteristics, thus they have different management needs. Data management practices need to maintain sufficient flexibility to meet the lifecycle needs of different types of data.Data is not only an asset but also represents organizational risk. Data can be lost, stolen, or misused. Organizations must consider the ethical implications of using data. Data-related risks must be managed as part of the data lifecycle.Data management involves complex processes that require coordination, collaboration, and commitment. To achieve goals, not only management skills are needed but also a vision and mission from the leadership.Value is the difference between the cost of something and the benefits derived from it. For some assets, like inventory, calculating value is very straightforward—it is the difference between its purchase cost and selling price. However, for data, neither the cost nor the profit has a unified standard, making these calculations complex.Each organization's data is unique, so assessing the value of data requires first calculating the general costs and various benefits incurred internally. Categories include:\nThe cost of acquiring and storing data.\nThe cost of replacing data if lost.\nThe impact of data loss on the organization.\nThe cost of risk mitigation and potential risk costs related to data.\nThe cost of improving data.\nThe advantages of high-quality data.\nThe fees competitors pay for data.\nThe potential selling price of data.\nThe expected revenue from innovative data applications.\nAs mentioned in section 1.1, deriving value from data is not accidental and requires planning in various forms. Organizations must recognize that they can control how they acquire and create data. If data is viewed as a product being created, better decisions can be made throughout its lifecycle. These decisions require systemic thinking because they involve:\nData may be considered as existing independently of business processes.\nThe relationship between business processes and the technologies that support them.\nThe design and architecture of systems and the data they generate and store.\nWays data might be used to drive organizational strategy.\nBetter data planning requires strategic paths for architecture, modeling, and feature design. It also depends on strategic collaboration between business and IT leadership, as well as the execution strength of individual projects.The challenge is that there is often long-term pressure from organizations, time, and money, which hinders the execution of optimization plans. Organizations must balance long-term goals with short-term objectives. Only by making clear trade-offs can effective decisions be achieved.Organizations need reliable metadata to manage data assets, which means comprehensively understanding metadata. It includes not only business and technical metadata and the metadata operations, but also metadata embedded in data architecture, data models, data security requirements, data integration standards, and data operation processes.Metadata describes what data an organization has, what it represents, how it is classified, where it comes from, how it moves within the organization, how it evolves in use, who can use it, and whether it is high-quality. Data is abstract, and definitions and other descriptions in the context make the data clear and explicit. They make the data, the data lifecycle, and the complex systems containing the data easier to understand.The challenge is that metadata is constituted in data form, thus requiring strict management. Organizations that do not manage their data well do not manage metadata at all. Metadata management is the starting point for comprehensive improvement of data management.\nCreation and use are key points in the data lifecycle. Managing data must be performed with an understanding of how data is generated or acquired and how it is used. Producing data costs money. Data only has value when it is consumed or applied.\nData quality management must permeate the entire data lifecycle. Data quality management is central to data management. Low-quality data means costs and risks, not value. Organizations often find managing data quality challenging, as mentioned previously, data is often created as a byproduct of operational processes, and organizations usually do not set clear standards for quality. Since the quality level of data may be affected by a range of lifecycle events, quality must be planned as part of the data lifecycle.\nMetadata quality management must permeate the entire data lifecycle. Because metadata is a form of data and because organizations rely on it to manage other data, metadata quality must be managed in the same way as other data quality.\nData management also includes ensuring data security and reducing risks related to data. Data that needs protection must be protected throughout its entire lifecycle (from creation to destruction).\nData management efforts should focus on key data. Organizations produce a large amount of data, much of which is never actually used. It is impossible to manage every piece of data. Lifecycle management requires focusing on organizationally critical data and minimizing data ROT (Redundant, Obsolete, Trivial) (Aiken, 2014).\nDifferent types of data have different lifecycle management needs, making data management more complex. Any management system needs to classify the objects it manages. Data can be classified by type, such as transaction data, reference data, master data, metadata, or category data, source data, event data, detailed transaction data; or it can be classified by data content (such as data domains, subject areas), required formats, or protection levels, or by the ways and locations of storage or access (see Chapters 5 and 10).Because different data types have different needs, are related to different risks, and play different roles in an organization, many data management tools focus on classification and control (Bryce, 2005). For example, master data and transaction data have different uses, so their management requirements differ.Data represents not only value but also risks. Inaccurate, incomplete, or outdated low-quality data, due to its incorrect information, clearly represents risks. The risk of data lies in that it may be misunderstood and misused.The highest quality data brings the greatest value to an organization—accessible, interrelated, complete, accurate, consistent, timely, applicable, meaningful, and easy to understand. However, there is often a gap in information—the difference between known information and needed information. This deficiency in the information gap can have a profound impact on operational efficiency and profits. Organizations aware of the value of high-quality data can take specific, proactive measures to improve the quality and availability of data and information within a regulatory and ethical cultural framework.As the role of information as an organizational asset grows across all departments, regulators and legislators are increasingly concerned about the potential misuse of information in use. From the Sarbanes-Oxley Act (focused on controlling the accuracy and effectiveness of financial transaction data from transactions to balance sheets) to Solvency II (focused on supporting risk models and capital adequacy in the insurance industry), to the rapid growth of data privacy regulations over the past decade (including handling personal data within various industries and jurisdictions), it is evident, although financial departments are still waiting to list information as an asset on balance sheets, the regulatory environment increasingly hopes to include it in risk registers and take appropriate mitigation and control measures.Similarly, as consumers become increasingly aware of how their data is used, they not only expect more smooth and efficient operational processes but also expect their information to be protected and their privacy respected. This means that for data management professionals, the range of strategic-level stakeholders is usually broader than in traditional circumstances (see Chapters 2 and 7).Unfortunately, when these risks are not managed, shareholders express their opinions by selling shares, regulators impose fines or restrictions on company operations, and customers make choices with their wallets, thus increasing the impact of information management on the balance sheet.\nA compelling data management vision.\nA summary of the business case for data management.\nGuiding principles, values, and management perspectives.\nThe mission and long-term goals of data management.\nRecommended measures for data management success.\nShort-term (12-24 months) data management plan objectives that adhere to the SMART principles (Specific, Measurable, Actionable, Realistic, Time-bound).\nA description of data management roles and organizations, and a summary of their responsibilities and decision-making authority.\nData management program components and initial tasks.\nA priority work plan with a specific, clear scope.\nA draft implementation roadmap containing projects and action tasks. Data management charter. Including overall vision, business case, objectives, guiding principles, success measurement standards, key success factors, identifiable risks, operational models, etc.\nData management scope statement. Including planning purposes and objectives (usually for 3 years), and the roles, organizations, and leaders responsible for achieving these goals.\nData management implementation roadmap. Identifying specific plans, project assignments, and delivery milestones (see Chapter 15).\nThe Strategic Alignment Model (SAM) abstracts the basic driving factors of various data management methods (Henderson and Venkatraman, 1999), with the relationship between data and information at its core. Information is usually related to business strategy and the operational use of data. Data is associated with information technology and processes, which support the physical systems that make data accessible. Around this concept are the four basic areas of strategic choice: business strategy, IT strategy, organization and processes, and information systems.The Amsterdam Information Model (AIM), like the Strategic Alignment Model, views the consistency between business and IT from a strategic perspective (Abcoower, Maes, and Truijens, 1997). It has nine units, abstracting a focus structure (including planning and architecture) and a strategic middle layer. Additionally, the necessity of information communication must be recognized.The SAM (Strategic Alignment Model) and AIM (Amsterdam Information Model) frameworks describe the relationships between components in detail from two dimensions: the horizontal axis (business/IT strategy) and the vertical axis (business strategy/business operations).The DAMA-DMBOK framework delves deeper into the knowledge areas that constitute the overall scope of data management. The DAMA data management framework is described through three diagrams:\nDAMA Wheel.\nAmsterdam Information Model\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Management","level":3,"id":"Data_Management_0"},{"heading":"Goals of organizational data management include:","level":3,"id":"Goals_of_organizational_data_management_include_0"},{"heading":"Data Management Principles","level":3,"id":"Data_Management_Principles_0"},{"heading":"(1) Data is an Asset with Unique Properties","level":4,"id":"(1)_Data_is_an_Asset_with_Unique_Properties_0"},{"heading":"(2) The Value of Data can be Expressed in Economic Terms","level":4,"id":"(2)_The_Value_of_Data_can_be_Expressed_in_Economic_Terms_0"},{"heading":"(3) Managing Data Means Managing Data Quality","level":4,"id":"(3)_Managing_Data_Means_Managing_Data_Quality_0"},{"heading":"(4) Managing Data Requires Metadata","level":4,"id":"(4)_Managing_Data_Requires_Metadata_0"},{"heading":"(5) Data Management Requires Planning","level":4,"id":"(5)_Data_Management_Requires_Planning_0"},{"heading":"(6) Data Management Must Drive IT Decisions","level":4,"id":"(6)_Data_Management_Must_Drive_IT_Decisions_0"},{"heading":"(7) Data Management is a Cross-Functional Job","level":4,"id":"(7)_Data_Management_is_a_Cross-Functional_Job_0"},{"heading":"(8) Data Management Requires an Enterprise Perspective","level":4,"id":"(8)_Data_Management_Requires_an_Enterprise_Perspective_0"},{"heading":"(9) Data Management Requires Multi-faceted Thinking","level":4,"id":"(9)_Data_Management_Requires_Multi-faceted_Thinking_0"},{"heading":"(10) Data Management Requires Full Lifecycle Management, Different Types of Data Have Different Lifecycle Characteristics","level":4,"id":"(10)_Data_Management_Requires_Full_Lifecycle_Management,_Different_Types_of_Data_Have_Different_Lifecycle_Characteristics_0"},{"heading":"(11) Data Management Must Include Risks Related to Data","level":4,"id":"(11)_Data_Management_Must_Include_Risks_Related_to_Data_0"},{"heading":"(12) Effective Data Management Requires Leadership Commitment","level":4,"id":"(12)_Effective_Data_Management_Requires_Leadership_Commitment_0"},{"heading":"Value","level":3,"id":"Value_0"},{"heading":"Data Optimization Plan","level":3,"id":"Data_Optimization_Plan_0"},{"heading":"Metadata and Data Management","level":3,"id":"Metadata_and_Data_Management_0"},{"heading":"Key impacts of data management on the data lifecycle:","level":3,"id":"Key_impacts_of_data_management_on_the_data_lifecycle_0"},{"heading":"Different Types of Data","level":3,"id":"Different_Types_of_Data_0"},{"heading":"Data and Risks","level":3,"id":"Data_and_Risks_0"},{"heading":"The composition of a data management strategy should include:","level":3,"id":"The_composition_of_a_data_management_strategy_should_include_0"},{"heading":"Deliverables of data management strategic planning include:","level":3,"id":"Deliverables_of_data_management_strategic_planning_include_0"},{"heading":"1.3.1 Strategic Alignment Model","level":3,"id":"1.3.1_Strategic_Alignment_Model_0"},{"heading":"1.3.2 The Amsterdam Information Model","level":3,"id":"1.3.2_The_Amsterdam_Information_Model_0"},{"heading":"DAMA-DMBOK Framework","level":3,"id":"DAMA-DMBOK_Framework_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/0.-data-management.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641397,"modifiedTime":1737554783000,"sourceSize":17524,"sourcePath":"Big Data/13. Certificate/CDGA/0. Data Management.md","exportPath":"big-data/13.-certificate/cdga/0.-data-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/1.-data-handling-ethics.html":{"title":"1. Data Handling Ethics","icon":"","description":"\nEthical principles focus on fairness, respect, responsibility, integrity, quality, reliability, transparency, and trust.\nDefinition of Data Processing Ethics: How to acquire, store, manage, use, and dispose of data in a manner that aligns with ethical principles. (P28)\nKey Areas of Focus: Impact on individuals.\nPotential misuse.\nEconomic value of data. Define ethical standards for data processing within an organization.\nEducate employees about the risks of improper data handling.\nChange or instill a culture of ethical data processing behavior.\nRegulate, measure, monitor, and adjust organizational ethical conduct.\n(Define standards; Educate employees; Change culture; Monitor behavior — [Define, Educate, Change, Monitor]) Communication planning checklists.\nAnnual ethics oath conferences. Number of employees trained.\nCompliance/non-compliance incidents.\nExecutive participation. Enhance trust in the organization, data, and data processing outcomes.\nReduce risks of misuse by employees, customers, and partners.\n“Z-Archive/Pictures/Data/dataHandlingEthics.png” could not be found.\nReview data processing practices.\nIdentify principles, methods, and risk factors.\nDevelop ethical data processing strategies.\nIdentify practical gaps.\nCommunicate and train employees.\nMonitor and correct behavior.\n(Review practices; Identify risks; Build strategies; Address gaps; Train employees; Monitor and correct) Respect for individuals.\nBeneficence (maximize benefits and minimize harm).\nJustice.\nRespect for laws and public interest.\n(Based on the Menlo Report from the U.S. Department of Homeland Security.) Limitation on data collection.\nData quality assurance.\nPurpose-specific data collection.\nRestrictions on data use.\nSecurity measures.\nExpectations for openness and transparency.\nIndividuals' ability to challenge inaccuracies in their data.\nOrganizational responsibility for compliance with guidelines. Fairness/legality/transparency.\nPurpose limitation.\nData minimization.\nAccuracy (updated, correctable, or deletable data).\nStorage limitation (store identifiable data only as long as necessary).\nIntegrity and confidentiality (secure processing).\nAccountability (responsibility for demonstrating compliance).\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Ethical Principles","level":3,"id":"Ethical_Principles_0"},{"heading":"Goals of Data Handling Ethics","level":3,"id":"Goals_of_Data_Handling_Ethics_0"},{"heading":"Methods","level":3,"id":"Methods_0"},{"heading":"Metrics","level":3,"id":"Metrics_0"},{"heading":"Business Drivers","level":3,"id":"Business_Drivers_0"},{"heading":"Activities in Data Processing Ethics","level":3,"id":"Activities_in_Data_Processing_Ethics_0"},{"heading":"Principles of Data Ethics","level":3,"id":"Principles_of_Data_Ethics_0"},{"heading":"OECD Fair Information Processing Standards","level":3,"id":"OECD_Fair_Information_Processing_Standards_0"},{"heading":"<mark style=\"background: #FFB8EBA6;\">EU GDPR Guidelines</mark> (2016)","level":3,"id":"<mark_style=\"background_#FFB8EBA6;\">EU_GDPR_Guidelines</mark>_(2016)_0"},{"heading":"Canadian PIPEDA Legal Obligations","level":3,"id":"Canadian_PIPEDA_Legal_Obligations_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/1.-data-handling-ethics.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641398,"modifiedTime":1737554783000,"sourceSize":5058,"sourcePath":"Big Data/13. Certificate/CDGA/1. Data Handling Ethics.md","exportPath":"big-data/13.-certificate/cdga/1.-data-handling-ethics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/2.-data-governance.html":{"title":"2. Data Governance","icon":"","description":"A series of activities for exercising authority, control, and shared decision-making (planning, monitoring, and execution) over data asset management. Data Governance Function: Guides all other data management domains. Its purpose is to ensure data is properly managed according to established policies and best practices.\nThe overarching driver of data management is to derive value from organizational data.\nData governance focuses on decision-making, behaviors, and processes regarding data. Common areas based on organizational needs:\nStrategy.\nPolicies.\nStandards and quality.\nOversight.\nCompliance.\nIssue management.\nData management projects.\nData asset valuation. To achieve these goals, organizations:\nEstablish rules and practices for data management across various levels.\nEngage in organizational change management to highlight the benefits of improved data governance.\nSecure C-level support (e.g., CRO, CFO, or CDO).\nAddress cultural change as a critical part of governance to gain leadership support. Compliance: Focus on reducing risks and improving processes.\nRisk Reduction: General risk management.\nData security.\nPrivacy. Process Improvement: Regulatory compliance.\nImproved data quality.\nMetadata management.\nProject development efficiency.\nVendor management. Enhance the ability to manage data assets.\nDefine, approve, communicate, and implement principles, policies, procedures, metrics, tools, and responsibilities for data management.\nMonitor and guide compliance with policies, data use, and management activities. Sustainable.\nEmbedded within processes, not additional.\nMeasurable. Leadership and Strategy: Requires visionary leadership and strategic alignment with business goals.\nBusiness-driven: A business management program that integrates IT and business decision-making.\nShared Responsibility: Involves both business data managers and data governance professionals.\nMulti-layered: Operates across enterprise, local, and intermediate levels.\nFramework-based: Requires an operational framework for cross-functional coordination.\nPrinciple-based: Founded on guiding principles for strategies and activities. Centralized: A single organization oversees activities across all business units.\nDistributed: Each business unit adopts the same governance model and standards.\nFederated: Collaboration between a central governance body and multiple business units for consistency in definitions and standards.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition","level":3,"id":"Definition_0"},{"heading":"Context and Responsibilities of Data Governance and Management","level":3,"id":"Context_and_Responsibilities_of_Data_Governance_and_Management_0"},{"heading":"<mark style=\"background: #FFB8EBA6;\">Scope and Focus of Data Governance Projects</mark>","level":3,"id":"<mark_style=\"background_#FFB8EBA6;\">Scope_and_Focus_of_Data_Governance_Projects</mark>_0"},{"heading":"Implementation Goals","level":3,"id":"Implementation_Goals_0"},{"heading":"Business Drivers","level":3,"id":"Business_Drivers_0"},{"heading":"<mark style=\"background: #ABF7F7A6;\">Data Governance Objectives</mark>","level":3,"id":"<mark_style=\"background_#ABF7F7A6;\">Data_Governance_Objectives</mark>_0"},{"heading":"Characteristics of Effective Data Governance","level":3,"id":"Characteristics_of_Effective_Data_Governance_0"},{"heading":"Core Principles of Data Governance","level":3,"id":"Core_Principles_of_Data_Governance_0"},{"heading":"Governance Models","level":3,"id":"Governance_Models_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/2.-data-governance.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641430,"modifiedTime":1737554783000,"sourceSize":4450,"sourcePath":"Big Data/13. Certificate/CDGA/2. Data Governance.md","exportPath":"big-data/13.-certificate/cdga/2.-data-governance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/3.-data-architecture.html":{"title":"3. Data Architecture","icon":"","description":"\nDefinition: An organized design of components aimed at optimizing the functionality, performance, feasibility, cost, and user experience of an overall structure or system. It is the fundamental structure of a system, reflected in the components, their interrelations, and the principles guiding their design and evolution. It operates across different scopes and levels, defining complex elements in a clear and explicit manner. Enterprise Architecture: Includes business architecture, data architecture, application architecture, and technology architecture. A good architecture helps organizations understand system status, accelerate improvements, and achieve regulatory compliance and efficiency goals. Data Architecture: Forms the foundation of data management and needs to be described at various levels for better understanding and decision-making. Components: Description of the current state, definition of data requirements, guidance for data integration, and data asset management standards. Goals: Build a bridge between business strategies and technical implementation. It is a part of enterprise architecture. Build a smooth bridge between business strategy and technical implementation. Key responsibilities include: Strategically assisting organizations in rapidly transforming products, services, and data using emerging technologies.\nConverting business needs into data and application requirements to ensure effective data support for business processes.\nManaging complex data and information across the enterprise.\nAligning business and IT technologies.\nSupporting organizational reform, transformation, and adaptability improvements. Outputs: Data storage and processing requirements.\nDesigning structures and plans to meet current and long-term data needs. Work: Define the current state of data.\nProvide standard business vocabulary for data and components.\nEnsure alignment of data architecture with enterprise strategy and business architecture.\nDescribe data strategy requirements.\nHigh-level data integration design.\nDevelop enterprise data architecture blueprints. Use data architecture components (main blueprints) to define data requirements, guide integration, manage data assets, and align data projects with enterprise strategy. Collaborate with stakeholders involved in business or IT system improvements to influence and learn from them.\nEstablish a common enterprise data language using the architecture and vocabulary.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Architecture","level":3,"id":"Architecture_0"},{"heading":"Components and Goals of Data Architecture","level":3,"id":"Components_and_Goals_of_Data_Architecture_0"},{"heading":"Business-Driven Factors","level":3,"id":"Business-Driven_Factors_0"},{"heading":"Key Outputs and Work of Data Architects","level":3,"id":"Key_Outputs_and_Work_of_Data_Architects_0"},{"heading":"Implementation of Data Architecture","level":3,"id":"Implementation_of_Data_Architecture_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/3.-data-architecture.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641431,"modifiedTime":1737554783000,"sourceSize":5081,"sourcePath":"Big Data/13. Certificate/CDGA/3. Data Architecture.md","exportPath":"big-data/13.-certificate/cdga/3.-data-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/4.-data-modeling-and-design.html":{"title":"4. Data Modeling and Design","icon":"","description":"\nDefinition: The process of discovering, analyzing, and determining data requirements, representing and communicating these requirements in a precise form called a data model. This process is iterative and may involve conceptual, logical, and physical models. Six Types: Relational model, multidimensional model, object-oriented model, fact-based model, time-series model, and NoSQL model.\nBased on levels of detail, models can be categorized as conceptual, logical, and physical models. Provide a common vocabulary for data.\nGather and document detailed information about data and systems within the organization.\nServe as a primary communication tool in projects.\nProvide a starting point for application customization, integration, and even replacement. To confirm and document an understanding of data requirements from different perspectives.\nEnsure that applications align better with current and future business needs.\nLay the foundation for further data applications or management, such as master data management and data governance projects. Planning data modeling.\nCreating data models (conceptual, logical, and physical).\nReviewing data models.\nMaintaining data models. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition of Data Modeling","level":4,"id":"Definition_of_Data_Modeling_0"},{"heading":"Common Types of Data Models","level":4,"id":"Common_Types_of_Data_Models_0"},{"heading":"Business Drivers","level":4,"id":"Business_Drivers_0"},{"heading":"Objectives of Data Modeling and Design","level":4,"id":"Objectives_of_Data_Modeling_and_Design_0"},{"heading":"Activities in Data Modeling and Design","level":4,"id":"Activities_in_Data_Modeling_and_Design_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/4.-data-modeling-and-design.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641432,"modifiedTime":1737554783000,"sourceSize":5866,"sourcePath":"Big Data/13. Certificate/CDGA/4. Data Modeling and Design.md","exportPath":"big-data/13.-certificate/cdga/4.-data-modeling-and-design.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/5.-data-storage-and-operations.html":{"title":"5. Data Storage and Operations","icon":"","description":"\nDefinition: Management of data storage design, implementation, and support activities to maximize its value. Manage data availability throughout its lifecycle.\nEnsure the integrity of data assets.\nManage the performance of data transactions. Database Technology Management: Understand database technologies.\nEvaluate database technologies.\nManage and monitor database technologies. Database Operations Management: Understand requirements.\nPlan business continuity.\nCreate database instances.\nManage database performance.\nManage test datasets.\nManage data migration. Data architecture.\nData requirements.\nData models.\nService Level Agreements (SLAs). Database technology evaluation standards.\nDatabase environment.\nMigration/replication/multiple versioned data.\nBusiness continuity planning.\nDatabase performance Operational Level Agreement (OLA). Data storage metrics.\nPerformance metrics.\nOperational metrics.\nService metrics. Ensure business continuity. Identify and act on automation opportunities.\nBuild with reuse in mind.\nUnderstand and apply best practices appropriately.\nSupport standardized database requirements.\nSet expectations for DBA roles in projects. Support development, testing, QA, and other special database environments.\nDBA is not the sole role for managing data storage and operations. Roles include: Production DBA.\nApplication DBA.\nProcess and Development DBA.\nNetwork Storage Administrator (NSA). Ensure database performance and reliability through activities like performance tuning, monitoring, and error reporting.\nImplement backup and recovery mechanisms.\nEnsure data availability through clustering and fault tolerance.\nPerform other maintenance activities like data archiving. Production database environment with proper security, reliability, and availability.\nMechanisms and processes to control database implementation changes.\nMechanisms to ensure data integrity, availability, and recovery.\nError detection and reporting mechanisms.\nDatabase services aligned with SLA.\nPerformance monitoring mechanisms and processes. Centralized database.\nDistributed database: Federated (autonomous).\nNon-federated (non-autonomous). Federated databases are suitable for heterogeneous, distributed integration projects like enterprise information integration, data visualization, schema matching, and master data management. Virtual machine images.\nDatabase-as-a-Service (DaaS).\nManaged databases on the cloud. Atomicity.\nConsistency.\nIsolation.\nDurability. Basically Available.\nSoft State.\nEventual Consistency. Consistency.\nAvailability.\nPartition Tolerance. (Choose any two) Disks and Storage Area Networks (SAN).\nMemory.\nColumn compression schemes.\nFlash storage. Hierarchical database.\nRelational database: Multidimensional databases.\nTemporal databases. Non-relational database: Columnar.\nSpatial.\nObject/Multimedia.\nFlat files.\nKey-value pairs.\nTriple stores. Data archiving.\nCapacity and growth forecasting.\nChange data capture.\nData cleansing.\nData replication: Active and passive replication.\nHorizontal and vertical scaling.\nMirroring and log shipping. Understand the technical characteristics of databases.\nComprehend how technologies work and how they provide value in specific business environments. Key considerations include:\nProduct architecture and complexity.\nCapacity and speed limitations, including data transfer rates.\nApplication categories (e.g., transaction processing, business intelligence, personal data).\nSpecial features (e.g., time computation support).\nHardware platform and OS support.\nAvailability of software support tools.\nPerformance evaluation, including real-time statistics.\nScalability.\nSoftware, memory, and storage requirements.\nResilience, including error handling and reporting.\nAdditional Factors:\nOrganizational tolerance for technical risks.\nAvailability of trained technical personnel.\nTotal cost of ownership (e.g., licensing fees, maintenance, computational resources).\nVendor reputation and support policies.\nVendor relationships and customer success stories. Emphasize regular training.\nPerform periodic backups and recovery tests. Define storage needs: Permanent or temporary storage.\nInitial capacity and growth projections.\nCompliance with data retention policies. Identify usage patterns.\nDefine access requirements. Database Usage Patterns:\nTransaction-based.\nLarge dataset reads or writes.\nTime-based.\nLocation-based.\nPriority-based. Plan for disaster events and adverse conditions impacting systems or data access.\nEnsure recovery plans are reviewed and approved by business continuity teams.\nPeriodically assess plan accuracy and comprehensiveness. Steps:\nBackup data.\nRecover data. Install and update DBMS software.\nMaintain installations across environments.\nInstall and manage related data technologies.\nManaging Related Data Technologies:\nManage physical storage environments.\nControl data access (controlled environments, physical security, monitoring).\nCreate storage containers.\nApply physical data models.\nLoad data.\nManage data replication (active/passive replication, distributed concurrency control). Steps to optimize performance:\nSet and optimize OS and application parameters.\nManage database connections.\nCollaborate with developers and network admins to optimize systems.\nProvide appropriate storage.\nPredict capacity growth.\nProvide operational workload and benchmarks for SLA management and planning. Performance Management Includes:\nSetting database performance service levels.\nManaging database availability: Manageability.\nRecoverability.\nReliability.\nMaintainability. Managing database operations.\nMaintaining performance service levels.\nMaintaining standby environments. Scheduled downtime.\nUnscheduled downtime.\nApplication issues.\nData issues.\nHuman error. Tools for Availability:\nBackup tools.\nReorganization tools.\nStatistical search tools.\nData integrity check tools.\nAutomated execution tools.\nData replication across databases. Memory allocation and contention.\nLocks and blocks.\nInaccurate database statistics.\nPoorly written code.\nInefficient or complex table joins.\nImproper indexing.\nApplication activities.\nServer overload.\nDatabase volatility.\nRunaway queries. Development environment.\nTesting environment.\nData sandbox.\nStandby production environment. Tools: Data modeling tools, monitoring tools, database management tools, development support tools. Methods: Testing in lower environments, physical naming standards, and scripting all changes. Data loss risks (SLA and data audit to mitigate risks).\nTechnical readiness assessment. DBAs often face challenges in showcasing their value to the organization. To address this:\nCommunicate proactively.\nUnderstand others’ perspectives.\nFocus on business outcomes.\nBe helpful to others.\nContinuously learn. Data Storage Metrics: Number of database types.\nAggregate transaction statistics.\nCapacity metrics.\nAmount of used storage.\nNumber of storage containers.\nNumber of committed and uncommitted blocks or pages in data objects.\nData queues.\nStorage service usage.\nNumber of requests made to storage services.\nImprovements in application performance using the service. Performance Metrics: Transaction frequency and volume.\nQuery performance.\nAPI service performance. Operational Metrics: Summary statistics on data retrieval times.\nBackup size.\nData quality assessments.\nAvailability. Service Metrics: Number of issue submissions, resolutions, and escalations by type.\nTime taken to resolve issues. Ensures databases comply with all licensing agreements and regulatory requirements.\nAudit data can help determine the total cost of ownership (TCO) for each technology and product. Data Audit: The process of evaluating datasets against defined standards.\nOften focuses on specific aspects of the dataset.\nObjective: Ensure data storage complies with contractual and procedural requirements.\nMethods may include: Project-specific and comprehensive checklists.\nRequired deliverables.\nQuality control standards. Data Validation: Assessing stored data based on established acceptance criteria to determine quality and usability.\nRelies on standards set by data quality teams or other users of the data. Assist in developing and reviewing methods.\nConduct initial data screening and review.\nDevelop data monitoring methods.\nApply statistical, geostatistical, or biostatistical techniques to optimize data analysis.\nSupport sampling and analysis.\nAudit data.\nProvide support for data discovery.\nAct as subject matter experts for issues related to database management.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Definition</strong>","level":3,"id":"**Definition**_0"},{"heading":"<strong>Objectives</strong>","level":3,"id":"**Objectives**_0"},{"heading":"<strong>Activities</strong>","level":3,"id":"**Activities**_0"},{"heading":"<strong>Inputs</strong>","level":3,"id":"**Inputs**_0"},{"heading":"<strong>Deliverables</strong>","level":3,"id":"**Deliverables**_0"},{"heading":"<strong>Metrics</strong>","level":3,"id":"**Metrics**_0"},{"heading":"<strong>Business Drivers</strong>","level":3,"id":"**Business_Drivers**_0"},{"heading":"<strong>Guidelines for DBAs</strong>","level":3,"id":"**Guidelines_for_DBAs**_0"},{"heading":"<strong>Roles of a DBA</strong>","level":3,"id":"**Roles_of_a_DBA**_0"},{"heading":"<strong>Production DBA Responsibilities</strong>","level":3,"id":"**Production_DBA_Responsibilities**_0"},{"heading":"<strong>Deliverables of Production DBA</strong>","level":4,"id":"**Deliverables_of_Production_DBA**_0"},{"heading":"<strong>Types of Database Architectures</strong>","level":3,"id":"**Types_of_Database_Architectures**_0"},{"heading":"<strong>Database Implementation on Cloud</strong>","level":3,"id":"**Database_Implementation_on_Cloud**_0"},{"heading":"<strong>ACID Properties</strong>","level":3,"id":"**ACID_Properties**_0"},{"heading":"<strong>BASE Properties</strong>","level":3,"id":"**BASE_Properties**_0"},{"heading":"<strong>CAP Theorem</strong>","level":3,"id":"**CAP_Theorem**_0"},{"heading":"<strong>Data Storage Media</strong>","level":3,"id":"**Data_Storage_Media**_0"},{"heading":"<strong>Database Organization Models</strong>","level":3,"id":"**Database_Organization_Models**_0"},{"heading":"<strong>Common Database Processes</strong>","level":3,"id":"**Common_Database_Processes**_0"},{"heading":"<strong>Key Activities</strong>","level":3,"id":"**Key_Activities**_0"},{"heading":"<strong>Activity 1-1: Understanding Database Technologies</strong>","level":4,"id":"**Activity_1-1_Understanding_Database_Technologies**_0"},{"heading":"<strong>Activity 1-2: Evaluating Database Technologies</strong>","level":4,"id":"**Activity_1-2_Evaluating_Database_Technologies**_0"},{"heading":"<strong>Activity 1-3: Managing and Monitoring Database Technologies</strong>","level":4,"id":"**Activity_1-3_Managing_and_Monitoring_Database_Technologies**_0"},{"heading":"<strong>Activity 2-1: <mark style=\"background: #FFF3A3A6;\">Understanding Requirements</mark></strong>","level":4,"id":"**Activity_2-1_<mark_style=\"background_#FFF3A3A6;\">Understanding_Requirements</mark>**_0"},{"heading":"<strong>Activity 2-2: Planning Business Continuity</strong>","level":4,"id":"**Activity_2-2_Planning_Business_Continuity**_0"},{"heading":"<strong>Activity 2-3: Creating Database Instances</strong>","level":4,"id":"**Activity_2-3_Creating_Database_Instances**_0"},{"heading":"<strong>Activity 2-4: Managing Database Performance</strong>","level":4,"id":"**Activity_2-4_Managing_Database_Performance**_0"},{"heading":"<strong>Factors Influencing Availability</strong>","level":3,"id":"**Factors_Influencing_Availability**_0"},{"heading":"<strong>Common Causes of Poor Performance</strong>","level":3,"id":"**Common_Causes_of_Poor_Performance**_0"},{"heading":"<strong>Environment Types</strong>","level":3,"id":"**Environment_Types**_0"},{"heading":"<strong>Tools and Methods</strong>","level":3,"id":"**Tools_and_Methods**_0"},{"heading":"<strong>Readiness and Risk Assessment</strong>","level":3,"id":"**Readiness_and_Risk_Assessment**_0"},{"heading":"<strong>Cultural and Organizational Changes</strong>","level":3,"id":"**Cultural_and_Organizational_Changes**_0"},{"heading":"<strong>Data Storage and Operations Governance</strong>","level":3,"id":"**Data_Storage_and_Operations_Governance**_0"},{"heading":"<strong>Metrics</strong>","level":4,"id":"**Metrics**_1"},{"heading":"<strong>Governance of Data Storage</strong>","level":3,"id":"**Governance_of_Data_Storage**_0"},{"heading":"<strong>Data Auditing and Validation</strong>","level":3,"id":"**Data_Auditing_and_Validation**_0"},{"heading":"<strong>DBA’s Role in Auditing and Validation</strong>","level":3,"id":"**DBA’s_Role_in_Auditing_and_Validation**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/5.-data-storage-and-operations.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641432,"modifiedTime":1737554783000,"sourceSize":11149,"sourcePath":"Big Data/13. Certificate/CDGA/5. Data Storage and Operations.md","exportPath":"big-data/13.-certificate/cdga/5.-data-storage-and-operations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/6.-data-security.html":{"title":"6. Data Security","icon":"","description":"\nData security involves the planning, establishment, and execution of security policies and procedures to provide proper authentication, authorization, access, and auditing for data and information assets. Stakeholders: Security needs are driven by protecting stakeholders' interests.\nGovernment Regulations: Regulations may: Restrict information access.\nEnsure transparency, accountability, and openness. Business-Specific Concerns: Proprietary data protection is critical for gaining a competitive edge.\nLegitimate Access Needs: Ensuring authorized access.\nContractual Obligations: Compliance with contractual data protection requirements. Risk Reduction: Enterprise-level data security strategies lower overall costs and risks compared to fragmented approaches.\nEffective security strategies ensure consistency across the organization and mitigate inefficiencies caused by weak security processes.\nData classification and categorization are essential to identify sensitive data requiring protection. Process for Classification and Categorization: Identify and classify sensitive data assets.\nLocate sensitive data across the enterprise.\nDetermine protection methods for each asset.\nAssess interactions between information and business processes, considering external threats and internal risks. Business Growth: Strong information security enhances product/service quality, drives transactions, and builds customer confidence. Security as an Asset: Metadata management of sensitive data optimizes protection and supports business and technical processes.\nSecurity-related metadata becomes a strategic asset, improving transaction quality and reducing risks. Support adaptive access control and prevent inappropriate access to enterprise data assets.\nEnsure compliance with privacy, protection, and confidentiality regulations.\nMeet stakeholder expectations for privacy and confidentiality Collaboration: Encourage cooperation across teams.\nEnterprise Oversight: Ensure organizational consistency in security practices.\nProactive Management: Success depends on active and dynamic management, stakeholder attention, and overcoming organizational silos.\nClear Accountability: Establish regulatory chains of responsibility.\nMetadata-Driven Security: Classification and categorization are integral to defining data security.\nRisk Reduction via Limited Exposure: Minimize the spread of sensitive/confidential data, especially in non-production environments. Identify data security requirements.\nDevelop data security policies.\nDefine detailed data security rules.\nAssess current security risks.\nImplement controls and procedures. Potential actions (internal or external, not always malicious) that could harm the organization. The likelihood of loss due to specific threats, conditions, or vulnerabilities. Critical Risk Data (CRD): Personal information that, if misused, can harm individuals and lead to severe penalties, customer/employee retention costs, and damage to the company's brand and reputation.\nResults in financial harm to the organization. High-Risk Data (HRD): Proprietary data that provides a competitive edge and holds financial value.\nMisuse can result in financial losses, legal risks, regulatory penalties, and reputational damage. Moderate-Risk Data (MRD): Non-public information with minimal financial value but can negatively impact the company if misused. Building an enterprise data model is crucial for effective data protection.\nWithout knowing the location of sensitive information, creating a comprehensive protection plan is impossible 4A1E: Access, Audit, Authentication, Authorization, Entitlement.\nMonitoring: Includes mechanisms to detect unexpected events. Access: Refers to enabling authorized individuals to access systems and data promptly.\nAs a verb, it implies actively connecting to and using data in an information system.\nAs a noun, it signifies valid authorization for specific data. Authentication: Validating the identity of users attempting to access the system. Authorization: Granting users permissions to specific data views aligned with their roles.\nAccess control systems validate authorization tokens upon user login. Entitlement: The aggregate of all data elements exposed to a user based on individual access authorization decisions. Passive Monitoring: Periodically captures system snapshots.\nCompares trends to benchmarks or standards to track changes.\nFunctions as an evaluation mechanism. Active Monitoring: Detects issues in real-time.\nActs as a detection mechanism. Hashing: Converts data of arbitrary length into fixed-length data.\nExamples: MD5, SHA. Symmetric Encryption: Uses the same key for encryption and decryption.\nExamples: DES, 3DES, AES, IDEA. Asymmetric Encryption: Sender encrypts with a public key; receiver decrypts with a private key.\nExamples: RSA, Diffie-Hellman, PGP. Obfuscation: Making data unclear or ambiguous. Data Masking: Changing data appearance without altering the original data. Types of Data Masking:\nStatic Masking: Permanent, irreversible changes to data.\nIncludes: Non-persistent masking: No intermediate files or unmasked data remain.\nPersistent masking: Used when the data source and target are the same; involves risks. Dynamic Masking: Temporarily alters data appearance for end-users or systems.\nExample: Changing 123456 to 12**56. Masking Techniques:\nSubstitution: Replace characters or values with predefined patterns.\nShuffling: Exchange data elements within a record or across records.\nTemporal Variance: Shift dates slightly to retain trends but obscure exact values.\nValue Variance: Apply random factors to values to preserve trends but make data unrecognizable.\nNulling or Deleting: Remove data not needed in test environments.\nRandomization: Replace elements with random characters or values.\nEncryption: Convert recognizable characters into unreadable streams.\nExpression Masking: Replace values with an expression result.\nKey Masking: Ensure unique, repeatable masking for database key fields. Backdoor: Hidden entry point bypassing authentication mechanisms.\nBot/Zombie: Compromised workstation under remote control for malicious tasks.\nCookie: Small data file stored on a computer for tracking preferences; may raise privacy concerns.\nFirewall: Software/hardware filtering traffic to protect against unauthorized access.\nPerimeter: Boundary between an organization and external systems, often protected by firewalls.\nDMZ (Demilitarized Zone): Perimeter area for data exchange between organizations and the Internet.\nSuperuser Account: Administrator-level account with heightened security requirements.\nKeylogger: Software capturing keystrokes, potentially compromising sensitive information.\nPenetration Testing: \"White-hat\" hacking to identify vulnerabilities and install security patches.\nVPN (Virtual Private Network): Secure, encrypted tunnel for communication between users and internal networks. Facility Security: First line of defense against malicious activity targeting physical infrastructure. Device Security: Policies for accessing data via mobile devices.\nData storage standards for portable devices.\nData erasure and disposal policies aligned with records management standards.\nInstallation of anti-malware and encryption software.\nAwareness of security vulnerabilities. Credential Security: Identity management systems, including single sign-on (SSO).\nUnique user ID standards for email systems.\nPassword policies requiring regular updates (e.g., every 45–180 days).\nMulti-factor authentication for sensitive information access. Electronic Communication Security: Preventing external interception or reading of communications.\nSecurity training for users.\nAwareness of email forwarding risks. Confidentiality Levels (\"Need to Know\"): Internal requirements for restricting access to sensitive data. Regulatory Requirements (\"Allowed to Know\"): External compliance standards for data access. Key Difference:\nConfidentiality originates internally, while regulatory requirements are externally imposed. For General Audiences: Public information.\nInternal Use Only: Restricted to employees/members, with minimal sharing risks.\nConfidential: Not to be shared outside the organization without appropriate agreements.\nRestricted Confidential: Accessible only to individuals with specific permissions.\nRegistered Confidential: Highly sensitive data requiring legal agreements for access. Enterprises must classify data into regulatory categories for compliance.\nSimplified, actionable classifications improve operability.\nSimilar regulations can be grouped into series for consistency.\nExamples: Regulation Series: Personal Identifiable Information (PII).\nFinancially Sensitive Data.\nMedical/Personal Health Information (PHI).\nEducational Records. Industry or Contract-Based Regulations: PCI-DSS (Payment Card Industry Data Security Standard).\nTrade secrets or competitive advantage.\nContractual restrictions. The first step in identifying risks is to determine:\nThe location of sensitive data storage.\nThe necessary protection measures for the data. Abuse of Privileges: Adopt the principle of least privilege to prevent excessive permissions.\nUsers, processes, or programs should only access information for legitimate purposes.\nSolution: Query-level access control to limit database permissions to the minimum required SQL operations and data scope. This includes granular control at the table, row, and column levels. Automated tools may be necessary for effective implementation. Misuse of Legitimate Privileges: Can be intentional or unintentional.\nSolution: Database access controls. Unauthorized Privilege Escalation: Combine IPS (Intrusion Prevention Systems) with query-level access control to prevent privilege escalation vulnerabilities.\nIntegrate IPS with other attack indicators to improve accuracy in detecting attacks. Service or Shared Account Abuse: Limit service accounts to specific commands or tasks on designated systems, requiring documentation and approval for credential distribution. Platform Intrusion Attacks. Injection Vulnerabilities. Default Passwords: Removing default passwords is a critical security step during implementation. Backup Data Abuse. White-Hat Hackers: Ethical hackers improving systems.\nMalicious Hackers: Intentionally compromise systems to steal information or cause harm. Social Engineering: Methods used by hackers to trick people into providing information or access. Phishing: Inducing recipients via phone, instant messages, or emails to unknowingly disclose valuable or private information.\nTargeted phishing for executives is known as \"whaling.\" Examples: Adware\nSpyware\nTrojans\nViruses\nWorms Instant messaging.\nSocial networks (e.g., employees posting sensitive corporate information).\nSpam: Hover over hyperlinks to verify authenticity.\nWatch for inability to unsubscribe as a spam indicator. Security governance focuses on outcomes, not methods.\nOrganizations must design security controls to meet or exceed legal requirements. Business Needs: Understand the organization's mission, strategy, and scale.\nTools: Data-Process Matrix and Data-Role Relationship Matrix. Regulatory Requirements: Compile a complete list of regulations, including: Affected data domains.\nRelevant security policy links.\nControl measures. Organizations should develop data security policies based on their specific business and regulatory requirements. A policy is a statement of the chosen course of action and the expected behavior necessary to achieve objectives. Data security policies describe the actions that align with the organization's best interests in protecting its data.To ensure these policies have measurable impacts, they must be auditable and regularly audited. Enterprise Security Policies: Global policies for employee access to facilities and other assets.\nEmail standards and policies.\nSecurity access levels based on roles or job positions.\nReporting protocols for security vulnerabilities. IT Security Policies: Directory structure standards.\nPassword policies.\nIdentity management frameworks. Data Security Policies: Categories for individual applications, database roles, user groups, and information sensitivity. Governance:\nData Governance Committee: Responsible for reviewing and approving data security policies.\nData Management Officer: Oversees and maintains these policies. Procedures complement policies with detailed instructions to fulfill policy intent.\nSteps: Define data confidentiality levels: Classification serves as critical metadata guiding access. Define data regulatory categories: Group similar regulations for efficient management using common protection measures. Define security roles: Tools: Role Allocation Matrix\nRole Allocation Hierarchy Ensure consistent and centrally managed user identities and group memberships.\nUse a change management system to track modifications. Evaluate risks to networks and databases:\nSensitivity of stored or transmitted data.\nRequirements for protecting data.\nExisting security measures.\nResponsibilities:\nSecurity Administrators lead implementation.\nCollaborate with Data Management Officers and Technical Teams.\nDBAs are typically responsible for database security.\nProcedures should include:\nHow users gain or lose system/application access.\nAssigning and removing roles for users.\nMonitoring privilege levels.\nHandling and monitoring access change requests.\nClassifying data based on confidentiality and regulations.\nResponding to data breaches. Assign Confidentiality Levels: The Data Management Officer is responsible for evaluating and determining the appropriate confidentiality levels for data. Assign Regulatory Categories: Data must be categorized based on relevant regulations to ensure compliance. Manage and Maintain Data Security: The primary objective is to prevent security breaches or detect them promptly if they occur.\nContinuous system monitoring and regular audits of security procedures are essential for maintaining data security. Key Tasks: Control Data Availability / Data-Centric Security: Defining and granting authorizations requires: A comprehensive data inventory.\nDetailed analysis of data needs.\nDocumentation of data accessible under each user’s permissions. Data Masking: Protects data even if unintentionally exposed.\nDecryption Key Authorization: Can be part of the user authorization process. Authorized users can view unencrypted data, while others only see randomized characters.\nRelational Database Views: Enforce data security levels by restricting access based on data values.\nLimit access to specific rows or columns to protect confidential or regulated fields. Monitor User Authentication and Access Behavior: Reporting access activities is a fundamental requirement for compliance audits.\nMonitoring authentication and access behavior provides insights into who is connecting to and accessing information assets.\nHelps identify anomalies, unexpected actions, or suspicious transactions that warrant investigation. Manage Compliance with Security Policies: Regulatory Compliance Management: Ensure adherence to laws and regulations. Audit Data Security and Compliance Activities: Regularly review security measures and practices to maintain regulatory and organizational compliance. Regulatory Risk: Non-compliance with regulations.\nDetection and Recovery Risk: Inefficiency in identifying and recovering from breaches.\nManagement and Audit Responsibility Risk: Audit responsibilities should be independent of DBAs and database platform support staff. Risk of Inadequate Local Audit Tools: Lack of user attribution for fraudulent database transactions. High performance.\nSeparation of duties.\nGranular transaction tracking. Measure adherence to policies and procedures.\nEnsure all data requirements are measurable and auditable.\nUse standard tools and processes to protect regulated data in storage and transit.\nReport and address potential compliance issues through escalation mechanisms. Inputs include: Policies, standard documents, implementation guidelines, change requests.\nAccess monitoring logs, report outputs, and other records (electronic or hard copy).\nTesting and inspections. Auditing Tasks:\nEvaluate policies and procedures to ensure clearly defined compliance controls meet regulatory requirements.\nAnalyze implementation processes and user authorization practices.\nAssess authorization standards and procedures for adequacy and technical compliance.\nReview escalation and notification mechanisms for violations or potential violations.\nExamine contracts with external vendors and data-sharing agreements.\nAssess the maturity of security practices and report \"regulatory compliance status\" to senior management.\nRecommend compliance policy changes and operational improvements. Common Tools: Antivirus/security software.\nHTTPS protocols.\nIdentity management technologies.\nIntrusion detection and prevention software.\nFirewalls.\nMetadata tracking.\nData masking and encryption. Apply CURDE Matrix (Create, Read, Update, Delete, Execute).\nDeploy security patches immediately.\nUse metadata for data security attributes.\nInclude security requirements in project needs.\nEnable efficient searches of encrypted data.\nPerform file cleanups. Perfect data security is nearly impossible, but the best approach is to establish awareness of security requirements, policies, and procedures. Training: Mandatory for employees and linked to performance evaluations. Consistency: Develop consistent data security policies for all teams and departments. Measuring Benefits: Include objective data security metrics in organizational evaluations. Vendor Security Requirements: Incorporate data security expectations in SLAs and outsourcing contracts. Emphasizing Urgency: Highlight legal, contractual, and regulatory obligations. Continuous Communication Risks: Loss of control over technical environments and data usage by external parties. Internal Responsibility: IT architecture and data security measures should remain internally managed, even if outsourced partners implement systems. Service Level Agreements (SLAs).\nLimited liability clauses in contracts.\nClauses specifying audit rights.\nClear consequences for breaches of contractual obligations.\nRegular data security reports from service providers.\nIndependent monitoring of vendor activities.\nRegular and thorough security audits.\nOngoing communication with providers.\nAwareness of legal differences in international disputes. Collaboration between IT and business stakeholders is essential.\nClear policies and procedures form the foundation of governance. Tools for managing data security.\nEncryption standards and mechanisms.\nVendor and contractor data access guidelines.\nInternet data transmission protocols.\nDocumentation requirements.\nRemote access standards.\nIncident reporting procedures. Internal systems and business units.\nExternal business partners.\nRegulatory bodies. Security Implementation Metrics\nSecurity Awareness Metrics\nData Protection Metrics\nSecurity Incident Metrics\nConfidential Data Diffusion Metrics Percentage of enterprise computers with the latest security patches installed.\nPercentage of computers with up-to-date anti-malware software installed and running.\nPercentage of new employees who successfully pass background checks.\nPercentage of employees scoring above 80% in annual security practice assessments.\nPercentage of business units that have completed formal risk assessment analyses.\nPercentage of business processes passing disaster recovery tests during events like fires, earthquakes, storms, floods, or explosions.\nPercentage of audit findings successfully resolved. (P200) Results of risk assessments.\nRisk events and their associated profiles.\nFormal feedback from surveys and interviews.\nIncident reviews, lessons learned, and interviews with affected individuals.\nPatch effectiveness audits. Criticality rankings for specific data types and information systems: Assessing their impact on business operations. Annual loss expectations from incidents such as data loss, breaches, hacks, thefts, or disasters.\nRisk assessment for specific data loss scenarios, including: Regulated information categories.\nPriority order for remediation actions. Risk mapping of data to specific business processes: Example: Risks associated with sales point devices included in financial payment system forecasts. Threat assessment of potential attacks on valuable data resources and their distribution channels.\nVulnerability assessments for business processes where sensitive information could be unintentionally or intentionally leaked. Number of detected and blocked intrusion attempts.\nROI (Return on Investment) from cost savings through intrusion prevention. Measure the number of copies of confidential data to reduce its proliferation.\nThe more locations confidential data is stored, the greater the risk of leakage. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Definition</strong>","level":3,"id":"**Definition**_0"},{"heading":"<strong><mark style=\"background: #FFB8EBA6;\">Sources of Data Security Requirements</mark></strong>","level":3,"id":"**<mark_style=\"background_#FFB8EBA6;\">Sources_of_Data_Security_Requirements</mark>**_0"},{"heading":"<strong>Business Drivers</strong>","level":3,"id":"**Business_Drivers**_0"},{"heading":"<strong>Objectives</strong>","level":3,"id":"**Objectives**_0"},{"heading":"<strong><mark style=\"background: #ADCCFFA6;\">Principles</mark></strong>","level":3,"id":"**<mark_style=\"background_#ADCCFFA6;\">Principles</mark>**_0"},{"heading":"<strong>Activities</strong>","level":3,"id":"**Activities**_0"},{"heading":"<strong>Vulnerability</strong>","level":3,"id":"**Vulnerability**_0"},{"heading":"<strong>Threat</strong>","level":4,"id":"**Threat**_0"},{"heading":"<strong>Risk</strong>","level":4,"id":"**Risk**_0"},{"heading":"<strong>Risk Categories</strong>","level":3,"id":"**Risk_Categories**_0"},{"heading":"<strong>Data Security Organization</strong>","level":3,"id":"**Data_Security_Organization**_0"},{"heading":"<strong>Data Security Requirements and Processes</strong>","level":3,"id":"**Data_Security_Requirements_and_Processes**_0"},{"heading":"<strong>4A1E Framework</strong>","level":3,"id":"**4A1E_Framework**_0"},{"heading":"<strong>Monitoring</strong>","level":3,"id":"**Monitoring**_0"},{"heading":"<strong>Encryption Methods</strong>","level":3,"id":"**Encryption_Methods**_0"},{"heading":"<strong>Obfuscation and Data Masking</strong>","level":3,"id":"**Obfuscation_and_Data_Masking**_0"},{"heading":"<strong>Network Security Terms</strong>","level":3,"id":"**Network_Security_Terms**_0"},{"heading":"<strong>Types of Data Security</strong>","level":3,"id":"**Types_of_Data_Security**_0"},{"heading":"<strong>Constraints on Data Security</strong>","level":3,"id":"**Constraints_on_Data_Security**_0"},{"heading":"<strong><mark style=\"background: #D2B3FFA6;\">Confidential Data Categories</mark></strong>","level":3,"id":"**<mark_style=\"background_#D2B3FFA6;\">Confidential_Data_Categories</mark>**_0"},{"heading":"<strong>Regulated Data</strong>","level":3,"id":"**Regulated_Data**_0"},{"heading":"<strong>System Security Risks</strong>","level":3,"id":"**System_Security_Risks**_0"},{"heading":"<strong>Identifying Risks</strong>","level":4,"id":"**Identifying_Risks**_0"},{"heading":"<strong>Common Risks</strong>","level":4,"id":"**Common_Risks**_0"},{"heading":"<strong>Hacking and Social Engineering Threats</strong>","level":3,"id":"**Hacking_and_Social_Engineering_Threats**_0"},{"heading":"<strong>Hackers</strong>","level":4,"id":"**Hackers**_0"},{"heading":"<strong>Phishing and Social Engineering</strong>","level":4,"id":"**Phishing_and_Social_Engineering**_0"},{"heading":"<strong>Malware</strong>","level":4,"id":"**Malware**_0"},{"heading":"<strong>Sources of Malware</strong>","level":4,"id":"**Sources_of_Malware**_0"},{"heading":"<strong>Security Governance</strong>","level":3,"id":"**Security_Governance**_0"},{"heading":"<strong>Key Activities in Data Security</strong>","level":3,"id":"**Key_Activities_in_Data_Security**_0"},{"heading":"<strong>Activity 1: Identifying Data Security Needs</strong>","level":4,"id":"**Activity_1_Identifying_Data_Security_Needs**_0"},{"heading":"<strong>Activity 2: Developing Data Security Policies</strong>","level":3,"id":"**Activity_2_Developing_Data_Security_Policies**_0"},{"heading":"<strong>Levels of Policies</strong>","level":4,"id":"**Levels_of_Policies**_0"},{"heading":"<strong>Activity 3: Defining Data Security Procedures</strong>","level":4,"id":"**Activity_3_Defining_Data_Security_Procedures**_0"},{"heading":"<strong>Activity 4: Assessing Current Security Risks</strong>","level":4,"id":"**Activity_4_Assessing_Current_Security_Risks**_0"},{"heading":"<strong>Activity 5: Implementing Controls and Procedures</strong>","level":4,"id":"**Activity_5_Implementing_Controls_and_Procedures**_0"},{"heading":"<strong>Controls and Procedures</strong>","level":3,"id":"**Controls_and_Procedures**_0"},{"heading":"<strong><mark style=\"background: #FFB8EBA6;\">Risks of Insufficient Automated Monitoring</mark></strong>","level":3,"id":"**<mark_style=\"background_#FFB8EBA6;\">Risks_of_Insufficient_Automated_Monitoring</mark>**_0"},{"heading":"<strong>Advantages of Network-Based Audit Devices</strong>","level":3,"id":"**Advantages_of_Network-Based_Audit_Devices**_0"},{"heading":"<strong>Managing Regulatory Compliance</strong>","level":3,"id":"**Managing_Regulatory_Compliance**_0"},{"heading":"<strong>Auditing Inputs and Processes</strong>","level":3,"id":"**Auditing_Inputs_and_Processes**_0"},{"heading":"<strong>Tools for Information Security Management</strong>","level":3,"id":"**Tools_for_Information_Security_Management**_0"},{"heading":"<strong>Methods for Information Security</strong>","level":3,"id":"**Methods_for_Information_Security**_0"},{"heading":"<strong>Readiness and Risk Assessments</strong>","level":3,"id":"**Readiness_and_Risk_Assessments**_0"},{"heading":"<strong>Improving Compliance</strong>","level":4,"id":"**Improving_Compliance**_0"},{"heading":"<strong>Data Security in Outsourcing</strong>","level":3,"id":"**Data_Security_in_Outsourcing**_0"},{"heading":"<strong>Risk Management Mechanisms</strong>:","level":4,"id":"**Risk_Management_Mechanisms**_0"},{"heading":"<strong>Data Security Governance</strong>","level":3,"id":"**Data_Security_Governance**_0"},{"heading":"<strong>Key Aspects of <mark style=\"background: #BBFABBA6;\">Security Architecture</mark></strong>:","level":4,"id":"**Key_Aspects_of_<mark_style=\"background_#BBFABBA6;\">Security_Architecture</mark>**_0"},{"heading":"<strong>Integration Needs</strong>:","level":4,"id":"**Integration_Needs**_0"},{"heading":"<strong>Metrics</strong>","level":3,"id":"**Metrics**_0"},{"heading":"<strong>Common Security Implementation Metrics</strong>","level":4,"id":"**Common_Security_Implementation_Metrics**_0"},{"heading":"<strong>Security Awareness Metrics</strong>","level":4,"id":"**Security_Awareness_Metrics**_0"},{"heading":"<strong><mark style=\"background: #FFF3A3A6;\">Data Protection Metrics</mark></strong>","level":4,"id":"**<mark_style=\"background_#FFF3A3A6;\">Data_Protection_Metrics</mark>**_0"},{"heading":"<strong>Security Incident Metrics</strong>","level":4,"id":"**Security_Incident_Metrics**_0"},{"heading":"<strong>Confidential Data Diffusion Metrics</strong>","level":4,"id":"**Confidential_Data_Diffusion_Metrics**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/6.-data-security.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641433,"modifiedTime":1737554783000,"sourceSize":26433,"sourcePath":"Big Data/13. Certificate/CDGA/6. Data Security.md","exportPath":"big-data/13.-certificate/cdga/6.-data-security.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html":{"title":"7. Data Integration and Interoperability","icon":"","description":"\nData integration and interoperability describe the processes for moving and integrating data within and between different data stores, applications, and organizations. Data Integration: Consolidates data into a physical or virtual consistent format.\nData Interoperability: The ability for multiple systems to communicate with each other. Data integration and interoperability provide essential data management functions for organizations, including:\nData migration and transformation.\nConsolidation of data into data centers or data marts.\nIntegration of vendor software packages into organizational application frameworks.\nData sharing across different applications or organizations.\nDistribution of data across repositories and data centers.\nData archiving.\nData interface management.\nAcquisition and ingestion of external data.\nIntegration of structured and unstructured data.\nSupporting operational intelligence and decision-making. Data Governance: Governs transformation rules and message structures. Data Architecture: Designs integration solutions. Data Security: Ensures security of data, whether persistent, virtual, or in motion. Metadata: Tracks technical inventories, business definitions, transformation rules, data operations history, and lineage. Data Storage and Operations: Manages the physical implementation of solutions. Data Modeling and Design: Designs data structures, including persistent, virtual, and messaging formats. Managing the complexity and cost of data integration.\nMaintaining operational management costs.\nSupporting organizational compliance with data processing standards and regulations. Deliver data in the required format to consumers (both human and system) in a timely manner.\nConsolidate data physically or virtually into data centers.\nReduce the cost and complexity of managing solutions by developing shared models and interfaces.\nIdentify significant events (opportunities or threats), trigger alerts, and take appropriate actions.\nSupport business intelligence, data analytics, master data management, and operational efficiency improvements. Design for future scalability using iterative and incremental delivery.\nBalance local and enterprise data needs, including support and maintenance.\nEnsure reliability in data integration and interoperability designs and activities.\nInvolve business experts in designing and modifying transformation rules. Define data integration and lifecycle requirements.\nConduct data discovery.\nRecord data lineage.\nProfile data.\nCollect business rules. Design data integration solutions: Consider both enterprise and individual solution levels.\nLeverage existing solutions and components where possible. Model interfaces, messages, and data services: Persistent structures like data warehouses, data marts, and operational repositories. Map data sources to targets: Specify formats, transformations, and calculations needed. Design data orchestration: Define the end-to-end flow of data, including intermediate steps and frequency of transfers. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Data Integration and Interoperability (DII)</strong>","level":3,"id":"**Data_Integration_and_Interoperability_(DII)**_0"},{"heading":"<strong>Definition</strong>","level":4,"id":"**Definition**_0"},{"heading":"<strong>Core Functions</strong>","level":3,"id":"**Core_Functions**_0"},{"heading":"<strong>Dependencies on Other Data Management Areas</strong>","level":3,"id":"**Dependencies_on_Other_Data_Management_Areas**_0"},{"heading":"<strong>Business Drivers</strong>","level":3,"id":"**Business_Drivers**_0"},{"heading":"<strong>Objectives</strong>","level":3,"id":"**Objectives**_0"},{"heading":"<strong><mark style=\"background: #FFB8EBA6;\">Principles</mark></strong>","level":3,"id":"**<mark_style=\"background_#FFB8EBA6;\">Principles</mark>**_0"},{"heading":"<strong><mark style=\"background: #FFB86CA6;\">Activities</mark></strong>","level":3,"id":"**<mark_style=\"background_#FFB86CA6;\">Activities</mark>**_0"},{"heading":"<strong>Planning and Analysis</strong>","level":4,"id":"**Planning_and_Analysis**_0"},{"heading":"<strong>Design</strong>","level":4,"id":"**Design**_0"},{"heading":"<strong>Development</strong>","level":4,"id":"**Development**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641435,"modifiedTime":1737554783000,"sourceSize":9772,"sourcePath":"Big Data/13. Certificate/CDGA/7. Data Integration and Interoperability.md","exportPath":"big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/8.-document-and-content-management.html":{"title":"8. Document and Content Management","icon":"","description":"File and content management involves the processes for collecting, storing, accessing, and using data and information stored outside relational databases. It focuses on maintaining integrity, ensuring accessibility, and securing high-quality content through reliable architecture and well-managed metadata.\nRegulatory compliance requirements.\nLitigation response capabilities.\nElectronic discovery requests.\nBusiness continuity requirements.\nImproving organizational efficiency. Ensure high-speed and efficient collection and use of unstructured data and information.\nEnable integration between structured and unstructured data.\nComply with legal obligations and meet customer expectations. Everyone in the organization should play a role in safeguarding its future by following established procedures for creating, using, retrieving, and disposing of records.\nRecords and content processing experts must actively participate in policy and planning development. Accountability.\nIntegrity.\nProtection.\nCompliance.\nAvailability.\nRetention.\nDisposition.\nTransparency. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Overview","level":3,"id":"Overview_0"},{"heading":"Business Drivers","level":3,"id":"Business_Drivers_0"},{"heading":"<mark style=\"background: #BBFABBA6;\">Goals</mark>","level":3,"id":"<mark_style=\"background_#BBFABBA6;\">Goals</mark>_0"},{"heading":"<mark style=\"background: #ABF7F7A6;\">Principles</mark>","level":3,"id":"<mark_style=\"background_#ABF7F7A6;\">Principles</mark>_0"},{"heading":"ARMA’s GARP Principles (2009)","level":3,"id":"ARMA’s_GARP_Principles_(2009)_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/8.-document-and-content-management.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641437,"modifiedTime":1737554783000,"sourceSize":7798,"sourcePath":"Big Data/13. Certificate/CDGA/8. Document and Content Management.md","exportPath":"big-data/13.-certificate/cdga/8.-document-and-content-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/9.-reference-and-master-data.html":{"title":"9. Reference and Master Data","icon":"","description":"\nMeeting organizational data needs: Ensuring complete, up-to-date, and consistent shared datasets.\nManaging data quality: Controlling the quality of critical entity data.\nReducing data integration costs.\nMinimizing risks.\n(Key points: Meeting needs, quality control, cost management, risk reduction) Lowering the risk and cost of data integration by using consistent reference data for multiple projects.\nImproving the quality of reference data.\n(Key points: Cost reduction, quality improvement) Ensure complete, consistent, up-to-date, and authoritative reference and master data across organizational processes.\nEnable sharing of reference and master data across business units and applications.\nReduce the cost and complexity of data usage and integration by adopting standardized models and integration practices.\n(Key points: Ensure consistency, promote sharing, control costs) Data Sharing: Managed data can be shared effectively.\nOwnership: Ownership spans the entire organization and requires comprehensive management.\nQuality: Continuous monitoring and governance of data quality are necessary.\nManagement Responsibility: Business data stewards are accountable for quality.\nChange Control: Master data values should represent the best understanding at any given time.\nChanges to reference data must follow explicit processes and require prior approval. Authorization: Master data should be copied only from a system of record.\nReference data management systems must enable cross-organizational sharing. ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong><mark style=\"background: #FFB8EBA6;\">Business Drivers</mark> for Master Data Management</strong>","level":3,"id":"**<mark_style=\"background_#FFB8EBA6;\">Business_Drivers</mark>_for_Master_Data_Management**_0"},{"heading":"<strong><mark style=\"background: #FFB86CA6;\">Drivers for Reference Data Management</mark></strong>","level":3,"id":"**<mark_style=\"background_#FFB86CA6;\">Drivers_for_Reference_Data_Management</mark>**_0"},{"heading":"<strong>Goals</strong>","level":3,"id":"**Goals**_0"},{"heading":"<strong>Guiding Principles</strong>","level":3,"id":"**Guiding_Principles**_0"},{"heading":"Key Concepts on Reference and Master Data Management","level":3,"id":"Key_Concepts_on_Reference_and_Master_Data_Management_0"},{"heading":"<strong>Malcolm Chisholm's Six-Level Data Classification</strong>","level":4,"id":"**Malcolm_Chisholm's_Six-Level_Data_Classification**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/9.-reference-and-master-data.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641438,"modifiedTime":1737554783000,"sourceSize":16305,"sourcePath":"Big Data/13. Certificate/CDGA/9. Reference and Master Data.md","exportPath":"big-data/13.-certificate/cdga/9.-reference-and-master-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html":{"title":"10. Data Warehousing and Business Intelligence","icon":"","description":"\nData Warehouse (DW):\nOriginating in the 1980s and evolving through the 1990s, data warehouses developed in tandem with Business Intelligence (BI) as the primary drivers of business decision-making. They enable organizations to integrate data from various sources into a unified data model. The consolidated data provides insights into business operations, supports enterprise decision-making, and unlocks new possibilities for creating organizational value. Data warehouses reduce redundancy, enhance information consistency, and empower enterprises to make better data-driven decisions. As a result, the data warehouse has become the cornerstone of enterprise data management. Business Drivers: Operational Support Functions\nCompliance Requirements (e.g., responding to historical data inquiries)\nBusiness Intelligence Activities (Primary driver: providing insights, improving efficiency, and enhancing competitive advantage). Support business intelligence activities.\nEmpower business analysis and efficient decision-making.\nDrive innovation through data insights. Focus on business goals: Address the highest-priority business needs and resolve them.\nStart with the end in mind: Prioritize business needs and drive warehouse creation by desired outcomes.\nThink globally, act locally: Adopt a holistic design while implementing incrementally.\nLearn and continuously optimize: Improvements should be iterative, not preemptively exhaustive.\nEnhance transparency and self-service: Ensure ease of access for stakeholders.\nBuild metadata alongside the data warehouse: Success depends on accurately interpreting the data.\nCollaborate: Coordinate with other data activities, especially data governance, data quality, and metadata management.\nAvoid one-size-fits-all solutions: Provide appropriate tools and products for different types of data consumers. Understand requirements.\nDefine and maintain DW (Data Warehouse) and BI (Business Intelligence) architectures.\nDevelop data warehouses and data marts.\nLoad data into the warehouse.\nImplement BI product portfolios.\nMaintain data products. Definition 1: BI refers to data analysis activities that understand organizational needs and identify opportunities. The insights generated aim to enhance decision-making success.\nDefinition 2: BI is also a set of technologies that support these data analysis activities. An integrated decision-support database, along with associated software programs for collecting, cleansing, transforming, and storing data from various operational and external sources. A data mart is a subset of data warehouse data. Broadly speaking, a data warehouse encompasses all data stores or operations that support BI goals. Processes include extracting, cleansing, transforming, controlling, and loading data into the warehouse. The emphasis is on creating an integrated, historical business environment from operational data through enforced business rules and appropriate data relationships. This also involves interacting with metadata repositories. Traditionally focused on structured data, data warehousing now includes semi-structured and unstructured data. Bill Inmon and Ralph Kimball are two thought leaders who used normalized modeling and dimensional modeling, respectively, for data warehouse modeling. (P293)\nBill Inmon: Defined a data warehouse in \"Building the Data Warehouse\" as “a subject-oriented, integrated, time-variant, and non-volatile collection of data to support management and decision-making in an enterprise.”\nRalph Kimball: Advocated for a bottom-up approach in \"The Data Warehouse Toolkit\", emphasizing the construction of data marts. He defined data marts as “copies of transaction data tailored for queries and analysis.” Data originates from other systems.\nData integration enhances value.\nData is made accessible and analyzable.\nReliable and integrated data is made available to authorized stakeholders.\nConstruction purposes include workflow support, operational management, and predictive analytics.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Warehouse Overview","level":3,"id":"Data_Warehouse_Overview_0"},{"heading":"<strong>Goals of Data Warehouse Construction</strong>:","level":3,"id":"**Goals_of_Data_Warehouse_Construction**_0"},{"heading":"<strong>Principles for Data Warehouse Construction</strong>:","level":3,"id":"**Principles_for_Data_Warehouse_Construction**_0"},{"heading":"Activities","level":3,"id":"Activities_0"},{"heading":"Business Intelligence (BI):","level":4,"id":"Business_Intelligence_(BI)_0"},{"heading":"Data Warehouse:","level":4,"id":"Data_Warehouse_0"},{"heading":"Data Warehouse Construction:","level":4,"id":"Data_Warehouse_Construction_0"},{"heading":"Data Warehouse Construction Approaches:","level":4,"id":"Data_Warehouse_Construction_Approaches_0"},{"heading":"Common Core Concepts:","level":4,"id":"Common_Core_Concepts_0"},{"heading":"<strong><mark style=\"background: #ADCCFFA6;\">Corporate Information Factory</mark> (CIF):</strong>","level":3,"id":"**<mark_style=\"background_#ADCCFFA6;\">Corporate_Information_Factory</mark>_(CIF)**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641399,"modifiedTime":1737554783000,"sourceSize":19076,"sourcePath":"Big Data/13. Certificate/CDGA/10. Data Warehousing and Business Intelligence.md","exportPath":"big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/11.-metadata-management.html":{"title":"11. Metadata Management","icon":"","description":"Metadata is data that defines and describes other data. This implies that metadata is data itself; when data is used in this specific way, it becomes metadata. Only in a specific context, for a specific purpose, or from a specific perspective does data become metadata.The collection of environments, purposes, or perspectives where data is used as metadata is referred to as the \"context.\" In a given context, metadata is \"data about data.\" Since metadata is also data, it can be stored in databases and organized using models. Some models are application-specific, while others are more generic.Models describing metadata are often called metamodels. In this sense, the conceptual model introduced in GB/T 18391.3 is a metamodel.Metadata includes technical and business processes, data rules and constraints, logical data structures, and physical data structures. It describes:\nThe data itself (e.g., databases, data elements, data models).\nThe concepts represented by data (e.g., business processes, application systems, software code, technical infrastructure).\nThe relationships between data and concepts.\nWithout reliable metadata, organizations would not know:\nWhat data they possess.\nWhat the data represents.\nWhere the data originates.\nHow it flows through systems.\nWho has access to it. Define a Metadata Strategy.\nUnderstand Metadata Requirements: (1) Business needs.\n(2) Technical needs. Define Metadata Architecture: (1) Create metamodels.\n(2) Apply metadata standards.\n(3) Manage metadata storage. Create and Maintain Metadata: (1) Integrate metadata.\n(2) Distribute and deliver metadata. Query, Report, and Analyze Metadata. Improved data credibility through context and quality checks.\nIncreased value of strategic information (e.g., master data) by expanding its use.\nEnhanced operational efficiency by identifying redundant data and processes.\nPrevention of outdated or incorrect data usage.\nReduced time spent on data research.\nImproved communication between data users and IT professionals.\nAccurate impact analysis, reducing project failure risk.\nShortened time-to-market by reducing system development lifecycle time.\nLower training costs and reduced impact of employee turnover through comprehensive documentation of data context, history, and origin.\nRegulatory compliance. Redundant data and data management processes.\nDuplicative and redundant dictionaries, repositories, and other metadata stores.\nInconsistent data element definitions and associated misuse risks.\nContradictory and conflicting metadata versions, undermining user confidence.\nDoubts about metadata and data reliability. Record and manage a knowledge system of business terms related to data to ensure consistency in understanding and using data content.\nCollect and integrate metadata from various sources to understand the similarities and differences in data across departments.\nEnsure the quality, consistency, timeliness, and security of metadata.\nProvide standard pathways for metadata users to access metadata.\nPromote or enforce the use of technical metadata standards to facilitate data exchange. Organizational Commitment: Manage data as a corporate asset.\nStrategy: Align metadata initiatives with strategic demands and business priorities.\nEnterprise Perspective: Ensure scalability for future growth.\nSubtle Influence: Advocate for the necessity of metadata and its various uses; emphasizing its value encourages business adoption and provides knowledge assistance.\nAccess: Ensure employees know how to access and use metadata.\nQuality: Process owners are responsible for metadata quality.\nAudit: Develop, implement, and review metadata standards to simplify integration and usage.\nImprovement: Create feedback mechanisms for users to report outdated or incorrect metadata to the metadata management team. From a practical standpoint, one person’s metadata could be another person’s data.\nRather than focusing on theoretical distinctions, prioritize the functions metadata serves, such as: Creating new data\nUnderstanding existing data\nEnabling system interoperability\nAccessing and sharing data Emphasize the source data that meets these needs. (Example: The NSA’s definition of metadata in phone surveillance highlights differences between metadata and general understanding.) Metadata is categorized into three main types:\nBusiness Metadata\nTechnical Metadata\nOperational Metadata\nIn library and information sciences, metadata can further be divided into:\nDescriptive Metadata\nStructural Metadata\nAdministrative Metadata Focuses on the content and conditions of data, including detailed information related to data governance. Examples include:\nDefinitions and descriptions of datasets, tables, and fields.\nBusiness rules, transformation rules, calculations, and derivations.\nData models.\nData quality rules and validation results.\nData update schedules.\nData lineage and traceability.\nData standards.\nSpecific systems for recording data elements.\nValid value constraints.\nContact information for stakeholders (e.g., data owners, data stewards).\nSecurity and privacy levels of data.\nKnown data issues.\nData usage instructions.\nProvides technical details about data, the systems storing it, and the processes of data flow within and between systems. Examples include:\nPhysical database table names and field names.\nField attributes.\nProperties of database objects.\nAccess permissions.\nCRUD (Create, Read, Update, Delete) rules for data.\nPhysical data models, including table names, keys, and indexes.\nRelationships between data models and physical assets.\nDetails of ETL (Extract, Transform, Load) jobs.\nFile format schema definitions.\nSource-to-target mapping documentation.\nData lineage documentation, including information about upstream and downstream change impacts.\nNames and descriptions of programs and applications.\nSchedules and dependencies for periodic jobs (content updates).\nRecovery and backup rules.\nPermissions, groups, and roles for data access.\nOperational metadata describes the details of processing and accessing data. Examples include:\nExecution logs of batch processing programs.\nExtraction history and results.\nScheduling exception handling.\nAudit, balancing, and control metrics results.\nError logs.\nAccess patterns, frequency, and execution time of reports and queries.\nMaintenance plans and execution details for patches and versions, including the current patch level.\nBackup details, retention rules, creation dates, and disaster recovery plans.\nService Level Agreement (SLA) requirements and provisions.\nCapacity and usage patterns.\nData archiving rules, retention policies, and associated archived files.\nData cleansing standards.\nData sharing rules and agreements.\nRoles, responsibilities, and contact information for technical personnel.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Definition in GB/T 18391","level":4,"id":"Definition_in_GB/T_18391_0"},{"heading":"Context","level":4,"id":"Context_0"},{"heading":"Metamodels","level":4,"id":"Metamodels_0"},{"heading":"Components of Metadata","level":4,"id":"Components_of_Metadata_0"},{"heading":"Importance of Metadata","level":4,"id":"Importance_of_Metadata_0"},{"heading":"Activities","level":3,"id":"Activities_0"},{"heading":"Business Drivers","level":3,"id":"Business_Drivers_0"},{"heading":"Risks of Poor Metadata Management:","level":4,"id":"Risks_of_Poor_Metadata_Management_0"},{"heading":"Goals","level":3,"id":"Goals_0"},{"heading":"Principles","level":3,"id":"Principles_0"},{"heading":"Boundary Between Metadata and Non-Metadata","level":3,"id":"Boundary_Between_Metadata_and_Non-Metadata_0"},{"heading":"Types of Metadata","level":3,"id":"Types_of_Metadata_0"},{"heading":"Business Metadata","level":3,"id":"Business_Metadata_0"},{"heading":"Technical Metadata","level":3,"id":"Technical_Metadata_0"},{"heading":"Operational Metadata","level":3,"id":"Operational_Metadata_0"},{"heading":"Metadata Types Overview","level":3,"id":"Metadata_Types_Overview_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/11.-metadata-management.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641400,"modifiedTime":1737554783000,"sourceSize":27717,"sourcePath":"Big Data/13. Certificate/CDGA/11. Metadata Management.md","exportPath":"big-data/13.-certificate/cdga/11.-metadata-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/12.-data-quality.html":{"title":"12. Data Quality","icon":"","description":"\nLack of organizational understanding of the impact of low-quality data.\nInadequate planning.\nSiloed system design.\nInconsistent development processes.\nIncomplete documentation.\nLack of standards or governance. No organization has perfect business processes, technical processes, or data management practices.\nAll organizations face data quality issues.\nOrganizations with formal data quality management experience fewer problems than those without. Data quality management is not a one-time project; it is an ongoing effort.\nLong-term success depends on cultural changes and the establishment of a quality mindset.\nHigh-quality data is not an end goal but a means to achieve organizational success. Enhance the value and utility of organizational data.\nReduce risks and costs associated with low-quality data.\nImprove organizational efficiency and productivity.\nProtect and enhance organizational reputation.\n[Create opportunities, reduce costs, boost efficiency, strengthen reputation] Inability to issue invoices correctly.\nIncreased call center volume and reduced problem-solving capacity.\nRevenue loss due to missed business opportunities.\nImpeded integration progress after mergers and acquisitions.\nIncreased fraud risk.\nLosses from incorrect business decisions driven by erroneous data.\nBusiness losses due to a lack of good reputation. Develop a managed approach to make data fit for purpose based on consumer needs.\nDefine standards and norms for data quality control as part of the entire data lifecycle.\nDefine and implement processes for measuring, monitoring, and reporting data quality levels. Significance: Prioritize improvements based on data importance and risks associated with inaccuracies.\nLifecycle Management: Address data quality across the entire lifecycle.\nPrevention: Focus on preventing data errors and reducing data inaccessibility.\nRoot Cause Correction: Modify processes and supporting systems to address underlying issues rather than superficial symptoms.\nGovernance: Data governance activities must support high-quality data, and data quality planning must sustain a governed data environment.\nStandards-Driven: Rely on established standards.\nObjective Measurement and Transparency: Ensure consistent and objective measurement of data quality levels.\nEmbedding in Business Processes: Business process owners are responsible for the quality of data generated through their processes and must implement data quality standards in their workflows.\nSystem Enforcement: System owners must ensure systems enforce data quality requirements.\nService Level Alignment: Data quality reporting and issue management should be integrated into Service Level Agreements (SLAs). Define High-Quality Data\nEstablish a Data Quality Strategy\nIdentify Key Data and Business Rules (1) Identify key data.\n(2) Identify existing rules and patterns. Perform an Initial Data Quality Assessment (1) Identify issues and prioritize them.\n(2) Conduct root cause analysis of issues. Determine Improvement Directions and Prioritize (1) Prioritize actions based on business impact.\n(2) Develop preventive and corrective measures.\n(3) Confirm planned actions. Define Data Quality Improvement Goals\nDevelop and Deploy Data Quality Operations (1) Develop data quality operational procedures.\n(2) Fix data quality defects.\n(3) Measure and monitor data quality.\n(4) Report data quality levels and findings. Data quality refers to the characteristics of high-quality data and the processes used to measure or improve it. It depends on the context in which the data is used and the needs of the data consumers. Expectations related to quality are not always explicit. Customers may not fully understand their quality expectations, and data managers may not proactively inquire about them.Data quality management should concentrate on the organization's and customers' most critical data. The priority can be determined by considering:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Factors Leading to Poor Data Quality","level":3,"id":"Factors_Leading_to_Poor_Data_Quality_0"},{"heading":"Universality of Data Quality Issues","level":3,"id":"Universality_of_Data_Quality_Issues_0"},{"heading":"Cross-Functional Commitment and Coordination","level":3,"id":"Cross-Functional_Commitment_and_Coordination_0"},{"heading":"Business Drivers for Data Quality Management","level":3,"id":"Business_Drivers_for_Data_Quality_Management_0"},{"heading":"Consequences of Poor Data Quality","level":3,"id":"Consequences_of_Poor_Data_Quality_0"},{"heading":"Goals of Data Quality Management","level":3,"id":"Goals_of_Data_Quality_Management_0"},{"heading":"Principles of Data Quality Management","level":3,"id":"Principles_of_Data_Quality_Management_0"},{"heading":"Activities in Data Quality Management","level":3,"id":"Activities_in_Data_Quality_Management_0"},{"heading":"Definition of Data Quality","level":3,"id":"Definition_of_Data_Quality_0"},{"heading":"Focus of Data Quality Management","level":3,"id":"Focus_of_Data_Quality_Management_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/12.-data-quality.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641401,"modifiedTime":1737554783000,"sourceSize":33450,"sourcePath":"Big Data/13. Certificate/CDGA/12. Data Quality.md","exportPath":"big-data/13.-certificate/cdga/12.-data-quality.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/13.-big-data-and-data-science.html":{"title":"13. Big Data and Data Science","icon":"","description":"Organizations should carefully manage metadata associated with big data sources to accurately and efficiently handle data files, their origins, and their value.\nVolume: The large scale of data.\nVelocity: The rapid speed of data updates.\nVariety: The diversity and variability in data types.\nViscosity: The resistance to flow or integration in data processes.\nVolatility: The high variability and short-lived nature of data.\nVeracity: The low accuracy or reliability of data.\nA data scientist is a professional who explores data, develops predictive models, machine learning models, prescriptive models, and analytical methods, and deploys these solutions for stakeholder analysis.\nMost data warehouses rely on ETL (Extract, Transform, Load).\nBig data solutions, such as data lakes, typically rely on ELT (Extract, Load, Transform).\nThe primary business driver for improving an organization's big data and data science capabilities is the expectation of seizing opportunities identified from insights within datasets generated by various processes.\nRich data sources.\nOrganized information and analytical processes.\nDelivery of insights.\nVisualization of findings and data insights. Define a big data strategy and business needs.\nSelect data sources.\nAcquire and ingest data.\nFormulate data hypotheses and methodologies.\nIntegrate and adjust data for analysis.\nUse models to explore data.\nDeploy and monitor the results.\nData Lake:\nAn environment designed to extract, store, evaluate, and analyze massive amounts of diverse and structured/unstructured data. It supports various use cases, providing:\nAn environment for data scientists to mine and analyze data.\nA centralized storage area for raw data, requiring minimal transformation (if any).\nA backup storage area for detailed historical data from data warehouses.\nAn online archive for information records.\nAn environment to extract streaming data through automated model recognition. Risk: Data lakes can quickly turn into \"data swamps\" if metadata is not managed during ingestion.\nServices-Based Architecture (SBA): A method for instant data access while ensuring accurate historical data updates.\nComponents: Batch Processing Layer: Provides services for both recent and historical data using the data lake.\nAcceleration Layer: Handles real-time data only.\nService Layer: Interfaces connecting batch and acceleration layer data. Definition: Combines unsupervised learning and supervised learning, with reinforcement learning as an emerging third branch.\nTypes: Supervised Learning: Based on numerical theories.\nUnsupervised Learning: Focused on discovering hidden patterns (e.g., data mining).\nReinforcement Learning: Goal-oriented optimization (e.g., winning a chess match). Purpose: Enables machines to learn and adapt quickly from queries in ever-changing datasets. Definition: An automated method for extracting insights from unstructured or semi-structured data to perceive people's opinions on brands, products, services, or topics. Branch: A subset of machine learning under unsupervised learning.\nTechniques: Profiling: Describes typical behaviors for anomaly detection.\nData Reduction: Replaces large datasets with smaller ones.\nAssociation: Finds relationships among transaction elements.\nClustering: Groups data based on shared features.\nSelf-Organizing Maps: Reduces dimensionality of evaluation spaces. Definition: A supervised learning subfield where users model data elements and evaluate probabilities to predict future outcomes.\nFocus: Probability-based modeling of potential events (e.g., purchases, price changes) using historical data. Definition: Goes beyond predicting outcomes by defining actions that influence results.\nPurpose: Predicts what, when, and why something might happen and suggests measures to address it.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Principles for Managing Metadata in Big Data","level":3,"id":"Principles_for_Managing_Metadata_in_Big_Data_0"},{"heading":"Characteristics of Big Data (The 6 Vs)","level":3,"id":"Characteristics_of_Big_Data_(The_6_Vs)_0"},{"heading":"Data Scientist","level":3,"id":"Data_Scientist_0"},{"heading":"ETL vs. ELT in Data Management","level":3,"id":"ETL_vs._ELT_in_Data_Management_0"},{"heading":"Business Drivers for Big Data","level":3,"id":"Business_Drivers_for_Big_Data_0"},{"heading":"<mark style=\"background: #FFB8EBA6;\">Dependencies of Data Science</mark>","level":3,"id":"<mark_style=\"background_#FFB8EBA6;\">Dependencies_of_Data_Science</mark>_0"},{"heading":"Stages in the Data Science Process","level":3,"id":"Stages_in_the_Data_Science_Process_0"},{"heading":"DW/BI and Big Data Conceptual Architecture","level":3,"id":"DW/BI_and_Big_Data_Conceptual_Architecture_0"},{"heading":"Key Concepts and Activities in Big Data and Analytics","level":3,"id":"Key_Concepts_and_Activities_in_Big_Data_and_Analytics_0"},{"heading":"<strong>Data Lakes</strong>","level":4,"id":"**Data_Lakes**_0"},{"heading":"<strong>Machine Learning</strong>","level":4,"id":"**Machine_Learning**_0"},{"heading":"<strong>Semantic Analysis</strong>","level":4,"id":"**Semantic_Analysis**_0"},{"heading":"<strong>Data Mining</strong>","level":4,"id":"**Data_Mining**_0"},{"heading":"<strong>Predictive Analytics</strong>","level":4,"id":"**Predictive_Analytics**_0"},{"heading":"<strong>Prescriptive Analytics</strong>","level":4,"id":"**Prescriptive_Analytics**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/13.-big-data-and-data-science.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641426,"modifiedTime":1737554783000,"sourceSize":12625,"sourcePath":"Big Data/13. Certificate/CDGA/13. Big Data and Data Science.md","exportPath":"big-data/13.-certificate/cdga/13.-big-data-and-data-science.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html":{"title":"14. Data Management Maturity Assessment","icon":"","description":"Definition:\nCMA is a capability enhancement framework based on the Capability Maturity Model (CMM). It describes the evolution of data management capabilities from an initial state to an optimized process.\nOrigin: The concept originated from the U.S. Department of Defense as a standard for evaluating software contractors.\nDevelopment: In the mid-1980s, Carnegie Mellon University's Software Engineering Institute released the Software Capability Maturity Model.\nThe maturity model defines maturity levels by describing characteristics of each stage. The levels follow a fixed progression and cannot be skipped.Typical Levels:\nLevel 0: Incapable Level\nLevel 1: Initial or Ad-Hoc Level Success depends on individual capabilities. Level 2: Repeatable Level Basic process rules are established. Level 3: Defined Level Standards are established and used. Level 4: Managed Level Capabilities are quantifiable and controlled. Level 5: Optimized Level Capability improvement goals are measurable. Organizations can develop a roadmap to achieve the following goals:\nIdentify high-value improvement opportunities related to processes, methods, resources, and automation.\nEnsure capabilities align with business strategies.\nGovern projects based on regular capability assessments.\nNote: Higher maturity levels are not always better—capabilities should align with business strategies and fit organizational needs.Purpose:\nDMMA can be used to comprehensively evaluate data management or focus on a specific knowledge area or process. It bridges the gap between business and IT departments regarding the health and effectiveness of data management practices.\nRegulation: Regulatory requirements set minimum maturity levels for data management.\nData Governance: Maturity assessments support planning and compliance in data governance.\nOrganizational Readiness for Process Improvement: Organizations need to evaluate their current state before improving practices.\nOrganizational Changes: Changes such as mergers introduce data management challenges.\nNew Technology: Assessments help determine the potential for success when adopting new technologies.\nData Management Issues: Identifying and resolving specific data management challenges.\nThe primary objective of a Data Management Capability Assessment is to evaluate the current state of key data management activities to develop improvement plans. This assessment helps organizations position themselves on the maturity scale by analyzing strengths and weaknesses, enabling them to recognize, prioritize, and implement improvement opportunities.Cultural Impact:\nIntroduces stakeholders to data management concepts, principles, and practices.\nClarifies stakeholder roles and responsibilities in organizing data.\nEmphasizes the necessity of managing data as a critical asset.\nExpands awareness of data management activities across the organization.\nFacilitates collaboration required for effective data governance. Plan the Assessment: Define the scope and method.\nPlan communication. Conduct the Maturity Assessment: Collect information.\nPerform the assessment.\nInterpret the results. Analyze Results and Provide Recommendations. Develop Targeted Improvement Plans. Reassess Maturity Periodically. Level 0: Incapable Unorganized state, defined as a baseline for further improvement. Level 1: Initial/Ad-Hoc Data management relies on limited tools.\nFew or no governance activities exist.\nProcesses depend heavily on a few experts, with roles defined separately across departments.\nData quality issues are widespread.\nInfrastructure is business-unit specific.\nEvaluation Standard: Control of individual processes, such as recording data quality issues. Level 2: Repeatable Consistent tools and roles support process execution.\nCentralized tools and monitoring mechanisms are used.\nDefined roles and processes are less dependent on specific experts.\nAwareness of master and reference data concepts begins.\nEvaluation Standard: Formal role definitions in the organization. Level 3: Defined Data management capabilities emerge.\nProcesses are institutionalized and seen as organizational enablers.\nData replication is controlled, improving overall data quality.\nPolicies and management practices are aligned.\nProcesses involve less manual intervention, leading to predictable outcomes.\nEvaluation Standard: Established data management policies, scalable processes, and consistent data models and controls. Level 4: Managed Predictable results for new projects based on experience from previous levels.\nRisk management and performance indicators are introduced.\nStandardized tools and centralized planning support governance.\nEvaluation Standard: Indicators tied to project success, system performance, and data quality. Level 5: Optimized Processes are automated with managed technology changes.\nContinuous improvement is emphasized.\nTools support cross-process data views and prevent unnecessary duplication.\nMetrics are easy to interpret for managing data quality and processes.\nEvaluation Standard: Change management components and process improvement initiatives. Evaluation Frameworks:\nUse scales for each level. Frameworks like DAMA-DMBOK focus on activities, tools, standards, personnel, and resources. CMMI Data Management Maturity Model (DMM):\nEvaluates six areas: Data management strategies\nData governance\nData quality\nPlatforms and architecture\nData operations\nSupporting processes DCAM (Data Management Capability Assessment Model): Developed by the EDM Council.\nFocuses on 37 capabilities and 115 sub-capabilities related to sustainable data management programs.\nEmphasizes stakeholder engagement and process formalization. IBM Data Governance Maturity Model: Helps organizations establish governance consistency and quality control.\nIncludes four categories: Outcomes: Risk management, compliance, and value creation.\nEnablers: Organizational structures, policies, and management.\nCore Elements: Data quality management, lifecycle management, security, and privacy.\nSupporting Elements: Data architecture, metadata, auditing, and reporting. Stanford Data Governance Maturity Model: Focuses on governance rather than management but lays the foundation for data management assessment.\nDifferentiates between foundational elements (e.g., awareness, formalization, metadata) and project-level elements (e.g., data quality, master data).\nDefines maturity levels with qualitative and quantitative measures. Gartner Enterprise Information Management Maturity Model: Evaluates vision, strategy, metrics, governance, roles, responsibilities, lifecycle, and infrastructure. Define Objectives: Identify drivers and align goals with strategic direction. Select a Framework: Choose the right framework for the organization. Define the Scope: Enterprise-wide implementation may not be practical for the first assessment.\nOptions: Conduct a localized assessment.\nConduct an enterprise assessment, composed of multiple localized assessments or independent tasks. Define Interaction Methods: Use workshops, interviews, surveys, and component reviews. Plan Communication: Inform stakeholders before the assessment starts about its expectations.\nKey Information: Purpose of the maturity assessment.\nHow the assessment will be conducted.\nThe stakeholder's role in the assessment.\nThe timeline for assessment activities. Gather inputs for the assessment based on the interaction model.\nThe collected information should include: Formal ratings for evaluation criteria.\nOutcomes of interviews and focus groups.\nSystem analysis and design documents.\nData surveys.\nEmail chains, procedure manuals, standards, and policies.\nDocument repositories, approval workflows, work products, metadata repositories, data and integration reference architectures, templates, and forms. The evaluation process involves multiple stages, including reaching a consensus on ratings and defining improvements.\nSteps: Review the rating method.\nDocument supporting evidence.\nDiscuss findings with participants and agree on final scores for each area. Use weighted scores based on the importance of criteria where appropriate.\nRecord statements about model criteria and reviewers' interpretations as supporting explanations for ratings.\nDevelop visualization tools to illustrate and explain assessment results. The assessment report should include:\nBusiness drivers for the evaluation.\nOverall results of the assessment.\nGap ratings categorized by topic.\nRecommended methods to address gaps.\nObserved organizational strengths.\nRisks to progress.\nInvestment and outcome options.\nGovernance and metrics for tracking progress.\nResource analysis and potential future utility.\nComponents that can be reused within the organization. Develop a briefing for management to provide decision support, including objectives, plans, and timelines. Create a roadmap or reference plan that includes: Improvements for specific data management functions.\nTimelines for implementing improvement activities.\nExpected improvement in DMMA ratings after implementation.\nMonitoring activities that reflect gradual maturity improvements over time. Establish a baseline rating through the initial assessment.\nDefine parameters for reassessment, including organizational scope.\nRepeat the DMM assessment on a published timeline as needed.\nTrack trends relative to the initial baseline.\nProvide recommendations based on reassessment results. Data Management Maturity Framework (DMM).\nCommunication Plan.\nCollaboration Tools: Enable sharing of assessment results.\nKnowledge Management and Metadata Repositories. Criteria for selecting a DMM framework: Ease of Use.\nComprehensiveness: Includes business involvement, not just IT processes.\nScalability and Flexibility.\nBuilt-in Future Evolution Path.\nNeutrality: Supports both industry-agnostic and industry-specific best practices.\nAbstraction or Detail: Balances generality and specificity.\nNon-Prescriptive: Focuses on executable, not mandatory, actions.\nScenario-Based Organization: Contextual, independent, and identifies dependencies.\nRepeatable.\nIndependent Support: Backed by neutral organizations.\nTechnology Neutrality: Emphasizes practices over tools.\nTraining Support. Use: Supports DMMA preparation or standard establishment.\nAdvantages: Widely used by experts across industries.\nSupported by a knowledgeable community.\nProvides a checklist approach to identify areas requiring deeper analysis or improvement. Common Risks and Mitigation Strategies:\nDMMA Ratings: Track improvements in maturity levels.\nResource Utilization: Measure the effectiveness of resources.\nRisk Exposure: Evaluate the organization’s ability to respond to risk scenarios.\nExpenditure Management: Sustainability of data management efforts.\nAchievement of proactive goals and objectives.\nCommunication effectiveness.\nTraining effectiveness.\nSpeed of change adoption.\nContribution of data management to business goals.\nRisk reduction and operational efficiency. DMMA Inputs: Assess the quality and relevance of input data.\nPace of Change: Track the organization's speed in improving its capabilities.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"<strong>Capability Maturity Assessment (CMA)</strong>","level":3,"id":"**Capability_Maturity_Assessment_(CMA)**_0"},{"heading":"<strong>Maturity Model Levels</strong>","level":3,"id":"**Maturity_Model_Levels**_0"},{"heading":"<strong>Goals and Roadmap</strong>","level":3,"id":"**Goals_and_Roadmap**_0"},{"heading":"<strong>Data Management Maturity Assessment (DMMA)</strong>","level":3,"id":"**Data_Management_Maturity_Assessment_(DMMA)**_0"},{"heading":"<strong>Business Drivers for Maturity Assessment</strong>","level":3,"id":"**Business_Drivers_for_Maturity_Assessment**_0"},{"heading":"<strong>Goals of Data Management Capability Assessment</strong>","level":3,"id":"**Goals_of_Data_Management_Capability_Assessment**_0"},{"heading":"<strong>Activities in Capability Assessment</strong>","level":3,"id":"**Activities_in_Capability_Assessment**_0"},{"heading":"<strong>Maturity Levels and Characteristics</strong>","level":3,"id":"**Maturity_Levels_and_Characteristics**_0"},{"heading":"<strong>Key Maturity Models</strong>","level":3,"id":"**Key_Maturity_Models**_0"},{"heading":"<strong>Activity 1: Plan the Assessment</strong>","level":3,"id":"**Activity_1_Plan_the_Assessment**_0"},{"heading":"<strong>Activity 2: Conduct Maturity Assessment</strong>","level":3,"id":"**Activity_2_Conduct_Maturity_Assessment**_0"},{"heading":"<strong>Step 2-1: Collect Information</strong>","level":4,"id":"**Step_2-1_Collect_Information**_0"},{"heading":"<strong>Step 2-2: Execute the Assessment</strong>","level":4,"id":"**Step_2-2_Execute_the_Assessment**_0"},{"heading":"<strong>Activity 3: Interpret Results and Provide Recommendations</strong>","level":3,"id":"**Activity_3_Interpret_Results_and_Provide_Recommendations**_0"},{"heading":"<strong>Step 3-1: Report Assessment Results</strong>","level":4,"id":"**Step_3-1_Report_Assessment_Results**_0"},{"heading":"<strong>Step 3-2: Prepare Management Briefing</strong>","level":4,"id":"**Step_3-2_Prepare_Management_Briefing**_0"},{"heading":"<strong>Activity 4: Develop Targeted Improvement Plans</strong>","level":3,"id":"**Activity_4_Develop_Targeted_Improvement_Plans**_0"},{"heading":"<strong>Activity 5: Reassess Maturity</strong>","level":3,"id":"**Activity_5_Reassess_Maturity**_0"},{"heading":"<strong>Tools for Maturity Assessment</strong>","level":3,"id":"**Tools_for_Maturity_Assessment**_0"},{"heading":"<strong>Method 1: Selecting the DMM Framework</strong>","level":3,"id":"**Method_1_Selecting_the_DMM_Framework**_0"},{"heading":"<strong>Method 2: Using DAMA-DMBOK Framework</strong>","level":3,"id":"**Method_2_Using_DAMA-DMBOK_Framework**_0"},{"heading":"<strong>Guidelines for Readiness and Risk Assessment</strong>","level":3,"id":"**Guidelines_for_Readiness_and_Risk_Assessment**_0"},{"heading":"<strong>Metrics for Maturity Assessment</strong>","level":3,"id":"**Metrics_for_Maturity_Assessment**_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html","pathToRoot":"../../..","attachments":[],"createdTime":1737584641428,"modifiedTime":1737554783000,"sourceSize":14630,"sourcePath":"Big Data/13. Certificate/CDGA/14. Data Management Maturity Assessment.md","exportPath":"big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/auto-keyword.html":{"title":"Auto keyword","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=2vOPEuiGXVo\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=2vOPEuiGXVo\" target=\"_self\">link</a>Using auto allows you to omit the explicit type when declaring a variable. The compiler will deduce the type based on the initializer expression:auto x = 5; // x is int\nauto y = 3.14; // y is double\nThis avoids having to spell out the type yourself and lets it be determined automatically.Some major cases where auto is helpful:\nDeclaring iterators or other long, complex STL types:\nstd::map&lt;std::string, std::vector&lt;int&gt;&gt;::iterator it;\n// vs\nauto it = myMap.begin(); Generic programming with templates\nLambda expressions and std::function\nInitializing variables with constructors\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Auto keyword","level":1,"id":"Auto_keyword_1"},{"heading":"Key Use Cases:","level":2,"id":"Key_Use_Cases_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/auto-keyword.html","pathToRoot":"..","attachments":[],"createdTime":1690694653862,"modifiedTime":1691392973897,"sourceSize":1280,"sourcePath":"C++/Auto keyword.md","exportPath":"c++/auto-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/c++.html":{"title":"C++","icon":"","description":"\n<a data-href=\"OOP\" href=\"c++/oop.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">OOP</a>\n<br><a data-href=\"Enum and Enum class\" href=\"c++/enum-and-enum-class.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Enum and Enum class</a>\n<br><a data-href=\"header files\" href=\"c++/header-files.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">header files</a>\n<br><a data-href=\"Const keyword\" href=\"c++/const-keyword.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Const keyword</a>\n<br><a data-href=\"Static keyword\" href=\"c++/static-keyword.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Static keyword</a>\n<br><a data-href=\"Inline keyword\" href=\"c++/inline-keyword.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Inline keyword</a>\n<br><a data-href=\"Auto keyword\" href=\"c++/auto-keyword.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Auto keyword</a>\n<br><a data-href=\"Type punning\" href=\"c++/type-punning.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Type punning</a>\n<br><a data-href=\"Timing\" href=\"c++/timing.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Timing</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=f3FVU-iwNuA\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=f3FVU-iwNuA\" target=\"_self\">static</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=9RJTQmK0YPI\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=9RJTQmK0YPI\" target=\"_self\">header files</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=4fJBrditnJU\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=4fJBrditnJU\" target=\"_self\">Const</a> ","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"C++","level":1,"id":"C++_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["c++/oop.html#_0","c++/enum-and-enum-class.html#_0","c++/header-files.html#_0","c++/const-keyword.html#_0","c++/static-keyword.html#_0","c++/inline-keyword.html#_0","c++/auto-keyword.html#_0","c++/type-punning.html#_0","c++/timing.html#_0"],"author":"","coverImageURL":"","fullURL":"c++/c++.html","pathToRoot":"..","attachments":[],"createdTime":1690545991464,"modifiedTime":1692560983000,"sourceSize":390,"sourcePath":"C++/C++.md","exportPath":"c++/c++.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/const-keyword.html":{"title":"Const keyword","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=4fJBrditnJU\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=4fJBrditnJU\" target=\"_self\">Const</a>In C++, the const keyword is used to indicate that a variable, pointer, reference, or function cannot be modified.When used with a variable, const makes the variable read-only, meaning that its value cannot be changed after it has been initialized. For example:const int x = 5;\nx = 6; // Error: assignment of read-only variable 'x'\nWhen used with a pointer, const indicates that the value pointed to by the pointer cannot be modified, but the pointer itself can be changed to point to a different memory location. This is indicated by const appearing to the right of the *. For example:int x = 5;\nconst int* ptr = &amp;x;\n*ptr = 6; // Error: assignment of read-only location '*ptr'\nptr = nullptr; // ok, changes the value of ptr to null\nWhen used with a pointer, const appearing to the left of the * indicates that the pointer itself is constant, but the value it points to can be changed. For example:int x = 5;\nint y = 10;\nint* const ptr = &amp;x;\n*ptr = 6; // ok, changes the value of x\nptr = &amp;y; // Error: cannot assign to variable 'ptr' with const-qualified type 'int *const'\nWhen used with a reference, const indicates that the reference cannot be used to modify the object it refers to. For example:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Const keyword","level":1,"id":"Const_keyword_1"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/const-keyword.html","pathToRoot":"..","attachments":[],"createdTime":1690545991260,"modifiedTime":1708699899608,"sourceSize":2894,"sourcePath":"C++/Const keyword.md","exportPath":"c++/const-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/enum-and-enum-class.html":{"title":"Enum and Enum class","icon":"","description":"In C++, enum and enum class are used to define enumerations, which are user-defined data types that represent a set of named integer constants.The main difference between enum and enum class is in how the underlying constants are scoped:enum: The constants defined in an enum are placed in the outer scope, which means that they can potentially clash with other names in the same scope. For example:enum Color { RED, GREEN, BLUE\n}; int RED = 0; // this will cause a compilation error\nenum class: The constants defined in an enum class are scoped within the enumeration itself, which means that they cannot clash with other names in the same scope. For example:enum class Color { RED, GREEN, BLUE\n}; int RED = 0; // this is allowed\nAnother difference between enum and enum class is in how the underlying constants are implicitly converted to integers:enum: The constants defined in an enum are implicitly converted to integers, which means that they can be used interchangeably with integers. For example: enum Color { RED, GREEN, BLUE }; int redValue = RED; // this is allowed int blueValue = BLUE; // this is allowed\nenum class: The constants defined in an enum class are not implicitly converted to integers, which means that they cannot be used interchangeably with integers. For example:enum class Color { RED, GREEN, BLUE\n}; int redValue = Color::RED; // this is not allowed\nint blueValue = static_cast&lt;int&gt;(Color::BLUE); // this is allowed with explicit conversion To summarize, enum class provides better scoping and type safety compared to enum, but requires explicit casting to convert the enum constants to integers.Here's an example of how to declare and use an enum class in C++:enum class Color { RED, GREEN, BLUE\n}; int main() { Color myColor = Color::RED; // declare a variable of type Color and assign it the value Color::RED switch (myColor) { case Color::RED: std::cout &lt;&lt; \"The color is red.\" &lt;&lt; std::endl; break; case Color::GREEN: std::cout &lt;&lt; \"The color is green.\" &lt;&lt; std::endl; break; case Color::BLUE: std::cout &lt;&lt; \"The color is blue.\" &lt;&lt; std::endl; break; default: std::cout &lt;&lt; \"Invalid color.\" &lt;&lt; std::endl; break; } return 0;\n}\nIn this example, we define an enum class called Color that contains three named constants: RED, GREEN, and BLUE. We then declare a variable myColor of type Color and assign it the value Color::RED. We use a switch statement to check the value of myColor and print a message depending on the color.Note that we use the scope resolution operator :: to access the named constants in the enum class. Also note that since the values in an enum class are not implicitly converted to integers, we must use the name of the enum class and the named constant to refer to the values.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Enum and Enum class","level":1,"id":"Enum_and_Enum_class_1"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/enum-and-enum-class.html","pathToRoot":"..","attachments":[],"createdTime":1690545991307,"modifiedTime":1692560973000,"sourceSize":3187,"sourcePath":"C++/Enum and Enum class.md","exportPath":"c++/enum-and-enum-class.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/header-files.html":{"title":"Header Files","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=9RJTQmK0YPI\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=9RJTQmK0YPI\" target=\"_self\">header files</a>In C++, header files are used to declare functions, classes, variables, and other entities that are defined in other source files. Header files are typically used to share declarations between files and to provide a modular structure for C++ programs.A header file typically has a .h or .hpp file extension and contains declarations of functions, classes, and other entities that are defined in one or more source files. Here's an example of a simple header file:#ifndef MY_HEADER_H\n#define MY_HEADER_H int add(int x, int y); #endif\nIn this example, MY_HEADER_H is a preprocessor macro that is used to prevent the header file from being included multiple times in the same source file. The add() function is declared in the header file, but its implementation is defined in a separate source file.To use a header file in a source file, you typically include the header file using the #include directive. For example:#include \"my_header.h\" int main() { int result = add(5, 10); return 0;\n}\nIn this example, the my_header.h header file is included in the main.cpp source file. This allows the add() function to be called in the main() function.Header files can also include other header files, which allows for a modular structure in C++ programs. For example:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Header Files","level":1,"id":"Header_Files_1"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/header-files.html","pathToRoot":"..","attachments":[],"createdTime":1690545991327,"modifiedTime":1691393043318,"sourceSize":2671,"sourcePath":"C++/Header Files.md","exportPath":"c++/header-files.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/inline-keyword.html":{"title":"Inline keyword","icon":"","description":"In C++, the inline keyword is used to suggest that a function be inlined, which means that the function's code is inserted directly into the calling code rather than being called as a separate function. This can improve performance by reducing the overhead of function calls.When a function is declared with the inline keyword, the compiler may choose to inline the function's code, but it is not required to do so. The decision to inline a function is ultimately up to the compiler, which will take into account factors such as the size of the function, how often it is called, and the available optimization options.Here's an example of using the inline keyword:inline int add(int x, int y) { return x + y;\n} int result = add(5, 10);\nIn this example, add() is declared as an inline function that adds two integers and returns the result. The function is called with the arguments 5 and 10, and the result is stored in the result variable. If the compiler chooses to inline the function, the resulting code will be equivalent to:int result = 5 + 10;\nNote that the inline keyword is only a suggestion to the compiler. In some cases, the compiler may choose not to inline the function, even if it is declared as inline. Additionally, inlining a function can increase the size of the resulting code, so it is not always the best option for improving performance.Overall, the inline keyword is a useful tool for improving performance by suggesting that a function be inlined, but its effectiveness depends on the compiler and the specifics of the code being compiled.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Inline keyword","level":1,"id":"Inline_keyword_1"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/inline-keyword.html","pathToRoot":"..","attachments":[],"createdTime":1690545991345,"modifiedTime":1691393052964,"sourceSize":1637,"sourcePath":"C++/Inline keyword.md","exportPath":"c++/inline-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/oop.html":{"title":"OOP","icon":"","description":"Object-oriented programming is a programming paradigm that emphasizes the use of objects and their interactions to design and write software. C++ is a popular programming language that supports OOP concepts such as abstraction, polymorphism, inheritance, and encapsulation. Let's start with the basics:Abstraction is the process of hiding the implementation details of a class while showing only the necessary information to the user. In C++, abstraction is achieved using classes and interfaces. A class is a user-defined data type that encapsulates data members and member functions. An interface is a collection of abstract methods that define the behavior of an object.Here's an example of an abstract class in C++:class Shape {\npublic: virtual void draw() = 0; // pure virtual function\n};\nIn this example, Shape is an abstract class that has a pure virtual function called draw. A pure virtual function is a virtual function that has no implementation in the base class and must be implemented in the derived class. This makes Shape an abstract class because it cannot be instantiated.Polymorphism is the ability of an object to take on many forms. In C++, polymorphism is achieved using virtual functions and function overloading.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"OOP","level":1,"id":"OOP_1"},{"heading":"1. Abstraction:","level":1,"id":"1._Abstraction_0"},{"heading":"2. Polymorphism:","level":1,"id":"2._Polymorphism_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/oop.html","pathToRoot":"..","attachments":[],"createdTime":1690545991361,"modifiedTime":1691393061952,"sourceSize":4257,"sourcePath":"C++/OOP.md","exportPath":"c++/oop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/static-keyword.html":{"title":"Static keyword","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=f3FVU-iwNuA\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=f3FVU-iwNuA\" target=\"_self\">static</a> In C++, the static keyword has several different uses, depending on where it is used.When used with a variable or function in global scope, static restricts the variable or function's scope to the current translation unit, which is the source file being compiled and any files included directly or indirectly by that file. This means that the variable or function cannot be accessed from other translation units. For example:// file1.cpp\nstatic int x = 5;\nstatic void foo() { /* ... */ }\nIn this example, x and foo() are declared as static variables and functions in the file1.cpp translation unit. They cannot be accessed from other translation units unless they are explicitly declared as extern.When used with a variable or function in a class or struct, static indicates that the variable or function is shared by all instances of the class or struct, rather than being associated with individual instances. For example:class MyClass {\npublic: static int x; static void foo() { /* ... */ }\n}; int MyClass::x = 5;\nIn this example, x and foo() are declared as static members of the MyClass class. x is a shared variable that is associated with the class rather than individual instances of the class. foo() is a shared function that can be called on the class itself, rather than on individual instances.When used with a local variable inside a function, static indicates that the variable retains its value between function calls, rather than being reinitialized each time the function is called. For example:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Static keyword","level":1,"id":"Static_keyword_1"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/static-keyword.html","pathToRoot":"..","attachments":[],"createdTime":1690545991407,"modifiedTime":1692560977000,"sourceSize":2858,"sourcePath":"C++/Static keyword.md","exportPath":"c++/static-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/timing.html":{"title":"Timing","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=oEx5vGNFrLk&amp;list=WL&amp;index=2\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=oEx5vGNFrLk&amp;list=WL&amp;index=2\" target=\"_self\">link</a>the first way to time the execution is by using the built in timer Chrono in C++:#include &lt;iostream&gt;\n#include &lt;chrono&gt;\n#include &lt;thread&gt;\nint main(){ using namespace std::literals::chrono_literals; auto start = std::chrono::high_resolution_clock::now(); std::this_thread()::sleep_for(1s); auto end = std::chrono::high_resolution_clock::now(); std::chrono::duration&lt;float&gt; duration = end - start; std::cout &lt;&lt; duration .count() &lt;&lt; \"s \" &lt;&lt; std::endl; std::cin.get()\n}\nnow a better way to use this with less code :#include &lt;iostream&gt;\n#include &lt;chrono&gt;\n#include &lt;thread&gt;\nstruct Timer { std::chrono::time_point&lt;std::chrono::high_resolution_clock&gt; start, end; std::chrono::duration&lt;float&gt; duration; Timer() { start = std::chrono::high_resolution_clock::now(); } ~Timer() { end = std::chrono::high_resolution_clock::now(); duration = end - start; float ms = duration.count() * 1000.0f; std::cout &lt;&lt; \"Timer took \" &lt;&lt; ms &lt;&lt; \"ms \" &lt;&lt; std::endl; }\n}; void function(){ Timer timer; for (int i=0; i&lt;100; i++){ std::cout &lt;&lt; i &lt;&lt; std::endl; }\n} void fasterFunction(){ Timer timer; for (int i=0; i&lt;100; i++){ std::cout &lt;&lt; i &lt;&lt; \"\\n\"; }\n} int main(){ //fasterfunction function(); std::cin.get();\n} you can also use the operating system timer e.g. in windows you can use something like winsock.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Timing","level":1,"id":"Timing_1"},{"heading":"Chrono","level":2,"id":"Chrono_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/timing.html","pathToRoot":"..","attachments":[],"createdTime":1691005752022,"modifiedTime":1691393086230,"sourceSize":1499,"sourcePath":"C++/Timing.md","exportPath":"c++/timing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"c++/type-punning.html":{"title":"Type punning","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=8egZ_5GA9Bc&amp;list=WL&amp;index=1&amp;t=438s\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=8egZ_5GA9Bc&amp;list=WL&amp;index=1&amp;t=438s\" target=\"_self\">link</a>Type punning refers to techniques that allow accessing an object as a different type by taking advantage of the memory layout. This allows bypassing the strict aliasing rules in C++. Here's an explanation of how it works and some common use cases.Type punning involves leveraging the fact that the memory layout of some types may match, even if they are considered different types by the compiler. Some examples:\nAn int and a float may both occupy 4 bytes.\nA struct with exactly one int member will match the layout of just an int.\nThis allows code to reinterpret the binary representation of the same memory as different types.Some methods of type punning include:union Data { int i; float f;\n}; Data data; data.i = 10; // Now access the same memory as a float\nstd::cout &lt;&lt; data.f;\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Type punning","level":1,"id":"Type_punning_1"},{"heading":"How Type Punning Works","level":2,"id":"How_Type_Punning_Works_0"},{"heading":"Unions","level":3,"id":"Unions_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"c++/type-punning.html","pathToRoot":"..","attachments":[],"createdTime":1691005840293,"modifiedTime":1691393100246,"sourceSize":3115,"sourcePath":"C++/Type punning.md","exportPath":"c++/type-punning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-base/data-base.html":{"title":"Data Base","icon":"","description":"\n<a data-href=\"MongoDB\" href=\"data-base/mongodb.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">MongoDB</a>\n<br><a data-href=\"MongoDB cheat Sheet\" href=\"data-base/mongodb-cheat-sheet.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">MongoDB cheat Sheet</a>\n<br><a data-href=\"SQL Cheat Sheet\" href=\"data-base/sql-cheat-sheet.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">SQL Cheat Sheet</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://gist.github.com/bradtraversy/f407d642bdc3b31681bc7e56d95485b6\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://gist.github.com/bradtraversy/f407d642bdc3b31681bc7e56d95485b6\" target=\"_self\">MongoDB</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://learnsql.com/blog/sql-basics-cheat-sheet/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://learnsql.com/blog/sql-basics-cheat-sheet/\" target=\"_self\">SQL cheat sheet</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Base","level":1,"id":"Data_Base_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["data-base/mongodb.html#_0","data-base/mongodb-cheat-sheet.html#_0","data-base/sql-cheat-sheet.html#_0"],"author":"","coverImageURL":"","fullURL":"data-base/data-base.html","pathToRoot":"..","attachments":[],"createdTime":1690694720777,"modifiedTime":1692560974000,"sourceSize":267,"sourcePath":"Data Base/Data Base.md","exportPath":"data-base/data-base.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-base/mongodb.html":{"title":"MongoDB","icon":"","description":"MongoDB is a popular open-source, document-oriented NoSQL database. Instead of storing data in tables like traditional relational databases, MongoDB stores structured data in JSON-like documents.\nDocument-oriented - stores data in flexible, JSON-like documents instead of rows and columns\nScalability - easy to scale horizontally with automatic sharding\nHigh performance - performs well with large datasets and heavy workloads\nHigh availability - supports replication and failover for high availability\nFlexible data model - dynamic schemas allow you to store different data in the same collection\nIndexing - indexes support faster queries and can include keys from embedded documents and arrays\nThe main components of MongoDB are:\nmongod - The MongoDB daemon that does the core data storage\nmongos - The query router that provides sharding\nDocuments - Data records composed of field-value pairs\nCollections - Groupings of MongoDB documents\nDatabases - Container for collections JSON-like documents contain field-value pairs\nCollections contain sets of documents\nDatabases contain collections\nUses BSON format to store documents (binary JSON)\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"MongoDB","level":1,"id":"MongoDB_1"},{"heading":"Introduction","level":2,"id":"Introduction_0"},{"heading":"Key Features","level":2,"id":"Key_Features_0"},{"heading":"Components","level":2,"id":"Components_0"},{"heading":"Basic Concepts","level":2,"id":"Basic_Concepts_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-base/mongodb.html","pathToRoot":"..","attachments":[],"createdTime":1690694662822,"modifiedTime":1691392853882,"sourceSize":6052,"sourcePath":"Data Base/MongoDB.md","exportPath":"data-base/mongodb.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-base/mongodb-cheat-sheet.html":{"title":"MongoDB cheat Sheet","icon":"","description":"show dbs\ndb\nuse acme\ndb.dropDatabase()\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"MongoDB cheat Sheet","level":1,"id":"MongoDB_cheat_Sheet_1"},{"heading":"Show All Databases","level":2,"id":"Show_All_Databases_0"},{"heading":"Show Current Database","level":2,"id":"Show_Current_Database_0"},{"heading":"Create Or Switch Database","level":2,"id":"Create_Or_Switch_Database_0"},{"heading":"Drop","level":2,"id":"Drop_0"},{"heading":"Create Collection","level":2,"id":"Create_Collection_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-base/mongodb-cheat-sheet.html","pathToRoot":"..","attachments":[],"createdTime":1690694869426,"modifiedTime":1691392864868,"sourceSize":3031,"sourcePath":"Data Base/MongoDB cheat Sheet.md","exportPath":"data-base/mongodb-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-base/sql-cheat-sheet.html":{"title":"SQL Cheat Sheet","icon":"","description":"SQL, or Structured Query Language, is a language to talk to databases. It allows you to select specific data and to build complex reports. Today, SQL is a universal language of data. It is used in practically all technologies that process data.Fetch all columns from the country table:SELECT *\nFROM country;Fetch id and name columns from the city table:SELECT id, name\nFROM city;Fetch city names sorted by the rating column in the default ASCending order:SELECT name\nFROM city\nORDER BY rating [ASC];Fetch city names sorted by the rating column in the DESCending order:SELECT name\nFROM city\nORDER BY rating DESC;SELECT name AS city_name\nFROM city;SELECT co.name, ci.name\nFROM city AS ci\nJOIN country AS co\nON ci.country_id = co.id;Fetch names of cities that have a rating above 3:SELECT name\nFROM city\nWHERE rating &gt; 3;Fetch names of cities that are neither Berlin nor Madrid:SELECT name\nFROM city\nWHERE name != 'Berlin'\nAND name != 'Madrid';Fetch names of cities that start with a 'P' or end with an 's':SELECT name\nFROM city\nWHERE name LIKE 'P%'\nOR name LIKE '%s';Fetch names of cities that start with any letter followed by 'ublin' (like Dublin in Ireland or Lublin in Poland):SELECT name\nFROM city\nWHERE name LIKE '_ublin';Fetch names of cities that have a population between 500K and 5M:SELECT name\nFROM city\nWHERE population BETWEEN 500000 AND 5000000;Fetch names of cities that don't miss a rating value:SELECT name\nFROM city\nWHERE rating IS NOT NULL;Fetch names of cities that are in countries with IDs 1, 4, 7, or 8:SELECT name\nFROM city\nWHERE country_id IN (1, 4, 7, 8);JOIN (or explicitly INNER JOIN) returns rows that have matching values in both tables.SELECT city.name, country.name\nFROM city\n[INNER] JOIN country\nON city.country_id = country.id;LEFT JOIN returns all rows from the left table with corresponding rows from the right table. If there's no matching row, NULLs are returned as values from the second table.SELECT city.name, country.name\nFROM city\nLEFT JOIN country\nON city.country_id = country.id;RIGHT JOIN returns all rows from the right table with corresponding rows from the left table. If there's no matching row, NULLs are returned as values from the left table.SELECT city.name, country.name\nFROM city\nRIGHT JOIN country\nON city.country_id = country.id;FULL JOIN (or explicitly FULL OUTER JOIN) returns all rows from both tables – if there's no matching row in the second table, NULLs are returned.SELECT city.name, country.name\nFROM city\nFULL [OUTER] JOIN country\nON city.country_id = country.id;CROSS JOIN returns all possible combinations of rows from both tables. There are two syntaxes available.SELECT city.name, country.name\nFROM city\nCROSS JOIN country;SELECT city.name, country.name\nFROM city, country;NATURAL JOIN will join tables by all columns with the same name.SELECT city.name, country.name\nFROM city\nNATURAL JOIN country;NATURAL JOIN used these columns to match rows:\ncity.id, city.name, country.id, country.name.\nNATURAL JOIN is very rarely used in practice.GROUP BY groups together rows that have the same values in specified columns. It computes summaries (aggregates) for each unique combination of values.\navg(expr) − average value for rows within the group\ncount(expr) − count of values for rows within the group\nmax(expr) − maximum value within the group\nmin(expr) − minimum value within the group\nsum(expr) − sum of values within the group\nFind out the number of cities:SELECT COUNT(*)\nFROM city;Find out the number of cities with non-null ratings:SELECT COUNT(rating)\nFROM city;Find out the number of distinctive country values:SELECT COUNT(DISTINCT country_id)\nFROM city;Find out the smallest and the greatest country populations:SELECT MIN(population), MAX(population)\nFROM country;Find out the total population of cities in respective countries:SELECT country_id, SUM(population)\nFROM city\nGROUP BY country_id;Find out the average rating for cities in respective countries if the average is above 3.0:SELECT country_id, AVG(rating)\nFROM city\nGROUP BY country_id\nHAVING AVG(rating) &gt; 3.0;A subquery is a query that is nested inside another query, or inside another subquery. There are different types of subqueries.The simplest subquery returns exactly one column and exactly one row. It can be used with comparison operators =, &lt;, &lt;=, &gt;, or &gt;=.This query finds cities with the same rating as Paris:SELECT name\nFROM city\nWHERE rating = (\nSELECT rating\nFROM city\nWHERE name = 'Paris'\n);A subquery can also return multiple columns or multiple rows. Such subqueries can be used with operators IN, EXISTS, ALL, or ANY.This query finds cities in countries that have a population above 20M:SELECT name\nFROM city\nWHERE country_id IN (\nSELECT country_id\nFROM country\nWHERE population &gt; 20000000\n);A correlated subquery refers to the tables introduced in the outer query. A correlated subquery depends on the outer query. It cannot be run independently from the outer query.This query finds cities with a population greater than the average population in the country:SELECT *\nFROM city main_city\nWHERE population &gt; (\nSELECT AVG(population)\nFROM city average_city\nWHERE average_city.country_id = main_city.country_id\n);This query finds countries that have at least one city:SELECT name\nFROM country\nWHERE EXISTS (\nSELECT *\nFROM city\nWHERE country_id = country.id\n);Set operations are used to combine the results of two or more queries into a single result. The combined queries must return the same number of columns and compatible data types. The names of the corresponding columns can be differentUNION combines the results of two result sets and removes duplicates. UNION ALL doesn't remove duplicate rows.This query displays German cyclists together with German skaters:SELECT name\nFROM cycling\nWHERE country = 'DE'\nUNION / UNION ALL\nSELECT name\nFROM skating\nWHERE country = 'DE';INTERSECT returns only rows that appear in both result sets.This query displays German cyclists who are also German skaters at the same time:SELECT name\nFROM cycling\nWHERE country = 'DE'\nINTERSECT\nSELECT name\nFROM skating\nWHERE country = 'DE';EXCEPT returns only the rows that appear in the first result set but do not appear in the second result set.This query displays German cyclists unless they are also German skaters at the same time:SELECT name\nFROM cycling\nWHERE country = 'DE'\nEXCEPT / MINUS\nSELECT name\nFROM skating\nWHERE country = 'DE';","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"SQL Basics Cheat Sheet","level":1,"id":"SQL_Basics_Cheat_Sheet_0"},{"heading":"SQL","level":2,"id":"SQL_0"},{"heading":"QUERYING SINGLE TABLE","level":3,"id":"QUERYING_SINGLE_TABLE_0"},{"heading":"Aliases","level":3,"id":"Aliases_0"},{"heading":"Columns","level":4,"id":"Columns_0"},{"heading":"Tables","level":4,"id":"Tables_0"},{"heading":"FILTERING THE OUTPUT","level":3,"id":"FILTERING_THE_OUTPUT_0"},{"heading":"COMPARISON OPERATORS","level":4,"id":"COMPARISON_OPERATORS_0"},{"heading":"TEXT OPERATORS","level":4,"id":"TEXT_OPERATORS_0"},{"heading":"OTHER OPERATORS","level":4,"id":"OTHER_OPERATORS_0"},{"heading":"QUERYING MULTIPLE TABLES","level":3,"id":"QUERYING_MULTIPLE_TABLES_0"},{"heading":"INNER JOIN","level":4,"id":"INNER_JOIN_0"},{"heading":"LEFT JOIN","level":4,"id":"LEFT_JOIN_0"},{"heading":"RIGHT JOIN","level":4,"id":"RIGHT_JOIN_0"},{"heading":"FULL JOIN","level":4,"id":"FULL_JOIN_0"},{"heading":"CROSS JOIN","level":4,"id":"CROSS_JOIN_0"},{"heading":"NATURAL JOIN","level":4,"id":"NATURAL_JOIN_0"},{"heading":"AGGREGATION AND GROUPING","level":3,"id":"AGGREGATION_AND_GROUPING_0"},{"heading":"AGGREGATE FUNCTIONS","level":4,"id":"AGGREGATE_FUNCTIONS_0"},{"heading":"EXAMPLE QUERIES","level":4,"id":"EXAMPLE_QUERIES_0"},{"heading":"SUBQUERIES","level":3,"id":"SUBQUERIES_0"},{"heading":"SINGLE VALUE","level":4,"id":"SINGLE_VALUE_0"},{"heading":"MULTIPLE VALUES","level":4,"id":"MULTIPLE_VALUES_0"},{"heading":"CORRELATED","level":4,"id":"CORRELATED_0"},{"heading":"SET OPERATIONS","level":3,"id":"SET_OPERATIONS_0"},{"heading":"UNION","level":4,"id":"UNION_0"},{"heading":"INTERSECT","level":4,"id":"INTERSECT_0"},{"heading":"EXCEPT","level":4,"id":"EXCEPT_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-base/sql-cheat-sheet.html","pathToRoot":"..","attachments":[],"createdTime":1691390817322,"modifiedTime":1691392878982,"sourceSize":7306,"sourcePath":"Data Base/SQL Cheat Sheet.md","exportPath":"data-base/sql-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/aggregation-analysis.html":{"title":"Aggregation analysis","icon":"","description":"imagine we have the code below:def increment_number(number): # Convert the number to a list of bits bits = list(number) # Start from the rightmost bit index = len(bits) - 1 # Iterate through the bits and perform the increment while index &gt;= 0: if bits[index] == '0': # Change 0 to 1 and stop bits[index] = '1' break else: # Change 1 to 0 and move one position to the left bits[index] = '0' index -= 1 # Convert the list of bits back to a string result = ''.join(bits) return result # Example usage\nnumber = '101'\nresult = increment_number(number)\nprint(result)\nthis is a program that increments a number with k bits in base 2 by one.\nif we try to calculate its execution time we find out that it is dependent on the value of input number.so we use a method named accounting; we do this task n time then calculate the average of those n time and that's our execution time:j=0\nn=10\nwhile j&lt;n : increment_basecar_number(number) j+=1\nso now we aggregate the terms by counting each bit, first bit gets changed time, second bit gets changed of the times and...Each aggregate operation is assigned a \"payment\". The payment is intended to cover the cost of elementary operations needed to complete this particular operation, with some of the payment left over, placed in a pool to be used later. it's basically saving money for each tasks so it can use them later on the more heavier tasks.now if look at the increment_number(number) example we can spend 1$ for every 0 to 1 and save 1$ for it. now if we have 1 to 0 change in bits we can do it because we have saved 1$ before hand.\nnote that this is for when we are counting from 00..0 if for example we start from a number 011..01 which has k ones in it then we add k$ for those ones and calculate the rest of them like before, .<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Accounting_method_(computer_science)\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Accounting_method_(computer_science)\" target=\"_self\">Wikipedia</a>we have a data structure called and a operation which is called which and for not defining to many variables its execution time is also we define the function and we define a fake payment like the formula below:so all is left to do is to define in a way that helps us calculate the execution time.\nfor example increment_number(number) we define to be the number of 1's in the number.\nfor example if the number has a ones in it and t adjacent ones from right side we calculate them like this:\nwhich add up to for which is the execution time.<br><a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Potential_method\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Potential_method\" target=\"_self\">Wikipedia</a><br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%B4%D8%B4%D9%85-%D8%AA%D8%AD%D9%84%DB%8C%D9%84-%D8%B3%D8%B1%D8%B4%DA%A9%D9%86/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%B4%D8%B4%D9%85-%D8%AA%D8%AD%D9%84%DB%8C%D9%84-%D8%B3%D8%B1%D8%B4%DA%A9%D9%86/\" target=\"_self\">session 6</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Aggregation analysis:","level":1,"id":"Aggregation_analysis_1"},{"heading":"Accounting method:","level":1,"id":"Accounting_method_0"},{"heading":"Potential method:","level":2,"id":"Potential_method_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/aggregation-analysis.html","pathToRoot":"..","attachments":[],"createdTime":1690545991727,"modifiedTime":1691392748961,"sourceSize":3779,"sourcePath":"Data Structures/Aggregation analysis.md","exportPath":"data-structures/aggregation-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/analyzing-exchanging-sorting-algorithms.html":{"title":"Analyzing Exchanging Sorting Algorithms","icon":"","description":"\nsimple operations like assigning values or multiply, subtraction, etc. = .\nfor calculating if statements we count the maximum bracket.\ndef bubbleSort(arr): n = len(arr) # Traverse through all array elements for i in range(n): swapped = False # Last i elements are already in place for j in range(0, n-i-1): # Traverse the array from 0 to n-i-1 # Swap if the element found is greater # than the next element if arr[j] &gt; arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] swapped = True if (swapped == False): break # Driver code to test above\nif __name__ == \"__main__\": arr = [64, 34, 25, 12, 22, 11, 90] bubbleSort(arr) print(\"Sorted array:\") for i in range(len(arr)): print(\"%d\" % arr[i], end=\" \")\nfor bubble sort best case, worst case and average are all equal to cause is does not check if the array is sorted or not and just repeats the comparison operations.def mergeSort(arr): if len(arr) &gt; 1: # Finding the mid of the array mid = len(arr)//2 # Dividing the array elements L = arr[:mid] # Into 2 halves R = arr[mid:] # Sorting the first half mergeSort(L) # Sorting the second half mergeSort(R) i = j = k = 0 # Copy data to temp arrays L[] and R[] while i &lt; len(L) and j &lt; len(R): if L[i] &lt;= R[j]: arr[k] = L[i] i += 1 else: arr[k] = R[j] j += 1 k += 1 # Checking if any element was left while i &lt; len(L): arr[k] = L[i] i += 1 k += 1 while j &lt; len(R): arr[k] = R[j] j += 1 k += 1 # Code to print the list\ndef printList(arr): for i in range(len(arr)): print(arr[i], end=\" \") print() # Driver Code\nif __name__ == '__main__': arr = [12, 11, 13, 5, 6, 7] print(\"Given array is\") printList(arr) mergeSort(arr) print(\"\\nSorted array is \") printList(arr)\n<a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%B3%D9%88%D9%85-%D8%AA%D8%AD%D9%84%DB%8C%D9%84-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%AA%D8%B1%D8%AA%DB%8C%D8%A8%DB%8C-%D9%85%D8%B1%D8%AA%D8%A8-%D8%B3%D8%A7%D8%B2%DB%8C/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%B3%D9%88%D9%85-%D8%AA%D8%AD%D9%84%DB%8C%D9%84-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%AA%D8%B1%D8%AA%DB%8C%D8%A8%DB%8C-%D9%85%D8%B1%D8%AA%D8%A8-%D8%B3%D8%A7%D8%B2%DB%8C/\" target=\"_self\">session 3</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Execution time for operations:","level":1,"id":"Execution_time_for_operations_0"},{"heading":"Bubble Sort:","level":3,"id":"Bubble_Sort_0"},{"heading":"Merge Sort:","level":3,"id":"Merge_Sort_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/analyzing-exchanging-sorting-algorithms.html","pathToRoot":"..","attachments":[],"createdTime":1690545991668,"modifiedTime":1691392771098,"sourceSize":2373,"sourcePath":"Data Structures/Analyzing Exchanging Sorting Algorithms.md","exportPath":"data-structures/analyzing-exchanging-sorting-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/avl-tree.html":{"title":"AVL Tree","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/introduction-to-avl-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/introduction-to-avl-tree/\" target=\"_self\">GFG</a> | <a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/AVL_tree\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/AVL_tree\" target=\"_self\">wiki</a>An AVL tree defined as a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees for any node cannot be more than one.<br><img alt=\"AVL tree.excalidraw.svg\" src=\"excalidraw/avl-tree.excalidraw.svg\" target=\"_self\">M(h) is the minimum number of node in a AVL tree with height h.\nthe proof is trivial with recursion.also we have and since the inequality we wrote earlier looks like Fibonacci and the general formula for Fibonacci is we can say that similarly there exist a number where this proves that the height of AVL tree is logarithmic.<br><a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/insertion-in-an-avl-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/insertion-in-an-avl-tree/\" target=\"_self\">GFG</a>To make sure that the given tree remains AVL after every insertion, we must augment the standard BST insert operation to perform some re-balancing.\nFollowing are two basic operations that can be performed to balance a BST without violating the BST property (keys(left) &lt; key(root) &lt; keys(right)).\nLeft Rotation&nbsp;\nRight Rotation <br>Perform the normal <a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/binary-search-tree-set-1-search-and-insertion/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/binary-search-tree-set-1-search-and-insertion/\" target=\"_self\">BST insertion.</a>&nbsp;\nThe current node must be one of the ancestors of the newly inserted node. Update the height of the current node.&nbsp;\nGet the balance factor (left subtree height – right subtree height) of the current node.&nbsp;\nIf the balance factor is greater than 1, then the current node is unbalanced and we are either in the Left Left case or left Right case. To check whether it is left left case or not, compare the newly inserted key with the key in the left subtree root.&nbsp;\nIf the balance factor is less than -1, then the current node is unbalanced and we are either in the Right Right case or Right-Left case. To check whether it is the Right Right case or not, compare the newly inserted key with the key in the right subtree root.\n<br><img alt=\"Black and red tree height diff.excalidraw.svg\" src=\"excalidraw/black-and-red-tree-height-diff.excalidraw.svg\" target=\"_self\">the difference between sub trees is equal to the difference between the number of red nodes we also know that this number is at most where is the height of the tree, and we have proven that in black and red tree so the difference is .<br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%A8%DB%8C%D8%B3%D8%AA-%D8%AF%D9%88%D9%85-%D8%AF%D8%B1%D8%AE%D8%AA-avl/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%A8%DB%8C%D8%B3%D8%AA-%D8%AF%D9%88%D9%85-%D8%AF%D8%B1%D8%AE%D8%AA-avl/\" target=\"_self\">session 21</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"AVL Tree","level":1,"id":"AVL_Tree_1"},{"heading":"Height of AVL tree","level":2,"id":"Height_of_AVL_tree_0"},{"heading":"Insertion Algorithm","level":2,"id":"Insertion_Algorithm_0"},{"heading":"Operation Step By Step","level":3,"id":"Operation_Step_By_Step_0"},{"heading":"Height Difference in Black and Red Tree","level":2,"id":"Height_Difference_in_Black_and_Red_Tree_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/avl-tree.html","pathToRoot":"..","attachments":["excalidraw/avl-tree.excalidraw.svg","excalidraw/black-and-red-tree-height-diff.excalidraw.svg"],"createdTime":1691390477575,"modifiedTime":1741256261570,"sourceSize":2838,"sourcePath":"Data Structures/AVL Tree.md","exportPath":"data-structures/avl-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/binary-search-tree.html":{"title":"Binary Search Tree","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/binary-search-tree-data-structure/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/binary-search-tree-data-structure/\" target=\"_self\">GFG</a>Binary Search Tree is a node-based binary tree data structure which has the following properties:\nThe left subtree of a node contains only nodes with keys lesser than the node’s key.\nThe right subtree of a node contains only nodes with keys greater than the node’s key.\nThe left and right subtree each must also be a binary search tree.\nalthough the number of permutations of numbers is but those permutations output similar trees so the number of different trees is less than and equal to:When inserting a new node into a binary search tree, we follow these steps:\nStart at the root of the tree.\nIf the tree is empty, the new node becomes the root.\nIf the new node's value is less than the current node's value, move to the left child.\nIf the new node's value is greater than the current node's value, move to the right child.\nRepeat steps 3 and 4 until a suitable empty position is found.\nInsert the new node in that position.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Binary Search Tree:","level":1,"id":"Binary_Search_Tree_1"},{"heading":"Number of trees:","level":2,"id":"Number_of_trees_0"},{"heading":"Inserting a element:","level":2,"id":"Inserting_a_element_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/binary-search-tree.html","pathToRoot":"..","attachments":[],"createdTime":1690545991589,"modifiedTime":1706562033136,"sourceSize":4007,"sourcePath":"Data Structures/Binary Search Tree.md","exportPath":"data-structures/binary-search-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/comparison-based-sorting.html":{"title":"Comparison based sorting","icon":"","description":"Average and worst case time complexity: n^2&nbsp;\nBest case time complexity: n when array is already sorted.&nbsp;\nWorst case: when the array is reverse sorted.&nbsp; Best, average and worst case time complexity: n^2 which is independent of distribution of data.Best, average and worst case time complexity: which is independent of distribution of data.Best, average and worst case time complexity: which is independent of distribution of data.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Comparison based sorting","level":1,"id":"Comparison_based_sorting_1"},{"heading":"Bubble sort and Insertion sort","level":2,"id":"Bubble_sort_and_Insertion_sort_0"},{"heading":"Selection sort","level":2,"id":"Selection_sort_0"},{"heading":"Merge sort","level":2,"id":"Merge_sort_0"},{"heading":"Heap sort","level":2,"id":"Heap_sort_0"},{"heading":"Quick sort","level":2,"id":"Quick_sort_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/comparison-based-sorting.html","pathToRoot":"..","attachments":[],"createdTime":1691390477683,"modifiedTime":1696098447000,"sourceSize":4176,"sourcePath":"Data Structures/Comparison based sorting.md","exportPath":"data-structures/comparison-based-sorting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/data-structures.html":{"title":"Data Structures","icon":"","description":"\n<a data-href=\"Insertion sort\" href=\"data-structures/insertion-sort.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Insertion sort</a>\n<br><a data-href=\"Time Complexity Comparison\" href=\"data-structures/time-complexity-comparison.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Time Complexity Comparison</a>\n<br><a data-href=\"Analyzing Exchanging Sorting Algorithms\" href=\"data-structures/analyzing-exchanging-sorting-algorithms.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Analyzing Exchanging Sorting Algorithms</a>\n<br><a data-href=\"Recusrsion\" href=\"data-structures/recusrsion.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Recusrsion</a>\n<br><a data-href=\"Master theorem\" href=\"data-structures/master-theorem.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Master theorem</a>\n<br><a data-href=\"Aggregation analysis\" href=\"data-structures/aggregation-analysis.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Aggregation analysis</a>\n<br><a data-href=\"Lists\" href=\"data-structures/lists.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Lists</a>\n<br><a data-href=\"Stacks &amp; Queues\" href=\"data-structures/stacks-&amp;-queues.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Stacks &amp; Queues</a>\n<br><a data-href=\"Trees\" href=\"data-structures/trees.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Trees</a>\n<br><a data-href=\"Tree and Trie\" href=\"data-structures/tree-and-trie.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Tree and Trie</a>\n<br><a data-href=\"Data Structures/Expression Tree\" href=\"data-structures/expression-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Structures/Expression Tree</a>\n<br><a data-href=\"Binary Search Tree\" href=\"data-structures/binary-search-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Binary Search Tree</a>\n<br><a data-href=\"Priority Queues\" href=\"data-structures/priority-queues.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Priority Queues</a>\n<br><a data-href=\"Hashing\" href=\"data-structures/hashing.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Hashing</a>\n<br><a data-href=\"Open Addressing\" href=\"data-structures/open-addressing.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Open Addressing</a>\n<br><a data-href=\"Order\" href=\"data-structures/order.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Order</a>\n<br><a data-href=\"Sorting Algorithms\" href=\"data-structures/sorting-algorithms.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Sorting Algorithms</a>\n<br><a data-href=\"Comparison based sorting\" href=\"data-structures/comparison-based-sorting.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Comparison based sorting</a>\n<br><a data-href=\"Linear Sorting Algorithm\" href=\"data-structures/linear-sorting-algorithm.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Linear Sorting Algorithm</a>\n<br><a data-href=\"Red-Black Tree\" href=\"data-structures/red-black-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Red-Black Tree</a>\n<br><a data-href=\"AVL Tree\" href=\"data-structures/avl-tree.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">AVL Tree</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%A7%D9%88%D9%84-%D8%B2%D9%85%D8%A7%D9%86-%D8%A7%D8%AC%D8%B1%D8%A7%DB%8C-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D9%85%D8%B1%D8%AA%D8%A8/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%A7%D9%88%D9%84-%D8%B2%D9%85%D8%A7%D9%86-%D8%A7%D8%AC%D8%B1%D8%A7%DB%8C-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D9%85%D8%B1%D8%AA%D8%A8/\" target=\"_self\">class recordings</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=RBSGKlAvoiM&amp;list=WL&amp;index=2\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=RBSGKlAvoiM&amp;list=WL&amp;index=2\" target=\"_self\">YouTube course</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/how-to-solve-time-complexity-recurrence-relations-using-recursion-tree-method/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/how-to-solve-time-complexity-recurrence-relations-using-recursion-tree-method/\" target=\"_self\">time complexity Recurrence Relations using Recursion Tree method</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Accounting_method_(computer_science)\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Accounting_method_(computer_science)\" target=\"_self\">Accounting method</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Potential_method\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Potential_method\" target=\"_self\">Potential method</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/open-addressing-collision-handling-technique-in-hashing/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/open-addressing-collision-handling-technique-in-hashing/\" target=\"_self\">Open addressing</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Open_addressing\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Open_addressing\" target=\"_self\">Open addressing</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/hashing-data-structure/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/hashing-data-structure/\" target=\"_self\">hashing</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Min-max_heap\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Min-max_heap\" target=\"_self\">Min max heap</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/binary-search-tree-data-structure/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/binary-search-tree-data-structure/\" target=\"_self\">binary search tree</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/sorting-algorithms/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/sorting-algorithms/\" target=\"_self\">Sorting algorithm</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/introduction-to-avl-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/introduction-to-avl-tree/\" target=\"_self\">AVL Tree</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"📒 Contents:","level":1,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":1,"id":"🔗_References_0"}],"links":["data-structures/insertion-sort.html#_0","data-structures/time-complexity-comparison.html#_0","data-structures/analyzing-exchanging-sorting-algorithms.html#_0","data-structures/recusrsion.html#_0","data-structures/master-theorem.html#_0","data-structures/aggregation-analysis.html#_0","data-structures/lists.html#_0","data-structures/stacks-&-queues.html#_0","data-structures/trees.html#_0","data-structures/tree-and-trie.html#_0","data-structures/expression-tree.html#_0","data-structures/binary-search-tree.html#_0","data-structures/priority-queues.html#_0","data-structures/hashing.html#_0","data-structures/open-addressing.html#_0","data-structures/order.html#_0","data-structures/sorting-algorithms.html#_0","data-structures/comparison-based-sorting.html#_0","data-structures/linear-sorting-algorithm.html#_0","data-structures/red-black-tree.html#_0","data-structures/avl-tree.html#_0"],"author":"","coverImageURL":"","fullURL":"data-structures/data-structures.html","pathToRoot":"..","attachments":[],"createdTime":1690545991794,"modifiedTime":1706716576160,"sourceSize":1956,"sourcePath":"Data Structures/Data Structures.md","exportPath":"data-structures/data-structures.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/expression-tree.html":{"title":"Expression Tree","icon":"","description":"Expression trees are a type of binary tree used to represent arithmetic expressions. In an expression tree, each node represents an operator or operand, and the tree's structure reflects the order of operations within the expression. These trees provide a structured way to evaluate and manipulate arithmetic expressions.In an expression tree, each node can be either an operator or an operand. The operator nodes represent arithmetic operations such as addition, subtraction, multiplication, and division. The operand nodes represent the numeric values or variables in the expression.The node structure typically contains three fields:\nValue/Label: The value or label associated with the node. For operator nodes, this represents the operator symbol (+, -, *, /). For operand nodes, it represents the numeric value or variable name.\nLeft Child: A reference to the left child node, which represents the left operand or subexpression.\nRight Child: A reference to the right child node, which represents the right operand or subexpression.\nfor example the tree for the infix notation would be:<img alt=\"expression tree.svg\" src=\"excalidraw/expression-tree.svg\" target=\"_self\">To construct an expression tree from an arithmetic expression, we typically use one of two approaches: prefix notation (also known as Polish notation) or postfix notation (also known as Reverse Polish notation).","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Expression Trees","level":1,"id":"Expression_Trees_0"},{"heading":"Node Structure","level":2,"id":"Node_Structure_0"},{"heading":"Constructing an Expression Tree","level":2,"id":"Constructing_an_Expression_Tree_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/expression-tree.html","pathToRoot":"..","attachments":["excalidraw/expression-tree.svg"],"createdTime":1690545991576,"modifiedTime":1706563115229,"sourceSize":10174,"sourcePath":"Data Structures/Expression Tree.md","exportPath":"data-structures/expression-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/hashing.html":{"title":"Hashing","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/hashing-data-structure/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/hashing-data-structure/\" target=\"_self\">GFG</a>Normally you'd have to search through the whole list of words every time you wanted to check if a word is there. That can take a long time if there are lots of words.With hashing, instead of storing the actual words, you store them based on a \"hash code\" you calculate from each word. For example, you might take the first letter of each word and convert it to a number:\napple → 1\nbanana → 2\norange → 3\nYou store these hash codes in a hash table. To check if a word is there, you just calculate its hash code and see if it's in the table. Way faster than searching through all the words!The hash code is called a \"hash function\". It's important that it maps words evenly across the hash table, and no two words get the same hash code.\nHash codes aren't usually as simple as taking the first letter. Usually it involves doing some math on the letters of the word to scramble them up into a number. A good hash function should be fast and also distribute the data evenly in memory space we can work with which is called Uniform Hash Function.if in each cell we have data where is time complexity is , the proof is simple… .","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Hashing","level":1,"id":"Hashing_1"},{"heading":"Hash function","level":2,"id":"Hash_function_0"},{"heading":"Time complexity","level":3,"id":"Time_complexity_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/hashing.html","pathToRoot":"..","attachments":[],"createdTime":1690545991629,"modifiedTime":1706561600000,"sourceSize":2573,"sourcePath":"Data Structures/Hashing.md","exportPath":"data-structures/hashing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/heaps-&-priority-queues.html":{"title":"Heaps & Priority Queues","icon":"","description":"A heap is a specialized tree-based data structure that satisfies the heap property - the value of each node is greater than or equal to (or less than or equal to) the values of its children. There are two common types of heaps:\nMax heap - Parent nodes are greater than or equal to children\nMin heap - Parent nodes are less than or equal to children\nHeaps are typically implemented using arrays, where:\nThe root element is at array For any node at array : Its left child is at array Its right child is at array Heaps are useful for implementing priority queues, as they allow efficient retrieval and removal of the maximum/minimum element.C++ provides several heap functions in the &lt;algorithm&gt; header:","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Heaps &amp; Priority Queues","level":1,"id":"Heaps_&_Priority_Queues_0"},{"heading":"Heaps and Priority Queues in C++","level":1,"id":"Heaps_and_Priority_Queues_in_C++_0"},{"heading":"Heaps","level":2,"id":"Heaps_0"},{"heading":"Heap Functions in C++","level":3,"id":"Heap_Functions_in_C++_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/heaps-&-priority-queues.html","pathToRoot":"..","attachments":[],"createdTime":1695752418964,"modifiedTime":1695752421914,"sourceSize":2943,"sourcePath":"Data Structures/Heaps & Priority Queues.md","exportPath":"data-structures/heaps-&-priority-queues.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/insertion-sort.html":{"title":"Insertion sort","icon":"","description":"# Function to do insertion sort\ndef insertionSort(arr): # Traverse through 1 to len(arr) for i in range(1, len(arr)): key = arr[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j &gt;= 0 and key &lt; arr[j] : arr[j + 1] = arr[j] j -= 1 arr[j + 1] = key # Driver code to test above\narr = [12, 11, 13, 5, 6]\ninsertionSort(arr)\nfor i in range(len(arr)): print (\"% d\" % arr[i]) best case: does not matter that much.\nworst case: this is the important one.\naverage case: in most algorithms this can not be calculated but here in particular we can.\ndef binary_search(arr, val, start, end): # we need to distinguish whether we # should insert before or after the # left boundary. imagine [0] is the last # step of the binary search and we need # to decide where to insert -1 if start == end: if arr[start] &gt; val: return start else: return start+1 # this occurs if we are moving # beyond left's boundary meaning # the left boundary is the least # position to find a number greater than val if start &gt; end: return start mid = (start+end)//2 if arr[mid] &lt; val: return binary_search(arr, val, mid+1, end) elif arr[mid] &gt; val: return binary_search(arr, val, start, mid-1) else: return mid def insertion_sort(arr): for i in range(1, len(arr)): val = arr[i] j = binary_search(arr, val, 0, i-1) arr = arr[:j] + [val] + arr[j:i] + arr[i+1:] return arr print(\"Sorted array:\")\nprint(insertion_sort([37, 23, 0, 31, 22, 17, 12, 72, 31, 46, 100, 88, 54])) this is faster and will result in a better algorithm but its not significant improvement cause the power of n remains 2 but coefficient is less.\nwe know the power of n is the most important thing for example an algorithm with complexity equal to is a lot slower than a algorithm with complexity .<a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%A7%D9%88%D9%84-%D8%B2%D9%85%D8%A7%D9%86-%D8%A7%D8%AC%D8%B1%D8%A7%DB%8C-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D9%85%D8%B1%D8%AA%D8%A8/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%A7%D9%88%D9%84-%D8%B2%D9%85%D8%A7%D9%86-%D8%A7%D8%AC%D8%B1%D8%A7%DB%8C-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D9%85%D8%B1%D8%AA%D8%A8/\" target=\"_self\">session 1</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Insertion sort:","level":1,"id":"Insertion_sort_1"},{"heading":"to calculate the time it takes this program to sort an algorithm we consider 3 case:","level":3,"id":"to_calculate_the_time_it_takes_this_program_to_sort_an_algorithm_we_consider_3_case_0"},{"heading":"Binary sort:","level":1,"id":"Binary_sort_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/insertion-sort.html","pathToRoot":"..","attachments":[],"createdTime":1690545991486,"modifiedTime":1691392788800,"sourceSize":2536,"sourcePath":"Data Structures/Insertion sort.md","exportPath":"data-structures/insertion-sort.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/linear-sorting-algorithm.html":{"title":"Linear Sorting Algorithm","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/counting-sort/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/counting-sort/\" target=\"_self\">GFG</a>Counting sort is a sorting technique based on keys between a specific range. It works by counting the number of objects having distinct key values (a kind of hashing). Then do some arithmetic operations to calculate the position of each object in the output sequence.lets assume we have an array where the biggest number in this array is less than or equal to n:# in this example n is 9.\na[] = {1,2,2,2,2,3,3,5,7,2,8,4,6,7,4,9};\nwe start by counting the occurences of each number and store them in a secondary array:for i in a: b[i] += 1\nthen we store the last position of the number i in the sorted array a in a different array called c or we can also overwrite b because we don’t need it anymore:for i in range(1,n): b[i] +=b[i-1]\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Linear Sorting Algorithm","level":1,"id":"Linear_Sorting_Algorithm_1"},{"heading":"Counting Sort","level":2,"id":"Counting_Sort_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/linear-sorting-algorithm.html","pathToRoot":"..","attachments":[],"createdTime":1691390477709,"modifiedTime":1696098452000,"sourceSize":5188,"sourcePath":"Data Structures/Linear Sorting Algorithm.md","exportPath":"data-structures/linear-sorting-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/lists.html":{"title":"Lists","icon":"","description":"is a way to save data which is efficient and what count as efficient differs from each case.\neach DS has some basic methods that we expect it to have. for example if we take a list as an example:\naddFirst(), addLast(), insertAt(), remove(),...it is vital that we implement these methods such they are efficient in both memory and speed at the same time.in most data structures decreasing time is equal to more allocation of more memory so we need to find the sweet spot in order to make a good data structure.if we want to add to an array we need to change and shift the whole array so we came up with a better solution called link list where each node has a pointer to the node after it(also before it).function reverseLinkedList(head): // initialize variables prev = null curr = head // loop through the linked list while curr is not null: // save the next node nextNode = curr.next // reverse the current node's pointer curr.next = prev // move pointers to the next nodes prev = curr curr = nextNode // update the head of the linked list head = prev // return the reversed linked list return head\nsee this <a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Josephus_problem\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Josephus_problem\" target=\"_self\">link</a> to get a better understanding of the problem.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Data Structure:","level":1,"id":"Data_Structure_0"},{"heading":"Linked List:","level":2,"id":"Linked_List_0"},{"heading":"write a program to reverse a linked list:","level":4,"id":"write_a_program_to_reverse_a_linked_list_0"},{"heading":"Josephus:","level":4,"id":"Josephus_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/lists.html","pathToRoot":"..","attachments":[],"createdTime":1690545991744,"modifiedTime":1691392737396,"sourceSize":4371,"sourcePath":"Data Structures/Lists.md","exportPath":"data-structures/lists.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/master-theorem.html":{"title":"Master theorem","icon":"","description":"general formula is:\nwe assume and because otherwise we can't solve for . now by knowing this we can guess for any function and then prove it by induction for our initial guess.<a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%BE%D9%86%D8%AC%D9%85-%D9%82%D8%B6%DB%8C%D9%87-%D8%A7%D8%B5%D9%84%DB%8C/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%BE%D9%86%D8%AC%D9%85-%D9%82%D8%B6%DB%8C%D9%87-%D8%A7%D8%B5%D9%84%DB%8C/\" target=\"_self\">session 5</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Master theorem:","level":1,"id":"Master_theorem_1"},{"heading":"we have 3 cases for every <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D453 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span> :","level":2,"id":"we_have_3_cases_for_every_$f(n)$__0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/master-theorem.html","pathToRoot":"..","attachments":[],"createdTime":1690545991710,"modifiedTime":1706561599000,"sourceSize":952,"sourcePath":"Data Structures/Master theorem.md","exportPath":"data-structures/master-theorem.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/open-addressing.html":{"title":"Open Addressing","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/open-addressing-collision-handling-technique-in-hashing/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/open-addressing-collision-handling-technique-in-hashing/\" target=\"_self\">GFG</a> | <a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Open_addressing\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Open_addressing\" target=\"_self\">wiki</a>Like separate chaining, open addressing is a method for handling collisions. In Open Addressing, all elements are stored in the hash table itself. So at any point, the size of the table must be greater than or equal to the total number of keys (Note that we can increase table size by copying old data if needed).&nbsp;This approach is also known as closed hashing. This entire procedure is based upon probing. We will understand the types of probing ahead:\nInsert(k): Keep probing until an empty slot is found. Once an empty slot is found, insert k.&nbsp;\nSearch(k): Keep probing until the slot’s key doesn’t become equal to k or an empty slot is reached.&nbsp;\nDelete(k): Delete operation is interesting. If we simply delete a key, then the search may fail. So slots of deleted keys are marked specially as “deleted”. The insert can insert an item in a deleted slot, but the search doesn’t stop at a deleted slot which make the search take longer and reduces performance.\nIn linear probing, the hash table is searched sequentially that starts from the original location of the hash. If in case the location that we get is already occupied, then we check for the next location.Linear probing is a collision handling technique used in hashing, where the algorithm looks for the next available slot in the hash table to store the collided key. Some of the applications of linear probing include:\nSymbol tables: Linear probing is commonly used in symbol tables, which are used in compilers and interpreters to store variables and their associated values. Since symbol tables can grow dynamically, linear probing can be used to handle collisions and ensure that variables are stored efficiently.\nCaching: Linear probing can be used in caching systems to store frequently accessed data in memory. When a cache miss occurs, the data can be loaded into the cache using linear probing, and when a collision occurs, the next available slot in the cache can be used to store the data.\nDatabases: Linear probing can be used in databases to store records and their associated keys. When a collision occurs, linear probing can be used to find the next available slot to store the record.\nCompiler design: Linear probing can be used in compiler design to implement symbol tables, error recovery mechanisms, and syntax analysis.\nSpell checking: Linear probing can be used in spell-checking software to store the dictionary of words and their associated frequency counts. When a collision occurs, linear probing can be used to store the word in the next available slot. Primary Clustering: One of the problems with linear probing is Primary clustering, many consecutive elements form groups and it starts taking time to find a free slot or to search for an element.\nSecondary Clustering: Secondary clustering is less severe, two records only have the same collision chain (Probe Sequence) if their initial position is the same.\nIf you observe carefully, then you will understand that the interval between probes will increase proportionally to the hash value. Quadratic probing is a method with the help of which we can solve the problem of clustering that was discussed above. &nbsp;This method is also known as the mid-square method. In this method, we look for the i2-th slot in the i-th iteration.&nbsp;We always start from the original hash location. If only the location is occupied then we check the other slots.let hash(x) be the slot index computed using hash function. &nbsp;\nIf slot hash(x) % S is full, then we try (hash(x) + 1*1) % S If (hash(x) + 1*1) % S is also full, then we try (hash(x) + 2*2) % S If (hash(x) + 2*2) % S is also full, then we try (hash(x) + 3*3) % S\n<br><a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/double-hashing/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/double-hashing/\" target=\"_self\">GFG</a>&nbsp;The intervals that lie between probes are computed by another hash function. Double hashing is a technique that reduces clustering in an optimized way. In this technique, the increments for the probing sequence are computed by using another hash function. We use another hash function hash2(x) and look for the i * hash2(x) slot in the i-th rotation.let hash(x) be the slot index computed using hash function. &nbsp;\nIf slot hash(x) % S is full, then we try (hash(x) + 1*hash2(x)) % S If (hash(x) + 1*hash2(x)) % S is also full, then we try (hash(x) + 2*hash2(x)) % S If (hash(x) + 2*hash2(x)) % S is also full, then we try (hash(x) + 3*hash2(x)) % S\nif we face a growth in our data which is larger than the prior capacity we can handle this by using the same method we used in creating Dynamic Arrays by doubling the capacity and copying the data to the new Array.\nif we want to calculate the time we spend to do such a task we face 2 cases:now if we assume we want to insert numbers to an Array we calculate the time it takes like below:which is a good time, although some times we are spending a lot of time and other time we are spending less time the final complexity is reasonable. if we search for an element that does not exist in the data that we have when we hash it and start searching for it the average time that it is going to take is equal to :<br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%BE%D8%A7%D9%86%D8%B2%D8%AF%D9%87%D9%85-%D8%AF%D8%B1%D9%87%D9%85-%D8%B3%D8%A7%D8%B2%DB%8C-%D8%A2%D8%AF%D8%B1%D8%B3-%D8%AF%D9%87%DB%8C-%D8%A8%D8%A7%D8%B2-%D8%AF%D8%B1%D9%87%D9%85/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%BE%D8%A7%D9%86%D8%B2%D8%AF%D9%87%D9%85-%D8%AF%D8%B1%D9%87%D9%85-%D8%B3%D8%A7%D8%B2%DB%8C-%D8%A2%D8%AF%D8%B1%D8%B3-%D8%AF%D9%87%DB%8C-%D8%A8%D8%A7%D8%B2-%D8%AF%D8%B1%D9%87%D9%85/\" target=\"_self\">session 15</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Open Addressing","level":1,"id":"Open_Addressing_1"},{"heading":"Different ways of Open Addressing:","level":2,"id":"Different_ways_of_Open_Addressing_0"},{"heading":"1. Linear Probing:","level":3,"id":"1._Linear_Probing_0"},{"heading":"Applications of linear probing:","level":4,"id":"Applications_of_linear_probing_0"},{"heading":"Challenges in Linear Probing :","level":4,"id":"Challenges_in_Linear_Probing__0"},{"heading":"2. Quadratic Probing","level":3,"id":"2._Quadratic_Probing_0"},{"heading":"3. Double Hashing","level":3,"id":"3._Double_Hashing_0"},{"heading":"Time Complexity","level":2,"id":"Time_Complexity_0"},{"heading":"Inserting Elements","level":3,"id":"Inserting_Elements_0"},{"heading":"Time complexity for a failed search","level":3,"id":"Time_complexity_for_a_failed_search_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/open-addressing.html","pathToRoot":"..","attachments":[],"createdTime":1690545991633,"modifiedTime":1706563455494,"sourceSize":6490,"sourcePath":"Data Structures/Open Addressing.md","exportPath":"data-structures/open-addressing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/order.html":{"title":"Order","icon":"","description":"The order of elements in a data structure refers to how the elements are organized and accessed. we start this by a simple example: let assume we want to find the maximum value in an array of n numbers. we can do this by using a for loop and it takes us comparison operations to achieve.\nnow what about finding minimum and maximum at the same time? if we want to probe the array linearly and do 2 comparison operations for every the amount of operations add up to : but here is a different approach we can compare 2 numbers times and separate them into 2 groups, now we look for maximum in the bigger numbers group and minimum in the other one with this algorithm the number of operations is : which is faster than the previous algorithm. There are three main types of order in data structures:In a linear data structure, elements are organized sequentially one after the other. Examples include:\nArrays\nLinked Lists\nStacks\nQueues\nElements in these data structures are accessed in a strict linear order - from beginning to end.In a hierarchical data structure, elements have a nested parent-child relationship. Examples include:\nTrees (<a data-href=\"Trees\" href=\"data-structures/trees.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Trees</a>)\nGraphs\nElements are organized in a hierarchy with root nodes at the top and child nodes nested under parent nodes. Elements can be accessed through various traversal methods like depth-first or breadth-first.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Order in Data Structures","level":1,"id":"Order_in_Data_Structures_0"},{"heading":"Linear Order","level":2,"id":"Linear_Order_0"},{"heading":"Hierarchical Order","level":2,"id":"Hierarchical_Order_0"}],"links":["data-structures/trees.html#_0"],"author":"","coverImageURL":"","fullURL":"data-structures/order.html","pathToRoot":"..","attachments":[],"createdTime":1691390477616,"modifiedTime":1696098455000,"sourceSize":2372,"sourcePath":"Data Structures/Order.md","exportPath":"data-structures/order.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/priority-queues.html":{"title":"Priority Queues","icon":"","description":"A priority queue is a data structure that maintains a set of elements, each with an associated priority. Elements are ordered by their priority, with the highest priority element at the front.Some key properties of priority queues:\nElements are removed from the queue in priority order; the element with the highest priority is removed first.\nNew elements can be inserted at any time. They are inserted in the proper position based on priority.\nCommon operations are: enqueue(element) - Inserts an element\ndequeue() - Removes and returns highest priority element\npeek() - Gets highest priority element without removing\nisEmpty() - Checks if queue is empty\nsize() - Returns number of elements Priority queues are commonly implemented using:\nHeaps - Provide O(log n) time for enqueue and dequeue operations. Easy to implement with arrays.\nBinary search trees - Operations take O(log n) time. Provides flexibility for ordering.\nArrays - Simple implementation but operations take O(n) time.\nHeaps are the most common because they provide fast querying and insertion.#include &lt;iostream&gt;\nusing namespace std; struct Node { int data; int priority; Node* next;\n}; class PriorityQueue {\nprivate: Node* head; public: PriorityQueue() : head(nullptr) {} void push(int data, int priority) { Node* newNode = createNode(data, priority); if (head == nullptr) { head = newNode; } else if (priority &lt; head-&gt;priority) { newNode-&gt;next = head; head = newNode; } else { Node* current = head; while (current-&gt;next &amp;&amp; priority &gt;= current-&gt;next-&gt;priority) { current = current-&gt;next; } newNode-&gt;next = current-&gt;next; current-&gt;next = newNode; } } int pop() { if (isEmpty()) { cout &lt;&lt; \"Priority Queue is empty!\" &lt;&lt; endl; return -1; } Node* temp = head; int data = temp-&gt;data; head = head-&gt;next; delete temp; return data; } int peek() { if (isEmpty()) { cout &lt;&lt; \"Priority Queue is empty!\" &lt;&lt; endl; return -1; } return head-&gt;data; } bool isEmpty() { return head == nullptr; } private: Node* createNode(int data, int priority) { Node* newNode = new Node; newNode-&gt;data = data; newNode-&gt;priority = priority; newNode-&gt;next = nullptr; return newNode; }\n}; int main() { PriorityQueue pq; pq.push(4, 1); pq.push(5, 2); pq.push(6, 3); pq.push(7, 0); while (!pq.isEmpty()) { cout &lt;&lt; pq.peek() &lt;&lt; \" \"; pq.pop(); } return 0;\n}\nPriority queues are used for:\nScheduling - OS uses priority queues to schedule processes based on priority.\nAlgorithms - Dijkstra's algorithm uses a priority queue to find shortest path.\nData compression - Huffman coding uses priority queues to build optimal prefix code.\nPriority queues provide an abstract data type that can order elements by importance. This makes them very versatile for solving problems involving priority!<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Min-max_heap\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Min-max_heap\" target=\"_self\">wiki</a>A min-max heap is a variant of the binary heap data structure that efficiently supports both min and max operations. It combines properties of a min-heap and a max-heap.\nEach node has a value, min, and max attribute. The value is the actual value of the node. Min and max track the minimum and maximum values in the subtree rooted at that node.\nThe root node's min attribute equals the minimum value in the entire heap. Its max attribute equals the maximum value.\nWhen inserting or removing nodes, the min and max attributes must be updated along the path from the node to the root. This maintains the min/max invariant.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Priority Queues","level":1,"id":"Priority_Queues_1"},{"heading":"Implementation","level":2,"id":"Implementation_0"},{"heading":"Applications","level":2,"id":"Applications_0"},{"heading":"Complexities Of All The Operations:","level":2,"id":"Complexities_Of_All_The_Operations_0"},{"heading":"Min-max heap","level":1,"id":"Min-max_heap_0"},{"heading":"Some key properties of min-max heaps:","level":2,"id":"Some_key_properties_of_min-max_heaps_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/priority-queues.html","pathToRoot":"..","attachments":[],"createdTime":1690545991601,"modifiedTime":1696100531326,"sourceSize":7378,"sourcePath":"Data Structures/Priority Queues.md","exportPath":"data-structures/priority-queues.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/recusrsion.html":{"title":"Recusrsion","icon":"","description":"we need to come up with a approximation for and first we calculate and it does not work so we go stepper higher and calculate and for it is correct.\nbut we are not sure for what approximation exactly does this work for where i is between 1 and 2.\nfor now we say till we prove it later.we prove this by induction: and it is obvious by Simplifying the .same goes for this part of the equation.\nby combining 1 and 2 steps we can prove .\nit's a bit long so I included this <a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/how-to-solve-time-complexity-recurrence-relations-using-recursion-tree-method/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/how-to-solve-time-complexity-recurrence-relations-using-recursion-tree-method/\" target=\"_self\">link</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Recursive functions:","level":1,"id":"Recursive_functions_0"},{"heading":"First step we need to prove that  <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D447 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c1D442 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-n\" space=\"2\"><mjx-c class=\"mjx-c6C\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c67\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2061\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span> :","level":3,"id":"First_step_we_need_to_prove_that_$T(n)_=_O(n\\log(n))$__0"},{"heading":"Second step is to Prove <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D447 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3A9\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-n\" space=\"2\"><mjx-c class=\"mjx-c6C\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c67\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2061\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span> :","level":3,"id":"Second_step_is_to_Prove_$T(n)_=_\\Omega(n\\log(n))$__0"},{"heading":"Recursion Tree method:","level":2,"id":"Recursion_Tree_method_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/recusrsion.html","pathToRoot":"..","attachments":[],"createdTime":1690545991691,"modifiedTime":1691392764531,"sourceSize":1971,"sourcePath":"Data Structures/Recusrsion.md","exportPath":"data-structures/recusrsion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/red-black-tree.html":{"title":"Red-Black Tree","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/introduction-to-red-black-tree/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/introduction-to-red-black-tree/\" target=\"_self\">GFG</a>A red-black tree is a self-balancing binary search tree with useful properties for efficient lookups and updates.\nEvery node is either red or black The root is always black\nEvery leaf (null) is black\nIf a node is red, then both children are black\nthe children of a red node are black. Hence possible parent of red node is a black node.\nAll paths from a node to leaves have the same number of black nodes\n<br><img alt=\"Red-Black Tree.excalidraw.svg\" src=\"excalidraw/red-black-tree.excalidraw.svg\" target=\"_self\">These properties ensure:\nBalanced tree O(log n) time for operations\nMost of the BST operations (e.g., search, max, min, insert, delete.. etc.) take O(h) time where h is the height of the BST. If we make sure that the height of the tree remains O(log n) after every insertion and deletion, then we can guarantee an upper bound of for all these operations. The height of a Red-Black tree is always O(log n) where n is the number of nodes in the tree.&nbsp;","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Red-Black Tree","level":1,"id":"Red-Black_Tree_1"},{"heading":"Properties","level":2,"id":"Properties_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/red-black-tree.html","pathToRoot":"..","attachments":["excalidraw/red-black-tree.excalidraw.svg"],"createdTime":1691390477733,"modifiedTime":1706562705522,"sourceSize":6902,"sourcePath":"Data Structures/Red-Black Tree.md","exportPath":"data-structures/red-black-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/sorting-algorithms.html":{"title":"Sorting Algorithms","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/sorting-algorithms/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/sorting-algorithms/\" target=\"_self\">GFG</a> | <a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Sorting_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Sorting_algorithm\" target=\"_self\">wiki</a>A Sorting Algorithm is used to rearrange a given array or list of elements according to a comparison operator on the elements. The comparison operator is used to decide the new order of elements in the respective data structure.the sorting algorithms we talked about in the beginning where base on comparison now we want to see if we can sort an array without comparing the elements an if it is even possible to sort without comparison?. YES it is possible lets see an example: assume we have an array where the value in each cell is an integer let say the numbers are (1,4,2,5) now we create another array of size 5 and read every element from the initial array and write them in the cell where the index is equal to the value of the cell in the original array. as you can see with have sorted the array with zero comparison.A sorting algorithm is said to be stable if two objects with equal keys appear in the same order in sorted output as they appear in the input data set<br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%87%D9%81%D8%AF%D9%87%D9%85-%D9%85%D8%B1%D8%AA%D8%A8-%D8%B3%D8%A7%D8%B2%DB%8C/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%87%D9%81%D8%AF%D9%87%D9%85-%D9%85%D8%B1%D8%AA%D8%A8-%D8%B3%D8%A7%D8%B2%DB%8C/\" target=\"_self\">session 17</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Sorting Algorithms","level":1,"id":"Sorting_Algorithms_1"},{"heading":"Stable or Unstable algorithms","level":2,"id":"Stable_or_Unstable_algorithms_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/sorting-algorithms.html","pathToRoot":"..","attachments":[],"createdTime":1691390477652,"modifiedTime":1695751911047,"sourceSize":4408,"sourcePath":"Data Structures/Sorting Algorithms.md","exportPath":"data-structures/sorting-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/stacks-&-queues.html":{"title":"Stacks & Queues","icon":"","description":"A Stack is a linear data structure in which the insertion of a new element and removal of an existing element takes place at the same end represented as the top of the stack.<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/possible-permutations-at-a-railway-track/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/possible-permutations-at-a-railway-track/\" target=\"_self\">railway track</a>A Queue is defined as a linear data structure that is open at both ends and the operations are performed in First In First Out (FIFO) order.<br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/queue-data-structure/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/queue-data-structure/\" target=\"_self\">Queue</a><br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%87%D8%B4%D8%AA%D9%85-%D9%BE%D8%B4%D8%AA%D9%87-%D8%B5%D9%81/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D9%87%D8%B4%D8%AA%D9%85-%D9%BE%D8%B4%D8%AA%D9%87-%D8%B5%D9%81/\" target=\"_self\">session 8</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Stacks &amp; Queues","level":1,"id":"Stacks_&_Queues_0"},{"heading":"Stacks &amp; Queues:","level":1,"id":"Stacks_&_Queues_1"},{"heading":"Stack:","level":2,"id":"Stack_0"},{"heading":"Queue:","level":2,"id":"Queue_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/stacks-&-queues.html","pathToRoot":"..","attachments":[],"createdTime":1690545991750,"modifiedTime":1706561603000,"sourceSize":839,"sourcePath":"Data Structures/Stacks & Queues.md","exportPath":"data-structures/stacks-&-queues.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/time-complexity-comparison.html":{"title":"Time Complexity Comparison","icon":"","description":" : : : = : : we can write the same thing for .\nin order to prove is not greater than we need to prove\nthat for all the , we have .\n<a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%AF%D9%88%D9%85-%D9%85%D9%82%D8%A7%DB%8C%D8%B3%D9%87-%D8%B2%D9%85%D8%A7%D9%86-%D8%A7%D8%AC%D8%B1%D8%A7%DB%8C-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%B1%D8%B4%D8%AF/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%AC%D9%84%D8%B3%D9%87-%D8%AF%D9%88%D9%85-%D9%85%D9%82%D8%A7%DB%8C%D8%B3%D9%87-%D8%B2%D9%85%D8%A7%D9%86-%D8%A7%D8%AC%D8%B1%D8%A7%DB%8C-%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85-%D8%B1%D8%B4%D8%AF/\" target=\"_self\">session 2</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"comparing functions in here has new symbols:","level":1,"id":"comparing_functions_in_here_has_new_symbols_0"},{"heading":"Definition for <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D442 TEX-I\"></mjx-c></mjx-mi></mjx-math></mjx-container></span> :","level":3,"id":"Definition_for_$O$__0"},{"heading":"Definition for <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D703 TEX-I\"></mjx-c></mjx-mi></mjx-math></mjx-container></span> :","level":3,"id":"Definition_for_$\\theta$__0"},{"heading":"Definition for <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45C TEX-I\"></mjx-c></mjx-mi></mjx-math></mjx-container></span> :","level":3,"id":"Definition_for_$o$__0"},{"heading":"or we can write this instead:","level":6,"id":"or_we_can_write_this_instead_0"},{"heading":"Definition for <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D714 TEX-I\"></mjx-c></mjx-mi></mjx-math></mjx-container></span> :","level":3,"id":"Definition_for_$\\omega$__0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/time-complexity-comparison.html","pathToRoot":"..","attachments":[],"createdTime":1690545991637,"modifiedTime":1691392783155,"sourceSize":1534,"sourcePath":"Data Structures/Time Complexity Comparison.md","exportPath":"data-structures/time-complexity-comparison.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/tree-and-trie.html":{"title":"Tree and Trie","icon":"","description":"struct TreeNode { int val; TreeNode* left; TreeNode* right; TreeNode(int x) : val(x), left(NULL), right(NULL) {}\n}; class Tree {\npublic: TreeNode* root; Tree() : root(NULL) {} void insert(int val) { if (root == NULL) { root = new TreeNode(val); return; } TreeNode* cur = root; while (true) { if (val &lt; cur-&gt;val) { if (cur-&gt;left == NULL) { cur-&gt;left = new TreeNode(val); return; } else { cur = cur-&gt;left; } } else { if (cur-&gt;right == NULL) { cur-&gt;right = new TreeNode(val); return; } else { cur = cur-&gt;right; } } } }\n};\n<a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Trie\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Trie\" target=\"_self\">link</a>\na trie is a rooted tree that represents a set of strings. From a computer science perspective, it's a data structure used for efficient string operations, commonly used in applications such as spell checkers, search engines, and network routing algorithms.structure Node Children Node[Alphabet-Size] Is-Terminal Boolean Value Data-Type\nend structure\n<br><a data-tooltip-position=\"top\" aria-label=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-%D8%AC%D9%84%D8%B3%D9%87-%D8%AF%D9%87%D9%85-%D8%AF%D8%B1%D8%AE%D8%AA-%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://maktabkhooneh.org/course/%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87-%D9%87%D8%A7-mk118/%D9%81%DB%8C%D9%84%D9%85-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-ch166/%D9%88%DB%8C%D8%AF%DB%8C%D9%88-%D8%A2%D9%85%D9%88%D8%B2%D8%B4%DB%8C-%D8%AC%D9%84%D8%B3%D9%87-%D8%AF%D9%87%D9%85-%D8%AF%D8%B1%D8%AE%D8%AA-%D8%B3%D8%A7%D8%AE%D8%AA%D9%85%D8%A7%D9%86-%D8%AF%D8%A7%D8%AF%D9%87/\" target=\"_self\">session 10</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Tree","level":1,"id":"Tree_0"},{"heading":"implementation of tree:","level":2,"id":"implementation_of_tree_0"},{"heading":"Trie:","level":2,"id":"Trie_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/tree-and-trie.html","pathToRoot":"..","attachments":[],"createdTime":1690545991524,"modifiedTime":1704139702000,"sourceSize":1580,"sourcePath":"Data Structures/Tree and Trie.md","exportPath":"data-structures/tree-and-trie.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"data-structures/trees.html":{"title":"Trees","icon":"","description":"<a data-tooltip-position=\"top\" aria-label=\"https://www.geeksforgeeks.org/tree-traversals-inorder-preorder-and-postorder/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/tree-traversals-inorder-preorder-and-postorder/\" target=\"_self\">GFG</a>A tree is a hierarchical data structure consisting of nodes connected by edges. It is a widely-used data structure in computer science and is used in many algorithms and applications.\nNode: A single element in a tree.\nEdge: A line connecting two nodes.\nRoot: The topmost node in a tree.\nParent: A node that has one or more child nodes.\nChild: A node that is connected to a parent node by an edge.\nLeaf: A node that has no child nodes.\nHeight: The number of edges on the longest path from the root to a leaf node.\nThere are many different types of trees, each with its own unique characteristics. Here are some of the most common types of trees:A free tree is a tree where each node can have any number of child nodes. In other words, there are no restrictions on the number of child nodes a node can have. Free trees are also known as general trees.A rooted tree is a tree where one node is designated as the root node. The root node is typically drawn at the top of the tree. All other nodes in the tree are either parent nodes, child nodes, or leaf nodes.A binary tree is a tree where each node can have at most two child nodes. The child nodes are typically referred to as the left child and the right child. Binary trees are commonly used in data structures such as binary search trees and heaps.A binary search tree is a binary tree where the left child of a node is less than the node and the right child of a node is greater than the node. This property makes binary search trees useful for searching and sorting operations.An AVL tree is a binary search tree where the difference in height between the left and right subtrees of any node is at most one. This property ensures that the tree is balanced, which leads to faster search and insertion times.A red-black tree is a binary search tree with the following properties:\nEach node is either red or black.\nThe root node is black.\nAll leaf nodes (i.e., null nodes) are black.\nIf a node is red, then both of its child nodes are black.\nEvery path from a node to a leaf node contains the same number of black nodes.\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Trees in Data Structure:","level":1,"id":"Trees_in_Data_Structure_0"},{"heading":"Basic Terminology:","level":2,"id":"Basic_Terminology_0"},{"heading":"Types of Trees:","level":2,"id":"Types_of_Trees_0"},{"heading":"Free Tree:","level":3,"id":"Free_Tree_0"},{"heading":"Rooted Tree:","level":3,"id":"Rooted_Tree_0"},{"heading":"Binary Tree:","level":3,"id":"Binary_Tree_0"},{"heading":"Binary Search Tree:","level":3,"id":"Binary_Search_Tree_0"},{"heading":"AVL Tree:","level":3,"id":"AVL_Tree_0"},{"heading":"Red-Black Tree:","level":3,"id":"Red-Black_Tree_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"data-structures/trees.html","pathToRoot":"..","attachments":[],"createdTime":1690545991768,"modifiedTime":1690046731490,"sourceSize":5071,"sourcePath":"Data Structures/Trees.md","exportPath":"data-structures/trees.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"git/git.html":{"title":"Git","icon":"","description":"\n<a data-href=\"Git Cheat Sheet\" href=\"git/git-cheat-sheet.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Git Cheat Sheet</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://education.github.com/git-cheat-sheet-education.pdf\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://education.github.com/git-cheat-sheet-education.pdf\" target=\"_self\">cheat sheet</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Git","level":1,"id":"Git_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["git/git-cheat-sheet.html#_0"],"author":"","coverImageURL":"","fullURL":"git/git.html","pathToRoot":"..","attachments":[],"createdTime":1690547585357,"modifiedTime":1691392897620,"sourceSize":144,"sourcePath":"Git/Git.md","exportPath":"git/git.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"git/git-cheat-sheet.html":{"title":"Git Cheat Sheet","icon":"","description":"git status\nShow modified files in working directory, staged for your next commit.git add [file]\nAdd a file as it looks now to your next commit (stage).git reset [file]\nUnstage a file while retaining the changes in working directory.git diff\nDiff of what is changed but not staged.git diff --staged\nDiff of what is staged but not yet committed.git commit -m “[descriptive message]”\nCommit your staged content as a new commit snapshot.git config --global user.name “[firstname lastname]”\nSet a name that is identifiable for credit when reviewing version history.git config --global user.email “[valid-email]”\nSet an email address that will be associated with each history marker.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"GIT CHEAT SHEET","level":1,"id":"GIT_CHEAT_SHEET_0"},{"heading":"STAGE &amp; SNAPSHOT","level":2,"id":"STAGE_&_SNAPSHOT_0"},{"heading":"SETUP","level":2,"id":"SETUP_0"},{"heading":"Configuring user information used across all local repositories","level":3,"id":"Configuring_user_information_used_across_all_local_repositories_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"git/git-cheat-sheet.html","pathToRoot":"..","attachments":[],"createdTime":1690545992007,"modifiedTime":1690553369846,"sourceSize":5117,"sourcePath":"Git/Git Cheat Sheet.md","exportPath":"git/git-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"graph/dijkstra.html":{"title":"Dijkstra","icon":"","description":"Dijkstra's algorithm is a popular graph traversal algorithm used to find the shortest path between nodes in a graph with non-negative edge weights. It guarantees the shortest path from a starting node to all other nodes in the graph.\nInitialize the algorithm by setting the starting node as the current node.\nAssign a tentative distance value of zero to the starting node and infinity to all other nodes. The tentative distance represents the shortest distance from the starting node to each node.\nSet the current node as visited.\nFor each neighbor of the current node, calculate the tentative distance by adding the weight of the current node to the distance to the neighbor. If the tentative distance is less than the current distance assigned to the neighbor, update the neighbor's distance.\nAfter considering all the neighbors, mark the current node as visited.\nIf all nodes have been visited or the smallest tentative distance among the unvisited nodes is infinity, the algorithm terminates.\nOtherwise, select the unvisited node with the smallest tentative distance as the next current node and go back to step 4.\nfunction Dijkstra(graph, start): distances := map of all nodes with infinity value distances[start] := 0 visited := empty set unvisited := set of all nodes while unvisited is not empty: current := node in unvisited with smallest distance for neighbor in neighbors of current: tentativeDistance := distances[current] + distance(current, neighbor) if tentativeDistance &lt; distances[neighbor]: distances[neighbor] := tentativeDistance visited.add(current) unvisited.remove(current) return distances\nDijkstra's algorithm starts by assigning the starting node a distance of zero and all other nodes a distance of infinity. It then iteratively selects the node with the smallest tentative distance and explores its neighbors. If the tentative distance to a neighbor through the current node is smaller than its current distance, the algorithm updates the neighbor's distance.The algorithm continues until all nodes have been visited or the smallest tentative distance among the unvisited nodes is infinity. At the end of the algorithm, the distances map holds the shortest distances from the starting node to all other nodes in the graph.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Dijkstra","level":1,"id":"Dijkstra_1"},{"heading":"Dijkstra's Algorithm","level":2,"id":"Dijkstra's_Algorithm_0"},{"heading":"Algorithm Steps:","level":3,"id":"Algorithm_Steps_0"},{"heading":"Pseudo Code:","level":3,"id":"Pseudo_Code_0"},{"heading":"Explanation:","level":3,"id":"Explanation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"graph/dijkstra.html","pathToRoot":"..","attachments":[],"createdTime":1706713203220,"modifiedTime":1706717715355,"sourceSize":4114,"sourcePath":"Graph/Dijkstra.md","exportPath":"graph/dijkstra.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"graph/graph.html":{"title":"Graph","icon":"","description":"\n<a data-href=\"SCC\" href=\"graph/scc.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">SCC</a>\n<br><a data-href=\"Dijkstra\" href=\"graph/dijkstra.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Dijkstra</a>\n<br><a data-href=\"Max Flow\" href=\"graph/max-flow.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Max Flow</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm\" target=\"_self\">Tarjan</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.youtube.com/watch?v=pSqmAO-m7Lk\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=pSqmAO-m7Lk\" target=\"_self\">Dijkstra</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Graph","level":1,"id":"Graph_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["graph/scc.html#_0","graph/dijkstra.html#_0","graph/max-flow.html#_0"],"author":"","coverImageURL":"","fullURL":"graph/graph.html","pathToRoot":"..","attachments":[],"createdTime":1706713261936,"modifiedTime":1706713712454,"sourceSize":238,"sourcePath":"Graph/Graph.md","exportPath":"graph/graph.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"graph/max-flow.html":{"title":"Max Flow","icon":"","description":"Max flow and min cut are fundamental concepts in network flow theory. They are used to model and analyze the maximum amount of flow that can be sent through a network, as well as the minimum capacity needed to disconnect certain parts of the network. The Ford-Fulkerson algorithm is an algorithm used to find the maximum flow in a network.A network flow is a directed graph where each edge has a capacity representing the maximum amount of flow that can be sent through it. It consists of a source node, a sink node, and intermediate nodes connected by edges with capacities. The goal is to find the maximum amount of flow that can be sent from the source to the sink while respecting the capacity constraints.The Ford-Fulkerson algorithm is an iterative algorithm that finds the maximum flow in a network. It starts with an initial flow of zero and repeatedly augments the flow along the augmenting paths until no more augmenting paths exist. An augmenting path is a path from the source to the sink where each edge has residual capacity (capacity minus current flow) greater than zero.The algorithm finds an augmenting path using a graph traversal technique such as depth-first search (DFS) or breadth-first search (BFS). Once an augmenting path is found, the algorithm determines the maximum amount of flow that can be sent along the path and updates the flow values accordingly.function FordFulkerson(graph, source, sink): residualGraph := createResidualGraph(graph) flow := 0 while there is an augmenting path in residualGraph: augmentingPath := findAugmentingPath(residualGraph, source, sink) bottleneck := findBottleneckCapacity(augmentingPath) updateFlow(residualGraph, augmentingPath, bottleneck) flow := flow + bottleneck return flow\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Max Flow","level":1,"id":"Max_Flow_1"},{"heading":"Max Flow and Min Cut","level":2,"id":"Max_Flow_and_Min_Cut_0"},{"heading":"Network Flow:","level":3,"id":"Network_Flow_0"},{"heading":"Ford-Fulkerson Algorithm:","level":3,"id":"Ford-Fulkerson_Algorithm_0"},{"heading":"Pseudo Code for Ford-Fulkerson Algorithm:","level":3,"id":"Pseudo_Code_for_Ford-Fulkerson_Algorithm_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"graph/max-flow.html","pathToRoot":"..","attachments":[],"createdTime":1706713543747,"modifiedTime":1706717727960,"sourceSize":4518,"sourcePath":"Graph/Max Flow.md","exportPath":"graph/max-flow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"graph/scc.html":{"title":"SCC","icon":"","description":"Tarjan's algorithm is a popular algorithm used to find the strongly connected components (SCCs) in a directed graph. A strongly connected component is a set of vertices where there is a path between every pair of vertices. This algorithm is based on depth-first search (DFS) and uses a stack to keep track of the visited nodes.\nInitialize the algorithm by setting the index and low-link value of each node to -1. Create an empty stack.\nPerform a DFS traversal of the graph, starting from each unvisited node.\nAssign a unique index to each visited node and set its low-link value to the index initially.\nPush the visited node onto the stack.\nFor each neighbor of the current node: If the neighbor has not been visited, recursively perform a DFS on it.\nUpdate the low-link value of the current node using the minimum of its own low-link value and the low-link value of the neighbor. If the current node's low-link value is equal to its index, the current node and all nodes above it in the stack belong to the same SCC. Pop these nodes from the stack and mark them as part of the SCC.\nContinue the DFS traversal until all nodes have been visited.\nfunction TarjanSCC(graph): index := 0 stack := empty stack indexMap := map of nodes to their index values lowLinkMap := map of nodes to their low-link values onStack := set of nodes on the stack SCCs := empty list for each node in graph: if node is unvisited: DFS(node) function DFS(node): indexMap[node] := index lowLinkMap[node] := index index := index + 1 stack.push(node) onStack.add(node) for each neighbor in neighbors of node: if neighbor is unvisited: DFS(neighbor) lowLinkMap[node] := minimum(lowLinkMap[node], lowLinkMap[neighbor]) else if neighbor is on stack: lowLinkMap[node] := minimum(lowLinkMap[node], indexMap[neighbor]) if lowLinkMap[node] == indexMap[node]: SCC := empty list while true: poppedNode := stack.pop() onStack.remove(poppedNode) SCC.add(poppedNode) if poppedNode == node: break SCCs.add(SCC) return SCCs\nTarjan's algorithm uses DFS to visit nodes in the graph and assigns each node an index and low-link value. The index represents the order in which the nodes are visited during the DFS traversal, and the low-link value is the minimum index of any node reachable from the current node.The algorithm maintains a stack to keep track of the visited nodes. When a node is visited, it is pushed onto the stack and marked as on the stack. The algorithm performs a recursive DFS on each unvisited neighbor of the current node, updating the low-link value of the current node based on the low-link values of its neighbors.If the low-link value of a node is equal to its index, it means that the current node and all nodes above it in the stack form a strongly connected component. These nodes are popped from the stack and added to a list representing the SCC.The algorithm continues the DFS traversal until all nodes have been visited. At the end, the algorithm returns a list of SCCs present in the graph.Tarjan's algorithm is efficient for finding SCCs in a graph and has a time complexity of , where V is the number of vertices and is the number of edges in the graph. This time complexity arises from the fact that each node and each edge is visited once during the DFS traversal.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Strongly Connected Components","level":1,"id":"Strongly_Connected_Components_0"},{"heading":"Tarjan's Algorithm for Finding Strongly Connected Components","level":2,"id":"Tarjan's_Algorithm_for_Finding_Strongly_Connected_Components_0"},{"heading":"Algorithm Steps:","level":3,"id":"Algorithm_Steps_0"},{"heading":"Pseudo Code:","level":3,"id":"Pseudo_Code_0"},{"heading":"Explanation:","level":3,"id":"Explanation_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"graph/scc.html","pathToRoot":"..","attachments":[],"createdTime":1706713203217,"modifiedTime":1706717747585,"sourceSize":3790,"sourcePath":"Graph/SCC.md","exportPath":"graph/scc.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"markdown/markdown.html":{"title":"Markdown","icon":"","description":"\n<a data-href=\"MathJax\" href=\"markdown/mathjax.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">MathJax</a>\n<br><a data-href=\"Markdown Cheat Sheet\" href=\"markdown/markdown-cheat-sheet.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Markdown Cheat Sheet</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\" target=\"_self\">markdown cheat sheet</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference\" target=\"_self\">MathJax cheat sheet</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Markdown","level":1,"id":"Markdown_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["markdown/mathjax.html#_0","markdown/markdown-cheat-sheet.html#_0"],"author":"","coverImageURL":"","fullURL":"markdown/markdown.html","pathToRoot":"..","attachments":[],"createdTime":1690547826439,"modifiedTime":1692560978000,"sourceSize":300,"sourcePath":"Markdown/Markdown.md","exportPath":"markdown/markdown.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"markdown/markdown-cheat-sheet.html":{"title":"Markdown Cheat Sheet","icon":"","description":"Thanks for visiting <a data-tooltip-position=\"top\" aria-label=\"https://www.markdownguide.org\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.markdownguide.org\" target=\"_self\">The Markdown Guide</a><br>This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for <a data-tooltip-position=\"top\" aria-label=\"https://www.markdownguide.org/basic-syntax\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.markdownguide.org/basic-syntax\" target=\"_self\">basic syntax</a> and <a data-tooltip-position=\"top\" aria-label=\"https://www.markdownguide.org/extended-syntax\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.markdownguide.org/extended-syntax\" target=\"_self\">extended syntax</a>.<br>\nalso for writing math equation check <a data-href=\"MathJax\" href=\"markdown/mathjax.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">MathJax</a>These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Markdown Cheat Sheet","level":1,"id":"Markdown_Cheat_Sheet_1"},{"heading":"Basic Syntax","level":2,"id":"Basic_Syntax_0"},{"heading":"Heading","level":3,"id":"Heading_0"},{"heading":"H1","level":1,"id":"H1_0"},{"heading":"H2","level":2,"id":"H2_0"},{"heading":"H3","level":3,"id":"H3_0"},{"heading":"Bold","level":3,"id":"Bold_0"}],"links":["markdown/mathjax.html#_0"],"author":"","coverImageURL":"","fullURL":"markdown/markdown-cheat-sheet.html","pathToRoot":"..","attachments":[],"createdTime":1690545992067,"modifiedTime":1691392356000,"sourceSize":1983,"sourcePath":"Markdown/Markdown Cheat Sheet.md","exportPath":"markdown/markdown-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"markdown/mathjax.html":{"title":"MathJax","icon":"","description":"\nInline formulas: \\( ... \\)\nDisplayed formulas: \\[ ... \\]\nCurly braces to group pieces of formulas: { ... }\nSuperscripts: ^, Subscripts: _ \\(\\frac {a} {b}\\): \\({ {x} \\over {y} }\\): Now the table is properly formatted with the MathJax symbols enclosed in appropriate delimiters.Now the symbols are enclosed in MathJax delimiters ($) as expected.\n\\mathrm{...} for Roman font\n\\mathit{...} for italic font\n\\mathbf{...} for bold font\n\\mathbb{...} for blackboard bold font\n\\mathcal{...} for calligraphic font\n\\mathfrak{...} for Fraktur font \\, for a small space\n\\: for a medium space\n\\; for a large space\n\\! for a negative small space \\hat{a} for a hat accent\n\\bar{a} for a bar accent\n\\tilde{a} for a tilde accent\n\\vec{a} for a vector arrow\n\\dot{a} for a single dot accent\n\\ddot{a} for a double dot accent\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"MathJax Cheat Sheet","level":1,"id":"MathJax_Cheat_Sheet_0"},{"heading":"General Syntax","level":3,"id":"General_Syntax_0"},{"heading":"Fractions","level":3,"id":"Fractions_0"},{"heading":"Parentheses","level":3,"id":"Parentheses_0"},{"heading":"Unscaled Parentheses","level":4,"id":"Unscaled_Parentheses_0"},{"heading":"Scaled Parentheses","level":4,"id":"Scaled_Parentheses_0"},{"heading":"Hidden Parentheses","level":4,"id":"Hidden_Parentheses_0"},{"heading":"Logical Symbols","level":3,"id":"Logical_Symbols_0"},{"heading":"Operators","level":3,"id":"Operators_0"},{"heading":"Set Symbols","level":3,"id":"Set_Symbols_0"},{"heading":"Arrows","level":3,"id":"Arrows_0"},{"heading":"Special Symbols","level":3,"id":"Special_Symbols_0"},{"heading":"Trigonometry","level":3,"id":"Trigonometry_0"},{"heading":"Functional Symbols","level":3,"id":"Functional_Symbols_0"},{"heading":"Greek Letters","level":3,"id":"Greek_Letters_0"},{"heading":"Lowercase","level":4,"id":"Lowercase_0"},{"heading":"Uppercase","level":4,"id":"Uppercase_0"},{"heading":"Fonts","level":3,"id":"Fonts_0"},{"heading":"Spaces","level":3,"id":"Spaces_0"},{"heading":"Accents and Marks","level":3,"id":"Accents_and_Marks_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"markdown/mathjax.html","pathToRoot":"..","attachments":[],"createdTime":1690545992083,"modifiedTime":1692560980000,"sourceSize":5812,"sourcePath":"Markdown/MathJax.md","exportPath":"markdown/mathjax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"python/beautiful-soup.html":{"title":"Beautiful Soup","icon":"","description":"Beautiful Soup is one of the most popular libraries for parsing HTML and scraping data from websites in Python. In this tutorial, we'll go through everything you need to know to start a full fledged scraping project with Beautiful Soup.The first step is to install Beautiful Soup. You can do this with pip:pip install beautifulsoup4\nThis will grab the latest version of Beautiful Soup and its dependencies.Now that Beautiful Soup is installed, we can start parsing HTML. To create a BeautifulSoup object, you need to pass in some HTML content and specify the parser to use.Common parsers include 'html.parser', 'lxml', and 'html5lib'. Here's an example:from bs4 import BeautifulSoup html = \"\"\"\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Page Title&lt;/title&gt;&lt;/head&gt; &lt;body&gt;\n&lt;p&gt;This is a paragraph.&lt;/p&gt;\n&lt;p&gt;This is another paragraph.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\" soup = BeautifulSoup(html, 'html.parser')\nNow the HTML is stored in a BeautifulSoup object called 'soup' that we can query.BeautifulSoup represents the HTML document as a parse tree that can be traversed and searched. Common navigation methods include:\nsoup.title - Get the &lt;title&gt; element\nsoup.head - Get the &lt;head&gt; element\nsoup.body - Get the &lt;body&gt; element\nsoup.p - Get all &lt;p&gt; elements\nsoup.find_all('p') - Get a list of all &lt;p&gt; tags\nWe can also navigate relationships like:\nsoup.head.title\nsoup.body.p\nThese return Tag objects that contain the parsed element.Now that we have Tag objects, we can extract useful data from them:\ntag.name - Tag name as a string\ntag.text - Tag contents as a NavigableString\ntag['id'] - Value of id attribute\ntag.attrs - Dictionary of all attributes\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Using Beautiful Soup for Web Scraping","level":1,"id":"Using_Beautiful_Soup_for_Web_Scraping_0"},{"heading":"Installing Beautiful Soup","level":2,"id":"Installing_Beautiful_Soup_0"},{"heading":"Parsing HTML with BeautifulSoup","level":2,"id":"Parsing_HTML_with_BeautifulSoup_0"},{"heading":"Navigating the Parse Tree","level":3,"id":"Navigating_the_Parse_Tree_0"},{"heading":"Extracting Data from Tags","level":3,"id":"Extracting_Data_from_Tags_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"python/beautiful-soup.html","pathToRoot":"..","attachments":[],"createdTime":1693680837530,"modifiedTime":1693683637000,"sourceSize":5637,"sourcePath":"Python/Beautiful Soup.md","exportPath":"python/beautiful-soup.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"python/python.html":{"title":"Python","icon":"","description":"\n<a data-href=\"Python OOP\" href=\"python/python-oop.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Python OOP</a>\n<br><a data-href=\"Beautiful Soup\" href=\"python/beautiful-soup.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Beautiful Soup</a>\n<br><a data-tooltip-position=\"top\" aria-label=\"https://santoshk.dev/posts/2022/__init__-vs-__new__-and-when-to-use-them/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://santoshk.dev/posts/2022/__init__-vs-__new__-and-when-to-use-them/\" target=\"_self\">init vs new</a><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.google.com/books/edition/Python_Crash_Course/igYvDwAAQBAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.google.com/books/edition/Python_Crash_Course/igYvDwAAQBAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover\" target=\"_self\">Python Crash Course Book</a>","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Python","level":1,"id":"Python_1"},{"heading":"📒 Contents:","level":2,"id":"📒_Contents_0"},{"heading":"🔗 References:","level":2,"id":"🔗_References_0"}],"links":["python/python-oop.html#_0","python/beautiful-soup.html#_0"],"author":"","coverImageURL":"","fullURL":"python/python.html","pathToRoot":"..","attachments":[],"createdTime":1690548039002,"modifiedTime":1693680859464,"sourceSize":309,"sourcePath":"Python/Python.md","exportPath":"python/python.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"},"python/python-oop.html":{"title":"Python OOP","icon":"","description":"Magic methods in Python are the special methods that start and end with the double underscores. They are also called dunder methods. Magic methods are not meant to be invoked directly by you, but the invocation happens internally from the class on a certain action.\n__abs__(self) - used to return the absolute value of an object.\n__add__(self, other) - used to define the behavior of the addition operator (+) for an object.\n__and__(self, other) - used to define the behavior of the bitwise AND operator (&amp;) for an object.\n__bool__(self) - used to return the boolean value of an object.\n__ceil__(self) - used to return the ceiling value of an object.\n__class__(self) - used to return the class of an object.\n__delattr__(self, name) - used to delete an attribute from an object.\n__dir__(self) - used to return a list of valid attributes for an object.\n__divmod__(self, other) - used to define the behavior of the divmod() function for an object.\n__doc - used to store the documentation string for a class or function.\n__eq__(self, other) - used to define the behavior of the equality operator (==) for an object.\n__float__(self) - used to convert an object to a floating-point number.\n__floor__(self) - used to return the floor value of an object.\n__floordiv__(self, other) - used to define the behavior of the floor division operator (//) for an object.\n__format__(self, format_spec) - used to define the behavior of the str.format() method for an object.\n__ge__(self, other) - used to define the behavior of the greater than or equal to operator (&gt;=) for an object.\n__getattribute__(self, name) - used to get the value of an attribute from an object.\n__getnewargs__(self) - used to return the arguments for the object constructor.\n__gt__(self, other) - used to define the behavior of the greater than operator (&gt;) for an object.\n__hash__(self) - used to return the hash value of an object.\n__index__(self) - used to return the integer value of an object.\n__init__(self) - used to initialize an object.\n__init_subclass(cls) - used to initialize a subclass.\n__int__(self) - used to convert an object to an integer.\n__invert__(self) - used to define the behavior of the bitwise NOT operator (~) for an object.\n__le__(self, other) - used to define the behavior of the less than or equal to operator (&lt;=) for an object.\n__lshift__(self, other) - used to define the behavior of the left shift operator (&lt;&lt;) for an object.\n__lt__(self, other) - used to define the behavior of the less than operator (&lt;) for an object.\n__mod__(self, other) - used to define the behavior of the modulo operator (%) for an object.\n__mul__(self, other) - used to define the behavior of the multiplication operator (*) for an object.\n__ne__(self, other) - used to define the behavior of the not equal operator (!=) for an object.\n__neg__(self) - used to define the behavior of the negation operator (-) for an object.\n__new(cls, *args, kwargs) - used to create a new instance of a class. This method is called before init() and is responsible for creating and returning the new object.\n__or__(self, other) - used to define the behavior of the bitwise OR operator (|) for an object.\n__pos__(self) - used to define the behavior of the unary plus operator (+) for an object.\n__pow__(self, other[, modulo]) - used to define the behavior of the exponent\n__rsub__(self, other) - used to define the behavior of the subtraction operator (-) when the object is on the right-hand side.\n__rtruediv__(self, other) - used to define the behavior of the true division operator (/) when the object is on the right-hand side.\n__rxor__(self, other) - used to define the behavior of the bitwise XOR operator (^) when the object is on the right-hand side.\n__setattr__(self, name, value) - used to set the value of an attribute on an object.\n__sizeof__(self) - used to return the size of an object in bytes.\n__str__(self) - used to return a string representation of an object.\n__sub__(self, other) - used to define the behavior of the subtraction operator (-) for an object.\n__subclasshook__(cls, subclass) - used to customize the behavior of the issubclass() built-in function.\n__truediv__(self, other) - used to define the behavior of the true division operator (/) for an object.\n__trunc__(self) - used to return the truncated integer value of a float or decimal number.\n__xor__(self, other) - used to define the behavior of the bitwise XOR operator (^) for an object.\n__bit_length__(self) - used to return the number of bits required to represent an integer in binary.\n__conjugate__(self) - used to return the complex conjugate of a complex number.\n__denominator__(self) - used to return the denominator of a rational number.\n__from_bytes(bytes, byteorder, *, signed=False) - used to convert a sequence of bytes into an integer.\n__imag__(self) - used to return the imaginary part of a complex number.\n__numerator__(self) - used to return the numerator of a rational number.\n__real__(self) - used to return the real part of a complex number.\n__to_bytes__(self, length, byteorder, *, signed=False) - used to convert an integer into a sequence of bytes. Both of them are called/invoked during the creation of the instance.\nTalking about the first point. __new__ is called when the instance is first created. This happens before the initialization of the class.also its important to mention that the first argument to __init__ is always self This self is the instance of the class. self is what __new__ returns.Coming to the third point, __new__ is supposed to return an instance of the class. Note that if __new__ does not returns anything, __init__ is not called.If you are coming from another language, you might be surprised that there are two similar things doing the same kind of work. Most languages have something called a constructor.In Python, that concept is broken down into constructor and initializer. And __new__ is the constructor and __init__ is the initializer.Please note that __new__ is implicit. Meaning that if you don’t actually need to modify the creation of an instance of the class, you don’t need to have a new method.instance variables are local to an instance. So anything you are doing in init is local to that instance only. But anything you are doing in new will be affecting anything created for that type.Consider this example:class Demo: def __new__(cls, *args): print(\"__new__ called\") return object.__new__(cls) def __init__(self): print(\"__init__ called\") d = Demo() This is the simplest example of both __new__ and __init__ in action. If you save the above code in a file and run it, you’d see something like this:$ python3 in_it.py __new__ called\n__init__ called\nAs you can see, the new method is called first and then execution is passed to the init method.Use case for __new__ :One of the best use cases we can take an example of is when creating a Singleton. As we know, Singleton ensures a class only has one instance and provides a global point of access to it.Some of the places singleton are used is in game programming where there is only one instance of the player. Doesn’t matter how many instances you create, you’ll end up having only one.Let’s see how to achieve similar behavior in Python:class Singleton: __instance = None def __new__(cls): if cls.__instance is None: print(\"creating...\") cls.__instance = object.__new__(cls) return cls.__instance s1 = Singleton()\ns2 = Singleton() print(s1)\nprint(s2) Output:$ python3 singleton.py creating...\n&lt;__main__.Singleton object at 0x7f943301d350&gt;\n&lt;__main__.Singleton object at 0x7f943301d350&gt;\nAs you can see, creating… is printed only once. they both point to the same memory location. In my case, it’s 0x7f943301d350.You might be guessing that can’t we do the same thing with __init__? No! That’s because __init__ does not return anything. We’ll see in the next section what __init__ is well suited for.Use case for __init__ :As we have already seen previously. init is there to initialize an instance variable. These instance variables can later be used in different methods of the instance.Here I’ll demonstrate one such example:class Window(QWidget): def __init__(self, parent = None): super(Window, self).__init__(parent) self.resize(200, 100) self.setWindowTitle(\"My App\") When initializing UI objects, you can set how wide or long the window could be. You can also read preferences from a file and apply that during the initialization phase of an application. Setting the window title could be another example.","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Python Object Oriented Programming","level":1,"id":"Python_Object_Oriented_Programming_0"},{"heading":"Python Magic Methods:","level":2,"id":"Python_Magic_Methods_0"},{"heading":"Descriptions :","level":3,"id":"Descriptions__0"},{"heading":"Diffrences between init  and new :","level":2,"id":"Diffrences_between_init_and_new__0"},{"heading":"Similarities :","level":3,"id":"Similarities__0"},{"heading":"Differences :","level":3,"id":"Differences__0"},{"heading":"Which one of them is a constructor?","level":3,"id":"Which_one_of_them_is_a_constructor?_0"},{"heading":"Use Cases :","level":3,"id":"Use_Cases__0"}],"links":[],"author":"","coverImageURL":"","fullURL":"python/python-oop.html","pathToRoot":"..","attachments":[],"createdTime":1690545992118,"modifiedTime":1690553460022,"sourceSize":22690,"sourcePath":"Python/Python OOP.md","exportPath":"python/python-oop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown"}},"fileInfo":{"site-lib/scripts/graph-wasm.wasm":{"createdTime":1741171522089,"modifiedTime":1732604506000,"sourceSize":23655,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.wasm","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"site-lib/fonts/94f2f163d4b698242fef.otf":{"createdTime":1741268488915,"modifiedTime":1741268488915,"sourceSize":66800,"sourcePath":"","exportPath":"site-lib/fonts/94f2f163d4b698242fef.otf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/72505e6a122c6acd5471.woff2":{"createdTime":1741268488923,"modifiedTime":1741268488923,"sourceSize":104232,"sourcePath":"","exportPath":"site-lib/fonts/72505e6a122c6acd5471.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/2d5198822ab091ce4305.woff2":{"createdTime":1741268488925,"modifiedTime":1741268488925,"sourceSize":104332,"sourcePath":"","exportPath":"site-lib/fonts/2d5198822ab091ce4305.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/c8ba52b05a9ef10f4758.woff2":{"createdTime":1741268488928,"modifiedTime":1741268488928,"sourceSize":98868,"sourcePath":"","exportPath":"site-lib/fonts/c8ba52b05a9ef10f4758.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cb10ffd7684cd9836a05.woff2":{"createdTime":1741268488931,"modifiedTime":1741268488931,"sourceSize":106876,"sourcePath":"","exportPath":"site-lib/fonts/cb10ffd7684cd9836a05.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/b5f0f109bc88052d4000.woff2":{"createdTime":1741268488933,"modifiedTime":1741268488933,"sourceSize":105804,"sourcePath":"","exportPath":"site-lib/fonts/b5f0f109bc88052d4000.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cbe0ae49c52c920fd563.woff2":{"createdTime":1741268488934,"modifiedTime":1741268488934,"sourceSize":106108,"sourcePath":"","exportPath":"site-lib/fonts/cbe0ae49c52c920fd563.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/535a6cf662596b3bd6a6.woff2":{"createdTime":1741268488935,"modifiedTime":1741268488935,"sourceSize":111708,"sourcePath":"","exportPath":"site-lib/fonts/535a6cf662596b3bd6a6.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/70cc7ff27245e82ad414.ttf":{"createdTime":1741268488935,"modifiedTime":1741268488935,"sourceSize":192740,"sourcePath":"","exportPath":"site-lib/fonts/70cc7ff27245e82ad414.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/454577c22304619db035.ttf":{"createdTime":1741268488936,"modifiedTime":1741268488936,"sourceSize":161376,"sourcePath":"","exportPath":"site-lib/fonts/454577c22304619db035.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/52ac8f3034507f1d9e53.ttf":{"createdTime":1741268488948,"modifiedTime":1741268488948,"sourceSize":191568,"sourcePath":"","exportPath":"site-lib/fonts/52ac8f3034507f1d9e53.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/05b618077343fbbd92b7.ttf":{"createdTime":1741268488951,"modifiedTime":1741268488951,"sourceSize":155288,"sourcePath":"","exportPath":"site-lib/fonts/05b618077343fbbd92b7.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2":{"createdTime":1741268488912,"modifiedTime":1741268488912,"sourceSize":7876,"sourcePath":"","exportPath":"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/media/6155340132a851f6089e.svg":{"createdTime":1741268488913,"modifiedTime":1741268488913,"sourceSize":315,"sourcePath":"","exportPath":"site-lib/media/6155340132a851f6089e.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/media/2308ab1944a6bfa5c5b8.svg":{"createdTime":1741268488914,"modifiedTime":1741268488914,"sourceSize":278,"sourcePath":"","exportPath":"site-lib/media/2308ab1944a6bfa5c5b8.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/fonts/mathjax_zero.woff":{"createdTime":1741257260609,"modifiedTime":1741257260609,"sourceSize":1368,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_zero.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-regular.woff":{"createdTime":1741257260615,"modifiedTime":1741257260615,"sourceSize":34160,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-bold.woff":{"createdTime":1741257260615,"modifiedTime":1741257260615,"sourceSize":34464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-italic.woff":{"createdTime":1741257260616,"modifiedTime":1741257260616,"sourceSize":19360,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-italic.woff":{"createdTime":1741257260616,"modifiedTime":1741257260616,"sourceSize":20832,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-bolditalic.woff":{"createdTime":1741257260616,"modifiedTime":1741257260616,"sourceSize":19776,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-bolditalic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size1-regular.woff":{"createdTime":1741257260616,"modifiedTime":1741257260616,"sourceSize":5792,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size1-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size2-regular.woff":{"createdTime":1741257260616,"modifiedTime":1741257260616,"sourceSize":5464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size2-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size3-regular.woff":{"createdTime":1741257260616,"modifiedTime":1741257260616,"sourceSize":3244,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size3-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size4-regular.woff":{"createdTime":1741257260617,"modifiedTime":1741257260617,"sourceSize":5148,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size4-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_ams-regular.woff":{"createdTime":1741257260617,"modifiedTime":1741257260617,"sourceSize":40808,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_ams-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-regular.woff":{"createdTime":1741257260617,"modifiedTime":1741257260617,"sourceSize":9600,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-bold.woff":{"createdTime":1741257260617,"modifiedTime":1741257260617,"sourceSize":9908,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-regular.woff":{"createdTime":1741257260617,"modifiedTime":1741257260617,"sourceSize":21480,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-bold.woff":{"createdTime":1741257260617,"modifiedTime":1741257260617,"sourceSize":22340,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-regular.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":12660,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-bold.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":15944,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-italic.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":14628,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_script-regular.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":11852,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_script-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_typewriter-regular.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":17604,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_typewriter-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-regular.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":1136,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-bold.woff":{"createdTime":1741257260618,"modifiedTime":1741257260618,"sourceSize":1116,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/html/file-tree-content.html":{"createdTime":1741268493836,"modifiedTime":1741268493836,"sourceSize":335058,"sourcePath":"","exportPath":"site-lib/html/file-tree-content.html","showInTree":false,"treeOrder":0,"backlinks":[],"type":"html","data":null},"site-lib/scripts/webpage.js":{"createdTime":1741171523932,"modifiedTime":1741171523932,"sourceSize":101494,"sourcePath":"","exportPath":"site-lib/scripts/webpage.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-wasm.js":{"createdTime":1741171523932,"modifiedTime":1741171523932,"sourceSize":12885,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-render-worker.js":{"createdTime":1741171523932,"modifiedTime":1741171523932,"sourceSize":5681,"sourcePath":"","exportPath":"site-lib/scripts/graph-render-worker.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/media/favicon.png":{"createdTime":1712066436384.252,"modifiedTime":1708604390900.5654,"sourceSize":28663,"sourcePath":"","exportPath":"site-lib/media/favicon.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/styles/obsidian.css":{"createdTime":1741268489124,"modifiedTime":1741268489124,"sourceSize":162839,"sourcePath":"","exportPath":"site-lib/styles/obsidian.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/theme.css":{"createdTime":1741255366597,"modifiedTime":1741255366597,"sourceSize":70366,"sourcePath":"","exportPath":"site-lib/styles/theme.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/global-variable-styles.css":{"createdTime":1741268488723,"modifiedTime":1741268488723,"sourceSize":397,"sourcePath":"","exportPath":"site-lib/styles/global-variable-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/supported-plugins.css":{"createdTime":1741268489127,"modifiedTime":1741268489127,"sourceSize":1713,"sourcePath":"","exportPath":"site-lib/styles/supported-plugins.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/main-styles.css":{"createdTime":1741171523990,"modifiedTime":1741171523990,"sourceSize":19129,"sourcePath":"","exportPath":"site-lib/styles/main-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"home.html":{"createdTime":1690548373184,"modifiedTime":1741268362292,"sourceSize":1237,"sourcePath":"Home.md","exportPath":"home.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"excalidraw/codeverse.svg":{"createdTime":1694726143351,"modifiedTime":1706716941232,"sourceSize":20515,"sourcePath":"Excalidraw/codeverse.svg","exportPath":"excalidraw/codeverse.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html":{"createdTime":1695306687377,"modifiedTime":1741171987763,"sourceSize":7278,"sourcePath":"Algorithms/Algo/Backtracking/A general approach to backtracking questions.md","exportPath":"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/backtracking/backtracking.html":{"createdTime":1741084475860,"modifiedTime":1737554783000,"sourceSize":138,"sourcePath":"Algorithms/Algo/Backtracking/Backtracking.md","exportPath":"algorithms/algo/backtracking/backtracking.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/divide-and-conquer/divide-and-conquer.html":{"createdTime":1741084475871,"modifiedTime":1737554783000,"sourceSize":1,"sourcePath":"Algorithms/Algo/Divide and Conquer/Divide and Conquer.md","exportPath":"algorithms/algo/divide-and-conquer/divide-and-conquer.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/dynamic-programming/dynamic-programing.html":{"createdTime":1741084475883,"modifiedTime":1737554783000,"sourceSize":399,"sourcePath":"Algorithms/Algo/Dynamic Programming/Dynamic Programing.md","exportPath":"algorithms/algo/dynamic-programming/dynamic-programing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/dynamic-programming/kadane's-algorithm.html":{"createdTime":1741084475888,"modifiedTime":1737554783000,"sourceSize":68,"sourcePath":"Algorithms/Algo/Dynamic Programming/Kadane's Algorithm.md","exportPath":"algorithms/algo/dynamic-programming/kadane's-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/graph/alpha-beta.html":{"createdTime":1741084475922,"modifiedTime":1737554783000,"sourceSize":4002,"sourcePath":"Algorithms/Algo/Graph/Alpha-Beta.md","exportPath":"algorithms/algo/graph/alpha-beta.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html":{"createdTime":1741084475956,"modifiedTime":1737554783000,"sourceSize":1502,"sourcePath":"Algorithms/Algo/Graph/Floyd's cycle-finding algorithm.md","exportPath":"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/graph/minimax.html":{"createdTime":1741084475982,"modifiedTime":1737554783000,"sourceSize":2827,"sourcePath":"Algorithms/Algo/Graph/MiniMax.md","exportPath":"algorithms/algo/graph/minimax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/graph/union-find.html":{"createdTime":1741084475990,"modifiedTime":1737554783000,"sourceSize":87,"sourcePath":"Algorithms/Algo/Graph/Union Find.md","exportPath":"algorithms/algo/graph/union-find.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/greedy/greedy.html":{"createdTime":1741084476010,"modifiedTime":1737554783000,"sourceSize":322,"sourcePath":"Algorithms/Algo/Greedy/Greedy.md","exportPath":"algorithms/algo/greedy/greedy.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/recursion/recursion.html":{"createdTime":1741084476021,"modifiedTime":1737554783000,"sourceSize":134,"sourcePath":"Algorithms/Algo/Recursion/Recursion.md","exportPath":"algorithms/algo/recursion/recursion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/search/a-star.html":{"createdTime":1741084476056,"modifiedTime":1737554783000,"sourceSize":3357,"sourcePath":"Algorithms/Algo/Search/A star.md","exportPath":"algorithms/algo/search/a-star.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/search/breadth-first-search.html":{"createdTime":1741084476060,"modifiedTime":1737554783000,"sourceSize":4052,"sourcePath":"Algorithms/Algo/Search/Breadth First Search.md","exportPath":"algorithms/algo/search/breadth-first-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/search/depth-first-search.html":{"createdTime":1741084476122,"modifiedTime":1737554783000,"sourceSize":5374,"sourcePath":"Algorithms/Algo/Search/Depth First Search.md","exportPath":"algorithms/algo/search/depth-first-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/search/dfs-vs-bfs.html":{"createdTime":1697824454391,"modifiedTime":1704139676000,"sourceSize":3966,"sourcePath":"Algorithms/Algo/Search/DFS vs BFS.md","exportPath":"algorithms/algo/search/dfs-vs-bfs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/search/dijkstra.html":{"createdTime":1741084476188,"modifiedTime":1737554783000,"sourceSize":3758,"sourcePath":"Algorithms/Algo/Search/Dijkstra.md","exportPath":"algorithms/algo/search/dijkstra.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/search/iterative-deepening.html":{"createdTime":1741084476197,"modifiedTime":1737554783000,"sourceSize":988,"sourcePath":"Algorithms/Algo/Search/Iterative Deepening.md","exportPath":"algorithms/algo/search/iterative-deepening.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/sliding-window/sliding-window.html":{"createdTime":1741084476231,"modifiedTime":1737554783000,"sourceSize":281,"sourcePath":"Algorithms/Algo/Sliding Window/Sliding Window.md","exportPath":"algorithms/algo/sliding-window/sliding-window.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/sorting/binary-search.html":{"createdTime":1741084476241,"modifiedTime":1737554783000,"sourceSize":73,"sourcePath":"Algorithms/Algo/Sorting/Binary Search.md","exportPath":"algorithms/algo/sorting/binary-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html":{"createdTime":1695306688443,"modifiedTime":1741084570719,"sourceSize":2865,"sourcePath":"Algorithms/Algo/Two Pointers/Floyd’s Cycle Finding Algorithm.md","exportPath":"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algo/two-pointers/two-pointer.html":{"createdTime":1741084476250,"modifiedTime":1737554783000,"sourceSize":533,"sourcePath":"Algorithms/Algo/Two Pointers/Two Pointer.md","exportPath":"algorithms/algo/two-pointers/two-pointer.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/45.-jump-game-ii.html":{"createdTime":1741084473623,"modifiedTime":1737554783000,"sourceSize":1055,"sourcePath":"Algorithms/Leetcode/Array/DP/45. Jump Game II.md","exportPath":"algorithms/leetcode/array/dp/45.-jump-game-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/62.-unique-paths.html":{"createdTime":1741084473685,"modifiedTime":1737554783000,"sourceSize":972,"sourcePath":"Algorithms/Leetcode/Array/DP/62. Unique Paths.md","exportPath":"algorithms/leetcode/array/dp/62.-unique-paths.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/70.-climbing-stairs.html":{"createdTime":1741084473690,"modifiedTime":1737554783000,"sourceSize":435,"sourcePath":"Algorithms/Leetcode/Array/DP/70. Climbing Stairs.md","exportPath":"algorithms/leetcode/array/dp/70.-climbing-stairs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html":{"createdTime":1741084473701,"modifiedTime":1737554783000,"sourceSize":419,"sourcePath":"Algorithms/Leetcode/Array/DP/96. Unique Binary Search Trees.md","exportPath":"algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/198.-house-robber.html":{"createdTime":1741084473382,"modifiedTime":1737554783000,"sourceSize":795,"sourcePath":"Algorithms/Leetcode/Array/DP/198. House Robber.md","exportPath":"algorithms/leetcode/array/dp/198.-house-robber.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/213.-house-robber-ii.html":{"createdTime":1741084473404,"modifiedTime":1737554783000,"sourceSize":1317,"sourcePath":"Algorithms/Leetcode/Array/DP/213. House Robber II.md","exportPath":"algorithms/leetcode/array/dp/213.-house-robber-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/221.-maximal-square.html":{"createdTime":1741084473457,"modifiedTime":1737554783000,"sourceSize":950,"sourcePath":"Algorithms/Leetcode/Array/DP/221. Maximal Square.md","exportPath":"algorithms/leetcode/array/dp/221.-maximal-square.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html":{"createdTime":1741084473569,"modifiedTime":1737554783000,"sourceSize":463,"sourcePath":"Algorithms/Leetcode/Array/DP/300. Longest Increasing Subsequence.md","exportPath":"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html":{"createdTime":1741084473573,"modifiedTime":1737554783000,"sourceSize":1200,"sourcePath":"Algorithms/Leetcode/Array/DP/309. Best Time to Buy and Sell Stock with Cooldown.md","exportPath":"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/413.-arithmetic-slices.html":{"createdTime":1741084473589,"modifiedTime":1737554783000,"sourceSize":803,"sourcePath":"Algorithms/Leetcode/Array/DP/413. Arithmetic Slices.md","exportPath":"algorithms/leetcode/array/dp/413.-arithmetic-slices.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/472.-concatenated-words.html":{"createdTime":1741084473649,"modifiedTime":1737554783000,"sourceSize":799,"sourcePath":"Algorithms/Leetcode/Array/DP/472. Concatenated Words.md","exportPath":"algorithms/leetcode/array/dp/472.-concatenated-words.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/542.-01-matrix.html":{"createdTime":1741084473681,"modifiedTime":1737554783000,"sourceSize":1352,"sourcePath":"Algorithms/Leetcode/Array/DP/542. 01 Matrix.md","exportPath":"algorithms/leetcode/array/dp/542.-01-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html":{"createdTime":1741084473693,"modifiedTime":1737554783000,"sourceSize":1160,"sourcePath":"Algorithms/Leetcode/Array/DP/714. Best Time to Buy and Sell Stock with Transaction Fee.md","exportPath":"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/740.-delete-and-earn.html":{"createdTime":1741084473697,"modifiedTime":1737554783000,"sourceSize":1028,"sourcePath":"Algorithms/Leetcode/Array/DP/740. Delete and Earn.md","exportPath":"algorithms/leetcode/array/dp/740.-delete-and-earn.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html":{"createdTime":1741084473329,"modifiedTime":1737554783000,"sourceSize":974,"sourcePath":"Algorithms/Leetcode/Array/DP/1143. Longest Common Subsequence.md","exportPath":"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html":{"createdTime":1741084473372,"modifiedTime":1737554783000,"sourceSize":1177,"sourcePath":"Algorithms/Leetcode/Array/DP/1626. Best Team With No Conflicts.md","exportPath":"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/hash/454.-4sum-ii.html":{"createdTime":1741084473711,"modifiedTime":1737554783000,"sourceSize":640,"sourcePath":"Algorithms/Leetcode/Array/Hash/454. 4Sum II.md","exportPath":"algorithms/leetcode/array/hash/454.-4sum-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/interval/57.-insert-interval.html":{"createdTime":1741084473719,"modifiedTime":1737554783000,"sourceSize":1237,"sourcePath":"Algorithms/Leetcode/Array/Interval/57. Insert Interval.md","exportPath":"algorithms/leetcode/array/interval/57.-insert-interval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html":{"createdTime":1741084473778,"modifiedTime":1737554783000,"sourceSize":852,"sourcePath":"Algorithms/Leetcode/Array/Matrix/74. Search a 2D Matrix.md","exportPath":"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/matrix/79.-word-search.html":{"createdTime":1741084473795,"modifiedTime":1737554783000,"sourceSize":1169,"sourcePath":"Algorithms/Leetcode/Array/Matrix/79. Word Search.md","exportPath":"algorithms/leetcode/array/matrix/79.-word-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html":{"createdTime":1741084473743,"modifiedTime":1737554783000,"sourceSize":1872,"sourcePath":"Algorithms/Leetcode/Array/Matrix/417. Pacific Atlantic Water Flow.md","exportPath":"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/matrix/695.-max-area-of-island.html":{"createdTime":1741084473748,"modifiedTime":1737554783000,"sourceSize":1033,"sourcePath":"Algorithms/Leetcode/Array/Matrix/695. Max Area of Island.md","exportPath":"algorithms/leetcode/array/matrix/695.-max-area-of-island.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html":{"createdTime":1741084473805,"modifiedTime":1737554783000,"sourceSize":2497,"sourcePath":"Algorithms/Leetcode/Array/Matrix/909. Snakes and Ladders.md","exportPath":"algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/matrix/934.-shortest-bridge.html":{"createdTime":1741084473893,"modifiedTime":1737554783000,"sourceSize":1678,"sourcePath":"Algorithms/Leetcode/Array/Matrix/934. Shortest Bridge.md","exportPath":"algorithms/leetcode/array/matrix/934.-shortest-bridge.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/permutation/46.-permutations.html":{"createdTime":1741084473950,"modifiedTime":1737554783000,"sourceSize":582,"sourcePath":"Algorithms/Leetcode/Array/Permutation/46. Permutations.md","exportPath":"algorithms/leetcode/array/permutation/46.-permutations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/permutation/77.-combinations.html":{"createdTime":1741084473955,"modifiedTime":1737554783000,"sourceSize":590,"sourcePath":"Algorithms/Leetcode/Array/Permutation/77. Combinations.md","exportPath":"algorithms/leetcode/array/permutation/77.-combinations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/sort/912.-sort-an-array.html":{"createdTime":1741084473968,"modifiedTime":1737554783000,"sourceSize":1855,"sourcePath":"Algorithms/Leetcode/Array/Sort/912. Sort an Array.md","exportPath":"algorithms/leetcode/array/sort/912.-sort-an-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/subarray/53.-maximum-subarray.html":{"createdTime":1741084474104,"modifiedTime":1737554783000,"sourceSize":437,"sourcePath":"Algorithms/Leetcode/Array/Subarray/53. Maximum Subarray.md","exportPath":"algorithms/leetcode/array/subarray/53.-maximum-subarray.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html":{"createdTime":1741084474038,"modifiedTime":1737554783000,"sourceSize":697,"sourcePath":"Algorithms/Leetcode/Array/Subarray/491. Non-decreasing Subsequences.md","exportPath":"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html":{"createdTime":1741084474121,"modifiedTime":1737554783000,"sourceSize":1073,"sourcePath":"Algorithms/Leetcode/Array/Subarray/918. Maximum Sum Circular Subarray.md","exportPath":"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html":{"createdTime":1741084474146,"modifiedTime":1737554783000,"sourceSize":779,"sourcePath":"Algorithms/Leetcode/Array/Subarray/974. Subarray Sums Divisible by K.md","exportPath":"algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html":{"createdTime":1741084474238,"modifiedTime":1737554783000,"sourceSize":2247,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/11. Container With Most Water.md","exportPath":"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/15.-3sum.html":{"createdTime":1741084474299,"modifiedTime":1737554783000,"sourceSize":1214,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/15. 3Sum.md","exportPath":"algorithms/leetcode/array/two-pointers/15.-3sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html":{"createdTime":1741084474306,"modifiedTime":1737554783000,"sourceSize":848,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/16. 3Sum Closest.md","exportPath":"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/18.-4sum.html":{"createdTime":1741084474335,"modifiedTime":1737554783000,"sourceSize":1582,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/18. 4Sum.md","exportPath":"algorithms/leetcode/array/two-pointers/18.-4sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html":{"createdTime":1741084474532,"modifiedTime":1737554783000,"sourceSize":1535,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/26. Remove Duplicates from Sorted Array.md","exportPath":"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/27.-remove-element.html":{"createdTime":1741084474570,"modifiedTime":1737554783000,"sourceSize":1308,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/27. Remove Element.md","exportPath":"algorithms/leetcode/array/two-pointers/27.-remove-element.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html":{"createdTime":1741084474645,"modifiedTime":1737554783000,"sourceSize":767,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/69. Sqrt(x).md","exportPath":"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html":{"createdTime":1741084474692,"modifiedTime":1737554783000,"sourceSize":1572,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/81. Search in Rotated Sorted Array II.md","exportPath":"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html":{"createdTime":1741084474717,"modifiedTime":1737554783000,"sourceSize":1525,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/88. Merge Sorted Array.md","exportPath":"algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html":{"createdTime":1741084474272,"modifiedTime":1737554783000,"sourceSize":1102,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/142. Linked List Cycle II.md","exportPath":"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html":{"createdTime":1741084474383,"modifiedTime":1737554783000,"sourceSize":1300,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/209. Minimum Size Subarray Sum.md","exportPath":"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html":{"createdTime":1741084474458,"modifiedTime":1737554783000,"sourceSize":732,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/253. Meeting Rooms II.md","exportPath":"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html":{"createdTime":1741084474525,"modifiedTime":1737554783000,"sourceSize":678,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/259. 3Sum Smaller.md","exportPath":"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html":{"createdTime":1741084474586,"modifiedTime":1737554783000,"sourceSize":934,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/360. Sort Transformed Array.md","exportPath":"algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html":{"createdTime":1741084474618,"modifiedTime":1737554783000,"sourceSize":819,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/392. Is Subsequence.md","exportPath":"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html":{"createdTime":1741084474641,"modifiedTime":1737554783000,"sourceSize":548,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/633. Sum of Square Numbers.md","exportPath":"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html":{"createdTime":1741084474676,"modifiedTime":1737554783000,"sourceSize":792,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/713. Subarray Product Less Than K.md","exportPath":"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html":{"createdTime":1741084474203,"modifiedTime":1737554783000,"sourceSize":582,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/1004. Max Consecutive Ones III.md","exportPath":"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html":{"createdTime":1741084474283,"modifiedTime":1737554783000,"sourceSize":1007,"sourcePath":"Algorithms/Leetcode/Array/Two-pointers/1493. Longest Subarray of 1's After Deleting One Element.md","exportPath":"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/bit/190.-reverse-bits.html":{"createdTime":1741084474744,"modifiedTime":1737554783000,"sourceSize":865,"sourcePath":"Algorithms/Leetcode/Bit/190. Reverse Bits.md","exportPath":"algorithms/leetcode/bit/190.-reverse-bits.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/bit/461.-hamming-distance.html":{"createdTime":1741084474759,"modifiedTime":1737554783000,"sourceSize":492,"sourcePath":"Algorithms/Leetcode/Bit/461. Hamming Distance.md","exportPath":"algorithms/leetcode/bit/461.-hamming-distance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/geometry/149.-max-points-on-a-line.html":{"createdTime":1741084474769,"modifiedTime":1737554783000,"sourceSize":693,"sourcePath":"Algorithms/Leetcode/Geometry/149. Max Points on a Line.md","exportPath":"algorithms/leetcode/geometry/149.-max-points-on-a-line.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/graph/490.-the-maze.html":{"createdTime":1741084474806,"modifiedTime":1737554783000,"sourceSize":1451,"sourcePath":"Algorithms/Leetcode/Graph/490. The Maze.md","exportPath":"algorithms/leetcode/graph/490.-the-maze.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html":{"createdTime":1741084474838,"modifiedTime":1737554783000,"sourceSize":988,"sourcePath":"Algorithms/Leetcode/Graph/797. All Paths From Source to Target.md","exportPath":"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/graph/841.-keys-and-rooms.html":{"createdTime":1741084474894,"modifiedTime":1737554783000,"sourceSize":1257,"sourcePath":"Algorithms/Leetcode/Graph/841. Keys and Rooms.md","exportPath":"algorithms/leetcode/graph/841.-keys-and-rooms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html":{"createdTime":1741084474785,"modifiedTime":1737554783000,"sourceSize":1263,"sourcePath":"Algorithms/Leetcode/Graph/1971. Find if Path Exists in Graph.md","exportPath":"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/greedy/134.-gas-station.html":{"createdTime":1741084474928,"modifiedTime":1737554783000,"sourceSize":982,"sourcePath":"Algorithms/Leetcode/Greedy/134. Gas Station.md","exportPath":"algorithms/leetcode/greedy/134.-gas-station.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/greedy/135.-candy.html":{"createdTime":1741084474991,"modifiedTime":1737554783000,"sourceSize":1032,"sourcePath":"Algorithms/Leetcode/Greedy/135. Candy.md","exportPath":"algorithms/leetcode/greedy/135.-candy.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html":{"createdTime":1741084475001,"modifiedTime":1737554783000,"sourceSize":747,"sourcePath":"Algorithms/Leetcode/Greedy/435. Non-overlapping Intervals.md","exportPath":"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/greedy/455.-assign-cookies.html":{"createdTime":1741084475010,"modifiedTime":1737554783000,"sourceSize":862,"sourcePath":"Algorithms/Leetcode/Greedy/455. Assign Cookies.md","exportPath":"algorithms/leetcode/greedy/455.-assign-cookies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html":{"createdTime":1741084474997,"modifiedTime":1737554783000,"sourceSize":759,"sourcePath":"Algorithms/Leetcode/Greedy/1833. Maximum Ice Cream Bars.md","exportPath":"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html":{"createdTime":1741084475031,"modifiedTime":1737554783000,"sourceSize":701,"sourcePath":"Algorithms/Leetcode/Linked List/24. Swap Nodes in Pairs.md","exportPath":"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html":{"createdTime":1741084475073,"modifiedTime":1737554783000,"sourceSize":408,"sourcePath":"Algorithms/Leetcode/Math/453. Minimum Moves to Equal Array Elements.md","exportPath":"algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html":{"createdTime":1741084475100,"modifiedTime":1737554783000,"sourceSize":1235,"sourcePath":"Algorithms/Leetcode/Shortest Path/787. Cheapest Flights Within K Stops.md","exportPath":"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/select/183.-customers-who-never-order.html":{"createdTime":1741084475178,"modifiedTime":1737554783000,"sourceSize":403,"sourcePath":"Algorithms/Leetcode/SQL/Select/183. Customers Who Never Order.md","exportPath":"algorithms/leetcode/sql/select/183.-customers-who-never-order.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/select/584.-find-customer-referee.html":{"createdTime":1741084475182,"modifiedTime":1737554783000,"sourceSize":453,"sourcePath":"Algorithms/Leetcode/SQL/Select/584. Find Customer Referee.md","exportPath":"algorithms/leetcode/sql/select/584.-find-customer-referee.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/select/595.-big-countries.html":{"createdTime":1741084475205,"modifiedTime":1737554783000,"sourceSize":925,"sourcePath":"Algorithms/Leetcode/SQL/Select/595. Big Countries.md","exportPath":"algorithms/leetcode/sql/select/595.-big-countries.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html":{"createdTime":1741084475174,"modifiedTime":1737554783000,"sourceSize":555,"sourcePath":"Algorithms/Leetcode/SQL/Select/1757. Recyclable and Low Fat Products.md","exportPath":"algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html":{"createdTime":1741084475215,"modifiedTime":1737554783000,"sourceSize":553,"sourcePath":"Algorithms/Leetcode/SQL/String Processing Functions/1484. Group Sold Products By The Date.md","exportPath":"algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html":{"createdTime":1741084475219,"modifiedTime":1737554783000,"sourceSize":488,"sourcePath":"Algorithms/Leetcode/SQL/String Processing Functions/1527. Patients With a Condition.md","exportPath":"algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html":{"createdTime":1741084475230,"modifiedTime":1737554783000,"sourceSize":519,"sourcePath":"Algorithms/Leetcode/SQL/String Processing Functions/1667. Fix Names in a Table.md","exportPath":"algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/union/197.-rising-temperature.html":{"createdTime":1741084475238,"modifiedTime":1737554783000,"sourceSize":1107,"sourcePath":"Algorithms/Leetcode/SQL/Union/197. Rising Temperature.md","exportPath":"algorithms/leetcode/sql/union/197.-rising-temperature.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/union&select/608.-tree-node.html":{"createdTime":1741084475276,"modifiedTime":1737554783000,"sourceSize":535,"sourcePath":"Algorithms/Leetcode/SQL/Union&Select/608. Tree Node.md","exportPath":"algorithms/leetcode/sql/union&select/608.-tree-node.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html":{"createdTime":1741084475262,"modifiedTime":1737554783000,"sourceSize":861,"sourcePath":"Algorithms/Leetcode/SQL/Union&Select/1965. Employees With Missing Information.md","exportPath":"algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/6.-zigzag-conversion.html":{"createdTime":1741084475478,"modifiedTime":1737554783000,"sourceSize":815,"sourcePath":"Algorithms/Leetcode/String/6. Zigzag Conversion.md","exportPath":"algorithms/leetcode/string/6.-zigzag-conversion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/131.-palindrome-partitioning.html":{"createdTime":1741084475307,"modifiedTime":1737554783000,"sourceSize":731,"sourcePath":"Algorithms/Leetcode/String/131. Palindrome Partitioning.md","exportPath":"algorithms/leetcode/string/131.-palindrome-partitioning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html":{"createdTime":1741084475324,"modifiedTime":1737554783000,"sourceSize":546,"sourcePath":"Algorithms/Leetcode/String/340. Longest Substring with At Most K Distinct Characters.md","exportPath":"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html":{"createdTime":1741084475380,"modifiedTime":1737554783000,"sourceSize":897,"sourcePath":"Algorithms/Leetcode/String/395. Longest Substring with At Least K Repeating Characters.md","exportPath":"algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html":{"createdTime":1741084475431,"modifiedTime":1737554783000,"sourceSize":875,"sourcePath":"Algorithms/Leetcode/String/438. Find All Anagrams in a String.md","exportPath":"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html":{"createdTime":1741084475285,"modifiedTime":1737554783000,"sourceSize":653,"sourcePath":"Algorithms/Leetcode/String/1061. Lexicographically Smallest Equivalent String.md","exportPath":"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html":{"createdTime":1741084475299,"modifiedTime":1737554783000,"sourceSize":621,"sourcePath":"Algorithms/Leetcode/String/1071. Greatest Common Divisor of Strings.md","exportPath":"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/100.-same-tree.html":{"createdTime":1741084475510,"modifiedTime":1737554783000,"sourceSize":1041,"sourcePath":"Algorithms/Leetcode/Tree/100. Same Tree.md","exportPath":"algorithms/leetcode/tree/100.-same-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html":{"createdTime":1741084475541,"modifiedTime":1737554783000,"sourceSize":701,"sourcePath":"Algorithms/Leetcode/Tree/102. Binary Tree Level Order Traversal.md","exportPath":"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html":{"createdTime":1741084475546,"modifiedTime":1737554783000,"sourceSize":907,"sourcePath":"Algorithms/Leetcode/Tree/103. Binary Tree Zigzag Level Order Traversal.md","exportPath":"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html":{"createdTime":1741084475555,"modifiedTime":1737554783000,"sourceSize":1073,"sourcePath":"Algorithms/Leetcode/Tree/114. Flatten Binary Tree to Linked List.md","exportPath":"algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html":{"createdTime":1741084475573,"modifiedTime":1737554783000,"sourceSize":481,"sourcePath":"Algorithms/Leetcode/Tree/144. Binary Tree Preorder Traversal.md","exportPath":"algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html":{"createdTime":1741084475803,"modifiedTime":1737554783000,"sourceSize":876,"sourcePath":"Algorithms/Leetcode/Tree/429. N-ary Tree Level Order Traversal.md","exportPath":"algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html":{"createdTime":1741084475807,"modifiedTime":1737554783000,"sourceSize":730,"sourcePath":"Algorithms/Leetcode/Tree/637. Average of Levels in Binary Tree.md","exportPath":"algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html":{"createdTime":1741084475550,"modifiedTime":1737554783000,"sourceSize":1110,"sourcePath":"Algorithms/Leetcode/Tree/1123. Lowest Common Ancestor of Deepest Leaves.md","exportPath":"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/1302.-deepest-leaves-sum.html":{"createdTime":1741084475568,"modifiedTime":1737554783000,"sourceSize":434,"sourcePath":"Algorithms/Leetcode/Tree/1302. Deepest Leaves Sum.md","exportPath":"algorithms/leetcode/tree/1302.-deepest-leaves-sum.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html":{"createdTime":1741084475577,"modifiedTime":1737554783000,"sourceSize":1205,"sourcePath":"Algorithms/Leetcode/Tree/1443. Minimum Time to Collect All Apples in a Tree.md","exportPath":"algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html":{"createdTime":1741084475617,"modifiedTime":1737554783000,"sourceSize":420,"sourcePath":"Algorithms/Leetcode/Tree/1490. Clone N-ary Tree.md","exportPath":"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html":{"createdTime":1741084475624,"modifiedTime":1737554783000,"sourceSize":1117,"sourcePath":"Algorithms/Leetcode/Tree/1602. Find Nearest Right Node in Binary Tree.md","exportPath":"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/1660.-correct-a-binary-tree.html":{"createdTime":1741084475668,"modifiedTime":1737554783000,"sourceSize":1461,"sourcePath":"Algorithms/Leetcode/Tree/1660. Correct a Binary Tree.md","exportPath":"algorithms/leetcode/tree/1660.-correct-a-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html":{"createdTime":1741084475703,"modifiedTime":1737554783000,"sourceSize":1508,"sourcePath":"Algorithms/Leetcode/Tree/2196. Create Binary Tree From Descriptions.md","exportPath":"algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html":{"createdTime":1741084475748,"modifiedTime":1737554783000,"sourceSize":1149,"sourcePath":"Algorithms/Leetcode/Tree/2415. Reverse Odd Levels of Binary Tree.md","exportPath":"algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/leetcode/road-map.html":{"createdTime":1693680696995,"modifiedTime":1695308042284,"sourceSize":13635,"sourcePath":"Algorithms/Leetcode/Road Map.md","exportPath":"algorithms/leetcode/road-map.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"algorithms/algorithms.html":{"createdTime":1693680747190,"modifiedTime":1697824483947,"sourceSize":251,"sourcePath":"Algorithms/Algorithms.md","exportPath":"algorithms/algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/computer-vision/0.-image-processing.html":{"createdTime":1741084156443,"modifiedTime":1737554783000,"sourceSize":2787,"sourcePath":"Artificial Intelligence/Computer Vision/0. Image Processing.md","exportPath":"artificial-intelligence/computer-vision/0.-image-processing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html":{"createdTime":1741084156447,"modifiedTime":1737554783000,"sourceSize":1260,"sourcePath":"Artificial Intelligence/Computer Vision/1. Loss Function & Optimization.md","exportPath":"artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/computer-vision/2.-neural-networks.html":{"createdTime":1741084156451,"modifiedTime":1737554783000,"sourceSize":127,"sourcePath":"Artificial Intelligence/Computer Vision/2. Neural Networks.md","exportPath":"artificial-intelligence/computer-vision/2.-neural-networks.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html":{"createdTime":1741084156474,"modifiedTime":1737554783000,"sourceSize":1110,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.1 Why Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html":{"createdTime":1741084156478,"modifiedTime":1737554783000,"sourceSize":3891,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.2 What is Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html":{"createdTime":1741084156483,"modifiedTime":1737554783000,"sourceSize":3451,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.3. What makes a pattern useful.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html":{"createdTime":1741084156486,"modifiedTime":1737554783000,"sourceSize":3361,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.4 What kind of patterns can be mined.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html":{"createdTime":1741084156490,"modifiedTime":1737554783000,"sourceSize":2075,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.5 Challenges in Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html":{"createdTime":1741084156522,"modifiedTime":1737554783000,"sourceSize":5844,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.6 Privacy Implications of Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html":{"createdTime":1741084156611,"modifiedTime":1737554783000,"sourceSize":4305,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.1 Data Types and Representations.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html":{"createdTime":1741084156622,"modifiedTime":1737554783000,"sourceSize":2597,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.2 Basic Statistical Descriptions of a Single Data Variable.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html":{"createdTime":1741084156626,"modifiedTime":1737554783000,"sourceSize":3479,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.3 Measuring the Dispersion of Data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html":{"createdTime":1741084156641,"modifiedTime":1737554783000,"sourceSize":3487,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.4 Computation of Measures.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html":{"createdTime":1741084156644,"modifiedTime":1737554783000,"sourceSize":1775,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.5 Measuring Correlation amongst Two Variables.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html":{"createdTime":1741084156648,"modifiedTime":1737554783000,"sourceSize":2113,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.6 Measuring Similarity and Dissimilarity of Multivariate Data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html":{"createdTime":1741084156652,"modifiedTime":1737554783000,"sourceSize":1047,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.7 Proximity Measures for Nominal Attributes.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html":{"createdTime":1741084156656,"modifiedTime":1737554783000,"sourceSize":2170,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.8  Proximity Measures for Binary Attributes.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html":{"createdTime":1741084156659,"modifiedTime":1737554783000,"sourceSize":1466,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.9 Normalisation of numeric data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html":{"createdTime":1741084156615,"modifiedTime":1737554783000,"sourceSize":496,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.10 Minkowski Distance.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html":{"createdTime":1741084156618,"modifiedTime":1737554783000,"sourceSize":3365,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.11 Exercises.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html":{"createdTime":1741084156667,"modifiedTime":1737554783000,"sourceSize":4293,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/0. Basic concepts.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html":{"createdTime":1741084156671,"modifiedTime":1737554783000,"sourceSize":2040,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/1. Multi-dimensional Data Cubes.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html":{"createdTime":1741084156675,"modifiedTime":1737554783000,"sourceSize":1406,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/2. Concept Hierarchies.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html":{"createdTime":1741084156678,"modifiedTime":1737554783000,"sourceSize":1462,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/3. Modelling the Cube in a Relational Database.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html":{"createdTime":1741084156682,"modifiedTime":1737554783000,"sourceSize":2917,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/4. Data Mining in Data Warehouses.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html":{"createdTime":1741084156770,"modifiedTime":1737554783000,"sourceSize":4723,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/5. Typical OLAP Operations.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html":{"createdTime":1741084156774,"modifiedTime":1737554783000,"sourceSize":8000,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/6. Processing OLAP queries.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html":{"createdTime":1741084156777,"modifiedTime":1737554783000,"sourceSize":10891,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/7. Exercises.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html":{"createdTime":1741084156875,"modifiedTime":1737554783000,"sourceSize":1045,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.1 Motivation.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html":{"createdTime":1741084156883,"modifiedTime":1737554783000,"sourceSize":3730,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.2 Basic Concepts.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html":{"createdTime":1741084156887,"modifiedTime":1737554783000,"sourceSize":4819,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.3 Interesting pattern.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html":{"createdTime":1741084156890,"modifiedTime":1737554783000,"sourceSize":434,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.4 Frequent Itemset Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html":{"createdTime":1741084156894,"modifiedTime":1737554783000,"sourceSize":2867,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.5 Apriori algorithm.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html":{"createdTime":1741084156935,"modifiedTime":1737554783000,"sourceSize":511,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.6 Generating Association rules.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html":{"createdTime":1741084156939,"modifiedTime":1737554783000,"sourceSize":1029,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.7 Efficient frequent Data Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html":{"createdTime":1741084156943,"modifiedTime":1737554783000,"sourceSize":1818,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.8 Closed and Maximal Frequent.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html":{"createdTime":1741084156946,"modifiedTime":1737554783000,"sourceSize":8141,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.9 Adavanced pattern Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html":{"createdTime":1741084156879,"modifiedTime":1737554783000,"sourceSize":5303,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.10 Exercises.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html":{"createdTime":1741084156954,"modifiedTime":1737554783000,"sourceSize":3640,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/0. Classification.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html":{"createdTime":1741084156958,"modifiedTime":1737554783000,"sourceSize":9630,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/1. Decision Tree Induction.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html":{"createdTime":1741084156966,"modifiedTime":1737554783000,"sourceSize":1574,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/2. Overfitting and tree pruning.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html":{"createdTime":1741084156999,"modifiedTime":1737554783000,"sourceSize":2743,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/3. Enhancements to the Basic Algorithm.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html":{"createdTime":1741084157002,"modifiedTime":1737554783000,"sourceSize":3903,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/4. Rule-Based Classification.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html":{"createdTime":1741084157005,"modifiedTime":1737554783000,"sourceSize":6786,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/5. Bayes Classifiers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html":{"createdTime":1741084157009,"modifiedTime":1737554783000,"sourceSize":18194,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/6. Evaluation of Classifiers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html":{"createdTime":1741084157013,"modifiedTime":1737554783000,"sourceSize":125,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/7. Linear Regression & Neural Nets.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html":{"createdTime":1741084157017,"modifiedTime":1737554783000,"sourceSize":1413,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/8. SVM, Lazy Learners, & Variants.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html":{"createdTime":1741084157021,"modifiedTime":1737554783000,"sourceSize":10590,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/9. Classification, Decision Trees, Bayes, Evaluation.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html":{"createdTime":1741084156962,"modifiedTime":1737554783000,"sourceSize":7867,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/10. Other Classification and Prediction Methods.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html":{"createdTime":1741084157029,"modifiedTime":1737554783000,"sourceSize":9085,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/1. Basics.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html":{"createdTime":1741084157033,"modifiedTime":1737554783000,"sourceSize":9859,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/2. Partitioning method.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html":{"createdTime":1741084157036,"modifiedTime":1737554783000,"sourceSize":3995,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/3. Hierachical Clustering.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html":{"createdTime":1741084157040,"modifiedTime":1737554783000,"sourceSize":880,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/4. Density-Based Methods.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html":{"createdTime":1741084157044,"modifiedTime":1737554783000,"sourceSize":2816,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/5. Grid-Based Approach.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html":{"createdTime":1741084157074,"modifiedTime":1737554783000,"sourceSize":10218,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/6. Evaluation of Clustering.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html":{"createdTime":1741084157167,"modifiedTime":1737554783000,"sourceSize":8393,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/7. Clustering.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html":{"createdTime":1741084157174,"modifiedTime":1737554783000,"sourceSize":4836,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/1. Outliers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html":{"createdTime":1741084157178,"modifiedTime":1737554783000,"sourceSize":10434,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/2. Statistical Approach.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html":{"createdTime":1741084157182,"modifiedTime":1737554783000,"sourceSize":6295,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/3. Proximity-Based Approaches.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html":{"createdTime":1741084157185,"modifiedTime":1737554783000,"sourceSize":1399,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/4. Classification-Based Approaches.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html":{"createdTime":1741084157189,"modifiedTime":1737554783000,"sourceSize":1731,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/5. Clustering Based Approaches.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html":{"createdTime":1741084157199,"modifiedTime":1737554783000,"sourceSize":2474,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/6. Contextual and Collective Outliers.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html":{"createdTime":1741084157202,"modifiedTime":1737554783000,"sourceSize":3060,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/7. Outlier.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html":{"createdTime":1741084157210,"modifiedTime":1737554783000,"sourceSize":1079,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/0. Ensemble Methods.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html":{"createdTime":1741084157214,"modifiedTime":1737554783000,"sourceSize":4778,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/1. Bagging.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html":{"createdTime":1741084157218,"modifiedTime":1737554783000,"sourceSize":2396,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/2. Boosting with AdaBoost.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html":{"createdTime":1741084157222,"modifiedTime":1737554783000,"sourceSize":1495,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/3. Class-imbalanced data.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html":{"createdTime":1741084157320,"modifiedTime":1737554783000,"sourceSize":5126,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/4. Data Stream Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html":{"createdTime":1741084157345,"modifiedTime":1737554783000,"sourceSize":3914,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/5. Stream OLAP.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html":{"createdTime":1741084157350,"modifiedTime":1737554783000,"sourceSize":1933,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/6. Frequent Pattern Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html":{"createdTime":1741084157357,"modifiedTime":1737554783000,"sourceSize":1415,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/7. Time Series.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html":{"createdTime":1741084157365,"modifiedTime":1737554783000,"sourceSize":6604,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/8. Ensemble, Time series, Streams.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html":{"createdTime":1741084157415,"modifiedTime":1737554783000,"sourceSize":1813,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/1. Basic Measures for Information Retrieval.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html":{"createdTime":1741084157427,"modifiedTime":1737554783000,"sourceSize":8082,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/2. Informaiton Retrieval Techniques.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html":{"createdTime":1741084157430,"modifiedTime":1737554783000,"sourceSize":2856,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/3. Text mining problems.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html":{"createdTime":1741084157434,"modifiedTime":1737554783000,"sourceSize":1434,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/4. Web mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html":{"createdTime":1741084157438,"modifiedTime":1737554783000,"sourceSize":3252,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/5. Word meaning by embedding.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html":{"createdTime":1741084157579,"modifiedTime":1737554783000,"sourceSize":5887,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/6. Text & Web Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html":{"createdTime":1741084156530,"modifiedTime":1737554783000,"sourceSize":1543,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/1. Semantic Web.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html":{"createdTime":1741084156534,"modifiedTime":1737554783000,"sourceSize":9571,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/2. Foundation Technologies.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html":{"createdTime":1741084156538,"modifiedTime":1737554783000,"sourceSize":9197,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/3. The Web Ontology Language.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html":{"createdTime":1741084156542,"modifiedTime":1737554783000,"sourceSize":3379,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/4. Semantic Web Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html":{"createdTime":1741084156545,"modifiedTime":1737554783000,"sourceSize":2909,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/5. Semantic Web.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html":{"createdTime":1741084156467,"modifiedTime":1737554783000,"sourceSize":35895,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/0. Concepts/Data Mining Links.md","exportPath":"artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html":{"createdTime":1741084157588,"modifiedTime":1737554783000,"sourceSize":1922,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/0. Data Types.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html":{"createdTime":1741084157593,"modifiedTime":1737554783000,"sourceSize":637,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/1. Data Preprocessing.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html":{"createdTime":1741084157596,"modifiedTime":1737554783000,"sourceSize":1371,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/2. Exploratory Data Analysis.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html":{"createdTime":1741084157600,"modifiedTime":1737554783000,"sourceSize":2120,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/3. Data Mining Techniques.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html":{"createdTime":1741084157606,"modifiedTime":1737554783000,"sourceSize":2284,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/4. Model Evaluation.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html":{"createdTime":1741084157657,"modifiedTime":1737554783000,"sourceSize":1649,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/5. Feature Selection.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html":{"createdTime":1741084157663,"modifiedTime":1737554783000,"sourceSize":1687,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/6. Model Deployment.md","exportPath":"artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html":{"createdTime":1741084157671,"modifiedTime":1737554783000,"sourceSize":1811,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/0. Supervised learning algorithms.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html":{"createdTime":1741084157676,"modifiedTime":1737554783000,"sourceSize":1618,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/1. Unsupervised learning algorithms.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html":{"createdTime":1741084157679,"modifiedTime":1737554783000,"sourceSize":1197,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/2. Association rule mining.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html":{"createdTime":1741084157683,"modifiedTime":1737554783000,"sourceSize":1507,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/3. Time series analysis.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html":{"createdTime":1741084157688,"modifiedTime":1737554783000,"sourceSize":1676,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/4. Text Mining.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html":{"createdTime":1741084157706,"modifiedTime":1737554783000,"sourceSize":2104,"sourcePath":"Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/5. Lossy Counting Algorithm.md","exportPath":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/0.-concepts.html":{"createdTime":1741084157715,"modifiedTime":1737554783000,"sourceSize":1181,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/0. Concepts.md","exportPath":"artificial-intelligence/data-science/data-wrangling/0.-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/1.-process.html":{"createdTime":1741084157719,"modifiedTime":1737554783000,"sourceSize":5153,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/1. Process.md","exportPath":"artificial-intelligence/data-science/data-wrangling/1.-process.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html":{"createdTime":1741084157730,"modifiedTime":1737554783000,"sourceSize":3025,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/2. Source of Data.md","exportPath":"artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html":{"createdTime":1741084157735,"modifiedTime":1737554783000,"sourceSize":1233,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/3. Types and Measurements of Data.md","exportPath":"artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html":{"createdTime":1741084157771,"modifiedTime":1737554783000,"sourceSize":2809,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/4. Record Linkage Process.md","exportPath":"artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/5.-bins.html":{"createdTime":1741084157775,"modifiedTime":1737554783000,"sourceSize":412,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/5. Bins.md","exportPath":"artificial-intelligence/data-science/data-wrangling/5.-bins.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html":{"createdTime":1741084157779,"modifiedTime":1737554783000,"sourceSize":2792,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/6. Record Comparison.md","exportPath":"artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/7.-record-classification.html":{"createdTime":1741084157782,"modifiedTime":1737554783000,"sourceSize":2128,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/7. Record classification.md","exportPath":"artificial-intelligence/data-science/data-wrangling/7.-record-classification.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html":{"createdTime":1741084157786,"modifiedTime":1737554783000,"sourceSize":954,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/8. Measure Linkage.md","exportPath":"artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html":{"createdTime":1741084157790,"modifiedTime":1737554783000,"sourceSize":2724,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/9. Blocking Process Evaluation Metrics.md","exportPath":"artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html":{"createdTime":1741084157794,"modifiedTime":1737554783000,"sourceSize":2709,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/9.1 Blocking Methods.md","exportPath":"artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html":{"createdTime":1741084157723,"modifiedTime":1737554783000,"sourceSize":2573,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/10. Classification Techniques.md","exportPath":"artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html":{"createdTime":1741084157727,"modifiedTime":1737554783000,"sourceSize":2723,"sourcePath":"Artificial Intelligence/Data Science/Data Wrangling/11. Ontology Matching.md","exportPath":"artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html":{"createdTime":1741084157805,"modifiedTime":1737554783000,"sourceSize":7271,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/0. Information Retrieval.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html":{"createdTime":1741084157810,"modifiedTime":1737554783000,"sourceSize":2982,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/1. Classic Search Model.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html":{"createdTime":1741084157825,"modifiedTime":1737554783000,"sourceSize":9878,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/2. Boolean Retrieval.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html":{"createdTime":1741084157873,"modifiedTime":1737554783000,"sourceSize":4973,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/3. Term-Document Incidence Matrix.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html":{"createdTime":1741084157877,"modifiedTime":1737554783000,"sourceSize":10605,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/4. Inverted Index.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html":{"createdTime":1741084157881,"modifiedTime":1737554783000,"sourceSize":1795,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/5. Merge Intersection Algorithm (p1, p2).md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html":{"createdTime":1741084157885,"modifiedTime":1737554783000,"sourceSize":2953,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/6. Text Preprocessing.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html":{"createdTime":1741084157889,"modifiedTime":1737554783000,"sourceSize":4511,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/7. Ranked Retrieval.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html":{"createdTime":1741084157893,"modifiedTime":1737554783000,"sourceSize":4980,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/8. TF & IDF.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html":{"createdTime":1741084157897,"modifiedTime":1737554783000,"sourceSize":4052,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/9. Vector Space Model.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html":{"createdTime":1741084157814,"modifiedTime":1737554783000,"sourceSize":10785,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/10. Evaluation.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html":{"createdTime":1741084157817,"modifiedTime":1737554783000,"sourceSize":4362,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/11. Web Search.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html":{"createdTime":1741084157821,"modifiedTime":1737554783000,"sourceSize":1529,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/12. HITS Algorithm.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html":{"createdTime":1741084157901,"modifiedTime":1737554783000,"sourceSize":3229,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/Review.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html":{"createdTime":1741084157909,"modifiedTime":1737554783000,"sourceSize":8727,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/0. Representation.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html":{"createdTime":1741084157913,"modifiedTime":1737554783000,"sourceSize":20,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/1.  Word2Vec.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html":{"createdTime":1741084157917,"modifiedTime":1737554783000,"sourceSize":84,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/2. Transformer.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html":{"createdTime":1741084157922,"modifiedTime":1737554783000,"sourceSize":3455,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/3. Neural Network Language Models.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html":{"createdTime":1741084157926,"modifiedTime":1737554783000,"sourceSize":4473,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/4. RNN LM.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html":{"createdTime":1741084157929,"modifiedTime":1737554783000,"sourceSize":3205,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5. Pre-trained LM.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html":{"createdTime":1741084157943,"modifiedTime":1737554783000,"sourceSize":2689,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.1 ELMO.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html":{"createdTime":1741084157947,"modifiedTime":1737554783000,"sourceSize":2747,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.2 GPT Models.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html":{"createdTime":1741084157950,"modifiedTime":1737554783000,"sourceSize":3175,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.3 Others.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html":{"createdTime":1741084157954,"modifiedTime":1737554783000,"sourceSize":1025,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/Review.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html":{"createdTime":1741084157962,"modifiedTime":1737554783000,"sourceSize":2072,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.1 Language Modelling.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html":{"createdTime":1741084157966,"modifiedTime":1737554783000,"sourceSize":14097,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.2 Smoothing.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html":{"createdTime":1741084157970,"modifiedTime":1737554783000,"sourceSize":4546,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.3 Evaluation of Language Models.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html":{"createdTime":1741084157975,"modifiedTime":1737554783000,"sourceSize":14669,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/2. Syntactic Parsing.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html":{"createdTime":1741084157979,"modifiedTime":1737554783000,"sourceSize":11027,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/3. Semantics & Coreference Resolution.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html":{"createdTime":1741084158007,"modifiedTime":1737554783000,"sourceSize":5505,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/4. Evaluation In NLP.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html":{"createdTime":1741084158010,"modifiedTime":1737554783000,"sourceSize":5812,"sourcePath":"Artificial Intelligence/Data Science/Documnet Analysis/NLP/5. Multilingual and Low Resource NLP.md","exportPath":"artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html":{"createdTime":1741084158026,"modifiedTime":1737554783000,"sourceSize":6338,"sourcePath":"Artificial Intelligence/Deep Learning/0. Basic/0. Backpropagation.md","exportPath":"artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html":{"createdTime":1741084158030,"modifiedTime":1737554783000,"sourceSize":2614,"sourcePath":"Artificial Intelligence/Deep Learning/0. Basic/1. Neural Network.md","exportPath":"artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/0.-basic/2.-resnet.html":{"createdTime":1741084158033,"modifiedTime":1737554783000,"sourceSize":5010,"sourcePath":"Artificial Intelligence/Deep Learning/0. Basic/2. ResNet.md","exportPath":"artificial-intelligence/deep-learning/0.-basic/2.-resnet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html":{"createdTime":1741084158041,"modifiedTime":1737554783000,"sourceSize":12027,"sourcePath":"Artificial Intelligence/Deep Learning/1. Convolutional Neural Networks/Convolutional neural networks (CNNs).md","exportPath":"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html":{"createdTime":1741084158050,"modifiedTime":1737554783000,"sourceSize":8660,"sourcePath":"Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/1. Recurrent Neural Networks (RNNs).md","exportPath":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html":{"createdTime":1741084158054,"modifiedTime":1737554783000,"sourceSize":2964,"sourcePath":"Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/2. Gated Recurrent Unit (GRU).md","exportPath":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html":{"createdTime":1741084158058,"modifiedTime":1737554783000,"sourceSize":3730,"sourcePath":"Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/3. Long Short-Term Memory.md","exportPath":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html":{"createdTime":1741084158066,"modifiedTime":1737554783000,"sourceSize":4252,"sourcePath":"Artificial Intelligence/Deep Learning/3. Generative Adversarial Networks/Generative Adversarial Networks (GANs).md","exportPath":"artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html":{"createdTime":1741084158106,"modifiedTime":1737554783000,"sourceSize":2330,"sourcePath":"Artificial Intelligence/Deep Learning/4. Transfer Learning/0. Transfer Learning.md","exportPath":"artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html":{"createdTime":1741084158110,"modifiedTime":1737554783000,"sourceSize":6938,"sourcePath":"Artificial Intelligence/Deep Learning/4. Transfer Learning/1. Low-Rank Adaptation.md","exportPath":"artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html":{"createdTime":1741084158118,"modifiedTime":1737554783000,"sourceSize":3403,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/0. Self-Attention.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html":{"createdTime":1741084158123,"modifiedTime":1737554783000,"sourceSize":4363,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/1. Encoder.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html":{"createdTime":1741084158127,"modifiedTime":1737554783000,"sourceSize":4189,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/2. Decorder.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html":{"createdTime":1741084158130,"modifiedTime":1737554783000,"sourceSize":5259,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/3. BERT.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html":{"createdTime":1741084158135,"modifiedTime":1737554783000,"sourceSize":3463,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/3.1 DeBERTa.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html":{"createdTime":1741084158139,"modifiedTime":1737554783000,"sourceSize":2930,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/4. XLNet.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html":{"createdTime":1741084158173,"modifiedTime":1737554783000,"sourceSize":3782,"sourcePath":"Artificial Intelligence/Deep Learning/5. Transformer/5. GPT.md","exportPath":"artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html":{"createdTime":1741084158181,"modifiedTime":1737554783000,"sourceSize":3710,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/0 Multimodal Learning.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html":{"createdTime":1741084158184,"modifiedTime":1737554783000,"sourceSize":3872,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/1. Alignment.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html":{"createdTime":1741084158188,"modifiedTime":1737554783000,"sourceSize":3952,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/2. Fusion.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html":{"createdTime":1741084158192,"modifiedTime":1737554783000,"sourceSize":3489,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/3. Representation Learning.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html":{"createdTime":1741084158195,"modifiedTime":1737554783000,"sourceSize":5223,"sourcePath":"Artificial Intelligence/Deep Learning/6. Multimodal/4. Key Multimodal Models.md","exportPath":"artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html":{"createdTime":1741084158212,"modifiedTime":1737554783000,"sourceSize":4213,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/0. Overview.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html":{"createdTime":1741084158216,"modifiedTime":1737554783000,"sourceSize":10710,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/1. Mathematical Expressions.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html":{"createdTime":1741084158220,"modifiedTime":1737554783000,"sourceSize":4219,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/2. Kalman Filter.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html":{"createdTime":1741084158224,"modifiedTime":1737554783000,"sourceSize":3629,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/3. Extended Kalman Filter.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html":{"createdTime":1741084158228,"modifiedTime":1737554783000,"sourceSize":4218,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/4. Particle Filter.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html":{"createdTime":1741084158203,"modifiedTime":1737554783000,"sourceSize":5490,"sourcePath":"Artificial Intelligence/Deep Learning/7. SLAM/0. Introduction to SLAM.md","exportPath":"artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/deep-learning/0.2.-resnet.html":{"createdTime":1741084158018,"modifiedTime":1737554783000,"sourceSize":5010,"sourcePath":"Artificial Intelligence/Deep Learning/0.2. ResNet.md","exportPath":"artificial-intelligence/deep-learning/0.2.-resnet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html":{"createdTime":1741084158241,"modifiedTime":1737554783000,"sourceSize":2901,"sourcePath":"Artificial Intelligence/Knowledge Graph/0. Foundations/0. Introduction.md","exportPath":"artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html":{"createdTime":1741084158245,"modifiedTime":1737554783000,"sourceSize":3161,"sourcePath":"Artificial Intelligence/Knowledge Graph/0. Foundations/1. Graph Theory Essentials.md","exportPath":"artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html":{"createdTime":1741084158249,"modifiedTime":1737554783000,"sourceSize":4283,"sourcePath":"Artificial Intelligence/Knowledge Graph/0. Foundations/2. Semantic Web Technologies.md","exportPath":"artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html":{"createdTime":1741084158258,"modifiedTime":1737554783000,"sourceSize":3284,"sourcePath":"Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/0. Building and Modeling.md","exportPath":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html":{"createdTime":1741084158262,"modifiedTime":1737554783000,"sourceSize":3130,"sourcePath":"Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/1. Querying and Integration.md","exportPath":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html":{"createdTime":1741084158266,"modifiedTime":1737554783000,"sourceSize":3490,"sourcePath":"Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/2. Visualizing.md","exportPath":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html":{"createdTime":1741084158274,"modifiedTime":1737554783000,"sourceSize":3346,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/0. AI Enhancement.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html":{"createdTime":1741084158278,"modifiedTime":1737554783000,"sourceSize":3754,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/1. Knowledge Graph Embeddings.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html":{"createdTime":1741084158283,"modifiedTime":1737554783000,"sourceSize":3392,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/2. Dynamic and Temporal Knowledge Graphs.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html":{"createdTime":1741084158287,"modifiedTime":1737554783000,"sourceSize":3748,"sourcePath":"Artificial Intelligence/Knowledge Graph/2. Advanced/3. Scalability and Real-Time Updates.md","exportPath":"artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html":{"createdTime":1741084158300,"modifiedTime":1737554783000,"sourceSize":3836,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/0. Feature Selection.md","exportPath":"artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html":{"createdTime":1741084158304,"modifiedTime":1737554783000,"sourceSize":6456,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/1. Linear Regression.md","exportPath":"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html":{"createdTime":1741084158339,"modifiedTime":1737554783000,"sourceSize":5800,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/2. Logistic Regression.md","exportPath":"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html":{"createdTime":1741084158343,"modifiedTime":1737554783000,"sourceSize":9029,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/3. K-nearest Neighbors.md","exportPath":"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html":{"createdTime":1741084158347,"modifiedTime":1737554783000,"sourceSize":7408,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/4. Naive Bayesian Classifier.md","exportPath":"artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html":{"createdTime":1741084158351,"modifiedTime":1737554783000,"sourceSize":13260,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/5. Support Vector Machine.md","exportPath":"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html":{"createdTime":1741084158355,"modifiedTime":1737554783000,"sourceSize":3882,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/6. Decision Tree.md","exportPath":"artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/7.-random-forest.html":{"createdTime":1741084158360,"modifiedTime":1737554783000,"sourceSize":2922,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/7. Random Forest.md","exportPath":"artificial-intelligence/machine-learning/algorithm/7.-random-forest.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/8.-adaboost.html":{"createdTime":1741084158363,"modifiedTime":1737554783000,"sourceSize":1595,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/8. AdaBoost.md","exportPath":"artificial-intelligence/machine-learning/algorithm/8.-adaboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/8.-k-means.html":{"createdTime":1741084158367,"modifiedTime":1737554783000,"sourceSize":3791,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/8. K means.md","exportPath":"artificial-intelligence/machine-learning/algorithm/8.-k-means.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html":{"createdTime":1741084158371,"modifiedTime":1737554783000,"sourceSize":5095,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/9. Ensemble Learning.md","exportPath":"artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/9.-k-means.html":{"createdTime":1741084158375,"modifiedTime":1737554783000,"sourceSize":3647,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/9. K means.md","exportPath":"artificial-intelligence/machine-learning/algorithm/9.-k-means.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/10.-gbdt.html":{"createdTime":1741084158307,"modifiedTime":1737554783000,"sourceSize":6338,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/10. GBDT.md","exportPath":"artificial-intelligence/machine-learning/algorithm/10.-gbdt.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/11.-adaboost.html":{"createdTime":1741084158311,"modifiedTime":1737554783000,"sourceSize":1595,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/11. AdaBoost.md","exportPath":"artificial-intelligence/machine-learning/algorithm/11.-adaboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/12.-xgboost.html":{"createdTime":1741084158316,"modifiedTime":1737554783000,"sourceSize":17210,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/12. XGBoost.md","exportPath":"artificial-intelligence/machine-learning/algorithm/12.-xgboost.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html":{"createdTime":1741084158319,"modifiedTime":1737554783000,"sourceSize":7012,"sourcePath":"Artificial Intelligence/Machine Learning/Algorithm/13. LightGBM.md","exportPath":"artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html":{"createdTime":1741084158391,"modifiedTime":1737554783000,"sourceSize":3134,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/0. Spliting.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html":{"createdTime":1741084158394,"modifiedTime":1737554783000,"sourceSize":1839,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/1. Feature scaling.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html":{"createdTime":1741084158398,"modifiedTime":1737554783000,"sourceSize":1089,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/2. Fix overfitting.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html":{"createdTime":1741084158401,"modifiedTime":1737554783000,"sourceSize":1576,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/3. Seasonal Dummy.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html":{"createdTime":1741084158405,"modifiedTime":1737554783000,"sourceSize":2368,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/4. Regular Expression (Regex).md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html":{"createdTime":1741084158409,"modifiedTime":1737554783000,"sourceSize":4274,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Modify Data/5. Principal Component Analysis.md","exportPath":"artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html":{"createdTime":1741084158457,"modifiedTime":1737554783000,"sourceSize":1786,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Special Evaluation Method/1. Cross Validation.md","exportPath":"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html":{"createdTime":1741084158460,"modifiedTime":1737554783000,"sourceSize":3630,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Special Evaluation Method/2. Regularization.md","exportPath":"artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/data/preprocessing.html":{"createdTime":1741084158383,"modifiedTime":1737554783000,"sourceSize":2035,"sourcePath":"Artificial Intelligence/Machine Learning/Data/Preprocessing.md","exportPath":"artificial-intelligence/machine-learning/data/preprocessing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/math/1.-linear-algebra.html":{"createdTime":1741084158468,"modifiedTime":1737554783000,"sourceSize":2388,"sourcePath":"Artificial Intelligence/Machine Learning/Math/1. Linear Algebra.md","exportPath":"artificial-intelligence/machine-learning/math/1.-linear-algebra.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/math/2.-gradient-descent.html":{"createdTime":1741084158473,"modifiedTime":1737554783000,"sourceSize":4303,"sourcePath":"Artificial Intelligence/Machine Learning/Math/2. Gradient descent.md","exportPath":"artificial-intelligence/machine-learning/math/2.-gradient-descent.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html":{"createdTime":1741084158476,"modifiedTime":1737554783000,"sourceSize":2516,"sourcePath":"Artificial Intelligence/Machine Learning/Math/3. Full Rank Matrix.md","exportPath":"artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/1.-variance.html":{"createdTime":1741084158488,"modifiedTime":1737554783000,"sourceSize":722,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/1. Variance.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/1.-variance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/2.-covariance.html":{"createdTime":1741084158534,"modifiedTime":1737554783000,"sourceSize":116,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/2. Covariance.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/2.-covariance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html":{"createdTime":1741084158538,"modifiedTime":1737554783000,"sourceSize":137,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/3.1 Cost function - Least Square.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html":{"createdTime":1741084158542,"modifiedTime":1737554783000,"sourceSize":971,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/3.2 Cost Function - Logistic Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/4.-correlation.html":{"createdTime":1741084158546,"modifiedTime":1737554783000,"sourceSize":112,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/4. Correlation.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/4.-correlation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html":{"createdTime":1741084158568,"modifiedTime":1737554783000,"sourceSize":65,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/5. Sum of Squares.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html":{"createdTime":1741084158583,"modifiedTime":1737554783000,"sourceSize":2026,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/6. Normal Equation.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html":{"createdTime":1741084158587,"modifiedTime":1737554783000,"sourceSize":1189,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Formula/7. Cosine Distance.md","exportPath":"artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html":{"createdTime":1741084158595,"modifiedTime":1737554783000,"sourceSize":1026,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/1. Linear Regression stats.md","exportPath":"artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html":{"createdTime":1741084158600,"modifiedTime":1737554783000,"sourceSize":1600,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/2. Polynomial Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html":{"createdTime":1741084158604,"modifiedTime":1737554783000,"sourceSize":1490,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/3. Ridge Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html":{"createdTime":1741084158608,"modifiedTime":1737554783000,"sourceSize":927,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/4. Elastic Net.md","exportPath":"artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html":{"createdTime":1741084158611,"modifiedTime":1737554783000,"sourceSize":958,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/5.  Logistic Regression.md","exportPath":"artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html":{"createdTime":1741084158615,"modifiedTime":1737554783000,"sourceSize":11582,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/6. Neural Network Learning.md","exportPath":"artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html":{"createdTime":1741084158647,"modifiedTime":1737554783000,"sourceSize":1958,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/7. Lazy Leaner.md","exportPath":"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html":{"createdTime":1741084158650,"modifiedTime":1737554783000,"sourceSize":1156,"sourcePath":"Artificial Intelligence/Machine Learning/Stats/Models/8. Eager Learning.md","exportPath":"artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/basics/1.-components.html":{"createdTime":1741084158711,"modifiedTime":1737554783000,"sourceSize":2869,"sourcePath":"Artificial Intelligence/Natural Language Processing/Basics/1. Components.md","exportPath":"artificial-intelligence/natural-language-processing/basics/1.-components.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html":{"createdTime":1741084158715,"modifiedTime":1737554783000,"sourceSize":2835,"sourcePath":"Artificial Intelligence/Natural Language Processing/Basics/1. Pipeline.md","exportPath":"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/basics/2.-tasks.html":{"createdTime":1741084158718,"modifiedTime":1737554783000,"sourceSize":2841,"sourcePath":"Artificial Intelligence/Natural Language Processing/Basics/2. Tasks.md","exportPath":"artificial-intelligence/natural-language-processing/basics/2.-tasks.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html":{"createdTime":1741084158726,"modifiedTime":1737554783000,"sourceSize":5931,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/0. Word Segmentation.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html":{"createdTime":1741084158730,"modifiedTime":1737554783000,"sourceSize":3287,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/1. Segmentation Method.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html":{"createdTime":1741084158734,"modifiedTime":1737554783000,"sourceSize":3436,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/2. Max Matching.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html":{"createdTime":1741084158737,"modifiedTime":1737554783000,"sourceSize":3459,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/3. Incorporate Semantic.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html":{"createdTime":1741084158741,"modifiedTime":1737554783000,"sourceSize":2389,"sourcePath":"Artificial Intelligence/Natural Language Processing/Language Processing/3.1 Viterbi Algorithm.md","exportPath":"artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html":{"createdTime":1741084158749,"modifiedTime":1737554783000,"sourceSize":1747,"sourcePath":"Artificial Intelligence/Natural Language Processing/Machine Translation/0. Basics.md","exportPath":"artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html":{"createdTime":1741084158754,"modifiedTime":1737554783000,"sourceSize":2844,"sourcePath":"Artificial Intelligence/Natural Language Processing/Machine Translation/1. Statistical Machine Translation (SMT).md","exportPath":"artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html":{"createdTime":1741084158762,"modifiedTime":1737554783000,"sourceSize":1828,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Conditional Random Fields (CRF).md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html":{"createdTime":1741084158811,"modifiedTime":1737554783000,"sourceSize":3357,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Decoding Algorithm.md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html":{"createdTime":1741084158815,"modifiedTime":1737554783000,"sourceSize":4524,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Hidden Markov Models (HMM).md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html":{"createdTime":1741084158818,"modifiedTime":1737554783000,"sourceSize":4520,"sourcePath":"Artificial Intelligence/Natural Language Processing/Models & Algorithm/Word2Vec.md","exportPath":"artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html":{"createdTime":1741084158826,"modifiedTime":1737554783000,"sourceSize":3834,"sourcePath":"Artificial Intelligence/Natural Language Processing/Parsing & Semantic/Parsing.md","exportPath":"artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html":{"createdTime":1741084158830,"modifiedTime":1737554783000,"sourceSize":3386,"sourcePath":"Artificial Intelligence/Natural Language Processing/Parsing & Semantic/Semantic.md","exportPath":"artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html":{"createdTime":1741084158872,"modifiedTime":1737554783000,"sourceSize":1794,"sourcePath":"Artificial Intelligence/Natural Language Processing/Techniques/0. Prepare Data.md","exportPath":"artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html":{"createdTime":1741084158875,"modifiedTime":1737554783000,"sourceSize":1931,"sourcePath":"Artificial Intelligence/Natural Language Processing/Techniques/1. Feature Extraction & Model.md","exportPath":"artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/reinforcement-learning/0.-q-learning.html":{"createdTime":1741084158884,"modifiedTime":1737554783000,"sourceSize":4669,"sourcePath":"Artificial Intelligence/Reinforcement Learning/0. Q-Learning.md","exportPath":"artificial-intelligence/reinforcement-learning/0.-q-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html":{"createdTime":1741084158946,"modifiedTime":1737554783000,"sourceSize":1132,"sourcePath":"Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/0. Introduction.md","exportPath":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html":{"createdTime":1741084158950,"modifiedTime":1737554783000,"sourceSize":4000,"sourcePath":"Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/1. Propositional Logic.md","exportPath":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html":{"createdTime":1741084158954,"modifiedTime":1737554783000,"sourceSize":3723,"sourcePath":"Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/2. First Order Logic.md","exportPath":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html":{"createdTime":1741084158966,"modifiedTime":1737554783000,"sourceSize":5226,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/0. Agents.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html":{"createdTime":1741084158970,"modifiedTime":1737554783000,"sourceSize":2586,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/1. Search.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html":{"createdTime":1741084158974,"modifiedTime":1737554783000,"sourceSize":9836,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/2. Uninformed search strategies.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html":{"createdTime":1741084158977,"modifiedTime":1737554783000,"sourceSize":4324,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/3. Informed search algorithms.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html":{"createdTime":1741084158981,"modifiedTime":1737554783000,"sourceSize":14346,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/4. Adversarial Search Problems.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html":{"createdTime":1741084159008,"modifiedTime":1737554783000,"sourceSize":6521,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/0. Exercises 1.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html":{"createdTime":1741084159012,"modifiedTime":1737554783000,"sourceSize":5888,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/1. Exercises 2.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html":{"createdTime":1741084159015,"modifiedTime":1737554783000,"sourceSize":17273,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/2. Exercises 3.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html":{"createdTime":1741084159019,"modifiedTime":1737554783000,"sourceSize":9391,"sourcePath":"Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/3. Exercises 4.md","exportPath":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html":{"createdTime":1741084159031,"modifiedTime":1737554783000,"sourceSize":13888,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/0. Intro.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html":{"createdTime":1741084159035,"modifiedTime":1737554783000,"sourceSize":7313,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/1. DPLL.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html":{"createdTime":1741084159039,"modifiedTime":1737554783000,"sourceSize":4654,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/2. Prenex Normal form.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html":{"createdTime":1741084159042,"modifiedTime":1737554783000,"sourceSize":10009,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/3. Constraint Satisfaction Problem.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html":{"createdTime":1741084159046,"modifiedTime":1737554783000,"sourceSize":6489,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/4. Constraint Learning.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html":{"createdTime":1741084159049,"modifiedTime":1737554783000,"sourceSize":12492,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/5. Optimal Solving.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html":{"createdTime":1741084159057,"modifiedTime":1737554783000,"sourceSize":4965,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/6. Temporal Constraint Networks.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html":{"createdTime":1741084159066,"modifiedTime":1737554783000,"sourceSize":6334,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/1. Exercises/Constraint Satisfaction 1.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html":{"createdTime":1741084159069,"modifiedTime":1737554783000,"sourceSize":7251,"sourcePath":"Artificial Intelligence/Symbolic AI/2. KRR/1. Exercises/Constraint Satisfaction 2.md","exportPath":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html":{"createdTime":1741084159263,"modifiedTime":1737554783000,"sourceSize":3961,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/0. Intro.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html":{"createdTime":1741084159267,"modifiedTime":1737554783000,"sourceSize":4617,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/1. Plans.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html":{"createdTime":1741084159271,"modifiedTime":1737554783000,"sourceSize":13213,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/2. Planning Algorithm.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html":{"createdTime":1741084159274,"modifiedTime":1737554783000,"sourceSize":10791,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/3. Heuristics.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html":{"createdTime":1741084159278,"modifiedTime":1737554783000,"sourceSize":1292,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/4. Regression.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html":{"createdTime":1741084159282,"modifiedTime":1737554783000,"sourceSize":3312,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/5. Plan space search.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html":{"createdTime":1741084159320,"modifiedTime":1737554783000,"sourceSize":7906,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/1. Exercises/Heuristics, Regression, and Partial-Order Planning.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html":{"createdTime":1741084159325,"modifiedTime":1737554783000,"sourceSize":6841,"sourcePath":"Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/1. Exercises/Planning Representations and Graph-Based Approaches.md","exportPath":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/0.-data-lake/0.-introduction.html":{"createdTime":1737584641308,"modifiedTime":1737554783000,"sourceSize":8744,"sourcePath":"Big Data/0. Data Lake/0. Introduction.md","exportPath":"big-data/0.-data-lake/0.-introduction.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/0.-data-lake/1.-architecture.html":{"createdTime":1737584641310,"modifiedTime":1737554783000,"sourceSize":4873,"sourcePath":"Big Data/0. Data Lake/1. Architecture.md","exportPath":"big-data/0.-data-lake/1.-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/0.-data-lake/2.-workflow.html":{"createdTime":1737584641311,"modifiedTime":1737554783000,"sourceSize":4811,"sourcePath":"Big Data/0. Data Lake/2. Workflow.md","exportPath":"big-data/0.-data-lake/2.-workflow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/0.-data-lake/3.-datax.html":{"createdTime":1737584641312,"modifiedTime":1737554783000,"sourceSize":14671,"sourcePath":"Big Data/0. Data Lake/3. DataX.md","exportPath":"big-data/0.-data-lake/3.-datax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/1.-hadoop.html":{"createdTime":1737584641314,"modifiedTime":1737554783000,"sourceSize":4526,"sourcePath":"Big Data/1. Hadoop/1. Hadoop.md","exportPath":"big-data/1.-hadoop/1.-hadoop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/2.-hdfs.html":{"createdTime":1737584641316,"modifiedTime":1737554783000,"sourceSize":5758,"sourcePath":"Big Data/1. Hadoop/2. HDFS.md","exportPath":"big-data/1.-hadoop/2.-hdfs.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/3.-mapreduce-&-yarn.html":{"createdTime":1737584641317,"modifiedTime":1737554783000,"sourceSize":7574,"sourcePath":"Big Data/1. Hadoop/3. MapReduce & Yarn.md","exportPath":"big-data/1.-hadoop/3.-mapreduce-&-yarn.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/3.-metadata.html":{"createdTime":1737584641318,"modifiedTime":1737554783000,"sourceSize":5705,"sourcePath":"Big Data/1. Hadoop/3. Metadata.md","exportPath":"big-data/1.-hadoop/3.-metadata.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/4.-hdfs-shell.html":{"createdTime":1737584641319,"modifiedTime":1737554783000,"sourceSize":5749,"sourcePath":"Big Data/1. Hadoop/4. HDFS Shell.md","exportPath":"big-data/1.-hadoop/4.-hdfs-shell.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/5.-java-api.html":{"createdTime":1737584641321,"modifiedTime":1737554783000,"sourceSize":1552,"sourcePath":"Big Data/1. Hadoop/5. Java API.md","exportPath":"big-data/1.-hadoop/5.-java-api.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/6.-mapreduce-&-yarn.html":{"createdTime":1737584641327,"modifiedTime":1737554783000,"sourceSize":7574,"sourcePath":"Big Data/1. Hadoop/6. MapReduce & Yarn.md","exportPath":"big-data/1.-hadoop/6.-mapreduce-&-yarn.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html":{"createdTime":1737584641352,"modifiedTime":1737554783000,"sourceSize":3997,"sourcePath":"Big Data/1. Hadoop/7. Single Node Deployment Guide - Tecent Cloud.md","exportPath":"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/0.-comprehensive-guide.html":{"createdTime":1737584641446,"modifiedTime":1737554783000,"sourceSize":5062,"sourcePath":"Big Data/2. Scala/0. Comprehensive Guide.md","exportPath":"big-data/2.-scala/0.-comprehensive-guide.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/1.-variables-and-data-types.html":{"createdTime":1737584641447,"modifiedTime":1737554783000,"sourceSize":8099,"sourcePath":"Big Data/2. Scala/1. Variables and Data Types.md","exportPath":"big-data/2.-scala/1.-variables-and-data-types.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/2.-operators.html":{"createdTime":1737584641457,"modifiedTime":1737554783000,"sourceSize":5866,"sourcePath":"Big Data/2. Scala/2. Operators.md","exportPath":"big-data/2.-scala/2.-operators.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/3.-control-flow.html":{"createdTime":1737584641458,"modifiedTime":1737554783000,"sourceSize":3739,"sourcePath":"Big Data/2. Scala/3. Control Flow.md","exportPath":"big-data/2.-scala/3.-control-flow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/4.-functional-programming.html":{"createdTime":1737584641459,"modifiedTime":1737554783000,"sourceSize":8659,"sourcePath":"Big Data/2. Scala/4. Functional Programming.md","exportPath":"big-data/2.-scala/4.-functional-programming.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/5.-package-management.html":{"createdTime":1737584641465,"modifiedTime":1737554783000,"sourceSize":4583,"sourcePath":"Big Data/2. Scala/5. Package Management.md","exportPath":"big-data/2.-scala/5.-package-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/6.-object-oriented-programming.html":{"createdTime":1737584641466,"modifiedTime":1737554783000,"sourceSize":7669,"sourcePath":"Big Data/2. Scala/6. Object-Oriented Programming.md","exportPath":"big-data/2.-scala/6.-object-oriented-programming.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/7.-collections.html":{"createdTime":1737584641467,"modifiedTime":1737554783000,"sourceSize":5552,"sourcePath":"Big Data/2. Scala/7. Collections.md","exportPath":"big-data/2.-scala/7.-collections.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/8.-pattern-matching.html":{"createdTime":1737584641468,"modifiedTime":1737554783000,"sourceSize":4509,"sourcePath":"Big Data/2. Scala/8. Pattern Matching.md","exportPath":"big-data/2.-scala/8.-pattern-matching.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/9.-exception-handling.html":{"createdTime":1737584641475,"modifiedTime":1737554783000,"sourceSize":5625,"sourcePath":"Big Data/2. Scala/9. Exception Handling.md","exportPath":"big-data/2.-scala/9.-exception-handling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/2.-scala/10.-sbt.html":{"createdTime":1737584641448,"modifiedTime":1737554783000,"sourceSize":6879,"sourcePath":"Big Data/2. Scala/10. Sbt.md","exportPath":"big-data/2.-scala/10.-sbt.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/0.-concept-&-architecture/0.-spark.html":{"createdTime":1737584641495,"modifiedTime":1737554783000,"sourceSize":6968,"sourcePath":"Big Data/3. Spark/0. Concept & Architecture/0. Spark.md","exportPath":"big-data/3.-spark/0.-concept-&-architecture/0.-spark.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html":{"createdTime":1737584641496,"modifiedTime":1737554783000,"sourceSize":11896,"sourcePath":"Big Data/3. Spark/0. Concept & Architecture/1. Concepts.md","exportPath":"big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html":{"createdTime":1737584641497,"modifiedTime":1737554783000,"sourceSize":8523,"sourcePath":"Big Data/3. Spark/0. Concept & Architecture/2. Architecture and Execution.md","exportPath":"big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/0.-rdd.html":{"createdTime":1737584641498,"modifiedTime":1737554783000,"sourceSize":10388,"sourcePath":"Big Data/3. Spark/1. RDD/0. RDD.md","exportPath":"big-data/3.-spark/1.-rdd/0.-rdd.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/1.-transformations.html":{"createdTime":1737584641499,"modifiedTime":1737554783000,"sourceSize":17006,"sourcePath":"Big Data/3. Spark/1. RDD/1. Transformations.md","exportPath":"big-data/3.-spark/1.-rdd/1.-transformations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/2.-actions.html":{"createdTime":1737584641500,"modifiedTime":1737554783000,"sourceSize":4070,"sourcePath":"Big Data/3. Spark/1. RDD/2. Actions.md","exportPath":"big-data/3.-spark/1.-rdd/2.-actions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html":{"createdTime":1737584641501,"modifiedTime":1737554783000,"sourceSize":13997,"sourcePath":"Big Data/3. Spark/1. RDD/3. Serialization & Dependencies & Persistence.md","exportPath":"big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/4.-collaboration.html":{"createdTime":1737584641502,"modifiedTime":1737554783000,"sourceSize":6263,"sourcePath":"Big Data/3. Spark/1. RDD/4. Collaboration.md","exportPath":"big-data/3.-spark/1.-rdd/4.-collaboration.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html":{"createdTime":1737584641535,"modifiedTime":1737554783000,"sourceSize":1268,"sourcePath":"Big Data/3. Spark/1. RDD/5. Reading and Saving RDD Files.md","exportPath":"big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html":{"createdTime":1737584641570,"modifiedTime":1737554783000,"sourceSize":4520,"sourcePath":"Big Data/3. Spark/1. RDD/6. Accumulators & Broadcast Variables.md","exportPath":"big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/2.-spark-sql/0.-sparksql.html":{"createdTime":1737584641572,"modifiedTime":1737554783000,"sourceSize":2910,"sourcePath":"Big Data/3. Spark/2. Spark SQL/0. SparkSQL.md","exportPath":"big-data/3.-spark/2.-spark-sql/0.-sparksql.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html":{"createdTime":1737584641574,"modifiedTime":1737554783000,"sourceSize":2306,"sourcePath":"Big Data/3. Spark/2. Spark SQL/1. DataFrame & DataSet.md","exportPath":"big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/2.-spark-sql/2.-coding.html":{"createdTime":1737584641591,"modifiedTime":1737554783000,"sourceSize":9653,"sourcePath":"Big Data/3. Spark/2. Spark SQL/2. Coding.md","exportPath":"big-data/3.-spark/2.-spark-sql/2.-coding.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html":{"createdTime":1737584641592,"modifiedTime":1737554783000,"sourceSize":4342,"sourcePath":"Big Data/3. Spark/2. Spark SQL/3. RDD & DataFrame & DataSet.md","exportPath":"big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html":{"createdTime":1737584641594,"modifiedTime":1737554783000,"sourceSize":6684,"sourcePath":"Big Data/3. Spark/2. Spark SQL/4. User-Defined Functions.md","exportPath":"big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html":{"createdTime":1737584641595,"modifiedTime":1737554783000,"sourceSize":11623,"sourcePath":"Big Data/3. Spark/2. Spark SQL/5. Data Loading and Saving.md","exportPath":"big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/3.-streaming/0.-spark-streaming.html":{"createdTime":1737584641623,"modifiedTime":1737554783000,"sourceSize":6251,"sourcePath":"Big Data/3. Spark/3. Streaming/0. Spark Streaming.md","exportPath":"big-data/3.-spark/3.-streaming/0.-spark-streaming.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/3.-streaming/1.-kafka-data-source.html":{"createdTime":1737584641624,"modifiedTime":1737554783000,"sourceSize":4788,"sourcePath":"Big Data/3. Spark/3. Streaming/1. Kafka Data Source.md","exportPath":"big-data/3.-spark/3.-streaming/1.-kafka-data-source.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/3.-streaming/2.-dstream-transformations.html":{"createdTime":1737584641651,"modifiedTime":1737554783000,"sourceSize":7886,"sourcePath":"Big Data/3. Spark/3. Streaming/2. DStream Transformations.md","exportPath":"big-data/3.-spark/3.-streaming/2.-dstream-transformations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/3.-streaming/3.-dstream-output.html":{"createdTime":1737584641652,"modifiedTime":1737554783000,"sourceSize":5237,"sourcePath":"Big Data/3. Spark/3. Streaming/3. DStream Output.md","exportPath":"big-data/3.-spark/3.-streaming/3.-dstream-output.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/4.-core/0.-spark-kernel.html":{"createdTime":1737584641654,"modifiedTime":1737554783000,"sourceSize":2681,"sourcePath":"Big Data/3. Spark/4. Core/0. Spark Kernel.md","exportPath":"big-data/3.-spark/4.-core/0.-spark-kernel.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/4.-core/1.-deployment.html":{"createdTime":1737584641655,"modifiedTime":1737554783000,"sourceSize":5523,"sourcePath":"Big Data/3. Spark/4. Core/1. Deployment.md","exportPath":"big-data/3.-spark/4.-core/1.-deployment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/4.-core/2.-spark-communication-architecture.html":{"createdTime":1737584641656,"modifiedTime":1737554783000,"sourceSize":5340,"sourcePath":"Big Data/3. Spark/4. Core/2. Spark Communication Architecture.md","exportPath":"big-data/3.-spark/4.-core/2.-spark-communication-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/4.-core/3.-task-scheduling.html":{"createdTime":1737584641658,"modifiedTime":1737554783000,"sourceSize":10236,"sourcePath":"Big Data/3. Spark/4. Core/3. Task Scheduling.md","exportPath":"big-data/3.-spark/4.-core/3.-task-scheduling.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html":{"createdTime":1737584641659,"modifiedTime":1737554783000,"sourceSize":4646,"sourcePath":"Big Data/3. Spark/4. Core/4. Spark Shuffle Analysis.md","exportPath":"big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/5.-pyspark/0.-installation.html":{"createdTime":1737584641660,"modifiedTime":1737554783000,"sourceSize":2905,"sourcePath":"Big Data/3. Spark/5. Pyspark/0. Installation.md","exportPath":"big-data/3.-spark/5.-pyspark/0.-installation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/5.-pyspark/1.-dataframe.html":{"createdTime":1737584641698,"modifiedTime":1737554783000,"sourceSize":10672,"sourcePath":"Big Data/3. Spark/5. Pyspark/1. DataFrame.md","exportPath":"big-data/3.-spark/5.-pyspark/1.-dataframe.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/5.-pyspark/2.-spark-connect.html":{"createdTime":1737584641699,"modifiedTime":1737554783000,"sourceSize":1629,"sourcePath":"Big Data/3. Spark/5. Pyspark/2. Spark Connect.md","exportPath":"big-data/3.-spark/5.-pyspark/2.-spark-connect.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html":{"createdTime":1737584641701,"modifiedTime":1737554783000,"sourceSize":613,"sourcePath":"Big Data/3. Spark/5. Pyspark/3. Pandas API on Spark.md","exportPath":"big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/3.-spark/6.-source-code/0.-spark-storage.html":{"createdTime":1737584641705,"modifiedTime":1737554783000,"sourceSize":5140,"sourcePath":"Big Data/3. Spark/6. Source Code/0. Spark Storage.md","exportPath":"big-data/3.-spark/6.-source-code/0.-spark-storage.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/4.-hive/0.-hive.html":{"createdTime":1737584641707,"modifiedTime":1737554783000,"sourceSize":5720,"sourcePath":"Big Data/4. Hive/0. Hive.md","exportPath":"big-data/4.-hive/0.-hive.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/4.-hive/1.-data-definition-language-(ddl).html":{"createdTime":1737584641708,"modifiedTime":1737554783000,"sourceSize":7840,"sourcePath":"Big Data/4. Hive/1. Data Definition Language (DDL).md","exportPath":"big-data/4.-hive/1.-data-definition-language-(ddl).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/4.-hive/2.-data-manipulation-language-(dml).html":{"createdTime":1737584641709,"modifiedTime":1737554783000,"sourceSize":6947,"sourcePath":"Big Data/4. Hive/2. Data Manipulation Language (DML).md","exportPath":"big-data/4.-hive/2.-data-manipulation-language-(dml).html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html":{"createdTime":1737584641710,"modifiedTime":1737554783000,"sourceSize":8310,"sourcePath":"Big Data/4. Hive/3. Single Node Deployment Guide - Tecent Cloud.md","exportPath":"big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/0.-hbase.html":{"createdTime":1737584641712,"modifiedTime":1737554783000,"sourceSize":3973,"sourcePath":"Big Data/5. Hbase/0. Hbase.md","exportPath":"big-data/5.-hbase/0.-hbase.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/2.-shell.html":{"createdTime":1737584641715,"modifiedTime":1737554783000,"sourceSize":2507,"sourcePath":"Big Data/5. Hbase/2. Shell.md","exportPath":"big-data/5.-hbase/2.-shell.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/3.-processes.html":{"createdTime":1737584641716,"modifiedTime":1737554783000,"sourceSize":5921,"sourcePath":"Big Data/5. Hbase/3. Processes.md","exportPath":"big-data/5.-hbase/3.-processes.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/4.-api.html":{"createdTime":1737584641724,"modifiedTime":1737554783000,"sourceSize":6112,"sourcePath":"Big Data/5. Hbase/4. API.md","exportPath":"big-data/5.-hbase/4.-api.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/5.-mapreduce.html":{"createdTime":1737584641725,"modifiedTime":1737554783000,"sourceSize":10917,"sourcePath":"Big Data/5. Hbase/5. MapReduce.md","exportPath":"big-data/5.-hbase/5.-mapreduce.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/6.-integration-with-hive.html":{"createdTime":1737584641749,"modifiedTime":1737554783000,"sourceSize":5330,"sourcePath":"Big Data/5. Hbase/6. Integration with Hive.md","exportPath":"big-data/5.-hbase/6.-integration-with-hive.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/5.-hbase/7.-optimization.html":{"createdTime":1737584641774,"modifiedTime":1737554783000,"sourceSize":8403,"sourcePath":"Big Data/5. Hbase/7. Optimization.md","exportPath":"big-data/5.-hbase/7.-optimization.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/6.-flink/0.-key-features-of-flink.html":{"createdTime":1737584641776,"modifiedTime":1737554783000,"sourceSize":4869,"sourcePath":"Big Data/6. Flink/0. Key Features of Flink.md","exportPath":"big-data/6.-flink/0.-key-features-of-flink.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/6.-flink/1.-deployment-and-startup.html":{"createdTime":1737584641777,"modifiedTime":1737554783000,"sourceSize":4833,"sourcePath":"Big Data/6. Flink/1. Deployment and Startup.md","exportPath":"big-data/6.-flink/1.-deployment-and-startup.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/6.-flink/2.-architecture.html":{"createdTime":1737584641778,"modifiedTime":1737554783000,"sourceSize":5305,"sourcePath":"Big Data/6. Flink/2. Architecture.md","exportPath":"big-data/6.-flink/2.-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/6.-flink/3.-flink-operators.html":{"createdTime":1737584641779,"modifiedTime":1737554783000,"sourceSize":10185,"sourcePath":"Big Data/6. Flink/3. Flink Operators.md","exportPath":"big-data/6.-flink/3.-flink-operators.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/6.-flink/4.-time-and-window-in-stream-processing.html":{"createdTime":1737584641805,"modifiedTime":1737554783000,"sourceSize":5387,"sourcePath":"Big Data/6. Flink/4. Time and Window in Stream Processing.md","exportPath":"big-data/6.-flink/4.-time-and-window-in-stream-processing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/7.-kafka/0.-kafka.html":{"createdTime":1737584641838,"modifiedTime":1737554783000,"sourceSize":4423,"sourcePath":"Big Data/7. Kafka/0. Kafka.md","exportPath":"big-data/7.-kafka/0.-kafka.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/7.-kafka/1.-deployment-&-commands.html":{"createdTime":1737584641839,"modifiedTime":1737554783000,"sourceSize":5381,"sourcePath":"Big Data/7. Kafka/1. Deployment & Commands.md","exportPath":"big-data/7.-kafka/1.-deployment-&-commands.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/7.-kafka/2.-architecture.html":{"createdTime":1737584641840,"modifiedTime":1737554783000,"sourceSize":14235,"sourcePath":"Big Data/7. Kafka/2. Architecture.md","exportPath":"big-data/7.-kafka/2.-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/7.-kafka/3.-api.html":{"createdTime":1737584641853,"modifiedTime":1737554783000,"sourceSize":23541,"sourcePath":"Big Data/7. Kafka/3. API.md","exportPath":"big-data/7.-kafka/3.-api.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/7.-kafka/4.-monitoring.html":{"createdTime":1737584641855,"modifiedTime":1737554783000,"sourceSize":4519,"sourcePath":"Big Data/7. Kafka/4. Monitoring.md","exportPath":"big-data/7.-kafka/4.-monitoring.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/7.-kafka/5.-flume-integration.html":{"createdTime":1737584641856,"modifiedTime":1737554783000,"sourceSize":2068,"sourcePath":"Big Data/7. Kafka/5. Flume Integration.md","exportPath":"big-data/7.-kafka/5.-flume-integration.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/8.-doris/0.-doris.html":{"createdTime":1737584641865,"modifiedTime":1737554783000,"sourceSize":3359,"sourcePath":"Big Data/8. Doris/0. Doris.md","exportPath":"big-data/8.-doris/0.-doris.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/8.-doris/1.-installation.html":{"createdTime":1737584641867,"modifiedTime":1737554783000,"sourceSize":10404,"sourcePath":"Big Data/8. Doris/1. Installation.md","exportPath":"big-data/8.-doris/1.-installation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/8.-doris/2.-usage.html":{"createdTime":1737584641868,"modifiedTime":1737554783000,"sourceSize":29342,"sourcePath":"Big Data/8. Doris/2. Usage.md","exportPath":"big-data/8.-doris/2.-usage.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/8.-doris/3.-monitoring-and-alerting.html":{"createdTime":1737584641869,"modifiedTime":1737554783000,"sourceSize":6419,"sourcePath":"Big Data/8. Doris/3. Monitoring and Alerting.md","exportPath":"big-data/8.-doris/3.-monitoring-and-alerting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/9.-elastic-search/0.-overview.html":{"createdTime":1737584641871,"modifiedTime":1737554783000,"sourceSize":8205,"sourcePath":"Big Data/9. Elastic Search/0. Overview.md","exportPath":"big-data/9.-elastic-search/0.-overview.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/9.-elastic-search/1.-index-&-shard.html":{"createdTime":1737584641872,"modifiedTime":1737554783000,"sourceSize":8494,"sourcePath":"Big Data/9. Elastic Search/1. Index & Shard.md","exportPath":"big-data/9.-elastic-search/1.-index-&-shard.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/10.-clickhouse/0.-introduction-and-installation.html":{"createdTime":1737584641354,"modifiedTime":1737554783000,"sourceSize":4733,"sourcePath":"Big Data/10. ClickHouse/0. Introduction and Installation.md","exportPath":"big-data/10.-clickhouse/0.-introduction-and-installation.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/10.-clickhouse/1.-data-types.html":{"createdTime":1737584641355,"modifiedTime":1737554783000,"sourceSize":2695,"sourcePath":"Big Data/10. ClickHouse/1. Data Types.md","exportPath":"big-data/10.-clickhouse/1.-data-types.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/10.-clickhouse/2.-database-engines.html":{"createdTime":1737584641356,"modifiedTime":1737554783000,"sourceSize":5859,"sourcePath":"Big Data/10. ClickHouse/2. Database Engines.md","exportPath":"big-data/10.-clickhouse/2.-database-engines.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/10.-clickhouse/3.-table-engines.html":{"createdTime":1737584641357,"modifiedTime":1737554783000,"sourceSize":6253,"sourcePath":"Big Data/10. ClickHouse/3. Table Engines.md","exportPath":"big-data/10.-clickhouse/3.-table-engines.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/11.-data-migration/0.-sqoop.html":{"createdTime":1737584641359,"modifiedTime":1737554783000,"sourceSize":6934,"sourcePath":"Big Data/11. Data Migration/0. Sqoop.md","exportPath":"big-data/11.-data-migration/0.-sqoop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/11.-data-migration/1.-datax.html":{"createdTime":1737584641360,"modifiedTime":1737554783000,"sourceSize":8056,"sourcePath":"Big Data/11. Data Migration/1. DataX.md","exportPath":"big-data/11.-data-migration/1.-datax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/12.-schedule/1.-cron-expressions.html":{"createdTime":1737584641394,"modifiedTime":1737554783000,"sourceSize":2861,"sourcePath":"Big Data/12. Schedule/1. Cron Expressions.md","exportPath":"big-data/12.-schedule/1.-cron-expressions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/12.-schedule/2.-quartz-cron-expressions.html":{"createdTime":1737584641395,"modifiedTime":1737554783000,"sourceSize":2757,"sourcePath":"Big Data/12. Schedule/2. Quartz Cron Expressions.md","exportPath":"big-data/12.-schedule/2.-quartz-cron-expressions.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/0.-data-management.html":{"createdTime":1737584641397,"modifiedTime":1737554783000,"sourceSize":17524,"sourcePath":"Big Data/13. Certificate/CDGA/0. Data Management.md","exportPath":"big-data/13.-certificate/cdga/0.-data-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/1.-data-handling-ethics.html":{"createdTime":1737584641398,"modifiedTime":1737554783000,"sourceSize":5058,"sourcePath":"Big Data/13. Certificate/CDGA/1. Data Handling Ethics.md","exportPath":"big-data/13.-certificate/cdga/1.-data-handling-ethics.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/2.-data-governance.html":{"createdTime":1737584641430,"modifiedTime":1737554783000,"sourceSize":4450,"sourcePath":"Big Data/13. Certificate/CDGA/2. Data Governance.md","exportPath":"big-data/13.-certificate/cdga/2.-data-governance.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/3.-data-architecture.html":{"createdTime":1737584641431,"modifiedTime":1737554783000,"sourceSize":5081,"sourcePath":"Big Data/13. Certificate/CDGA/3. Data Architecture.md","exportPath":"big-data/13.-certificate/cdga/3.-data-architecture.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/4.-data-modeling-and-design.html":{"createdTime":1737584641432,"modifiedTime":1737554783000,"sourceSize":5866,"sourcePath":"Big Data/13. Certificate/CDGA/4. Data Modeling and Design.md","exportPath":"big-data/13.-certificate/cdga/4.-data-modeling-and-design.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/5.-data-storage-and-operations.html":{"createdTime":1737584641432,"modifiedTime":1737554783000,"sourceSize":11149,"sourcePath":"Big Data/13. Certificate/CDGA/5. Data Storage and Operations.md","exportPath":"big-data/13.-certificate/cdga/5.-data-storage-and-operations.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/6.-data-security.html":{"createdTime":1737584641433,"modifiedTime":1737554783000,"sourceSize":26433,"sourcePath":"Big Data/13. Certificate/CDGA/6. Data Security.md","exportPath":"big-data/13.-certificate/cdga/6.-data-security.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html":{"createdTime":1737584641435,"modifiedTime":1737554783000,"sourceSize":9772,"sourcePath":"Big Data/13. Certificate/CDGA/7. Data Integration and Interoperability.md","exportPath":"big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/8.-document-and-content-management.html":{"createdTime":1737584641437,"modifiedTime":1737554783000,"sourceSize":7798,"sourcePath":"Big Data/13. Certificate/CDGA/8. Document and Content Management.md","exportPath":"big-data/13.-certificate/cdga/8.-document-and-content-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/9.-reference-and-master-data.html":{"createdTime":1737584641438,"modifiedTime":1737554783000,"sourceSize":16305,"sourcePath":"Big Data/13. Certificate/CDGA/9. Reference and Master Data.md","exportPath":"big-data/13.-certificate/cdga/9.-reference-and-master-data.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html":{"createdTime":1737584641399,"modifiedTime":1737554783000,"sourceSize":19076,"sourcePath":"Big Data/13. Certificate/CDGA/10. Data Warehousing and Business Intelligence.md","exportPath":"big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/11.-metadata-management.html":{"createdTime":1737584641400,"modifiedTime":1737554783000,"sourceSize":27717,"sourcePath":"Big Data/13. Certificate/CDGA/11. Metadata Management.md","exportPath":"big-data/13.-certificate/cdga/11.-metadata-management.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/12.-data-quality.html":{"createdTime":1737584641401,"modifiedTime":1737554783000,"sourceSize":33450,"sourcePath":"Big Data/13. Certificate/CDGA/12. Data Quality.md","exportPath":"big-data/13.-certificate/cdga/12.-data-quality.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/13.-big-data-and-data-science.html":{"createdTime":1737584641426,"modifiedTime":1737554783000,"sourceSize":12625,"sourcePath":"Big Data/13. Certificate/CDGA/13. Big Data and Data Science.md","exportPath":"big-data/13.-certificate/cdga/13.-big-data-and-data-science.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html":{"createdTime":1737584641428,"modifiedTime":1737554783000,"sourceSize":14630,"sourcePath":"Big Data/13. Certificate/CDGA/14. Data Management Maturity Assessment.md","exportPath":"big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/auto-keyword.html":{"createdTime":1690694653862,"modifiedTime":1691392973897,"sourceSize":1280,"sourcePath":"C++/Auto keyword.md","exportPath":"c++/auto-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/c++.html":{"createdTime":1690545991464,"modifiedTime":1692560983000,"sourceSize":390,"sourcePath":"C++/C++.md","exportPath":"c++/c++.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/const-keyword.html":{"createdTime":1690545991260,"modifiedTime":1708699899608,"sourceSize":2894,"sourcePath":"C++/Const keyword.md","exportPath":"c++/const-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/enum-and-enum-class.html":{"createdTime":1690545991307,"modifiedTime":1692560973000,"sourceSize":3187,"sourcePath":"C++/Enum and Enum class.md","exportPath":"c++/enum-and-enum-class.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/header-files.html":{"createdTime":1690545991327,"modifiedTime":1691393043318,"sourceSize":2671,"sourcePath":"C++/Header Files.md","exportPath":"c++/header-files.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/inline-keyword.html":{"createdTime":1690545991345,"modifiedTime":1691393052964,"sourceSize":1637,"sourcePath":"C++/Inline keyword.md","exportPath":"c++/inline-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/oop.html":{"createdTime":1690545991361,"modifiedTime":1691393061952,"sourceSize":4257,"sourcePath":"C++/OOP.md","exportPath":"c++/oop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/static-keyword.html":{"createdTime":1690545991407,"modifiedTime":1692560977000,"sourceSize":2858,"sourcePath":"C++/Static keyword.md","exportPath":"c++/static-keyword.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/timing.html":{"createdTime":1691005752022,"modifiedTime":1691393086230,"sourceSize":1499,"sourcePath":"C++/Timing.md","exportPath":"c++/timing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"c++/type-punning.html":{"createdTime":1691005840293,"modifiedTime":1691393100246,"sourceSize":3115,"sourcePath":"C++/Type punning.md","exportPath":"c++/type-punning.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-base/data-base.html":{"createdTime":1690694720777,"modifiedTime":1692560974000,"sourceSize":267,"sourcePath":"Data Base/Data Base.md","exportPath":"data-base/data-base.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-base/mongodb.html":{"createdTime":1690694662822,"modifiedTime":1691392853882,"sourceSize":6052,"sourcePath":"Data Base/MongoDB.md","exportPath":"data-base/mongodb.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-base/mongodb-cheat-sheet.html":{"createdTime":1690694869426,"modifiedTime":1691392864868,"sourceSize":3031,"sourcePath":"Data Base/MongoDB cheat Sheet.md","exportPath":"data-base/mongodb-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-base/sql-cheat-sheet.html":{"createdTime":1691390817322,"modifiedTime":1691392878982,"sourceSize":7306,"sourcePath":"Data Base/SQL Cheat Sheet.md","exportPath":"data-base/sql-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/aggregation-analysis.html":{"createdTime":1690545991727,"modifiedTime":1691392748961,"sourceSize":3779,"sourcePath":"Data Structures/Aggregation analysis.md","exportPath":"data-structures/aggregation-analysis.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/analyzing-exchanging-sorting-algorithms.html":{"createdTime":1690545991668,"modifiedTime":1691392771098,"sourceSize":2373,"sourcePath":"Data Structures/Analyzing Exchanging Sorting Algorithms.md","exportPath":"data-structures/analyzing-exchanging-sorting-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/avl-tree.html":{"createdTime":1691390477575,"modifiedTime":1741256261570,"sourceSize":2838,"sourcePath":"Data Structures/AVL Tree.md","exportPath":"data-structures/avl-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/binary-search-tree.html":{"createdTime":1690545991589,"modifiedTime":1706562033136,"sourceSize":4007,"sourcePath":"Data Structures/Binary Search Tree.md","exportPath":"data-structures/binary-search-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/comparison-based-sorting.html":{"createdTime":1691390477683,"modifiedTime":1696098447000,"sourceSize":4176,"sourcePath":"Data Structures/Comparison based sorting.md","exportPath":"data-structures/comparison-based-sorting.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/data-structures.html":{"createdTime":1690545991794,"modifiedTime":1706716576160,"sourceSize":1956,"sourcePath":"Data Structures/Data Structures.md","exportPath":"data-structures/data-structures.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/expression-tree.html":{"createdTime":1690545991576,"modifiedTime":1706563115229,"sourceSize":10174,"sourcePath":"Data Structures/Expression Tree.md","exportPath":"data-structures/expression-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/hashing.html":{"createdTime":1690545991629,"modifiedTime":1706561600000,"sourceSize":2573,"sourcePath":"Data Structures/Hashing.md","exportPath":"data-structures/hashing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/heaps-&-priority-queues.html":{"createdTime":1695752418964,"modifiedTime":1695752421914,"sourceSize":2943,"sourcePath":"Data Structures/Heaps & Priority Queues.md","exportPath":"data-structures/heaps-&-priority-queues.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/insertion-sort.html":{"createdTime":1690545991486,"modifiedTime":1691392788800,"sourceSize":2536,"sourcePath":"Data Structures/Insertion sort.md","exportPath":"data-structures/insertion-sort.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/linear-sorting-algorithm.html":{"createdTime":1691390477709,"modifiedTime":1696098452000,"sourceSize":5188,"sourcePath":"Data Structures/Linear Sorting Algorithm.md","exportPath":"data-structures/linear-sorting-algorithm.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/lists.html":{"createdTime":1690545991744,"modifiedTime":1691392737396,"sourceSize":4371,"sourcePath":"Data Structures/Lists.md","exportPath":"data-structures/lists.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/master-theorem.html":{"createdTime":1690545991710,"modifiedTime":1706561599000,"sourceSize":952,"sourcePath":"Data Structures/Master theorem.md","exportPath":"data-structures/master-theorem.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/open-addressing.html":{"createdTime":1690545991633,"modifiedTime":1706563455494,"sourceSize":6490,"sourcePath":"Data Structures/Open Addressing.md","exportPath":"data-structures/open-addressing.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/order.html":{"createdTime":1691390477616,"modifiedTime":1696098455000,"sourceSize":2372,"sourcePath":"Data Structures/Order.md","exportPath":"data-structures/order.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/priority-queues.html":{"createdTime":1690545991601,"modifiedTime":1696100531326,"sourceSize":7378,"sourcePath":"Data Structures/Priority Queues.md","exportPath":"data-structures/priority-queues.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/recusrsion.html":{"createdTime":1690545991691,"modifiedTime":1691392764531,"sourceSize":1971,"sourcePath":"Data Structures/Recusrsion.md","exportPath":"data-structures/recusrsion.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/red-black-tree.html":{"createdTime":1691390477733,"modifiedTime":1706562705522,"sourceSize":6902,"sourcePath":"Data Structures/Red-Black Tree.md","exportPath":"data-structures/red-black-tree.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/sorting-algorithms.html":{"createdTime":1691390477652,"modifiedTime":1695751911047,"sourceSize":4408,"sourcePath":"Data Structures/Sorting Algorithms.md","exportPath":"data-structures/sorting-algorithms.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/stacks-&-queues.html":{"createdTime":1690545991750,"modifiedTime":1706561603000,"sourceSize":839,"sourcePath":"Data Structures/Stacks & Queues.md","exportPath":"data-structures/stacks-&-queues.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/time-complexity-comparison.html":{"createdTime":1690545991637,"modifiedTime":1691392783155,"sourceSize":1534,"sourcePath":"Data Structures/Time Complexity Comparison.md","exportPath":"data-structures/time-complexity-comparison.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/tree-and-trie.html":{"createdTime":1690545991524,"modifiedTime":1704139702000,"sourceSize":1580,"sourcePath":"Data Structures/Tree and Trie.md","exportPath":"data-structures/tree-and-trie.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"data-structures/trees.html":{"createdTime":1690545991768,"modifiedTime":1690046731490,"sourceSize":5071,"sourcePath":"Data Structures/Trees.md","exportPath":"data-structures/trees.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"git/git.html":{"createdTime":1690547585357,"modifiedTime":1691392897620,"sourceSize":144,"sourcePath":"Git/Git.md","exportPath":"git/git.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"git/git-cheat-sheet.html":{"createdTime":1690545992007,"modifiedTime":1690553369846,"sourceSize":5117,"sourcePath":"Git/Git Cheat Sheet.md","exportPath":"git/git-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"graph/dijkstra.html":{"createdTime":1706713203220,"modifiedTime":1706717715355,"sourceSize":4114,"sourcePath":"Graph/Dijkstra.md","exportPath":"graph/dijkstra.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"graph/graph.html":{"createdTime":1706713261936,"modifiedTime":1706713712454,"sourceSize":238,"sourcePath":"Graph/Graph.md","exportPath":"graph/graph.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"graph/max-flow.html":{"createdTime":1706713543747,"modifiedTime":1706717727960,"sourceSize":4518,"sourcePath":"Graph/Max Flow.md","exportPath":"graph/max-flow.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"graph/scc.html":{"createdTime":1706713203217,"modifiedTime":1706717747585,"sourceSize":3790,"sourcePath":"Graph/SCC.md","exportPath":"graph/scc.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"markdown/markdown.html":{"createdTime":1690547826439,"modifiedTime":1692560978000,"sourceSize":300,"sourcePath":"Markdown/Markdown.md","exportPath":"markdown/markdown.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"markdown/markdown-cheat-sheet.html":{"createdTime":1690545992067,"modifiedTime":1691392356000,"sourceSize":1983,"sourcePath":"Markdown/Markdown Cheat Sheet.md","exportPath":"markdown/markdown-cheat-sheet.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"markdown/mathjax.html":{"createdTime":1690545992083,"modifiedTime":1692560980000,"sourceSize":5812,"sourcePath":"Markdown/MathJax.md","exportPath":"markdown/mathjax.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"python/beautiful-soup.html":{"createdTime":1693680837530,"modifiedTime":1693683637000,"sourceSize":5637,"sourcePath":"Python/Beautiful Soup.md","exportPath":"python/beautiful-soup.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"python/python.html":{"createdTime":1690548039002,"modifiedTime":1693680859464,"sourceSize":309,"sourcePath":"Python/Python.md","exportPath":"python/python.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"python/python-oop.html":{"createdTime":1690545992118,"modifiedTime":1690553460022,"sourceSize":22690,"sourcePath":"Python/Python OOP.md","exportPath":"python/python-oop.html","showInTree":true,"treeOrder":0,"backlinks":[],"type":"markdown","data":null},"excalidraw/avl-tree.excalidraw.svg":{"createdTime":1691391380394,"modifiedTime":1706717101546,"sourceSize":97394,"sourcePath":"Excalidraw/AVL tree.excalidraw.svg","exportPath":"excalidraw/avl-tree.excalidraw.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"excalidraw/black-and-red-tree-height-diff.excalidraw.svg":{"createdTime":1691391380344,"modifiedTime":1706717203068,"sourceSize":91190,"sourcePath":"Excalidraw/Black and red tree height diff.excalidraw.svg","exportPath":"excalidraw/black-and-red-tree-height-diff.excalidraw.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"excalidraw/expression-tree.svg":{"createdTime":1690547564089,"modifiedTime":1706717275133,"sourceSize":104119,"sourcePath":"Excalidraw/expression tree.svg","exportPath":"excalidraw/expression-tree.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"excalidraw/red-black-tree.excalidraw.svg":{"createdTime":1691391380535,"modifiedTime":1706717288655,"sourceSize":150740,"sourcePath":"Excalidraw/Red-Black Tree.excalidraw.svg","exportPath":"excalidraw/red-black-tree.excalidraw.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null}},"sourceToTarget":{"":"site-lib/styles/supported-plugins.css","Algorithms/Algo/Backtracking/A general approach to backtracking questions.md":"algorithms/algo/backtracking/a-general-approach-to-backtracking-questions.html","Algorithms/Algo/Backtracking/Backtracking.md":"algorithms/algo/backtracking/backtracking.html","Algorithms/Algo/Divide and Conquer/Divide and Conquer.md":"algorithms/algo/divide-and-conquer/divide-and-conquer.html","Algorithms/Algo/Dynamic Programming/Dynamic Programing.md":"algorithms/algo/dynamic-programming/dynamic-programing.html","Algorithms/Algo/Dynamic Programming/Kadane's Algorithm.md":"algorithms/algo/dynamic-programming/kadane's-algorithm.html","Algorithms/Algo/Graph/Alpha-Beta.md":"algorithms/algo/graph/alpha-beta.html","Algorithms/Algo/Graph/Floyd's cycle-finding algorithm.md":"algorithms/algo/graph/floyd's-cycle-finding-algorithm.html","Algorithms/Algo/Graph/MiniMax.md":"algorithms/algo/graph/minimax.html","Algorithms/Algo/Graph/Union Find.md":"algorithms/algo/graph/union-find.html","Algorithms/Algo/Greedy/Greedy.md":"algorithms/algo/greedy/greedy.html","Algorithms/Algo/Recursion/Recursion.md":"algorithms/algo/recursion/recursion.html","Algorithms/Algo/Search/A star.md":"algorithms/algo/search/a-star.html","Algorithms/Algo/Search/Breadth First Search.md":"algorithms/algo/search/breadth-first-search.html","Algorithms/Algo/Search/Depth First Search.md":"algorithms/algo/search/depth-first-search.html","Algorithms/Algo/Search/DFS vs BFS.md":"algorithms/algo/search/dfs-vs-bfs.html","Algorithms/Algo/Search/Dijkstra.md":"algorithms/algo/search/dijkstra.html","Algorithms/Algo/Search/Iterative Deepening.md":"algorithms/algo/search/iterative-deepening.html","Algorithms/Algo/Sliding Window/Sliding Window.md":"algorithms/algo/sliding-window/sliding-window.html","Algorithms/Algo/Sorting/Binary Search.md":"algorithms/algo/sorting/binary-search.html","Algorithms/Algo/Two Pointers/Floyd’s Cycle Finding Algorithm.md":"algorithms/algo/two-pointers/floyd’s-cycle-finding-algorithm.html","Algorithms/Algo/Two Pointers/Two Pointer.md":"algorithms/algo/two-pointers/two-pointer.html","Algorithms/Leetcode/Array/DP/45. Jump Game II.md":"algorithms/leetcode/array/dp/45.-jump-game-ii.html","Algorithms/Leetcode/Array/DP/62. Unique Paths.md":"algorithms/leetcode/array/dp/62.-unique-paths.html","Algorithms/Leetcode/Array/DP/70. Climbing Stairs.md":"algorithms/leetcode/array/dp/70.-climbing-stairs.html","Algorithms/Leetcode/Array/DP/96. Unique Binary Search Trees.md":"algorithms/leetcode/array/dp/96.-unique-binary-search-trees.html","Algorithms/Leetcode/Array/DP/198. House Robber.md":"algorithms/leetcode/array/dp/198.-house-robber.html","Algorithms/Leetcode/Array/DP/213. House Robber II.md":"algorithms/leetcode/array/dp/213.-house-robber-ii.html","Algorithms/Leetcode/Array/DP/221. Maximal Square.md":"algorithms/leetcode/array/dp/221.-maximal-square.html","Algorithms/Leetcode/Array/DP/300. Longest Increasing Subsequence.md":"algorithms/leetcode/array/dp/300.-longest-increasing-subsequence.html","Algorithms/Leetcode/Array/DP/309. Best Time to Buy and Sell Stock with Cooldown.md":"algorithms/leetcode/array/dp/309.-best-time-to-buy-and-sell-stock-with-cooldown.html","Algorithms/Leetcode/Array/DP/413. Arithmetic Slices.md":"algorithms/leetcode/array/dp/413.-arithmetic-slices.html","Algorithms/Leetcode/Array/DP/472. Concatenated Words.md":"algorithms/leetcode/array/dp/472.-concatenated-words.html","Algorithms/Leetcode/Array/DP/542. 01 Matrix.md":"algorithms/leetcode/array/dp/542.-01-matrix.html","Algorithms/Leetcode/Array/DP/714. Best Time to Buy and Sell Stock with Transaction Fee.md":"algorithms/leetcode/array/dp/714.-best-time-to-buy-and-sell-stock-with-transaction-fee.html","Algorithms/Leetcode/Array/DP/740. Delete and Earn.md":"algorithms/leetcode/array/dp/740.-delete-and-earn.html","Algorithms/Leetcode/Array/DP/1143. Longest Common Subsequence.md":"algorithms/leetcode/array/dp/1143.-longest-common-subsequence.html","Algorithms/Leetcode/Array/DP/1626. Best Team With No Conflicts.md":"algorithms/leetcode/array/dp/1626.-best-team-with-no-conflicts.html","Algorithms/Leetcode/Array/Hash/454. 4Sum II.md":"algorithms/leetcode/array/hash/454.-4sum-ii.html","Algorithms/Leetcode/Array/Interval/57. Insert Interval.md":"algorithms/leetcode/array/interval/57.-insert-interval.html","Algorithms/Leetcode/Array/Matrix/74. Search a 2D Matrix.md":"algorithms/leetcode/array/matrix/74.-search-a-2d-matrix.html","Algorithms/Leetcode/Array/Matrix/79. Word Search.md":"algorithms/leetcode/array/matrix/79.-word-search.html","Algorithms/Leetcode/Array/Matrix/417. Pacific Atlantic Water Flow.md":"algorithms/leetcode/array/matrix/417.-pacific-atlantic-water-flow.html","Algorithms/Leetcode/Array/Matrix/695. Max Area of Island.md":"algorithms/leetcode/array/matrix/695.-max-area-of-island.html","Algorithms/Leetcode/Array/Matrix/909. Snakes and Ladders.md":"algorithms/leetcode/array/matrix/909.-snakes-and-ladders.html","Algorithms/Leetcode/Array/Matrix/934. Shortest Bridge.md":"algorithms/leetcode/array/matrix/934.-shortest-bridge.html","Algorithms/Leetcode/Array/Permutation/46. Permutations.md":"algorithms/leetcode/array/permutation/46.-permutations.html","Algorithms/Leetcode/Array/Permutation/77. Combinations.md":"algorithms/leetcode/array/permutation/77.-combinations.html","Algorithms/Leetcode/Array/Sort/912. Sort an Array.md":"algorithms/leetcode/array/sort/912.-sort-an-array.html","Algorithms/Leetcode/Array/Subarray/53. Maximum Subarray.md":"algorithms/leetcode/array/subarray/53.-maximum-subarray.html","Algorithms/Leetcode/Array/Subarray/491. Non-decreasing Subsequences.md":"algorithms/leetcode/array/subarray/491.-non-decreasing-subsequences.html","Algorithms/Leetcode/Array/Subarray/918. Maximum Sum Circular Subarray.md":"algorithms/leetcode/array/subarray/918.-maximum-sum-circular-subarray.html","Algorithms/Leetcode/Array/Subarray/974. Subarray Sums Divisible by K.md":"algorithms/leetcode/array/subarray/974.-subarray-sums-divisible-by-k.html","Algorithms/Leetcode/Array/Two-pointers/11. Container With Most Water.md":"algorithms/leetcode/array/two-pointers/11.-container-with-most-water.html","Algorithms/Leetcode/Array/Two-pointers/15. 3Sum.md":"algorithms/leetcode/array/two-pointers/15.-3sum.html","Algorithms/Leetcode/Array/Two-pointers/16. 3Sum Closest.md":"algorithms/leetcode/array/two-pointers/16.-3sum-closest.html","Algorithms/Leetcode/Array/Two-pointers/18. 4Sum.md":"algorithms/leetcode/array/two-pointers/18.-4sum.html","Algorithms/Leetcode/Array/Two-pointers/26. Remove Duplicates from Sorted Array.md":"algorithms/leetcode/array/two-pointers/26.-remove-duplicates-from-sorted-array.html","Algorithms/Leetcode/Array/Two-pointers/27. Remove Element.md":"algorithms/leetcode/array/two-pointers/27.-remove-element.html","Algorithms/Leetcode/Array/Two-pointers/69. Sqrt(x).md":"algorithms/leetcode/array/two-pointers/69.-sqrt(x).html","Algorithms/Leetcode/Array/Two-pointers/81. Search in Rotated Sorted Array II.md":"algorithms/leetcode/array/two-pointers/81.-search-in-rotated-sorted-array-ii.html","Algorithms/Leetcode/Array/Two-pointers/88. Merge Sorted Array.md":"algorithms/leetcode/array/two-pointers/88.-merge-sorted-array.html","Algorithms/Leetcode/Array/Two-pointers/142. Linked List Cycle II.md":"algorithms/leetcode/array/two-pointers/142.-linked-list-cycle-ii.html","Algorithms/Leetcode/Array/Two-pointers/209. Minimum Size Subarray Sum.md":"algorithms/leetcode/array/two-pointers/209.-minimum-size-subarray-sum.html","Algorithms/Leetcode/Array/Two-pointers/253. Meeting Rooms II.md":"algorithms/leetcode/array/two-pointers/253.-meeting-rooms-ii.html","Algorithms/Leetcode/Array/Two-pointers/259. 3Sum Smaller.md":"algorithms/leetcode/array/two-pointers/259.-3sum-smaller.html","Algorithms/Leetcode/Array/Two-pointers/360. Sort Transformed Array.md":"algorithms/leetcode/array/two-pointers/360.-sort-transformed-array.html","Algorithms/Leetcode/Array/Two-pointers/392. Is Subsequence.md":"algorithms/leetcode/array/two-pointers/392.-is-subsequence.html","Algorithms/Leetcode/Array/Two-pointers/633. Sum of Square Numbers.md":"algorithms/leetcode/array/two-pointers/633.-sum-of-square-numbers.html","Algorithms/Leetcode/Array/Two-pointers/713. Subarray Product Less Than K.md":"algorithms/leetcode/array/two-pointers/713.-subarray-product-less-than-k.html","Algorithms/Leetcode/Array/Two-pointers/1004. Max Consecutive Ones III.md":"algorithms/leetcode/array/two-pointers/1004.-max-consecutive-ones-iii.html","Algorithms/Leetcode/Array/Two-pointers/1493. Longest Subarray of 1's After Deleting One Element.md":"algorithms/leetcode/array/two-pointers/1493.-longest-subarray-of-1's-after-deleting-one-element.html","Algorithms/Leetcode/Bit/190. Reverse Bits.md":"algorithms/leetcode/bit/190.-reverse-bits.html","Algorithms/Leetcode/Bit/461. Hamming Distance.md":"algorithms/leetcode/bit/461.-hamming-distance.html","Algorithms/Leetcode/Geometry/149. Max Points on a Line.md":"algorithms/leetcode/geometry/149.-max-points-on-a-line.html","Algorithms/Leetcode/Graph/490. The Maze.md":"algorithms/leetcode/graph/490.-the-maze.html","Algorithms/Leetcode/Graph/797. All Paths From Source to Target.md":"algorithms/leetcode/graph/797.-all-paths-from-source-to-target.html","Algorithms/Leetcode/Graph/841. Keys and Rooms.md":"algorithms/leetcode/graph/841.-keys-and-rooms.html","Algorithms/Leetcode/Graph/1971. Find if Path Exists in Graph.md":"algorithms/leetcode/graph/1971.-find-if-path-exists-in-graph.html","Algorithms/Leetcode/Greedy/134. Gas Station.md":"algorithms/leetcode/greedy/134.-gas-station.html","Algorithms/Leetcode/Greedy/135. Candy.md":"algorithms/leetcode/greedy/135.-candy.html","Algorithms/Leetcode/Greedy/435. Non-overlapping Intervals.md":"algorithms/leetcode/greedy/435.-non-overlapping-intervals.html","Algorithms/Leetcode/Greedy/455. Assign Cookies.md":"algorithms/leetcode/greedy/455.-assign-cookies.html","Algorithms/Leetcode/Greedy/1833. Maximum Ice Cream Bars.md":"algorithms/leetcode/greedy/1833.-maximum-ice-cream-bars.html","Algorithms/Leetcode/Linked List/24. Swap Nodes in Pairs.md":"algorithms/leetcode/linked-list/24.-swap-nodes-in-pairs.html","Algorithms/Leetcode/Math/453. Minimum Moves to Equal Array Elements.md":"algorithms/leetcode/math/453.-minimum-moves-to-equal-array-elements.html","Algorithms/Leetcode/Shortest Path/787. Cheapest Flights Within K Stops.md":"algorithms/leetcode/shortest-path/787.-cheapest-flights-within-k-stops.html","Algorithms/Leetcode/SQL/Select/183. Customers Who Never Order.md":"algorithms/leetcode/sql/select/183.-customers-who-never-order.html","Algorithms/Leetcode/SQL/Select/584. Find Customer Referee.md":"algorithms/leetcode/sql/select/584.-find-customer-referee.html","Algorithms/Leetcode/SQL/Select/595. Big Countries.md":"algorithms/leetcode/sql/select/595.-big-countries.html","Algorithms/Leetcode/SQL/Select/1757. Recyclable and Low Fat Products.md":"algorithms/leetcode/sql/select/1757.-recyclable-and-low-fat-products.html","Algorithms/Leetcode/SQL/String Processing Functions/1484. Group Sold Products By The Date.md":"algorithms/leetcode/sql/string-processing-functions/1484.-group-sold-products-by-the-date.html","Algorithms/Leetcode/SQL/String Processing Functions/1527. Patients With a Condition.md":"algorithms/leetcode/sql/string-processing-functions/1527.-patients-with-a-condition.html","Algorithms/Leetcode/SQL/String Processing Functions/1667. Fix Names in a Table.md":"algorithms/leetcode/sql/string-processing-functions/1667.-fix-names-in-a-table.html","Algorithms/Leetcode/SQL/Union/197. Rising Temperature.md":"algorithms/leetcode/sql/union/197.-rising-temperature.html","Algorithms/Leetcode/SQL/Union&Select/608. Tree Node.md":"algorithms/leetcode/sql/union&select/608.-tree-node.html","Algorithms/Leetcode/SQL/Union&Select/1965. Employees With Missing Information.md":"algorithms/leetcode/sql/union&select/1965.-employees-with-missing-information.html","Algorithms/Leetcode/String/6. Zigzag Conversion.md":"algorithms/leetcode/string/6.-zigzag-conversion.html","Algorithms/Leetcode/String/131. Palindrome Partitioning.md":"algorithms/leetcode/string/131.-palindrome-partitioning.html","Algorithms/Leetcode/String/340. Longest Substring with At Most K Distinct Characters.md":"algorithms/leetcode/string/340.-longest-substring-with-at-most-k-distinct-characters.html","Algorithms/Leetcode/String/395. Longest Substring with At Least K Repeating Characters.md":"algorithms/leetcode/string/395.-longest-substring-with-at-least-k-repeating-characters.html","Algorithms/Leetcode/String/438. Find All Anagrams in a String.md":"algorithms/leetcode/string/438.-find-all-anagrams-in-a-string.html","Algorithms/Leetcode/String/1061. Lexicographically Smallest Equivalent String.md":"algorithms/leetcode/string/1061.-lexicographically-smallest-equivalent-string.html","Algorithms/Leetcode/String/1071. Greatest Common Divisor of Strings.md":"algorithms/leetcode/string/1071.-greatest-common-divisor-of-strings.html","Algorithms/Leetcode/Tree/100. Same Tree.md":"algorithms/leetcode/tree/100.-same-tree.html","Algorithms/Leetcode/Tree/102. Binary Tree Level Order Traversal.md":"algorithms/leetcode/tree/102.-binary-tree-level-order-traversal.html","Algorithms/Leetcode/Tree/103. Binary Tree Zigzag Level Order Traversal.md":"algorithms/leetcode/tree/103.-binary-tree-zigzag-level-order-traversal.html","Algorithms/Leetcode/Tree/114. Flatten Binary Tree to Linked List.md":"algorithms/leetcode/tree/114.-flatten-binary-tree-to-linked-list.html","Algorithms/Leetcode/Tree/144. Binary Tree Preorder Traversal.md":"algorithms/leetcode/tree/144.-binary-tree-preorder-traversal.html","Algorithms/Leetcode/Tree/429. N-ary Tree Level Order Traversal.md":"algorithms/leetcode/tree/429.-n-ary-tree-level-order-traversal.html","Algorithms/Leetcode/Tree/637. Average of Levels in Binary Tree.md":"algorithms/leetcode/tree/637.-average-of-levels-in-binary-tree.html","Algorithms/Leetcode/Tree/1123. Lowest Common Ancestor of Deepest Leaves.md":"algorithms/leetcode/tree/1123.-lowest-common-ancestor-of-deepest-leaves.html","Algorithms/Leetcode/Tree/1302. Deepest Leaves Sum.md":"algorithms/leetcode/tree/1302.-deepest-leaves-sum.html","Algorithms/Leetcode/Tree/1443. Minimum Time to Collect All Apples in a Tree.md":"algorithms/leetcode/tree/1443.-minimum-time-to-collect-all-apples-in-a-tree.html","Algorithms/Leetcode/Tree/1490. Clone N-ary Tree.md":"algorithms/leetcode/tree/1490.-clone-n-ary-tree.html","Algorithms/Leetcode/Tree/1602. Find Nearest Right Node in Binary Tree.md":"algorithms/leetcode/tree/1602.-find-nearest-right-node-in-binary-tree.html","Algorithms/Leetcode/Tree/1660. Correct a Binary Tree.md":"algorithms/leetcode/tree/1660.-correct-a-binary-tree.html","Algorithms/Leetcode/Tree/2196. Create Binary Tree From Descriptions.md":"algorithms/leetcode/tree/2196.-create-binary-tree-from-descriptions.html","Algorithms/Leetcode/Tree/2415. Reverse Odd Levels of Binary Tree.md":"algorithms/leetcode/tree/2415.-reverse-odd-levels-of-binary-tree.html","Algorithms/Leetcode/Road Map.md":"algorithms/leetcode/road-map.html","Algorithms/Algorithms.md":"algorithms/algorithms.html","Artificial Intelligence/Computer Vision/0. Image Processing.md":"artificial-intelligence/computer-vision/0.-image-processing.html","Artificial Intelligence/Computer Vision/1. Loss Function & Optimization.md":"artificial-intelligence/computer-vision/1.-loss-function-&-optimization.html","Artificial Intelligence/Computer Vision/2. Neural Networks.md":"artificial-intelligence/computer-vision/2.-neural-networks.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.1 Why Data Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.1-why-data-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.2 What is Data Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.2-what-is-data-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.3. What makes a pattern useful.md":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.3.-what-makes-a-pattern-useful.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.4 What kind of patterns can be mined.md":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.4-what-kind-of-patterns-can-be-mined.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.5 Challenges in Data Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.5-challenges-in-data-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/1. Introduction/1.6 Privacy Implications of Data Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/1.-introduction/1.6-privacy-implications-of-data-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.1 Data Types and Representations.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.1-data-types-and-representations.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.2 Basic Statistical Descriptions of a Single Data Variable.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.2-basic-statistical-descriptions-of-a-single-data-variable.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.3 Measuring the Dispersion of Data.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.3-measuring-the-dispersion-of-data.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.4 Computation of Measures.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.4-computation-of-measures.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.5 Measuring Correlation amongst Two Variables.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.5-measuring-correlation-amongst-two-variables.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.6 Measuring Similarity and Dissimilarity of Multivariate Data.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.6-measuring-similarity-and-dissimilarity-of-multivariate-data.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.7 Proximity Measures for Nominal Attributes.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.7-proximity-measures-for-nominal-attributes.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.8  Proximity Measures for Binary Attributes.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.8-proximity-measures-for-binary-attributes.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.9 Normalisation of numeric data.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.9-normalisation-of-numeric-data.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.10 Minkowski Distance.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.10-minkowski-distance.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/2. Foundation/2.11 Exercises.md":"artificial-intelligence/data-science/data-mining/0.-concepts/2.-foundation/2.11-exercises.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/0. Basic concepts.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/0.-basic-concepts.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/1. Multi-dimensional Data Cubes.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/1.-multi-dimensional-data-cubes.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/2. Concept Hierarchies.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/2.-concept-hierarchies.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/3. Modelling the Cube in a Relational Database.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/3.-modelling-the-cube-in-a-relational-database.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/4. Data Mining in Data Warehouses.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/4.-data-mining-in-data-warehouses.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/5. Typical OLAP Operations.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/5.-typical-olap-operations.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/6. Processing OLAP queries.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/6.-processing-olap-queries.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/3. Data Warehousing/7. Exercises.md":"artificial-intelligence/data-science/data-mining/0.-concepts/3.-data-warehousing/7.-exercises.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.1 Motivation.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.1-motivation.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.2 Basic Concepts.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.2-basic-concepts.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.3 Interesting pattern.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.3-interesting-pattern.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.4 Frequent Itemset Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.4-frequent-itemset-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.5 Apriori algorithm.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.5-apriori-algorithm.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.6 Generating Association rules.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.6-generating-association-rules.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.7 Efficient frequent Data Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.7-efficient-frequent-data-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.8 Closed and Maximal Frequent.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.8-closed-and-maximal-frequent.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.9 Adavanced pattern Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.9-adavanced-pattern-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/4. Association Mining/4.10 Exercises.md":"artificial-intelligence/data-science/data-mining/0.-concepts/4.-association-mining/4.10-exercises.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/0. Classification.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/0.-classification.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/1. Decision Tree Induction.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/1.-decision-tree-induction.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/2. Overfitting and tree pruning.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/2.-overfitting-and-tree-pruning.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/3. Enhancements to the Basic Algorithm.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/3.-enhancements-to-the-basic-algorithm.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/4. Rule-Based Classification.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/4.-rule-based-classification.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/5. Bayes Classifiers.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/5.-bayes-classifiers.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/6. Evaluation of Classifiers.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/6.-evaluation-of-classifiers.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/7. Linear Regression & Neural Nets.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/7.-linear-regression-&-neural-nets.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/8. SVM, Lazy Learners, & Variants.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/8.-svm,-lazy-learners,-&-variants.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/9. Classification, Decision Trees, Bayes, Evaluation.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/9.-classification,-decision-trees,-bayes,-evaluation.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/5. Classification & Prediction/10. Other Classification and Prediction Methods.md":"artificial-intelligence/data-science/data-mining/0.-concepts/5.-classification-&-prediction/10.-other-classification-and-prediction-methods.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/1. Basics.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/1.-basics.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/2. Partitioning method.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/2.-partitioning-method.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/3. Hierachical Clustering.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/3.-hierachical-clustering.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/4. Density-Based Methods.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/4.-density-based-methods.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/5. Grid-Based Approach.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/5.-grid-based-approach.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/6. Evaluation of Clustering.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/6.-evaluation-of-clustering.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/6. Cluster Analysis/7. Clustering.md":"artificial-intelligence/data-science/data-mining/0.-concepts/6.-cluster-analysis/7.-clustering.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/1. Outliers.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/1.-outliers.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/2. Statistical Approach.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/2.-statistical-approach.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/3. Proximity-Based Approaches.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/3.-proximity-based-approaches.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/4. Classification-Based Approaches.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/4.-classification-based-approaches.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/5. Clustering Based Approaches.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/5.-clustering-based-approaches.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/6. Contextual and Collective Outliers.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/6.-contextual-and-collective-outliers.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/7. Outlier Detection/7. Outlier.md":"artificial-intelligence/data-science/data-mining/0.-concepts/7.-outlier-detection/7.-outlier.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/0. Ensemble Methods.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/0.-ensemble-methods.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/1. Bagging.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/1.-bagging.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/2. Boosting with AdaBoost.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/2.-boosting-with-adaboost.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/3. Class-imbalanced data.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/3.-class-imbalanced-data.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/4. Data Stream Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/4.-data-stream-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/5. Stream OLAP.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/5.-stream-olap.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/6. Frequent Pattern Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/6.-frequent-pattern-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/7. Time Series.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/7.-time-series.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/8. Specialist topics/8. Ensemble, Time series, Streams.md":"artificial-intelligence/data-science/data-mining/0.-concepts/8.-specialist-topics/8.-ensemble,-time-series,-streams.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/1. Basic Measures for Information Retrieval.md":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/1.-basic-measures-for-information-retrieval.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/2. Informaiton Retrieval Techniques.md":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/2.-informaiton-retrieval-techniques.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/3. Text mining problems.md":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/3.-text-mining-problems.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/4. Web mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/4.-web-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/5. Word meaning by embedding.md":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/5.-word-meaning-by-embedding.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/9. Web Mining/6. Text & Web Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/9.-web-mining/6.-text-&-web-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/1. Semantic Web.md":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/1.-semantic-web.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/2. Foundation Technologies.md":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/2.-foundation-technologies.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/3. The Web Ontology Language.md":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/3.-the-web-ontology-language.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/4. Semantic Web Mining.md":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/4.-semantic-web-mining.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/10. Semantic Web and Knowledge Graphs/5. Semantic Web.md":"artificial-intelligence/data-science/data-mining/0.-concepts/10.-semantic-web-and-knowledge-graphs/5.-semantic-web.html","Artificial Intelligence/Data Science/Data Mining/0. Concepts/Data Mining Links.md":"artificial-intelligence/data-science/data-mining/0.-concepts/data-mining-links.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/0. Data Types.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/0.-data-types.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/1. Data Preprocessing.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/1.-data-preprocessing.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/2. Exploratory Data Analysis.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/2.-exploratory-data-analysis.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/3. Data Mining Techniques.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/3.-data-mining-techniques.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/4. Model Evaluation.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/4.-model-evaluation.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/5. Feature Selection.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/5.-feature-selection.html","Artificial Intelligence/Data Science/Data Mining/1. Fundamentals/6. Model Deployment.md":"artificial-intelligence/data-science/data-mining/1.-fundamentals/6.-model-deployment.html","Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/0. Supervised learning algorithms.md":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/0.-supervised-learning-algorithms.html","Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/1. Unsupervised learning algorithms.md":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/1.-unsupervised-learning-algorithms.html","Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/2. Association rule mining.md":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/2.-association-rule-mining.html","Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/3. Time series analysis.md":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/3.-time-series-analysis.html","Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/4. Text Mining.md":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/4.-text-mining.html","Artificial Intelligence/Data Science/Data Mining/2. General Algorithm/5. Lossy Counting Algorithm.md":"artificial-intelligence/data-science/data-mining/2.-general-algorithm/5.-lossy-counting-algorithm.html","Artificial Intelligence/Data Science/Data Wrangling/0. Concepts.md":"artificial-intelligence/data-science/data-wrangling/0.-concepts.html","Artificial Intelligence/Data Science/Data Wrangling/1. Process.md":"artificial-intelligence/data-science/data-wrangling/1.-process.html","Artificial Intelligence/Data Science/Data Wrangling/2. Source of Data.md":"artificial-intelligence/data-science/data-wrangling/2.-source-of-data.html","Artificial Intelligence/Data Science/Data Wrangling/3. Types and Measurements of Data.md":"artificial-intelligence/data-science/data-wrangling/3.-types-and-measurements-of-data.html","Artificial Intelligence/Data Science/Data Wrangling/4. Record Linkage Process.md":"artificial-intelligence/data-science/data-wrangling/4.-record-linkage-process.html","Artificial Intelligence/Data Science/Data Wrangling/5. Bins.md":"artificial-intelligence/data-science/data-wrangling/5.-bins.html","Artificial Intelligence/Data Science/Data Wrangling/6. Record Comparison.md":"artificial-intelligence/data-science/data-wrangling/6.-record-comparison.html","Artificial Intelligence/Data Science/Data Wrangling/7. Record classification.md":"artificial-intelligence/data-science/data-wrangling/7.-record-classification.html","Artificial Intelligence/Data Science/Data Wrangling/8. Measure Linkage.md":"artificial-intelligence/data-science/data-wrangling/8.-measure-linkage.html","Artificial Intelligence/Data Science/Data Wrangling/9. Blocking Process Evaluation Metrics.md":"artificial-intelligence/data-science/data-wrangling/9.-blocking-process-evaluation-metrics.html","Artificial Intelligence/Data Science/Data Wrangling/9.1 Blocking Methods.md":"artificial-intelligence/data-science/data-wrangling/9.1-blocking-methods.html","Artificial Intelligence/Data Science/Data Wrangling/10. Classification Techniques.md":"artificial-intelligence/data-science/data-wrangling/10.-classification-techniques.html","Artificial Intelligence/Data Science/Data Wrangling/11. Ontology Matching.md":"artificial-intelligence/data-science/data-wrangling/11.-ontology-matching.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/0. Information Retrieval.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/0.-information-retrieval.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/1. Classic Search Model.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/1.-classic-search-model.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/2. Boolean Retrieval.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/2.-boolean-retrieval.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/3. Term-Document Incidence Matrix.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/3.-term-document-incidence-matrix.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/4. Inverted Index.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/4.-inverted-index.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/5. Merge Intersection Algorithm (p1, p2).md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/5.-merge-intersection-algorithm-(p1,-p2).html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/6. Text Preprocessing.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/6.-text-preprocessing.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/7. Ranked Retrieval.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/7.-ranked-retrieval.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/8. TF & IDF.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/8.-tf-&-idf.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/9. Vector Space Model.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/9.-vector-space-model.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/10. Evaluation.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/10.-evaluation.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/11. Web Search.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/11.-web-search.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/12. HITS Algorithm.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/12.-hits-algorithm.html","Artificial Intelligence/Data Science/Documnet Analysis/Information Retrieval/Review.md":"artificial-intelligence/data-science/documnet-analysis/information-retrieval/review.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/0. Representation.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/0.-representation.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/1.  Word2Vec.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/1.-word2vec.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/2. Transformer.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/2.-transformer.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/3. Neural Network Language Models.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/3.-neural-network-language-models.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/4. RNN LM.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/4.-rnn-lm.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5. Pre-trained LM.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.-pre-trained-lm.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.1 ELMO.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.1-elmo.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.2 GPT Models.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.2-gpt-models.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/5.3 Others.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/5.3-others.html","Artificial Intelligence/Data Science/Documnet Analysis/ML in NLP/Review.md":"artificial-intelligence/data-science/documnet-analysis/ml-in-nlp/review.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.1 Language Modelling.md":"artificial-intelligence/data-science/documnet-analysis/nlp/1.1-language-modelling.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.2 Smoothing.md":"artificial-intelligence/data-science/documnet-analysis/nlp/1.2-smoothing.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/1.3 Evaluation of Language Models.md":"artificial-intelligence/data-science/documnet-analysis/nlp/1.3-evaluation-of-language-models.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/2. Syntactic Parsing.md":"artificial-intelligence/data-science/documnet-analysis/nlp/2.-syntactic-parsing.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/3. Semantics & Coreference Resolution.md":"artificial-intelligence/data-science/documnet-analysis/nlp/3.-semantics-&-coreference-resolution.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/4. Evaluation In NLP.md":"artificial-intelligence/data-science/documnet-analysis/nlp/4.-evaluation-in-nlp.html","Artificial Intelligence/Data Science/Documnet Analysis/NLP/5. Multilingual and Low Resource NLP.md":"artificial-intelligence/data-science/documnet-analysis/nlp/5.-multilingual-and-low-resource-nlp.html","Artificial Intelligence/Deep Learning/0. Basic/0. Backpropagation.md":"artificial-intelligence/deep-learning/0.-basic/0.-backpropagation.html","Artificial Intelligence/Deep Learning/0. Basic/1. Neural Network.md":"artificial-intelligence/deep-learning/0.-basic/1.-neural-network.html","Artificial Intelligence/Deep Learning/0. Basic/2. ResNet.md":"artificial-intelligence/deep-learning/0.-basic/2.-resnet.html","Artificial Intelligence/Deep Learning/1. Convolutional Neural Networks/Convolutional neural networks (CNNs).md":"artificial-intelligence/deep-learning/1.-convolutional-neural-networks/convolutional-neural-networks-(cnns).html","Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/1. Recurrent Neural Networks (RNNs).md":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/1.-recurrent-neural-networks-(rnns).html","Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/2. Gated Recurrent Unit (GRU).md":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/2.-gated-recurrent-unit-(gru).html","Artificial Intelligence/Deep Learning/2. Recurrent Neural Networks/3. Long Short-Term Memory.md":"artificial-intelligence/deep-learning/2.-recurrent-neural-networks/3.-long-short-term-memory.html","Artificial Intelligence/Deep Learning/3. Generative Adversarial Networks/Generative Adversarial Networks (GANs).md":"artificial-intelligence/deep-learning/3.-generative-adversarial-networks/generative-adversarial-networks-(gans).html","Artificial Intelligence/Deep Learning/4. Transfer Learning/0. Transfer Learning.md":"artificial-intelligence/deep-learning/4.-transfer-learning/0.-transfer-learning.html","Artificial Intelligence/Deep Learning/4. Transfer Learning/1. Low-Rank Adaptation.md":"artificial-intelligence/deep-learning/4.-transfer-learning/1.-low-rank-adaptation.html","Artificial Intelligence/Deep Learning/5. Transformer/0. Self-Attention.md":"artificial-intelligence/deep-learning/5.-transformer/0.-self-attention.html","Artificial Intelligence/Deep Learning/5. Transformer/1. Encoder.md":"artificial-intelligence/deep-learning/5.-transformer/1.-encoder.html","Artificial Intelligence/Deep Learning/5. Transformer/2. Decorder.md":"artificial-intelligence/deep-learning/5.-transformer/2.-decorder.html","Artificial Intelligence/Deep Learning/5. Transformer/3. BERT.md":"artificial-intelligence/deep-learning/5.-transformer/3.-bert.html","Artificial Intelligence/Deep Learning/5. Transformer/3.1 DeBERTa.md":"artificial-intelligence/deep-learning/5.-transformer/3.1-deberta.html","Artificial Intelligence/Deep Learning/5. Transformer/4. XLNet.md":"artificial-intelligence/deep-learning/5.-transformer/4.-xlnet.html","Artificial Intelligence/Deep Learning/5. Transformer/5. GPT.md":"artificial-intelligence/deep-learning/5.-transformer/5.-gpt.html","Artificial Intelligence/Deep Learning/6. Multimodal/0 Multimodal Learning.md":"artificial-intelligence/deep-learning/6.-multimodal/0-multimodal-learning.html","Artificial Intelligence/Deep Learning/6. Multimodal/1. Alignment.md":"artificial-intelligence/deep-learning/6.-multimodal/1.-alignment.html","Artificial Intelligence/Deep Learning/6. Multimodal/2. Fusion.md":"artificial-intelligence/deep-learning/6.-multimodal/2.-fusion.html","Artificial Intelligence/Deep Learning/6. Multimodal/3. Representation Learning.md":"artificial-intelligence/deep-learning/6.-multimodal/3.-representation-learning.html","Artificial Intelligence/Deep Learning/6. Multimodal/4. Key Multimodal Models.md":"artificial-intelligence/deep-learning/6.-multimodal/4.-key-multimodal-models.html","Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/0. Overview.md":"artificial-intelligence/deep-learning/7.-slam/1.-localization/0.-overview.html","Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/1. Mathematical Expressions.md":"artificial-intelligence/deep-learning/7.-slam/1.-localization/1.-mathematical-expressions.html","Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/2. Kalman Filter.md":"artificial-intelligence/deep-learning/7.-slam/1.-localization/2.-kalman-filter.html","Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/3. Extended Kalman Filter.md":"artificial-intelligence/deep-learning/7.-slam/1.-localization/3.-extended-kalman-filter.html","Artificial Intelligence/Deep Learning/7. SLAM/1. Localization/4. Particle Filter.md":"artificial-intelligence/deep-learning/7.-slam/1.-localization/4.-particle-filter.html","Artificial Intelligence/Deep Learning/7. SLAM/0. Introduction to SLAM.md":"artificial-intelligence/deep-learning/7.-slam/0.-introduction-to-slam.html","Artificial Intelligence/Deep Learning/0.2. ResNet.md":"artificial-intelligence/deep-learning/0.2.-resnet.html","Artificial Intelligence/Knowledge Graph/0. Foundations/0. Introduction.md":"artificial-intelligence/knowledge-graph/0.-foundations/0.-introduction.html","Artificial Intelligence/Knowledge Graph/0. Foundations/1. Graph Theory Essentials.md":"artificial-intelligence/knowledge-graph/0.-foundations/1.-graph-theory-essentials.html","Artificial Intelligence/Knowledge Graph/0. Foundations/2. Semantic Web Technologies.md":"artificial-intelligence/knowledge-graph/0.-foundations/2.-semantic-web-technologies.html","Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/0. Building and Modeling.md":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/0.-building-and-modeling.html","Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/1. Querying and Integration.md":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/1.-querying-and-integration.html","Artificial Intelligence/Knowledge Graph/1. Constructing and Querying/2. Visualizing.md":"artificial-intelligence/knowledge-graph/1.-constructing-and-querying/2.-visualizing.html","Artificial Intelligence/Knowledge Graph/2. Advanced/0. AI Enhancement.md":"artificial-intelligence/knowledge-graph/2.-advanced/0.-ai-enhancement.html","Artificial Intelligence/Knowledge Graph/2. Advanced/1. Knowledge Graph Embeddings.md":"artificial-intelligence/knowledge-graph/2.-advanced/1.-knowledge-graph-embeddings.html","Artificial Intelligence/Knowledge Graph/2. Advanced/2. Dynamic and Temporal Knowledge Graphs.md":"artificial-intelligence/knowledge-graph/2.-advanced/2.-dynamic-and-temporal-knowledge-graphs.html","Artificial Intelligence/Knowledge Graph/2. Advanced/3. Scalability and Real-Time Updates.md":"artificial-intelligence/knowledge-graph/2.-advanced/3.-scalability-and-real-time-updates.html","Artificial Intelligence/Machine Learning/Algorithm/0. Feature Selection.md":"artificial-intelligence/machine-learning/algorithm/0.-feature-selection.html","Artificial Intelligence/Machine Learning/Algorithm/1. Linear Regression.md":"artificial-intelligence/machine-learning/algorithm/1.-linear-regression.html","Artificial Intelligence/Machine Learning/Algorithm/2. Logistic Regression.md":"artificial-intelligence/machine-learning/algorithm/2.-logistic-regression.html","Artificial Intelligence/Machine Learning/Algorithm/3. K-nearest Neighbors.md":"artificial-intelligence/machine-learning/algorithm/3.-k-nearest-neighbors.html","Artificial Intelligence/Machine Learning/Algorithm/4. Naive Bayesian Classifier.md":"artificial-intelligence/machine-learning/algorithm/4.-naive-bayesian-classifier.html","Artificial Intelligence/Machine Learning/Algorithm/5. Support Vector Machine.md":"artificial-intelligence/machine-learning/algorithm/5.-support-vector-machine.html","Artificial Intelligence/Machine Learning/Algorithm/6. Decision Tree.md":"artificial-intelligence/machine-learning/algorithm/6.-decision-tree.html","Artificial Intelligence/Machine Learning/Algorithm/7. Random Forest.md":"artificial-intelligence/machine-learning/algorithm/7.-random-forest.html","Artificial Intelligence/Machine Learning/Algorithm/8. AdaBoost.md":"artificial-intelligence/machine-learning/algorithm/8.-adaboost.html","Artificial Intelligence/Machine Learning/Algorithm/8. K means.md":"artificial-intelligence/machine-learning/algorithm/8.-k-means.html","Artificial Intelligence/Machine Learning/Algorithm/9. Ensemble Learning.md":"artificial-intelligence/machine-learning/algorithm/9.-ensemble-learning.html","Artificial Intelligence/Machine Learning/Algorithm/9. K means.md":"artificial-intelligence/machine-learning/algorithm/9.-k-means.html","Artificial Intelligence/Machine Learning/Algorithm/10. GBDT.md":"artificial-intelligence/machine-learning/algorithm/10.-gbdt.html","Artificial Intelligence/Machine Learning/Algorithm/11. AdaBoost.md":"artificial-intelligence/machine-learning/algorithm/11.-adaboost.html","Artificial Intelligence/Machine Learning/Algorithm/12. XGBoost.md":"artificial-intelligence/machine-learning/algorithm/12.-xgboost.html","Artificial Intelligence/Machine Learning/Algorithm/13. LightGBM.md":"artificial-intelligence/machine-learning/algorithm/13.-lightgbm.html","Artificial Intelligence/Machine Learning/Data/Modify Data/0. Spliting.md":"artificial-intelligence/machine-learning/data/modify-data/0.-spliting.html","Artificial Intelligence/Machine Learning/Data/Modify Data/1. Feature scaling.md":"artificial-intelligence/machine-learning/data/modify-data/1.-feature-scaling.html","Artificial Intelligence/Machine Learning/Data/Modify Data/2. Fix overfitting.md":"artificial-intelligence/machine-learning/data/modify-data/2.-fix-overfitting.html","Artificial Intelligence/Machine Learning/Data/Modify Data/3. Seasonal Dummy.md":"artificial-intelligence/machine-learning/data/modify-data/3.-seasonal-dummy.html","Artificial Intelligence/Machine Learning/Data/Modify Data/4. Regular Expression (Regex).md":"artificial-intelligence/machine-learning/data/modify-data/4.-regular-expression-(regex).html","Artificial Intelligence/Machine Learning/Data/Modify Data/5. Principal Component Analysis.md":"artificial-intelligence/machine-learning/data/modify-data/5.-principal-component-analysis.html","Artificial Intelligence/Machine Learning/Data/Special Evaluation Method/1. Cross Validation.md":"artificial-intelligence/machine-learning/data/special-evaluation-method/1.-cross-validation.html","Artificial Intelligence/Machine Learning/Data/Special Evaluation Method/2. Regularization.md":"artificial-intelligence/machine-learning/data/special-evaluation-method/2.-regularization.html","Artificial Intelligence/Machine Learning/Data/Preprocessing.md":"artificial-intelligence/machine-learning/data/preprocessing.html","Artificial Intelligence/Machine Learning/Math/1. Linear Algebra.md":"artificial-intelligence/machine-learning/math/1.-linear-algebra.html","Artificial Intelligence/Machine Learning/Math/2. Gradient descent.md":"artificial-intelligence/machine-learning/math/2.-gradient-descent.html","Artificial Intelligence/Machine Learning/Math/3. Full Rank Matrix.md":"artificial-intelligence/machine-learning/math/3.-full-rank-matrix.html","Artificial Intelligence/Machine Learning/Stats/Formula/1. Variance.md":"artificial-intelligence/machine-learning/stats/formula/1.-variance.html","Artificial Intelligence/Machine Learning/Stats/Formula/2. Covariance.md":"artificial-intelligence/machine-learning/stats/formula/2.-covariance.html","Artificial Intelligence/Machine Learning/Stats/Formula/3.1 Cost function - Least Square.md":"artificial-intelligence/machine-learning/stats/formula/3.1-cost-function-least-square.html","Artificial Intelligence/Machine Learning/Stats/Formula/3.2 Cost Function - Logistic Regression.md":"artificial-intelligence/machine-learning/stats/formula/3.2-cost-function-logistic-regression.html","Artificial Intelligence/Machine Learning/Stats/Formula/4. Correlation.md":"artificial-intelligence/machine-learning/stats/formula/4.-correlation.html","Artificial Intelligence/Machine Learning/Stats/Formula/5. Sum of Squares.md":"artificial-intelligence/machine-learning/stats/formula/5.-sum-of-squares.html","Artificial Intelligence/Machine Learning/Stats/Formula/6. Normal Equation.md":"artificial-intelligence/machine-learning/stats/formula/6.-normal-equation.html","Artificial Intelligence/Machine Learning/Stats/Formula/7. Cosine Distance.md":"artificial-intelligence/machine-learning/stats/formula/7.-cosine-distance.html","Artificial Intelligence/Machine Learning/Stats/Models/1. Linear Regression stats.md":"artificial-intelligence/machine-learning/stats/models/1.-linear-regression-stats.html","Artificial Intelligence/Machine Learning/Stats/Models/2. Polynomial Regression.md":"artificial-intelligence/machine-learning/stats/models/2.-polynomial-regression.html","Artificial Intelligence/Machine Learning/Stats/Models/3. Ridge Regression.md":"artificial-intelligence/machine-learning/stats/models/3.-ridge-regression.html","Artificial Intelligence/Machine Learning/Stats/Models/4. Elastic Net.md":"artificial-intelligence/machine-learning/stats/models/4.-elastic-net.html","Artificial Intelligence/Machine Learning/Stats/Models/5.  Logistic Regression.md":"artificial-intelligence/machine-learning/stats/models/5.-logistic-regression.html","Artificial Intelligence/Machine Learning/Stats/Models/6. Neural Network Learning.md":"artificial-intelligence/machine-learning/stats/models/6.-neural-network-learning.html","Artificial Intelligence/Machine Learning/Stats/Models/7. Lazy Leaner.md":"artificial-intelligence/machine-learning/stats/models/7.-lazy-leaner.html","Artificial Intelligence/Machine Learning/Stats/Models/8. Eager Learning.md":"artificial-intelligence/machine-learning/stats/models/8.-eager-learning.html","Artificial Intelligence/Natural Language Processing/Basics/1. Components.md":"artificial-intelligence/natural-language-processing/basics/1.-components.html","Artificial Intelligence/Natural Language Processing/Basics/1. Pipeline.md":"artificial-intelligence/natural-language-processing/basics/1.-pipeline.html","Artificial Intelligence/Natural Language Processing/Basics/2. Tasks.md":"artificial-intelligence/natural-language-processing/basics/2.-tasks.html","Artificial Intelligence/Natural Language Processing/Language Processing/0. Word Segmentation.md":"artificial-intelligence/natural-language-processing/language-processing/0.-word-segmentation.html","Artificial Intelligence/Natural Language Processing/Language Processing/1. Segmentation Method.md":"artificial-intelligence/natural-language-processing/language-processing/1.-segmentation-method.html","Artificial Intelligence/Natural Language Processing/Language Processing/2. Max Matching.md":"artificial-intelligence/natural-language-processing/language-processing/2.-max-matching.html","Artificial Intelligence/Natural Language Processing/Language Processing/3. Incorporate Semantic.md":"artificial-intelligence/natural-language-processing/language-processing/3.-incorporate-semantic.html","Artificial Intelligence/Natural Language Processing/Language Processing/3.1 Viterbi Algorithm.md":"artificial-intelligence/natural-language-processing/language-processing/3.1-viterbi-algorithm.html","Artificial Intelligence/Natural Language Processing/Machine Translation/0. Basics.md":"artificial-intelligence/natural-language-processing/machine-translation/0.-basics.html","Artificial Intelligence/Natural Language Processing/Machine Translation/1. Statistical Machine Translation (SMT).md":"artificial-intelligence/natural-language-processing/machine-translation/1.-statistical-machine-translation-(smt).html","Artificial Intelligence/Natural Language Processing/Models & Algorithm/Conditional Random Fields (CRF).md":"artificial-intelligence/natural-language-processing/models-&-algorithm/conditional-random-fields-(crf).html","Artificial Intelligence/Natural Language Processing/Models & Algorithm/Decoding Algorithm.md":"artificial-intelligence/natural-language-processing/models-&-algorithm/decoding-algorithm.html","Artificial Intelligence/Natural Language Processing/Models & Algorithm/Hidden Markov Models (HMM).md":"artificial-intelligence/natural-language-processing/models-&-algorithm/hidden-markov-models-(hmm).html","Artificial Intelligence/Natural Language Processing/Models & Algorithm/Word2Vec.md":"artificial-intelligence/natural-language-processing/models-&-algorithm/word2vec.html","Artificial Intelligence/Natural Language Processing/Parsing & Semantic/Parsing.md":"artificial-intelligence/natural-language-processing/parsing-&-semantic/parsing.html","Artificial Intelligence/Natural Language Processing/Parsing & Semantic/Semantic.md":"artificial-intelligence/natural-language-processing/parsing-&-semantic/semantic.html","Artificial Intelligence/Natural Language Processing/Techniques/0. Prepare Data.md":"artificial-intelligence/natural-language-processing/techniques/0.-prepare-data.html","Artificial Intelligence/Natural Language Processing/Techniques/1. Feature Extraction & Model.md":"artificial-intelligence/natural-language-processing/techniques/1.-feature-extraction-&-model.html","Artificial Intelligence/Reinforcement Learning/0. Q-Learning.md":"artificial-intelligence/reinforcement-learning/0.-q-learning.html","Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/0. Introduction.md":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/0.-introduction.html","Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/1. Propositional Logic.md":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/1.-propositional-logic.html","Artificial Intelligence/Symbolic AI/0. Reasoning and Inference/2. First Order Logic.md":"artificial-intelligence/symbolic-ai/0.-reasoning-and-inference/2.-first-order-logic.html","Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/0. Agents.md":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/0.-agents.html","Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/1. Search.md":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/1.-search.html","Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/2. Uninformed search strategies.md":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/2.-uninformed-search-strategies.html","Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/3. Informed search algorithms.md":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/3.-informed-search-algorithms.html","Artificial Intelligence/Symbolic AI/1. Search/0. Concepts/4. Adversarial Search Problems.md":"artificial-intelligence/symbolic-ai/1.-search/0.-concepts/4.-adversarial-search-problems.html","Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/0. Exercises 1.md":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/0.-exercises-1.html","Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/1. Exercises 2.md":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/1.-exercises-2.html","Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/2. Exercises 3.md":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/2.-exercises-3.html","Artificial Intelligence/Symbolic AI/1. Search/1. Exercises/3. Exercises 4.md":"artificial-intelligence/symbolic-ai/1.-search/1.-exercises/3.-exercises-4.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/0. Intro.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/0.-intro.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/1. DPLL.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/1.-dpll.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/2. Prenex Normal form.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/2.-prenex-normal-form.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/3. Constraint Satisfaction Problem.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/3.-constraint-satisfaction-problem.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/4. Constraint Learning.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/4.-constraint-learning.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/5. Optimal Solving.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/5.-optimal-solving.html","Artificial Intelligence/Symbolic AI/2. KRR/0. Concepts/6. Temporal Constraint Networks.md":"artificial-intelligence/symbolic-ai/2.-krr/0.-concepts/6.-temporal-constraint-networks.html","Artificial Intelligence/Symbolic AI/2. KRR/1. Exercises/Constraint Satisfaction 1.md":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-1.html","Artificial Intelligence/Symbolic AI/2. KRR/1. Exercises/Constraint Satisfaction 2.md":"artificial-intelligence/symbolic-ai/2.-krr/1.-exercises/constraint-satisfaction-2.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/0. Intro.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/0.-intro.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/1. Plans.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/1.-plans.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/2. Planning Algorithm.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/2.-planning-algorithm.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/3. Heuristics.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/3.-heuristics.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/4. Regression.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/4.-regression.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/0. Concepts/5. Plan space search.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/0.-concepts/5.-plan-space-search.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/1. Exercises/Heuristics, Regression, and Partial-Order Planning.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/heuristics,-regression,-and-partial-order-planning.html","Artificial Intelligence/Symbolic AI/3. Planning and Decision Making/1. Exercises/Planning Representations and Graph-Based Approaches.md":"artificial-intelligence/symbolic-ai/3.-planning-and-decision-making/1.-exercises/planning-representations-and-graph-based-approaches.html","Big Data/0. Data Lake/0. Introduction.md":"big-data/0.-data-lake/0.-introduction.html","Big Data/0. Data Lake/1. Architecture.md":"big-data/0.-data-lake/1.-architecture.html","Big Data/0. Data Lake/2. Workflow.md":"big-data/0.-data-lake/2.-workflow.html","Big Data/0. Data Lake/3. DataX.md":"big-data/0.-data-lake/3.-datax.html","Big Data/1. Hadoop/1. Hadoop.md":"big-data/1.-hadoop/1.-hadoop.html","Big Data/1. Hadoop/2. HDFS.md":"big-data/1.-hadoop/2.-hdfs.html","Big Data/1. Hadoop/3. MapReduce & Yarn.md":"big-data/1.-hadoop/3.-mapreduce-&-yarn.html","Big Data/1. Hadoop/3. Metadata.md":"big-data/1.-hadoop/3.-metadata.html","Big Data/1. Hadoop/4. HDFS Shell.md":"big-data/1.-hadoop/4.-hdfs-shell.html","Big Data/1. Hadoop/5. Java API.md":"big-data/1.-hadoop/5.-java-api.html","Big Data/1. Hadoop/6. MapReduce & Yarn.md":"big-data/1.-hadoop/6.-mapreduce-&-yarn.html","Big Data/1. Hadoop/7. Single Node Deployment Guide - Tecent Cloud.md":"big-data/1.-hadoop/7.-single-node-deployment-guide-tecent-cloud.html","Big Data/2. Scala/0. Comprehensive Guide.md":"big-data/2.-scala/0.-comprehensive-guide.html","Big Data/2. Scala/1. Variables and Data Types.md":"big-data/2.-scala/1.-variables-and-data-types.html","Big Data/2. Scala/2. Operators.md":"big-data/2.-scala/2.-operators.html","Big Data/2. Scala/3. Control Flow.md":"big-data/2.-scala/3.-control-flow.html","Big Data/2. Scala/4. Functional Programming.md":"big-data/2.-scala/4.-functional-programming.html","Big Data/2. Scala/5. Package Management.md":"big-data/2.-scala/5.-package-management.html","Big Data/2. Scala/6. Object-Oriented Programming.md":"big-data/2.-scala/6.-object-oriented-programming.html","Big Data/2. Scala/7. Collections.md":"big-data/2.-scala/7.-collections.html","Big Data/2. Scala/8. Pattern Matching.md":"big-data/2.-scala/8.-pattern-matching.html","Big Data/2. Scala/9. Exception Handling.md":"big-data/2.-scala/9.-exception-handling.html","Big Data/2. Scala/10. Sbt.md":"big-data/2.-scala/10.-sbt.html","Big Data/3. Spark/0. Concept & Architecture/0. Spark.md":"big-data/3.-spark/0.-concept-&-architecture/0.-spark.html","Big Data/3. Spark/0. Concept & Architecture/1. Concepts.md":"big-data/3.-spark/0.-concept-&-architecture/1.-concepts.html","Big Data/3. Spark/0. Concept & Architecture/2. Architecture and Execution.md":"big-data/3.-spark/0.-concept-&-architecture/2.-architecture-and-execution.html","Big Data/3. Spark/1. RDD/0. RDD.md":"big-data/3.-spark/1.-rdd/0.-rdd.html","Big Data/3. Spark/1. RDD/1. Transformations.md":"big-data/3.-spark/1.-rdd/1.-transformations.html","Big Data/3. Spark/1. RDD/2. Actions.md":"big-data/3.-spark/1.-rdd/2.-actions.html","Big Data/3. Spark/1. RDD/3. Serialization & Dependencies & Persistence.md":"big-data/3.-spark/1.-rdd/3.-serialization-&-dependencies-&-persistence.html","Big Data/3. Spark/1. RDD/4. Collaboration.md":"big-data/3.-spark/1.-rdd/4.-collaboration.html","Big Data/3. Spark/1. RDD/5. Reading and Saving RDD Files.md":"big-data/3.-spark/1.-rdd/5.-reading-and-saving-rdd-files.html","Big Data/3. Spark/1. RDD/6. Accumulators & Broadcast Variables.md":"big-data/3.-spark/1.-rdd/6.-accumulators-&-broadcast-variables.html","Big Data/3. Spark/2. Spark SQL/0. SparkSQL.md":"big-data/3.-spark/2.-spark-sql/0.-sparksql.html","Big Data/3. Spark/2. Spark SQL/1. DataFrame & DataSet.md":"big-data/3.-spark/2.-spark-sql/1.-dataframe-&-dataset.html","Big Data/3. Spark/2. Spark SQL/2. Coding.md":"big-data/3.-spark/2.-spark-sql/2.-coding.html","Big Data/3. Spark/2. Spark SQL/3. RDD & DataFrame & DataSet.md":"big-data/3.-spark/2.-spark-sql/3.-rdd-&-dataframe-&-dataset.html","Big Data/3. Spark/2. Spark SQL/4. User-Defined Functions.md":"big-data/3.-spark/2.-spark-sql/4.-user-defined-functions.html","Big Data/3. Spark/2. Spark SQL/5. Data Loading and Saving.md":"big-data/3.-spark/2.-spark-sql/5.-data-loading-and-saving.html","Big Data/3. Spark/3. Streaming/0. Spark Streaming.md":"big-data/3.-spark/3.-streaming/0.-spark-streaming.html","Big Data/3. Spark/3. Streaming/1. Kafka Data Source.md":"big-data/3.-spark/3.-streaming/1.-kafka-data-source.html","Big Data/3. Spark/3. Streaming/2. DStream Transformations.md":"big-data/3.-spark/3.-streaming/2.-dstream-transformations.html","Big Data/3. Spark/3. Streaming/3. DStream Output.md":"big-data/3.-spark/3.-streaming/3.-dstream-output.html","Big Data/3. Spark/4. Core/0. Spark Kernel.md":"big-data/3.-spark/4.-core/0.-spark-kernel.html","Big Data/3. Spark/4. Core/1. Deployment.md":"big-data/3.-spark/4.-core/1.-deployment.html","Big Data/3. Spark/4. Core/2. Spark Communication Architecture.md":"big-data/3.-spark/4.-core/2.-spark-communication-architecture.html","Big Data/3. Spark/4. Core/3. Task Scheduling.md":"big-data/3.-spark/4.-core/3.-task-scheduling.html","Big Data/3. Spark/4. Core/4. Spark Shuffle Analysis.md":"big-data/3.-spark/4.-core/4.-spark-shuffle-analysis.html","Big Data/3. Spark/5. Pyspark/0. Installation.md":"big-data/3.-spark/5.-pyspark/0.-installation.html","Big Data/3. Spark/5. Pyspark/1. DataFrame.md":"big-data/3.-spark/5.-pyspark/1.-dataframe.html","Big Data/3. Spark/5. Pyspark/2. Spark Connect.md":"big-data/3.-spark/5.-pyspark/2.-spark-connect.html","Big Data/3. Spark/5. Pyspark/3. Pandas API on Spark.md":"big-data/3.-spark/5.-pyspark/3.-pandas-api-on-spark.html","Big Data/3. Spark/6. Source Code/0. Spark Storage.md":"big-data/3.-spark/6.-source-code/0.-spark-storage.html","Big Data/4. Hive/0. Hive.md":"big-data/4.-hive/0.-hive.html","Big Data/4. Hive/1. Data Definition Language (DDL).md":"big-data/4.-hive/1.-data-definition-language-(ddl).html","Big Data/4. Hive/2. Data Manipulation Language (DML).md":"big-data/4.-hive/2.-data-manipulation-language-(dml).html","Big Data/4. Hive/3. Single Node Deployment Guide - Tecent Cloud.md":"big-data/4.-hive/3.-single-node-deployment-guide-tecent-cloud.html","Big Data/5. Hbase/0. Hbase.md":"big-data/5.-hbase/0.-hbase.html","Big Data/5. Hbase/2. Shell.md":"big-data/5.-hbase/2.-shell.html","Big Data/5. Hbase/3. Processes.md":"big-data/5.-hbase/3.-processes.html","Big Data/5. Hbase/4. API.md":"big-data/5.-hbase/4.-api.html","Big Data/5. Hbase/5. MapReduce.md":"big-data/5.-hbase/5.-mapreduce.html","Big Data/5. Hbase/6. Integration with Hive.md":"big-data/5.-hbase/6.-integration-with-hive.html","Big Data/5. Hbase/7. Optimization.md":"big-data/5.-hbase/7.-optimization.html","Big Data/6. Flink/0. Key Features of Flink.md":"big-data/6.-flink/0.-key-features-of-flink.html","Big Data/6. Flink/1. Deployment and Startup.md":"big-data/6.-flink/1.-deployment-and-startup.html","Big Data/6. Flink/2. Architecture.md":"big-data/6.-flink/2.-architecture.html","Big Data/6. Flink/3. Flink Operators.md":"big-data/6.-flink/3.-flink-operators.html","Big Data/6. Flink/4. Time and Window in Stream Processing.md":"big-data/6.-flink/4.-time-and-window-in-stream-processing.html","Big Data/7. Kafka/0. Kafka.md":"big-data/7.-kafka/0.-kafka.html","Big Data/7. Kafka/1. Deployment & Commands.md":"big-data/7.-kafka/1.-deployment-&-commands.html","Big Data/7. Kafka/2. Architecture.md":"big-data/7.-kafka/2.-architecture.html","Big Data/7. Kafka/3. API.md":"big-data/7.-kafka/3.-api.html","Big Data/7. Kafka/4. Monitoring.md":"big-data/7.-kafka/4.-monitoring.html","Big Data/7. Kafka/5. Flume Integration.md":"big-data/7.-kafka/5.-flume-integration.html","Big Data/8. Doris/0. Doris.md":"big-data/8.-doris/0.-doris.html","Big Data/8. Doris/1. Installation.md":"big-data/8.-doris/1.-installation.html","Big Data/8. Doris/2. Usage.md":"big-data/8.-doris/2.-usage.html","Big Data/8. Doris/3. Monitoring and Alerting.md":"big-data/8.-doris/3.-monitoring-and-alerting.html","Big Data/9. Elastic Search/0. Overview.md":"big-data/9.-elastic-search/0.-overview.html","Big Data/9. Elastic Search/1. Index & Shard.md":"big-data/9.-elastic-search/1.-index-&-shard.html","Big Data/10. ClickHouse/0. Introduction and Installation.md":"big-data/10.-clickhouse/0.-introduction-and-installation.html","Big Data/10. ClickHouse/1. Data Types.md":"big-data/10.-clickhouse/1.-data-types.html","Big Data/10. ClickHouse/2. Database Engines.md":"big-data/10.-clickhouse/2.-database-engines.html","Big Data/10. ClickHouse/3. Table Engines.md":"big-data/10.-clickhouse/3.-table-engines.html","Big Data/11. Data Migration/0. Sqoop.md":"big-data/11.-data-migration/0.-sqoop.html","Big Data/11. Data Migration/1. DataX.md":"big-data/11.-data-migration/1.-datax.html","Big Data/12. Schedule/1. Cron Expressions.md":"big-data/12.-schedule/1.-cron-expressions.html","Big Data/12. Schedule/2. Quartz Cron Expressions.md":"big-data/12.-schedule/2.-quartz-cron-expressions.html","Big Data/13. Certificate/CDGA/0. Data Management.md":"big-data/13.-certificate/cdga/0.-data-management.html","Big Data/13. Certificate/CDGA/1. Data Handling Ethics.md":"big-data/13.-certificate/cdga/1.-data-handling-ethics.html","Big Data/13. Certificate/CDGA/2. Data Governance.md":"big-data/13.-certificate/cdga/2.-data-governance.html","Big Data/13. Certificate/CDGA/3. Data Architecture.md":"big-data/13.-certificate/cdga/3.-data-architecture.html","Big Data/13. Certificate/CDGA/4. Data Modeling and Design.md":"big-data/13.-certificate/cdga/4.-data-modeling-and-design.html","Big Data/13. Certificate/CDGA/5. Data Storage and Operations.md":"big-data/13.-certificate/cdga/5.-data-storage-and-operations.html","Big Data/13. Certificate/CDGA/6. Data Security.md":"big-data/13.-certificate/cdga/6.-data-security.html","Big Data/13. Certificate/CDGA/7. Data Integration and Interoperability.md":"big-data/13.-certificate/cdga/7.-data-integration-and-interoperability.html","Big Data/13. Certificate/CDGA/8. Document and Content Management.md":"big-data/13.-certificate/cdga/8.-document-and-content-management.html","Big Data/13. Certificate/CDGA/9. Reference and Master Data.md":"big-data/13.-certificate/cdga/9.-reference-and-master-data.html","Big Data/13. Certificate/CDGA/10. Data Warehousing and Business Intelligence.md":"big-data/13.-certificate/cdga/10.-data-warehousing-and-business-intelligence.html","Big Data/13. Certificate/CDGA/11. Metadata Management.md":"big-data/13.-certificate/cdga/11.-metadata-management.html","Big Data/13. Certificate/CDGA/12. Data Quality.md":"big-data/13.-certificate/cdga/12.-data-quality.html","Big Data/13. Certificate/CDGA/13. Big Data and Data Science.md":"big-data/13.-certificate/cdga/13.-big-data-and-data-science.html","Big Data/13. Certificate/CDGA/14. Data Management Maturity Assessment.md":"big-data/13.-certificate/cdga/14.-data-management-maturity-assessment.html","C++/Auto keyword.md":"c++/auto-keyword.html","C++/C++.md":"c++/c++.html","C++/Const keyword.md":"c++/const-keyword.html","C++/Enum and Enum class.md":"c++/enum-and-enum-class.html","C++/Header Files.md":"c++/header-files.html","C++/Inline keyword.md":"c++/inline-keyword.html","C++/OOP.md":"c++/oop.html","C++/Static keyword.md":"c++/static-keyword.html","C++/Timing.md":"c++/timing.html","C++/Type punning.md":"c++/type-punning.html","Data Base/Data Base.md":"data-base/data-base.html","Data Base/MongoDB.md":"data-base/mongodb.html","Data Base/MongoDB cheat Sheet.md":"data-base/mongodb-cheat-sheet.html","Data Base/SQL Cheat Sheet.md":"data-base/sql-cheat-sheet.html","Data Structures/Aggregation analysis.md":"data-structures/aggregation-analysis.html","Data Structures/Analyzing Exchanging Sorting Algorithms.md":"data-structures/analyzing-exchanging-sorting-algorithms.html","Data Structures/AVL Tree.md":"data-structures/avl-tree.html","Data Structures/Binary Search Tree.md":"data-structures/binary-search-tree.html","Data Structures/Comparison based sorting.md":"data-structures/comparison-based-sorting.html","Data Structures/Data Structures.md":"data-structures/data-structures.html","Data Structures/Expression Tree.md":"data-structures/expression-tree.html","Data Structures/Hashing.md":"data-structures/hashing.html","Data Structures/Heaps & Priority Queues.md":"data-structures/heaps-&-priority-queues.html","Data Structures/Insertion sort.md":"data-structures/insertion-sort.html","Data Structures/Linear Sorting Algorithm.md":"data-structures/linear-sorting-algorithm.html","Data Structures/Lists.md":"data-structures/lists.html","Data Structures/Master theorem.md":"data-structures/master-theorem.html","Data Structures/Open Addressing.md":"data-structures/open-addressing.html","Data Structures/Order.md":"data-structures/order.html","Data Structures/Priority Queues.md":"data-structures/priority-queues.html","Data Structures/Recusrsion.md":"data-structures/recusrsion.html","Data Structures/Red-Black Tree.md":"data-structures/red-black-tree.html","Data Structures/Sorting Algorithms.md":"data-structures/sorting-algorithms.html","Data Structures/Stacks & Queues.md":"data-structures/stacks-&-queues.html","Data Structures/Time Complexity Comparison.md":"data-structures/time-complexity-comparison.html","Data Structures/Tree and Trie.md":"data-structures/tree-and-trie.html","Data Structures/Trees.md":"data-structures/trees.html","Git/Git.md":"git/git.html","Git/Git Cheat Sheet.md":"git/git-cheat-sheet.html","Graph/Dijkstra.md":"graph/dijkstra.html","Graph/Graph.md":"graph/graph.html","Graph/Max Flow.md":"graph/max-flow.html","Graph/SCC.md":"graph/scc.html","Markdown/Markdown.md":"markdown/markdown.html","Markdown/Markdown Cheat Sheet.md":"markdown/markdown-cheat-sheet.html","Markdown/MathJax.md":"markdown/mathjax.html","Python/Beautiful Soup.md":"python/beautiful-soup.html","Python/Python.md":"python/python.html","Python/Python OOP.md":"python/python-oop.html","Home.md":"home.html","Excalidraw/codeverse.svg":"excalidraw/codeverse.svg","Excalidraw/AVL tree.excalidraw.svg":"excalidraw/avl-tree.excalidraw.svg","Excalidraw/Black and red tree height diff.excalidraw.svg":"excalidraw/black-and-red-tree-height-diff.excalidraw.svg","Excalidraw/expression tree.svg":"excalidraw/expression-tree.svg","Excalidraw/Red-Black Tree.excalidraw.svg":"excalidraw/red-black-tree.excalidraw.svg","Excalidraw/rotation.excalidraw.svg":"excalidraw/rotation.excalidraw.svg","Excalidraw/Min-max heap.excalidraw.svg":"excalidraw/min-max-heap.excalidraw.svg","Excalidraw/Traversing binary tree.svg":"excalidraw/traversing-binary-tree.svg","Excalidraw/Convert to binary tree.svg":"excalidraw/convert-to-binary-tree.svg"},"featureOptions":{"backlinks":{"featureId":"backlinks","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"Backlinks","featurePlacement":{"selector":".footer","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""}},"tags":{"featureId":"tags","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"","featurePlacement":{"selector":".header .data-bar","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"showInlineTags":true,"showFrontmatterTags":true,"info_showInlineTags":{"show":true,"name":"","description":"Show tags defined inside the document at the top of the page.","placeholder":""},"info_showFrontmatterTags":{"show":true,"name":"","description":"Show tags defined in the frontmatter of the document at the top of the page.","placeholder":""}},"alias":{"featureId":"aliases","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"","featurePlacement":{"selector":".header .data-bar","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""}},"properties":{"featureId":"properties","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"Properties","featurePlacement":{"selector":".header","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_hideProperties":{"show":true,"name":"","description":"A list of properties to hide from the properties view","placeholder":""}},"fileNavigation":{"featureId":"file-navigation","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"","featurePlacement":{"selector":"#left-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"showCustomIcons":false,"showDefaultFolderIcons":false,"showDefaultFileIcons":false,"defaultFolderIcon":"lucide//folder","defaultFileIcon":"lucide//file","defaultMediaIcon":"lucide//file-image","exposeStartingPath":true,"info_showCustomIcons":{"show":true,"name":"","description":"Show custom icons for files and folders","placeholder":""},"info_showDefaultFolderIcons":{"show":true,"name":"","description":"Show a default icon of a folder for every folder in the tree","placeholder":""},"info_showDefaultFileIcons":{"show":true,"name":"","description":"Show a default icon of a file for every file in the tree","placeholder":""},"info_defaultFolderIcon":{"show":true,"name":"","description":"The icon to use for folders. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultFileIcon":{"show":true,"name":"","description":"The icon to use for files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultMediaIcon":{"show":true,"name":"","description":"The icon to use for media files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_exposeStartingPath":{"show":true,"name":"","description":"Whether or not to show the current file in the file tree when the page is first loaded","placeholder":""},"includePath":"site-lib/html/file-tree.html"},"search":{"featureId":"search","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"Search...","featurePlacement":{"selector":"#left-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""}},"outline":{"featureId":"outline","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"Outline","featurePlacement":{"selector":"#right-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"startCollapsed":false,"minCollapseDepth":0,"info_startCollapsed":{"show":true,"name":"","description":"Should the outline start collapsed?","placeholder":""},"info_minCollapseDepth":{"show":true,"name":"","description":"Only allow outline items to be collapsed if they are at least this many levels deep in the tree.","placeholder":"","dropdownOptions":{"1":1,"2":2,"No Collapse":100}}},"themeToggle":{"featureId":"theme-toggle","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"","featurePlacement":{"selector":"#right-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""}},"graphView":{"featureId":"graph-view","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"Graph View","featurePlacement":{"selector":"#right-sidebar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"showOrphanNodes":true,"showAttachments":false,"allowGlobalGraph":true,"allowExpand":true,"attractionForce":1,"linkLength":15,"repulsionForce":80,"centralForce":2,"edgePruning":100,"minNodeRadius":3,"maxNodeRadius":7,"info_showOrphanNodes":{"show":true,"name":"","description":"Show nodes that are not connected to any other nodes.","placeholder":""},"info_showAttachments":{"show":true,"name":"","description":"Show attachments like images and PDFs as nodes in the graph.","placeholder":""},"info_allowGlobalGraph":{"show":true,"name":"","description":"Allow the user to view the global graph of all nodes.","placeholder":""},"info_allowExpand":{"show":true,"name":"","description":"Allow the user to pop-out the graph view to take up the whole screen","placeholder":""},"info_attractionForce":{"show":true,"name":"","description":"How much should linked nodes attract each other? This will make the graph appear more clustered.","placeholder":""},"info_linkLength":{"show":true,"name":"","description":"How long should the links between nodes be? The shorter the links the more connected nodes will cluster together.","placeholder":""},"info_repulsionForce":{"show":true,"name":"","description":"How much should nodes repel each other? This will make disconnected parts more spread out.","placeholder":""},"info_centralForce":{"show":true,"name":"","description":"How much should nodes be attracted to the center? This will make the graph appear more dense and circular.","placeholder":""},"info_edgePruning":{"show":true,"name":"","description":"Edges with a length above this threshold will not be rendered, however they will still contribute to the simulation. This can help large tangled graphs look more organised. Hovering over a node will still display these links.","placeholder":""},"info_minNodeRadius":{"show":true,"name":"","description":"How small should the smallest nodes be? The smaller a node is the less it will attract other nodes.","placeholder":""},"info_maxNodeRadius":{"show":true,"name":"","description":"How large should the largest nodes be? Nodes are sized by how many links they have. The larger a node is the more it will attract other nodes. This can be used to create a good grouping around the most important nodes.","placeholder":""}},"sidebar":{"featureId":"sidebar","enabled":true,"unavailable":false,"alwaysEnabled":false,"allowResizing":true,"allowCollapsing":true,"rightDefaultWidth":"20em","leftDefaultWidth":"20em","info_allowResizing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be resized","placeholder":""},"info_allowCollapsing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be collapsed","placeholder":""},"info_rightDefaultWidth":{"show":true,"name":"","description":"The default width of the right sidebar","placeholder":""},"info_leftDefaultWidth":{"show":true,"name":"","description":"The default width of the left sidebar","placeholder":""}},"customHead":{"featureId":"custom-head","enabled":true,"unavailable":false,"alwaysEnabled":false,"displayTitle":"","featurePlacement":{"selector":"head","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"sourcePath":"","info_sourcePath":{"show":true,"name":"","description":"The local path to the source .html file which will be included.","placeholder":"","fileInputOptions":{"makeRelativeToVault":true,"browseButton":true}},"includePath":"site-lib/html/custom-head.html"},"document":{"featureId":"obsidian-document","enabled":true,"unavailable":false,"alwaysEnabled":true,"allowFoldingLists":true,"allowFoldingHeadings":true,"documentWidth":"40em","info_allowFoldingLists":{"show":true,"name":"","description":"Whether or not to allow lists to be folded","placeholder":""},"info_allowFoldingHeadings":{"show":true,"name":"","description":"Whether or not to allow headings to be folded","placeholder":""},"info_documentWidth":{"show":true,"name":"","description":"The width of the document","placeholder":""}},"rss":{"featureId":"obsidian-document","enabled":false,"unavailable":false,"alwaysEnabled":false,"siteUrl":"","siteName":"test","authorName":"","info_siteUrl":{"show":true,"name":"","description":"The url that this site will be hosted at","placeholder":"https://example.com/mysite"},"info_siteName":{"show":true,"name":"","description":"The name of the vault / exported site","placeholder":""},"info_authorName":{"show":true,"name":"","description":"The name of the author of the site","placeholder":""}}},"modifiedTime":1741268489193,"siteName":"test","vaultName":"Code Verse MD","exportRoot":"","baseURL":"","pluginVersion":"1.9.0-3b","themeName":"","bodyClasses":"publish css-settings-manager mod-windows is-hidden-frameless is-maximized show-inline-title show-ribbon encore-theme-dark-blackout encore-colors-colorful encore-bg-sapphire is-focused","hasFavicon":true}